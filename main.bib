@article{McCulloch_Pitts_1943,
	title={A logical calculus of the ideas immanent in nervous activity},
	volume={5},
	ISSN={0007-4985, 1522-9602},
	DOI={10.1007/BF02478259},
	number={4},
	journal={The Bulletin of Mathematical Biophysics},
	author={McCulloch, Warren S. and Pitts, Walter},
	year={1943},
	month=Dec,
	pages={115–133},
	language={en}
}

 @article{Rosenblatt_1958,
 title={The perceptron: A probabilistic model for information storage and organization in the brain.},
 volume={65},
 ISSN={1939-1471, 0033-295X},
 DOI={10.1037/h0042519},
 number={6},
 journal={Psychological Review},
 author={Rosenblatt, F.},
 year={1958},
 pages={386–408},
 language={en}
}

@article{Cybenko_1989,
 title={Approximation by superpositions of a sigmoidal function},
 volume={2},
 ISSN={0932-4194, 1435-568X},
 DOI={10.1007/BF02551274},
 number={4},
 journal={Mathematics of Control, Signals, and Systems},
 author={Cybenko, G.},
 year={1989},
 month=Dec,
 pages={303–314},
 language={en}
}
 
@article{Rumelhart_Hinton_Williams_1986,
	title={Learning representations by back-propagating errors},
	volume={323},
	ISSN={0028-0836, 1476-4687},
	DOI={10.1038/323533a0},
	number={6088},
	journal={Nature},
	author={Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	year={1986},
	month=Oct,
	pages={533–536},
	language={en}
}

@online{Coursera,
  author = {Coursera Inc.},
  title = {Deep Learning Specialization},
  year = {2022},
  url = {https://www.coursera.org/specializations/deep-learning},
  urldate = {2022-08-19}
}

@online{NCAs_distill,
  author = {Distill},
  title = {Growing Neural Cellular Automata},
  year = {2022},
  url = {https://distill.pub/2020/growing-ca},
  urldate = {2022-09-05}
}

@online{axios_hinton,
  author = {Axios Media Inc.},
  title = {Artificial intelligence pioneer says we need to start over},
  year = 2017,
  url = {https://www.axios.com/2017/12/15/artificial-intelligence-pioneer-says-we-need-to-start-over-1513305524},
  urldate = {2022-09-04}
}

@article{Kingma2015AdamAM,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2015},
  volume={abs/1412.6980}
}

@online{OpenAI_compute,
  author = {Open AI},
  title = {AI and Compute},
  year = 2018,
  url = {https://openai.com/blog/ai-and-compute/},
  urldate = {2022-08-19}
}

@online{Lambda_GPT3,
  author = {Lambda},
  title = {OpenAI's GPT-3 Language Model: A Technical Overview},
  year = 2021,
  url = {https://lambdalabs.com/blog/demystifying-gpt-3/},
  urldate = {2022-08-19}
}

 @article{Moore_2006, title={Cramming more components onto integrated circuits, Reprinted from Electronics, volume 38, number 8, April 19, 1965, pp.114 ff.}, volume={11}, ISSN={1098-4232}, DOI={10.1109/N-SSC.2006.4785860}, number={3}, journal={IEEE Solid-State Circuits Society Newsletter}, author={Moore, Gordon E.}, year={2006}, month=Sep, pages={33–35} }

 @article{Kumar_2015, title={Fundamental Limits to Moore’s Law}, url={http://arxiv.org/abs/1511.05956}, abstractNote={The theoretical and practical aspects of the fundamental, ultimate, physical limits to scaling, or Moore-s law, is presented.}, note={arXiv:1511.05956 [cond-mat]}, number={arXiv:1511.05956}, publisher={arXiv}, author={Kumar, Suhas}, year={2015}, month=Nov }

 @inproceedings{Peters_Neumann_Iyyer_Gardner_Clark_Lee_Zettlemoyer_2018, address={New Orleans, Louisiana}, title={Deep Contextualized Word Representations}, url={http://aclweb.org/anthology/N18-1202}, DOI={10.18653/v1/N18-1202}, booktitle={Proceedings of the 2018 Conference of the North American Chapter of           the Association for Computational Linguistics: Human Language           Technologies, Volume 1 (Long Papers)}, publisher={Association for Computational Linguistics}, author={Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke}, year={2018}, pages={2227–2237}, language={en} }
 
@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

 @article{Shoeybi_Patwary_Puri_LeGresley_Casper_Catanzaro_2020, title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}, url={http://arxiv.org/abs/1909.08053}, abstractNote={Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).}, note={arXiv:1909.08053 [cs]}, number={arXiv:1909.08053}, publisher={arXiv}, author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan}, year={2020}, month=Mar }
 
 @article{Wu_Judd_Zhang_Isaev_Micikevicius_2020, title={Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation}, url={http://arxiv.org/abs/2004.09602}, abstractNote={Quantization techniques can reduce the size of Deep Neural Networks and improve inference latency and throughput by taking advantage of high throughput integer instructions. In this paper we review the mathematical aspects of quantization parameters and evaluate their choices on a wide range of neural network models for different application domains, including vision, speech, and language. We focus on quantization techniques that are amenable to acceleration by processors with high-throughput integer math pipelines. We also present a workflow for 8-bit quantization that is able to maintain accuracy within 1% of the floating-point baseline on all networks studied, including models that are more difficult to quantize, such as MobileNets and BERT-large.}, note={arXiv:2004.09602 [cs, stat]}, number={arXiv:2004.09602}, publisher={arXiv}, author={Wu, Hao and Judd, Patrick and Zhang, Xiaojie and Isaev, Mikhail and Micikevicius, Paulius}, year={2020}, month=Apr }
 
@article{Choudhary_Mishra_Goswami_Sarangapani_2020, title={A comprehensive survey on model compression and acceleration}, volume={53}, ISSN={0269-2821, 1573-7462}, DOI={10.1007/s10462-020-09816-7}, number={7}, journal={Artificial Intelligence Review}, author={Choudhary, Tejalal and Mishra, Vipul and Goswami, Anurag and Sarangapani, Jagannathan}, year={2020}, month=Oct, pages={5113–5155}, language={en} }

  @article{Hinton_Vinyals_Dean_2015, title={Distilling the Knowledge in a Neural Network}, url={http://arxiv.org/abs/1503.02531}, abstractNote={A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.}, note={arXiv:1503.02531 [cs, stat]}, number={arXiv:1503.02531}, publisher={arXiv}, author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff}, year={2015}, month=Mar }

  @article{Zhang_Yang_2021, title={A Survey on Multi-Task Learning}, url={http://arxiv.org/abs/1707.08114}, abstractNote={Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a definition of MTL and then classify different MTL algorithms into five categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL.}, note={arXiv:1707.08114 [cs]}, number={arXiv:1707.08114}, publisher={arXiv}, author={Zhang, Yu and Yang, Qiang}, year={2021}, month=Mar }

  @article{Parisi_Kemker_Part_Kanan_Wermter_2019, title={Continual lifelong learning with neural networks: A review}, volume={113}, ISSN={08936080}, DOI={10.1016/j.neunet.2019.01.012}, journal={Neural Networks}, author={Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan}, year={2019}, month=May, pages={54–71}, language={en} }

 @article{Sahoo_Pham_Lu_Hoi_2017, title={Online Deep Learning: Learning Deep Neural Networks on the Fly}, url={http://arxiv.org/abs/1711.03705}, abstractNote={Deep Neural Networks (DNNs) are typically trained by backpropagation in a batch learning setting, which requires the entire training data to be made available prior to the learning task. This is not scalable for many real-world scenarios where new data arrives sequentially in a stream form. We aim to address an open challenge of “Online Deep Learning” (ODL) for learning DNNs on the fly in an online setting. Unlike traditional online learning that often optimizes some convex objective function with respect to a shallow model (e.g., a linear/kernel-based hypothesis), ODL is significantly more challenging since the optimization of the DNN objective function is non-convex, and regular backpropagation does not work well in practice, especially for online learning settings. In this paper, we present a new online deep learning framework that attempts to tackle the challenges by learning DNN models of adaptive depth from a sequence of training data in an online learning setting. In particular, we propose a novel Hedge Backpropagation (HBP) method for online updating the parameters of DNN effectively, and validate the efficacy of our method on large-scale data sets, including both stationary and concept drifting scenarios.}, note={arXiv:1711.03705 [cs]}, number={arXiv:1711.03705}, publisher={arXiv}, author={Sahoo, Doyen and Pham, Quang and Lu, Jing and Hoi, Steven C. H.}, year={2017}, month=Nov }

 @article{Madan_Henry_Dozier_Ho_Bhandari_Sasaki_Durand_Pfister_Boix_2022, title={When and how convolutional neural networks generalize to out-of-distribution category–viewpoint combinations}, volume={4}, ISSN={2522-5839}, DOI={10.1038/s42256-021-00437-5}, number={2}, journal={Nature Machine Intelligence}, author={Madan, Spandan and Henry, Timothy and Dozier, Jamell and Ho, Helen and Bhandari, Nishchal and Sasaki, Tomotake and Durand, Frédo and Pfister, Hanspeter and Boix, Xavier}, year={2022}, month=Feb, pages={146–153}, language={en} }

 @article{Marcus_2018, title={Deep Learning: A Critical Appraisal}, url={http://arxiv.org/abs/1801.00631}, abstractNote={Although deep learning has historical roots going back decades, neither the term “deep learning” nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton’s now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.}, note={arXiv:1801.00631 [cs, stat]}, number={arXiv:1801.00631}, publisher={arXiv}, author={Marcus, Gary}, year={2018}, month=Jan }

 @book{Moravec_1995, address={Cambridge}, edition={4. print}, title={Mind children: the future of robot and human intelligence}, ISBN={978-0-674-57618-6}, publisher={Harvard Univ. Press}, author={Moravec, Hans}, year={1995}, language={eng} }


 @article{Felleman_Van_Essen_1991, title={Distributed Hierarchical Processing in the Primate Cerebral Cortex}, volume={1}, ISSN={1047-3211, 1460-2199}, DOI={10.1093/cercor/1.1.1}, number={1}, journal={Cerebral Cortex}, author={Felleman, D. J. and Van Essen, D. C.}, year={1991}, month=Jan, pages={1–47}, language={en} }

 @article{Lillicrap_Cownden_Tweed_Akerman_2016, title={Random synaptic feedback weights support error backpropagation for deep learning}, volume={7}, ISSN={2041-1723}, DOI={10.1038/ncomms13276}, number={1}, journal={Nature Communications}, author={Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.}, year={2016}, month=Dec, pages={13276}, language={en} }

 @article{Hopfield_1982, title={Neural networks and physical systems with emergent collective computational abilities.}, volume={79}, ISSN={0027-8424, 1091-6490}, DOI={10.1073/pnas.79.8.2554}, abstractNote={Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.}, number={8}, journal={Proceedings of the National Academy of Sciences}, author={Hopfield, J J}, year={1982}, month=Apr, pages={2554–2558}, language={en} }

 @article{Fix_Hodges_1989, title={Discriminatory Analysis. Nonparametric Discrimination: Consistency Properties}, volume={57}, ISSN={03067734}, DOI={10.2307/1403797}, number={3}, journal={International Statistical Review / Revue Internationale de Statistique}, author={Fix, Evelyn and Hodges, J. L.}, year={1989}, month=Dec, pages={238} }

 @article{Weston_Chopra_Bordes_2015, title={Memory Networks}, url={http://arxiv.org/abs/1410.3916}, abstractNote={We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.}, note={arXiv:1410.3916 [cs, stat]}, number={arXiv:1410.3916}, publisher={arXiv}, author={Weston, Jason and Chopra, Sumit and Bordes, Antoine}, year={2015}, month=Nov }

@ARTICLE{1057328,
  author={McEliece, R. and Posner, E. and Rodemich, E. and Venkatesh, S.},
  journal={IEEE Transactions on Information Theory}, 
  title={The capacity of the Hopfield associative memory}, 
  year={1987},
  volume={33},
  number={4},
  pages={461-482},
  doi={10.1109/TIT.1987.1057328}}

@Article{Hopfield1983,
author={Hopfield, J. J.
and Feinstein, D. I.
and Palmer, R. G.},
title={`Unlearning' has a stabilizing effect in collective memories},
journal={Nature},
year={1983},
month=Jul,
day={01},
volume={304},
number={5922},
pages={158-159},
abstract={Crick and Mitchison1 have presented a hypothesis for the functional role of dream sleep involving an `unlearning' process. We have independently carried out mathematical and computer modelling of learning and `unlearning' in a collective neural network of 30--1,000 neurones. The model network has a content-addressable memory or `associative memory' which allows it to learn and store many memories. A particular memory can be evoked in its entirety when the network is stimulated by any adequate-sized subpart of the information of that memory2. But different memories of the same size are not equally easy to recall. Also, when memories are learned, spurious memories are also created and can also be evoked. Applying an `unlearning' process, similar to the learning processes but with a reversed sign and starting from a noise input, enhances the performance of the network in accessing real memories and in minimizing spurious ones. Although our model was not motivated by higher nervous function, our system displays behaviours which are strikingly parallel to those needed for the hypothesized role of `unlearning' in rapid eye movement (REM) sleep.},
issn={1476-4687},
doi={10.1038/304158a0},
url={https://doi.org/10.1038/304158a0}
}

@inproceedings{10.5555/3157096.3157228, author = {Krotov, Dmitry and Hopfield, John J.}, title = {Dense Associative Memory for Pattern Recognition}, year = {2016}, isbn = {9781510838819}, publisher = {Curran Associates Inc.}, address = {Red Hook, NY, USA}, abstract = {A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions - the higher rectified polynomials which until now have not been used in deep learning. The utility of the dense memories is illustrated for two test cases: the logical gate XOR and the recognition of handwritten digits from the MNIST data set.}, booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems}, pages = {1180–1188}, numpages = {9}, location = {Barcelona, Spain}, series = {NIPS'16} }

 @article{Demircigil_Heusel_Löwe_Upgang_Vermet_2017, title={On a Model of Associative Memory with Huge Storage Capacity}, volume={168}, ISSN={0022-4715, 1572-9613}, DOI={10.1007/s10955-017-1806-y}, number={2}, journal={Journal of Statistical Physics}, author={Demircigil, Mete and Heusel, Judith and Löwe, Matthias and Upgang, Sven and Vermet, Franck}, year={2017}, month=Jul, pages={288–299}, language={en} }

 @article{Ramsauer, title={Hopfield Networks is All You Need}, url={http://arxiv.org/abs/2008.02217}, abstractNote={We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers}, note={arXiv:2008.02217 [cs, stat]}, number={arXiv:2008.02217}, publisher={arXiv}, author={Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp}, year={2021}, month=Apr }

 @book{Hebb_1949, address={Oxford,  England}, series={The organization of behavior; a neuropsychological theory.}, title={The organization of behavior; a neuropsychological theory.}, abstractNote={“This book presents a theory of behavior that is based as far as possible on the physiology of the nervous system, and makes a sedulous attempt to find some community of neurological and psychological conceptions.” Using the concept of the reverbatory circuit and the assumption that “some growth process or metabolic change” in neurones takes place as a result of repeated transmission across synapses, perceptual integration is described in terms of “cell-assemblies.” Of 11 chapters, 4 are devoted to perceptual problems, 2 to learning, 2 to motivation, and 1 each to emotional disturbances and intelligence. 14-page bibliography. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}, publisher={Wiley}, author={Hebb, D. O.}, year={1949}, pages={xix, 335}, collection={The organization of behavior; a neuropsychological theory.} }

 @article{Oja_1982, title={Simplified neuron model as a principal component analyzer}, volume={15}, ISSN={0303-6812, 1432-1416}, DOI={10.1007/BF00275687}, number={3}, journal={Journal of Mathematical Biology}, author={Oja, Erkki}, year={1982}, month=Nov, pages={267–273}, language={en} }

 @article{Bienenstock_Cooper_Munro_1982, title={Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex}, volume={2}, ISSN={0270-6474, 1529-2401}, DOI={10.1523/JNEUROSCI.02-01-00032.1982}, number={1}, journal={The Journal of Neuroscience}, author={Bienenstock, El and Cooper, Ln and Munro, Pw}, year={1982}, month=Jan, pages={32–48}, language={en} }

 @article{Intrator_Cooper_1992, title={Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions}, volume={5}, ISSN={08936080}, DOI={10.1016/S0893-6080(05)80003-6}, number={1}, journal={Neural Networks}, author={Intrator, Nathan and Cooper, Leon N}, year={1992}, month=Jan, pages={3–17}, language={en} }

 @article{Simoncelli_Olshausen_2001, title={Natural Image Statistics and Neural Representation}, volume={24}, ISSN={0147-006X, 1545-4126}, DOI={10.1146/annurev.neuro.24.1.1193}, abstractNote={▪ Abstract  It has long been assumed that sensory neurons are adapted, through both evolutionary and developmental processes, to the statistical properties of the signals to which they are exposed. Attneave (1954) , Barlow (1961) proposed that information theory could provide a link between environmental statistics and neural responses through the concept of coding efficiency. Recent developments in statistical modeling, along with powerful computational tools, have enabled researchers to study more sophisticated statistical models for visual images, to validate these models empirically against large sets of data, and to begin experimentally testing the efficient coding hypothesis for both individual neurons and populations of neurons.}, number={1}, journal={Annual Review of Neuroscience}, author={Simoncelli, Eero P and Olshausen, Bruno A}, year={2001}, month=Mar, pages={1193–1216}, language={en} }

 @article{Vogels_Sprekeler_Zenke_Clopath_Gerstner_2011, title={Inhibitory Plasticity Balances Excitation and Inhibition in Sensory Pathways and Memory Networks}, volume={334}, ISSN={0036-8075, 1095-9203}, DOI={10.1126/science.1211095}, number={6062}, journal={Science}, author={Vogels, T. P. and Sprekeler, H. and Zenke, F. and Clopath, C. and Gerstner, W.}, year={2011}, month=Dec, pages={1569–1573}, language={en} }

@INPROCEEDINGS{5178625,
  author={Joshi, Prashant and Triesch, Jochen},
  booktitle={2009 International Joint Conference on Neural Networks}, 
  title={Rules for information maximization in spiking neurons using intrinsic plasticity}, 
  year={2009},
  volume={},
  number={},
  pages={1456-1461},
  doi={10.1109/IJCNN.2009.5178625}}

@inproceedings{Teichmann,
author = {Teichmann, Michael and Hamker, Fred},
year = {2015},
month = {03},
pages = {},
title = {Intrinsic plasticity: A simple mechanism to stabilize Hebbian learning in multilayer neural networks.}
}

 @article{Grossberg_1988, title={Nonlinear neural networks: Principles, mechanisms, and architectures}, volume={1}, ISSN={08936080}, DOI={10.1016/0893-6080(88)90021-4}, number={1}, journal={Neural Networks}, author={Grossberg, Stephen}, year={1988}, month=Jan, pages={17–61}, language={en} }

@article{esn,
author = {Jaeger, Herbert},
year = {2001},
month = {01},
pages = {},
title = {The" echo state" approach to analysing and training recurrent neural networks-with an erratum note'},
volume = {148},
journal = {Bonn, Germany: German National Research Center for Information Technology GMD Technical Report}
}

 @article{Maass_Natschlager_Markram_2002, title={Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations}, volume={14}, ISSN={0899-7667, 1530-888X}, DOI={10.1162/089976602760407955}, abstractNote={A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.}, number={11}, journal={Neural Computation}, author={Maass, Wolfgang and Natschläger, Thomas and Markram, Henry}, year={2002}, month=Nov, pages={2531–2560}, language={en} }

 @inbook{Konkoli_2018, address={New York, NY}, title={Reservoir Computing}, ISBN={978-1-4939-6882-4}, url={http://link.springer.com/10.1007/978-1-4939-6883-1_683}, DOI={10.1007/978-1-4939-6883-1_683}, booktitle={Unconventional Computing}, publisher={Springer US}, author={Konkoli, Zoran}, editor={Adamatzky, Andrew}, year={2018}, pages={619–629}, language={en} }

 @article{Tanaka_Yamane_Héroux_Nakane_Kanazawa_Takeda_Numata_Nakano_Hirose_2019, title={Recent advances in physical reservoir computing: A review}, volume={115}, ISSN={08936080}, DOI={10.1016/j.neunet.2019.03.005}, journal={Neural Networks}, author={Tanaka, Gouhei and Yamane, Toshiyuki and Héroux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira}, year={2019}, month=Jul, pages={100–123}, language={en} }


@article{erdos59a,
  author = {Erdös, P. and Rényi, A.},
  journal = {Publicationes Mathematicae Debrecen},
  keywords = {graph sna},
  pages = 290,
  title = {On Random Graphs I},
  volume = 6,
  year = 1959
}

 @inbook{Lukoševičius_2012, address={Berlin, Heidelberg}, series={Lecture Notes in Computer Science}, title={A Practical Guide to Applying Echo State Networks}, volume={7700}, ISBN={978-3-642-35288-1}, url={http://link.springer.com/10.1007/978-3-642-35289-8_36}, DOI={10.1007/978-3-642-35289-8_36}, booktitle={Neural Networks: Tricks of the Trade}, publisher={Springer Berlin Heidelberg}, author={Lukoševičius, Mantas}, editor={Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert}, year={2012}, pages={659–686}, collection={Lecture Notes in Computer Science}, language={en} }

@article{Abbott1999LapicquesIO,
  title={Lapicque’s introduction of the integrate-and-fire model neuron (1907)},
  author={L. F. Abbott},
  journal={Brain Research Bulletin},
  year={1999},
  volume={50},
  pages={303-304}
}

 @article{Izhikevich_2003, title={Simple model of spiking neurons}, volume={14}, ISSN={1045-9227}, DOI={10.1109/TNN.2003.820440}, number={6}, journal={IEEE Transactions on Neural Networks}, author={Izhikevich, E.M.}, year={2003}, month=Nov, pages={1569–1572}, language={en} }

 @article{Brette_Gerstner_2005, title={Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity}, volume={94}, ISSN={0022-3077, 1522-1598}, DOI={10.1152/jn.00686.2005}, abstractNote={We introduce a two-dimensional integrate-and-fire model that combines an exponential spike mechanism with an adaptation equation, based on recent theoretical findings. We describe a systematic method to estimate its parameters with simple electrophysiological protocols (current-clamp injection of pulses and ramps) and apply it to a detailed conductance-based model of a regular spiking neuron. Our simple model predicts correctly the timing of 96% of the spikes (±2 ms) of the detailed model in response to injection of noisy synaptic conductances. The model is especially reliable in high-conductance states, typical of cortical activity in vivo, in which intrinsic conductances were found to have a reduced role in shaping spike trains. These results are promising because this simple model has enough expressive power to reproduce qualitatively several electrophysiological classes described in vitro.}, number={5}, journal={Journal of Neurophysiology}, author={Brette, Romain and Gerstner, Wulfram}, year={2005}, month=Nov, pages={3637–3642}, language={en} }

@article{Paugam_Moisy,
      title = {Spiking Neuron Networks A survey},
      author = {Paugam-Moisy, Hélène},
      publisher = {IDIAP},
      year = {2006},
      url = {http://infoscience.epfl.ch/record/83371},
}

 @article{Bi_Poo_2001, title={Synaptic Modification by Correlated Activity: Hebb’s Postulate Revisited}, volume={24}, ISSN={0147-006X, 1545-4126}, DOI={10.1146/annurev.neuro.24.1.139}, number={1}, journal={Annual Review of Neuroscience}, author={Bi, Guo-qiang and Poo, Mu-ming}, year={2001}, month=Mar, pages={139–166}, language={en} }

@article{KHERADPISHEH201856,
title = {STDP-based spiking deep convolutional neural networks for object recognition},
journal = {Neural Networks},
volume = {99},
pages = {56-67},
year = {2018},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2017.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608017302903},
author = {Saeed Reza Kheradpisheh and Mohammad Ganjtabesh and Simon J. Thorpe and Timothée Masquelier},
keywords = {Spiking neural network, STDP, Deep learning, Object recognition, Temporal coding},
}

 @article{von_der_Malsburg_Stadelmann_Grewe_2022, title={A Theory of Natural Intelligence}, url={http://arxiv.org/abs/2205.00002}, abstractNote={Introduction: In contrast to current AI technology, natural intelligence -- the kind of autonomous intelligence that is realized in the brains of animals and humans to attain in their natural environment goals defined by a repertoire of innate behavioral schemata -- is far superior in terms of learning speed, generalization capabilities, autonomy and creativity. How are these strengths, by what means are ideas and imagination produced in natural neural networks? Methods: Reviewing the literature, we put forward the argument that both our natural environment and the brain are of low complexity, that is, require for their generation very little information and are consequently both highly structured. We further argue that the structures of brain and natural environment are closely related. Results: We propose that the structural regularity of the brain takes the form of net fragments (self-organized network patterns) and that these serve as the powerful inductive bias that enables the brain to learn quickly, generalize from few examples and bridge the gap between abstractly defined general goals and concrete situations. Conclusions: Our results have important bearings on open problems in artificial neural network research.}, note={arXiv:2205.00002 [cs, q-bio]}, number={arXiv:2205.00002}, publisher={arXiv}, author={von der Malsburg, Christoph and Stadelmann, Thilo and Grewe, Benjamin F.}, year={2022}, month=Apr }

@mastersthesis{lehmann,
  author  = "Lehmann, Claude",
  title   = "Leveraging Neuroscience for Deep Learning Based Object Recognition",
  school  = "Zurich University of Applied Sciences",
  year    = "2022"
}

@article{Dresp2020SevenPO,
  title={Seven Properties of Self-Organization in the Human Brain},
  author={Birgitta Dresp},
  journal={Big Data Cogn. Comput.},
  year={2020},
  volume={4},
  pages={10}
}

@book{kelso1995dynamic,
  title={Dynamic patterns: The self-organization of brain and behavior},
  author={Kelso, JA Scott},
  year={1995},
  publisher={MIT press}
}

@Article{hbcrd,
author={McPherson, John D.
and Marra, Marco
and Hillier, LaDeana
and Waterston, Robert H.
and Chinwalla, Asif
and Wallis, John
and Sekhon, Mandeep
and Wylie, Kristine
and Mardis, Elaine R.
and Wilson, Richard K.
and Fulton, Robert
and Kucaba, Tamara A.
and Wagner-McPherson, Caryn
and Barbazuk, William B.
and Gregory, Simon G.
and Humphray, Sean J.
and French, Lisa
and Evans, Richard S.
and Bethel, Graeme
and Whittaker, Adam
and Holden, Jane L.
and McCann, Owen T.
and Dunham, Andrew
and Soderlund, Carol
and Scott, Carol E.
and Bentley, David R.
and Schuler, Gregory
and Chen, Hsiu-Chuan
and Jang, Wonhee
and Green, Eric D.
and Idol, Jacquelyn R.
and Maduro, Valerie V. Braden
and Montgomery, Kate T.
and Lee, Eunice
and Miller, Ashley
and Emerling, Suzanne
and Kucherlapati, Raju
and Gibbs, Richard
and Scherer, Steve
and Gorrell, J. Harley
and Sodergren, Erica
and Clerc-Blankenburg, Kerstin
and Tabor, Paul
and Naylor, Susan
and Garcia, Dawn
and de Jong, Pieter J.
and Catanese, Joseph J.
and Nowak, Norma
and Osoegawa, Kazutoyo
and Qin, Shizhen
and Rowen, Lee
and Madan, Anuradha
and Dors, Monica
and Hood, Leroy
and Trask, Barbara
and Friedman, Cynthia
and Massa, Hillary
and Cheung, Vivian G.
and Kirsch, Ilan R.
and Reid, Thomas
and Yonescu, Raluca
and Weissenbach, Jean
and Bruls, Thomas
and Heilig, Roland
and Branscomb, Elbert
and Olsen, Anne
and Doggett, Norman
and Cheng, Jan-Fang
and Hawkins, Trevor
and Myers, Richard M.
and Shang, Jin
and Ramirez, Lucia
and Schmutz, Jeremy
and Velasquez, Olivia
and Dixon, Kami
and Stone, Nancy E.
and Cox, David R.
and Haussler, David
and Kent, W. James
and Furey, Terrence
and Rogic, Sanja
and Kennedy, Scot
and Jones, Steven
and Rosenthal, Andre
and Wen, Gaiping
and Schilhabel, Markus
and Gloeckner, Gernot
and Nyakatura, Gerald
and Siebert, Reiner
and Schlegelberger, Brigitte
and Korenberg, Julie
and Chen, Xiao-Ning
and Fujiyama, Asao
and Hattori, Masahira
and Toyoda, Atsushi
and Yada, Tetsushi
and Park, Hong-Seok
and Sakaki, Yoshiyuki
and Shimizu, Nobuyoshi
and Asakawa, Shuichi
and Kawasaki, Kazuhiko
and Sasaki, Takashi
and Shintani, Ai
and Shimizu, Atsushi
and Shibuya, Kazunori
and Kudoh, Jun
and Minoshima, Shinsei
and Ramser, Juliane
and Seranski, Peter
and Hoff, Celine
and Poustka, Annemarie
and Reinhardt, Richard
and Lehrach, Hans},
title={A physical map of the human genome},
journal={Nature},
year={2001},
month=Feb,
day={01},
volume={409},
number={6822},
pages={934-941},
issn={1476-4687},
doi={10.1038/35057157},
url={https://doi.org/10.1038/35057157}
}

 @article{Kolmogorov_1998, title={On tables of random numbers}, volume={207}, ISSN={03043975}, DOI={10.1016/S0304-3975(98)00075-9}, number={2}, journal={Theoretical Computer Science}, author={Kolmogorov, A.N.}, year={1998}, month=Nov, pages={387–395}, language={en} }

 @article{Willshaw_VonDerMalsburg_1976, title={How patterned neural connections can be set up by self-organization}, volume={194}, ISSN={0080-4649, 2053-9193}, DOI={10.1098/rspb.1976.0087}, abstractNote={An important problem in biology is to explain how patterned neural connections are set up during ontogenesis. Topographically ordered mappings, found widely in nervous systems, are those in which neighbouring elements in one sheet of cells project to neighbouring elements in a second sheet. Exploiting this neighbourhood property leads to a new theory for the establishment of topographical mappings, in which the distance between two cells is expressed in terms of their similarity with respect to certain physical properties assigned to them. This topographical code can be realized in a model employing either synchronization of nervous activity or exchange of specific molecules between neighbouring cells. By means of modifiable synapses the code is used to set up a topographical mapping between two sheets with the same internal structure. We have investigated the neural activity version. Without needing to make any elaborate assumptions about its structure or about the operations its elements are to carry out we have shown that the mappings are set up in a system-to-system rather than a cell-to-cell fashion. The pattern of connections develops in a step-by-step and orderly fashion, the orientation of the mappings being laid down in the earliest stages of development.}, number={1117}, journal={Proceedings of the Royal Society of London. Series B. Biological Sciences}, author={Willshaw, D. J. and Von Der Malsburg, Christoph}, year={1976}, month=Nov, pages={431–445}, language={en} }

 @article{Willshaw_VonDerMalsburg_1979, title={A marker induction mechanism for the establishment of ordered neural mappings: its application to the retinotectal problem}, volume={287}, ISSN={0080-4622, 2054-0280}, DOI={10.1098/rstb.1979.0056}, abstractNote={This paper examines the idea that ordered patterns of nerve connections are set up by means of markers carried by the individual cells. The case of the ordered retinotectal projection in amphibia and fishes is discussed in great detail. It is suggested that retinotectal mappings are the result of two mechanisms acting in concert. One mechanism induces a set of retinal markers into the tectum. By this means, an initially haphazard pattern of synapses is transformed into a continuous or piece-wise continuous projection. The other mechanism places the individual pieces of the map in the correct orientation. The machinery necessary for this inductive scheme has been expressed in terms of a set of differential equations, which have been solved numerically for a number of cases. Straightforward assumptions are made as to how markers are distributed in the retina; how they are induced into the tectum; and how the induced markers bring about alterations in the pattern of synaptic contacts. A detailed physiological interpretation of the model is given. The inductive mechanism has been formulated at the level of the individual synaptic interactions. Therefore, it is possible to specify, in a given situation, not only the nature of the end state of the mapping but also how the mapping develops over time. The role of the modes of growth of retina and tectum in shaping the developing projection becomes clear. Since, on this model, the tectum is initially devoid of markers, there is an important difference between the development and the regeneration of ordered mappings. In the development of duplicate maps from various types of compound-eyes, it is suggested that the tectum, rather than the retina, contains an abnormal distribution of markers. An important parameter in these experiments, and also in the regeneration experiments where part-duplication has been found, is the range of interaction amongst the retinal cells. It is suggested that the results of many of the regeneration experiments (including apparently contradictory ones) are manifestations of a conflict between the two alternative ways of specifying the orientation of the map: through the information carried by the markers previously induced into the tectum and through the orientation mechanism itself.}, number={1021}, journal={Philosophical Transactions of the Royal Society of London. B, Biological Sciences}, author={Willshaw, D. J. and Von Der Malsburg, Christoph}, year={1979}, month=Nov, pages={203–243}, language={en} }

@article{Malsburg_1987,
	doi = {10.1209/0295-5075/3/11/015},
	url = {https://doi.org/10.1209/0295-5075/3/11/015},
	year = 1987,
	month = Jun,
	publisher = {{IOP} Publishing},
	volume = {3},
	number = {11},
	pages = {1243--1249},
	author = {C. von der Malsburg and E Bienenstock},
	title = {A Neural Network for the Retrieval of Superimposed Connection Patterns},
	journal = {Europhysics Letters ({EPL})},
	abstract = {The principle of associative memory is extended to a system with dynamical links capable of retrieval of superimposed connection patterns. The system consists of formalized neurons. Its dynamics is described by two separate Hamiltonians, one for spins and one for links. The spin part is treated in analogy to the Ising system on a 2D grid. Several such network patterns, related by permutations of neurons, are superimposed. Energy minima correspond to the activation of one connection pattern and the deactivation of all others. One important application of this system is invariant pattern recognition.}
}

 @article{vonderMalsburg_2018, title={Concerning the Neuronal Code}, volume={19}, DOI={10.17791/JCS.2018.19.4.511}, number={4}, journal={Journal of Cognitive Science}, author={C. von der Malsburg}, year={2018}, month=Dec, pages={511–550} }

@article{freeman1990representations,
  title={Representations: Who needs them?},
  author={Freeman III, Walter J and Skarda, Christine A},
  year={1990}
}

 @article{Fernandes_vonderMalsburg_2015, title={Self-Organization of Control Circuits for Invariant Fiber Projections}, volume={27}, ISSN={0899-7667, 1530-888X}, DOI={10.1162/NECO_a_00725}, abstractNote={Assuming that patterns in memory are represented as two-dimensional arrays of local features, just as they are in primary visual cortices, pattern recognition can take the form of elastic graph matching (Lades et al., 1993 ). Neural implementation of this may be based on preorganized fiber projections that can be activated rapidly with the help of control units (Wolfrum, Wolff, Lücke, & von der Malsburg, 2008 ). Each control unit governs a set of projection fibers that form part of a coherent mapping. We describe a mathematical model for the ontogenesis of the underlying connectivity based on a principle of network self-organization as described by the Häussler system (Häussler & von der Malsburg, 1983 ), modified to be sensitive to pattern similarity and to support formation of multiple mappings, each under the command of a control unit. The process takes the form of a soft-winner-take-all, where units compete for the representation of maps. We show simulations for invariant point-to-point and feature-to-feature mappings.}, number={5}, journal={Neural Computation}, author={Fernandes, Tomas and von der Malsburg, Christoph}, year={2015}, month=May, pages={1005–1032}, language={en} }

 @book{Arathorn_2002, address={Stanford, Calif}, title={Map-seeking circuits in visual cognition: a computational mechanism for biological and machine vision}, ISBN={978-0-8047-4277-1}, callNumber={QP383.15 .A736 2002}, publisher={Stanford University Press}, author={Arathorn, D. W.}, year={2002} }

@Article{Olshausen1995,
author={Olshausen, Bruno A.
and Anderson, Charles H.
and Van Essen, David C.},
title={A multiscale dynamic routing circuit for forming size- and position-invariant object representations},
journal={Journal of Computational Neuroscience},
year={1995},
month=Mar,
day={01},
volume={2},
number={1},
pages={45-62},
abstract={We describe a neural model for forming size- and position-invariant representations of visual objects. The model is based on a previously proposed dynamic routing circuit that remaps selected portions of an input array into an object-centered reference frame. Here, we show how a multiscale representation may be incorporated at the input stage of the model, and we describe the control architecture and dynamics for a hierarchical, multistage routing circuit. Specific neurobiological substrates and mechanisms for the model are proposed, and a number of testable predictions are described.},
issn={1573-6873},
doi={10.1007/BF00962707},
url={https://doi.org/10.1007/BF00962707}
}

@article{dorigo1997ant,
  title={Ant colony system: a cooperative learning approach to the traveling salesman problem},
  author={Dorigo, Marco and Gambardella, Luca Maria},
  journal={IEEE Transactions on evolutionary computation},
  volume={1},
  number={1},
  pages={53--66},
  year={1997},
  publisher={IEEE}
}

 @book{Hamann_2018, address={New York, NY}, title={Swarm robotics: a formal approach}, ISBN={978-3-319-74526-8}, publisher={Springer Science+Business Media}, author={Hamann, Heiko}, year={2018} }

 @article{Rubenstein_Cornejo_Nagpal_2014, title={Programmable self-assembly in a thousand-robot swarm}, volume={345}, ISSN={0036-8075, 1095-9203}, DOI={10.1126/science.1254295}, number={6198}, journal={Science}, author={Rubenstein, Michael and Cornejo, Alejandro and Nagpal, Radhika}, year={2014}, month=Aug, pages={795–799}, language={en} }

@inproceedings{Wulff1992LearningCA,
  title={Learning Cellular Automation Dynamics with Neural Networks},
  author={N. H. Wulff and John A. Hertz},
  booktitle={NIPS},
  year={1992}
}

 @inbook{Byla_Pang_2020, address={Cham}, series={Advances in Intelligent Systems and Computing}, title={DeepSwarm: Optimising Convolutional Neural Networks Using Swarm Intelligence}, volume={1043}, ISBN={978-3-030-29932-3}, url={http://link.springer.com/10.1007/978-3-030-29933-0_10}, DOI={10.1007/978-3-030-29933-0_10}, booktitle={Advances in Computational Intelligence Systems}, publisher={Springer International Publishing}, author={Byla, Edvinas and Pang, Wei}, editor={Ju, Zhaojie and Yang, Longzhi and Yang, Chenguang and Gegov, Alexander and Zhou, Dalin}, year={2020}, pages={119–130}, collection={Advances in Intelligent Systems and Computing}, language={en} }

@Article{Wolfram1984,
author={Wolfram, Stephen},
title={Cellular automata as models of complexity},
journal={Nature},
year={1984},
month=Oct,
day={01},
volume={311},
number={5985},
pages={419-424},
abstract={Natural systems from snowflakes to mollusc shells show a great diversity of complex patterns. The origins of such complexity can be investigated through mathematical models termed `cellular automata'. Cellular automata consist of many identical components, each simple., but together capable of complex behaviour. They are analysed both as discrete dynamical systems, and as information-processing systems. Here some of their universal features are discussed, and some general principles are suggested.},
issn={1476-4687},
doi={10.1038/311419a0},
url={https://doi.org/10.1038/311419a0}
}

@article{VICHNIAC198496,
title = {Simulating physics with cellular automata},
journal = {Physica D: Nonlinear Phenomena},
volume = {10},
number = {1},
pages = {96-116},
year = {1984},
issn = {0167-2789},
doi = {https://doi.org/10.1016/0167-2789(84)90253-7},
url = {https://www.sciencedirect.com/science/article/pii/0167278984902537},
author = {Gérard Y. Vichniac},
abstract = {Cellular automata are dynamical systems where space, time, and variables are discrete. They are shown on two-dimensional examples to be capable of non-numerical simulations of physics. They are useful for faithful parallel processing of lattice models. At another level, they exhibit behaviours and illustrate concepts that are unmistakably physical, such as non-ergodicity and order parameters, frustration, relaxation to chaos through period doublings, a conspicuous arrow of time in reversible microscopic dynamics, causality and light-cone, and non-separability. In general, they constitute exactly computable models for complex phenomena and large-scale correlations that result from very simple short-range interactions. We study their space, time, and intrinsic symmetries and the corresponding conservation laws, with an emphasis on the conservation of information obeyed by reversible cellular automata.}
}

@article{science.1372754,
author = {Siegrid Löwel  and Wolf Singer },
title = {Selection of Intrinsic Horizontal Connections in the Visual Cortex by Correlated Neuronal Activity},
journal = {Science},
volume = {255},
number = {5041},
pages = {209-212},
year = {1992},
doi = {10.1126/science.1372754},
URL = {https://www.science.org/doi/abs/10.1126/science.1372754},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1372754}
}

@article{PhysRevE,
  title = {Cellular automata as convolutional neural networks},
  author = {Gilpin, William},
  journal = {Phys. Rev. E},
  volume = {100},
  issue = {3},
  pages = {032402},
  numpages = {11},
  year = {2019},
  month = Sep,
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.100.032402},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.100.032402}
}

@article{48963,
title	= {Growing Neural Cellular Automata},
author	= {Alexander Mordvintsev and Ettore Randazzo and Eyvind Niklasson and Michael Levin},
year	= {2020},
URL	= {https://distill.pub/2020/growing-ca/},
journal	= {Distill}
}

 @article{Mordvintsev_Randazzo_Fouts_2022, title={Growing Isotropic Neural Cellular Automata}, url={http://arxiv.org/abs/2205.01681}, abstractNote={Modeling the ability of multicellular organisms to build and maintain their bodies through local interactions between individual cells (morphogenesis) is a long-standing challenge of developmental biology. Recently, the Neural Cellular Automata (NCA) model was proposed as a way to find local system rules that produce a desired global behaviour, such as growing and persisting a predefined target pattern, by repeatedly applying the same rule over a grid starting from a single cell. In this work, we argue that the original Growing NCA model has an important limitation: anisotropy of the learned update rule. This implies the presence of an external factor that orients the cells in a particular direction. In other words, “physical” rules of the underlying system are not invariant to rotation, thus prohibiting the existence of differently oriented instances of the target pattern on the same grid. We propose a modified Isotropic NCA (IsoNCA) model that does not have this limitation. We demonstrate that such cell systems can be trained to grow accurate asymmetrical patterns through either of two methods: (1) by breaking symmetries using structured seeds or (2) by introducing a rotation-reflection invariant training objective and relying on symmetry-breaking caused by asynchronous cell updates.}, note={arXiv:2205.01681 [cs, q-bio]}, number={arXiv:2205.01681}, publisher={arXiv}, author={Mordvintsev, Alexander and Randazzo, Ettore and Fouts, Craig}, year={2022}, month=Jun }

 @article{Palm_GonDuque_Sudhakaran_Risi_2022, title={Variational Neural Cellular Automata}, url={http://arxiv.org/abs/2201.12360}, abstractNote={In nature, the process of cellular growth and differentiation has lead to an amazing diversity of organisms -- algae, starfish, giant sequoia, tardigrades, and orcas are all created by the same generative process. Inspired by the incredible diversity of this biological generative process, we propose a generative model, the Variational Neural Cellular Automata (VNCA), which is loosely inspired by the biological processes of cellular growth and differentiation. Unlike previous related works, the VNCA is a proper probabilistic generative model, and we evaluate it according to best practices. We find that the VNCA learns to reconstruct samples well and that despite its relatively few parameters and simple local-only communication, the VNCA can learn to generate a large variety of output from information encoded in a common vector format. While there is a significant gap to the current state-of-the-art in terms of generative modeling performance, we show that the VNCA can learn a purely self-organizing generative process of data. Additionally, we show that the VNCA can learn a distribution of stable attractors that can recover from significant damage.}, note={arXiv:2201.12360 [cs]}, number={arXiv:2201.12360}, publisher={arXiv}, author={Palm, Rasmus Berg and González-Duque, Miguel and Sudhakaran, Shyam and Risi, Sebastian}, year={2022}, month=Feb }

 @article{Kingma_Welling_2014, title={Auto-Encoding Variational Bayes}, url={http://arxiv.org/abs/1312.6114}, abstractNote={How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.}, note={arXiv:1312.6114 [cs, stat]}, number={arXiv:1312.6114}, publisher={arXiv}, author={Kingma, Diederik P. and Welling, Max}, year={2014}, month=May }

 @article{Sudhakaran_Grbic_Li_Katona_Najarro_Glanois_Risi_2021, title={Growing 3D Artefacts and Functional Machines with Neural Cellular Automata}, url={http://arxiv.org/abs/2103.08737}, abstractNote={Neural Cellular Automata (NCAs) have been proven effective in simulating morphogenetic processes, the continuous construction of complex structures from very few starting cells. Recent developments in NCAs lie in the 2D domain, namely reconstructing target images from a single pixel or infinitely growing 2D textures. In this work, we propose an extension of NCAs to 3D, utilizing 3D convolutions in the proposed neural network architecture. Minecraft is selected as the environment for our automaton since it allows the generation of both static structures and moving machines. We show that despite their simplicity, NCAs are capable of growing complex entities such as castles, apartment blocks, and trees, some of which are composed of over 3,000 blocks. Additionally, when trained for regeneration, the system is able to regrow parts of simple functional machines, significantly expanding the capabilities of simulated morphogenetic systems. The code for the experiment in this paper can be found at: https://github.com/real-itu/3d-artefacts-nca.}, note={arXiv:2103.08737 [cs]}, number={arXiv:2103.08737}, publisher={arXiv}, author={Sudhakaran, Shyam and Grbic, Djordje and Li, Siyan and Katona, Adam and Najarro, Elias and Glanois, Claire and Risi, Sebastian}, year={2021}, month=Jun }

 @article{Horibe_Walker_Risi_2021, title={Regenerating Soft Robots through Neural Cellular Automata}, url={http://arxiv.org/abs/2102.02579}, abstractNote={Morphological regeneration is an important feature that highlights the environmental adaptive capacity of biological systems. Lack of this regenerative capacity significantly limits the resilience of machines and the environments they can operate in. To aid in addressing this gap, we develop an approach for simulated soft robots to regrow parts of their morphology when being damaged. Although numerical simulations using soft robots have played an important role in their design, evolving soft robots with regenerative capabilities have so far received comparable little attention. Here we propose a model for soft robots that regenerate through a neural cellular automata. Importantly, this approach only relies on local cell information to regrow damaged components, opening interesting possibilities for physical regenerable soft robots in the future. Our approach allows simulated soft robots that are damaged to partially regenerate their original morphology through local cell interactions alone and regain some of their ability to locomote. These results take a step towards equipping artificial systems with regenerative capacities and could potentially allow for more robust operations in a variety of situations and environments. The code for the experiments in this paper is available at: url{github.com/KazuyaHoribe/RegeneratingSoftRobots}.}, note={arXiv:2102.02579 [cs, q-bio]}, number={arXiv:2102.02579}, publisher={arXiv}, author={Horibe, Kazuya and Walker, Kathryn and Risi, Sebastian}, year={2021}, month=Feb }

 @article{Grattarola_Livi_Alippi_2021, title={Learning Graph Cellular Automata}, url={http://arxiv.org/abs/2110.14237}, abstractNote={Cellular automata (CA) are a class of computational models that exhibit rich dynamics emerging from the local interaction of cells arranged in a regular lattice. In this work we focus on a generalised version of typical CA, called graph cellular automata (GCA), in which the lattice structure is replaced by an arbitrary graph. In particular, we extend previous work that used convolutional neural networks to learn the transition rule of conventional CA and we use graph neural networks to learn a variety of transition rules for GCA. First, we present a general-purpose architecture for learning GCA, and we show that it can represent any arbitrary GCA with finite and discrete state space. Then, we test our approach on three different tasks: 1) learning the transition rule of a GCA on a Voronoi tessellation; 2) imitating the behaviour of a group of flocking agents; 3) learning a rule that converges to a desired target state.}, note={arXiv:2110.14237 [cs]}, number={arXiv:2110.14237}, publisher={arXiv}, author={Grattarola, Daniele and Livi, Lorenzo and Alippi, Cesare}, year={2021}, month=Oct }

 @article{Zhou_Cui_Hu_Zhang_Yang_Liu_Wang_Li_Sun_2021, title={Graph Neural Networks: A Review of Methods and Applications}, url={http://arxiv.org/abs/1812.08434}, abstractNote={Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.}, note={arXiv:1812.08434 [cs, stat]}, number={arXiv:1812.08434}, publisher={arXiv}, author={Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong}, year={2021}, month=Oct }

@inproceedings{NEURIPS2020_ee23e7ad,
 author = {Najarro, Elias and Risi, Sebastian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {20719--20731},
 publisher = {Curran Associates, Inc.},
 title = {Meta-Learning through Hebbian Plasticity in Random Networks},
 url = {https://proceedings.neurips.cc/paper/2020/file/ee23e7ad9b473ad072d57aaa9b2a5222-Paper.pdf},
 volume = {33},
 year = {2020}
}

 @inproceedings{Pedersen_Risi_2021, title={Evolving and Merging Hebbian Learning Rules: Increasing Generalization by Decreasing the Number of Rules}, url={http://arxiv.org/abs/2104.07959}, DOI={10.1145/3449639.3459317}, abstractNote={Generalization to out-of-distribution (OOD) circumstances after training remains a challenge for artificial agents. To improve the robustness displayed by plastic Hebbian neural networks, we evolve a set of Hebbian learning rules, where multiple connections are assigned to a single rule. Inspired by the biological phenomenon of the genomic bottleneck, we show that by allowing multiple connections in the network to share the same local learning rule, it is possible to drastically reduce the number of trainable parameters, while obtaining a more robust agent. During evolution, by iteratively using simple K-Means clustering to combine rules, our Evolve and Merge approach is able to reduce the number of trainable parameters from 61,440 to 1,920, while at the same time improving robustness, all without increasing the number of generations used. While optimization of the agents is done on a standard quadruped robot morphology, we evaluate the agents’ performances on slight morphology modifications in a total of 30 unseen morphologies. Our results add to the discussion on generalization, overfitting and OOD adaptation. To create agents that can adapt to a wider array of unexpected situations, Hebbian learning combined with a regularising “genomic bottleneck” could be a promising research direction.}, note={arXiv:2104.07959 [cs]}, booktitle={Proceedings of the Genetic and Evolutionary Computation Conference}, author={Pedersen, Joachim Winther and Risi, Sebastian}, year={2021}, month=Jun, pages={892–900} }


@inproceedings{kirsch2021meta,
title={Meta Learning Backpropagation And Improving It},
author={Louis Kirsch and J{\"u}rgen Schmidhuber},
booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
year={2021},
}

 @article{Variengien_Nichele_Glover_Pontes_Filho_2021, title={Towards self-organized control: Using neural cellular automata to robustly control a cart-pole agent}, url={http://arxiv.org/abs/2106.15240}, abstractNote={Neural cellular automata (Neural CA) are a recent framework used to model biological phenomena emerging from multicellular organisms. In these systems, artificial neural networks are used as update rules for cellular automata. Neural CA are end-to-end differentiable systems where the parameters of the neural network can be learned to achieve a particular task. In this work, we used neural CA to control a cart-pole agent. The observations of the environment are transmitted in input cells, while the values of output cells are used as a readout of the system. We trained the model using deep-Q learning, where the states of the output cells were used as the Q-value estimates to be optimized. We found that the computing abilities of the cellular automata were maintained over several hundreds of thousands of iterations, producing an emergent stable behavior in the environment it controls for thousands of steps. Moreover, the system demonstrated life-like phenomena such as a developmental phase, regeneration after damage, stability despite a noisy environment, and robustness to unseen disruption such as input deletion.}, note={arXiv:2106.15240 [cs]}, number={arXiv:2106.15240}, publisher={arXiv}, author={Variengien, Alexandre and Nichele, Stefano and Glover, Tom and Pontes-Filho, Sidney}, year={2021}, month=Jul }

 @article{Mnih_Kavukcuoglu_Silver_Graves_Antonoglou_Wierstra_Riedmiller_2013, title={Playing Atari with Deep Reinforcement Learning}, url={http://arxiv.org/abs/1312.5602}, abstractNote={We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.}, note={arXiv:1312.5602 [cs]}, number={arXiv:1312.5602}, publisher={arXiv}, author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin}, year={2013}, month=Dec }

@article{risi2021selfassemblingAI, 
title = "The Future of Artificial Intelligence is Self-Organizing and Self-Assembling", 
author = "Risi, Sebastian", 
journal = "sebastianrisi.com", 
year = "2021", 
url = "https://sebastianrisi.com/self_assembling_ai"
}

@ARTICLE{8259423,
  author={Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and McCoy, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},
  journal={IEEE Micro}, 
  title={Loihi: A Neuromorphic Manycore Processor with On-Chip Learning}, 
  year={2018},
  volume={38},
  number={1},
  pages={82-99},
  doi={10.1109/MM.2018.112130359}}

@article{masquelier2007unsupervised,
  title={Unsupervised learning of visual features through spike timing dependent plasticity},
  author={Masquelier, Timoth{\'e}e and Thorpe, Simon J},
  journal={PLoS computational biology},
  volume={3},
  number={2},
  pages={e31},
  year={2007},
  publisher={Public Library of Science San Francisco, USA}
}

@ARTICLE{6469239,
  author={Yu, Qiang and Tang, Huajin and Tan, Kay Chen and Li, Haizhou},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Rapid Feedforward Computation by Temporal Encoding and Learning With Spiking Neurons}, 
  year={2013},
  volume={24},
  number={10},
  pages={1539-1552},
  doi={10.1109/TNNLS.2013.2245677}}

@article{kheradpisheh2018stdp,
  title={STDP-based spiking deep convolutional neural networks for object recognition},
  author={Kheradpisheh, Saeed Reza and Ganjtabesh, Mohammad and Thorpe, Simon J and Masquelier, Timoth{\'e}e},
  journal={Neural Networks},
  volume={99},
  pages={56--67},
  year={2018},
  publisher={Elsevier}
}

@article{mozafari2019bio,
  title={Bio-inspired digit recognition using reward-modulated spike-timing-dependent plasticity in deep convolutional networks},
  author={Mozafari, Milad and Ganjtabesh, Mohammad and Nowzari-Dalini, Abbas and Thorpe, Simon J and Masquelier, Timoth{\'e}e},
  journal={Pattern recognition},
  volume={94},
  pages={87--95},
  year={2019},
  publisher={Elsevier}
}

@article{diehl2015unsupervised,
  title={Unsupervised learning of digit recognition using spike-timing-dependent plasticity},
  author={Diehl, Peter U and Cook, Matthew},
  journal={Frontiers in computational neuroscience},
  volume={9},
  pages={99},
  year={2015},
  publisher={Frontiers Media SA}
}

@article{cao2015spiking,
  title={Spiking deep convolutional neural networks for energy-efficient object recognition},
  author={Cao, Yongqiang and Chen, Yang and Khosla, Deepak},
  journal={International Journal of Computer Vision},
  volume={113},
  number={1},
  pages={54--66},
  year={2015},
  publisher={Springer}
}

@article{tavanaei2016bio,
  title={Bio-inspired spiking convolutional neural network using layer-wise sparse coding and STDP learning},
  author={Tavanaei, Amirhossein and Maida, Anthony S},
  journal={arXiv preprint arXiv:1611.03000},
  year={2016}
}

@article{ferre2018unsupervised,
  title={Unsupervised feature learning with winner-takes-all based stdp},
  author={Ferr{\'e}, Paul and Mamalet, Franck and Thorpe, Simon J},
  journal={Frontiers in computational neuroscience},
  volume={12},
  pages={24},
  year={2018},
  publisher={Frontiers Media SA}
}

@article{mozafari2018first,
  title={First-spike-based visual categorization using reward-modulated STDP},
  author={Mozafari, Milad and Kheradpisheh, Saeed Reza and Masquelier, Timoth{\'e}e and Nowzari-Dalini, Abbas and Ganjtabesh, Mohammad},
  journal={IEEE transactions on neural networks and learning systems},
  volume={29},
  number={12},
  pages={6178--6190},
  year={2018},
  publisher={IEEE}
}

@inproceedings{diehl2015fast,
  title={Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing},
  author={Diehl, Peter U and Neil, Daniel and Binas, Jonathan and Cook, Matthew and Liu, Shih-Chii and Pfeiffer, Michael},
  booktitle={2015 International joint conference on neural networks (IJCNN)},
  pages={1--8},
  year={2015},
  organization={ieee}
}

@article{zenke2018superspike,
  title={Superspike: Supervised learning in multilayer spiking neural networks},
  author={Zenke, Friedemann and Ganguli, Surya},
  journal={Neural computation},
  volume={30},
  number={6},
  pages={1514--1541},
  year={2018},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}


 @article{Raghavan_Lin_Thomson_2020, title={Self-organization of multi-layer spiking neural networks}, url={http://arxiv.org/abs/2006.06902}, abstractNote={Living neural networks in our brains autonomously self-organize into large, complex architectures during early development to result in an organized and functional organic computational device. A key mechanism that enables the formation of complex architecture in the developing brain is the emergence of traveling spatio-temporal waves of neuronal activity across the growing brain. Inspired by this strategy, we attempt to efficiently self-organize large neural networks with an arbitrary number of layers into a wide variety of architectures. To achieve this, we propose a modular tool-kit in the form of a dynamical system that can be seamlessly stacked to assemble multi-layer neural networks. The dynamical system encapsulates the dynamics of spiking units, their inter/intra layer interactions as well as the plasticity rules that control the flow of information between layers. The key features of our tool-kit are (1) autonomous spatio-temporal waves across multiple layers triggered by activity in the preceding layer and (2) Spike-timing dependent plasticity (STDP) learning rules that update the inter-layer connectivity based on wave activity in the connecting layers. Our framework leads to the self-organization of a wide variety of architectures, ranging from multi-layer perceptrons to autoencoders. We also demonstrate that emergent waves can self-organize spiking network architecture to perform unsupervised learning, and networks can be coupled with a linear classifier to perform classification on classic image datasets like MNIST. Broadly, our work shows that a dynamical systems framework for learning can be used to self-organize large computational devices.}, note={arXiv:2006.06902 [cs, q-bio]}, number={arXiv:2006.06902}, publisher={arXiv}, author={Raghavan, Guruprasad and Lin, Cong and Thomson, Matt}, year={2020}, month=Jun }

 @article{Vaila_Chiasson_Saxena_2019, title={Deep Convolutional Spiking Neural Networks for Image Classification}, url={http://arxiv.org/abs/1903.12272}, abstractNote={Spiking neural networks are biologically plausible counterparts of the artificial neural networks, artificial neural networks are usually trained with stochastic gradient descent and spiking neural networks are trained with spike timing dependant plasticity. Training deep convolutional neural networks is a memory and power intensive job. Spiking networks could potentially help in reducing the power usage. There is a large pool of tools for one to chose to train artificial neural networks of any size, on the other hand all the available tools to simulate spiking neural networks are geared towards computational neuroscience applications and they are not suitable for real life applications. In this work we focus on implementing a spiking CNN using Tensorflow to examine behaviour of the network and empirically study the effect of various parameters on learning capabilities and also study catastrophic forgetting in the spiking CNN and weight initialization problem in R-STDP using MNIST and N-MNIST data sets.}, note={arXiv:1903.12272 [cs]}, number={arXiv:1903.12272}, publisher={arXiv}, author={Vaila, Ruthvik and Chiasson, John and Saxena, Vishal}, year={2019}, month=Sep }

@inproceedings{Raghavan2019NeuralNG,
  title={Neural networks grown and self-organized by noise},
  author={Guruprasad Raghavan and Matt Thomson},
  booktitle={NeurIPS},
  year={2019}
}

@article{Gatys_Ecker_Bethge_2015, title={A Neural Algorithm of Artistic Style}, url={http://arxiv.org/abs/1508.06576}, abstractNote={In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.}, note={arXiv:1508.06576 [cs, q-bio]}, number={arXiv:1508.06576}, publisher={arXiv}, author={Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias}, year={2015}, month=Sep }

@article{Gatys_Ecker_Bethge_20152, title={Texture Synthesis Using Convolutional Neural Networks}, url={http://arxiv.org/abs/1505.07376}, abstractNote={Here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition. Samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion. Within the model, textures are represented by the correlations between feature maps in several layers of the network. We show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit. The model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks.}, note={arXiv:1505.07376 [cs, q-bio]}, number={arXiv:1505.07376}, publisher={arXiv}, author={Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias}, year={2015}, month=Nov }

@INPROCEEDINGS{7780634,
  author={Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Image Style Transfer Using Convolutional Neural Networks}, 
  year={2016},
  volume={},
  number={},
  pages={2414-2423},
  doi={10.1109/CVPR.2016.265}}

@inproceedings{10555531720773172198, author = {Li, Yanghao and Wang, Naiyan and Liu, Jiaying and Hou, Xiaodi}, title = {Demystifying Neural Style Transfer}, year = {2017}, isbn = {9780999241103}, publisher = {AAAI Press}, abstract = {Neural Style Transfer [Gatys et al. , 2016] has recently demonstrated very exciting results which catches eyes in both academia and industry. Despite the amazing results, the principle of neural style transfer, especially why the Gram matrices could represent style remains unclear. In this paper, we propose a novel interpretation of neural style transfer by treating it as a domain adaptation problem. Specifically, we theoretically show that matching the Gram matrices of feature maps is equivalent to minimize the Maximum Mean Discrepancy (MMD) with the second order polynomial kernel. Thus, we argue that the essence of neural style transfer is to match the feature distributions between the style images and the generated images. To further support our standpoint, we experiment with several other distribution alignment methods, and achieve appealing results. We believe this novel interpretation connects these two important research fields, and could enlighten future researches.}, booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence}, pages = {2230–2236}, numpages = {7}, location = {Melbourne, Australia}, series = {IJCAI'17} }

@article{JMLRv13gretton12a,
  author  = {Arthur Gretton and Karsten M. Borgwardt and Malte J. Rasch and Bernhard Sch{{\"o}}lkopf and Alexander Smola},
  title   = {A Kernel Two-Sample Test},
  journal = {Journal of Machine Learning Research},
  year    = {2012},
  volume  = {13},
  number  = {25},
  pages   = {723-773},
  url     = {http://jmlr.org/papers/v13/gretton12a.html}
}

 @article{Kohonen_1982, title={Self-organized formation of topologically correct feature maps}, volume={43}, ISSN={0340-1200, 1432-0770}, DOI={10.1007/BF00337288}, number={1}, journal={Biological Cybernetics}, author={Kohonen, Teuvo}, year={1982}, pages={59–69}, language={en} }

 @book{Kohonen_1989, address={Berlin, Heidelberg}, edition={Third edition}, title={Self-Organization and Associative Memory}, ISBN={978-3-642-88163-3}, abstractNote={This monograph gives a tutorial treatment of new approaches to self-organization, adaptation, learning and memory. It is based on recent research results, both mathematical and computer simulations, and lends itself to graduate and postgraduate courses in the natural sciences. The book presents new formalisms of pattern processing: orthogonal projectors, optimal associative mappings, novelty filters, subspace methods, feature-sensitive units, and self-organization of topological maps, with all their computable algorithms. The main objective is to provide an understanding of the properties of information representations from a general point of view and of their use in pattern information processing, as well as an understanding of many functions of the brain. In the third edition two new discussions have been added and a proof has been revised. The author has developed this book from Associative Memory - A System-Theoretical Approach (Volume 17 of Springer Series in Communication and Cybernetics, 1977), the first ever monograph on distributed associative memories}, publisher={Springer Berlin Heidelberg}, author={Kohonen, Teuvo}, year={1989}, language={eng} }

 @article{Marsland_Shapiro_Nehmzow_2002, title={A self-organising network that grows when required}, volume={15}, ISSN={08936080}, DOI={10.1016/S0893-6080(02)00078-3}, number={8–9}, journal={Neural Networks}, author={Marsland, Stephen and Shapiro, Jonathan and Nehmzow, Ulrich}, year={2002}, month={Oct}, pages={1041–1058}, language={en} }

@inproceedings{NIPS1994_d56b9fc4,
 author = {Fritzke, Bernd},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 pages = {},
 publisher = {MIT Press},
 title = {A Growing Neural Gas Network Learns Topologies},
 url = {https://proceedings.neurips.cc/paper/1994/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf},
 volume = {7},
 year = {1994}
}

 @article{Reilly_Cooper_Elbaum_1982, title={A neural model for category learning}, volume={45}, ISSN={0340-1200, 1432-0770}, DOI={10.1007/BF00387211}, number={1}, journal={Biological Cybernetics}, author={Reilly, Douglas L. and Cooper, Leon N. and Elbaum, Charles}, year={1982}, month={Aug}, pages={35–41}, language={en} }

 @article{Fritzke_1994, title={Growing cell structures—A self-organizing network for unsupervised and supervised learning}, volume={7}, ISSN={08936080}, DOI={10.1016/0893-6080(94)90091-4}, number={9}, journal={Neural Networks}, author={Fritzke, Bernd}, year={1994}, month={Jan}, pages={1441–1460}, language={en} }

 @article{Mici_Parisi_Wermter_2018, title={A self-organizing neural network architecture for learning human-object interactions}, volume={307}, ISSN={09252312}, DOI={10.1016/j.neucom.2018.04.015}, journal={Neurocomputing}, author={Mici, Luiza and Parisi, German I. and Wermter, Stefan}, year={2018}, month={Sep}, pages={14–24}, language={en} }

@online{LeCun_AMI,
  author = {LeCun, Yann},
  title = {A Path Towards Autonomous Machine Intelligence},
  year = {2022},
  url = {https://openreview.net/forum?id=BZ5a1r-kVsf},
  urldate = {2022-09-21}
}

 @article{Keurti_Pan_Besserve_Grewe_Schölkopf_2022, title={Homomorphism Autoencoder -- Learning Group Structured Representations from Observed Transitions}, url={http://arxiv.org/abs/2207.12067}, abstractNote={How can we acquire world models that veridically represent the outside world both in terms of what is there and in terms of how our actions affect it? Can we acquire such models by interacting with the world, and can we state mathematical desiderata for their relationship with a hypothetical reality existing outside our heads? As machine learning is moving towards representations containing not just observational but also interventional knowledge, we study these problems using tools from representation learning and group theory. Under the assumption that our actuators act upon the world, we propose methods to learn internal representations of not just sensory information but also of actions that modify our sensory representations in a way that is consistent with the actions and transitions in the world. We use an autoencoder equipped with a group representation linearly acting on its latent space, trained on 2-step reconstruction such as to enforce a suitable homomorphism property on the group representation. Compared to existing work, our approach makes fewer assumptions on the group representation and on which transformations the agent can sample from the group. We motivate our method theoretically, and demonstrate empirically that it can learn the correct representation of the groups and the topology of the environment. We also compare its performance in trajectory prediction with previous methods.}, note={arXiv:2207.12067 [cs, math, stat]}, number={arXiv:2207.12067}, publisher={arXiv}, author={Keurti, Hamza and Pan, Hsiao-Ru and Besserve, Michel and Grewe, Benjamin F. and Schölkopf, Bernhard}, year={2022}, month={Jul} }

 @article{Keller_Bonhoeffer_Hübener_2012, title={Sensorimotor Mismatch Signals in Primary Visual Cortex of the Behaving Mouse}, volume={74}, ISSN={08966273}, DOI={10.1016/j.neuron.2012.03.040}, number={5}, journal={Neuron}, author={Keller, Georg B. and Bonhoeffer, Tobias and Hübener, Mark}, year={2012}, month={Jun}, pages={809–815}, language={en} }

 @article{Dosovitskiy_Beyer_Kolesnikov_Weissenborn_Zhai_Unterthiner_Dehghani_Minderer_Heigold_Gelly_2021, title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, url={http://arxiv.org/abs/2010.11929}, abstractNote={While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.}, note={arXiv:2010.11929 [cs]}, number={arXiv:2010.11929}, publisher={arXiv}, author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil}, year={2021}, month={Jun} }



 @article{Sager_Salzmann_Burn_Stadelmann_2022, title={Unsupervised Domain Adaptation for Vertebrae Detection and Identification in 3D CT Volumes Using a Domain Sanity Loss}, volume={8}, rights={All rights reserved}, ISSN={2313-433X}, DOI={10.3390/jimaging8080222}, abstractNote={A variety of medical computer vision applications analyze 2D slices of computed tomography (CT) scans, whereas axial slices from the body trunk region are usually identified based on their relative position to the spine. A limitation of such systems is that either the correct slices must be extracted manually or labels of the vertebrae are required for each CT scan to develop an automated extraction system. In this paper, we propose an unsupervised domain adaptation (UDA) approach for vertebrae detection and identification based on a novel Domain Sanity Loss (DSL) function. With UDA the model’s knowledge learned on a publicly available (source) data set can be transferred to the target domain without using target labels, where the target domain is defined by the specific setup (CT modality, study protocols, applied pre- and processing) at the point of use (e.g., a specific clinic with its specific CT study protocols). With our approach, a model is trained on the source and target data set in parallel. The model optimizes a supervised loss for labeled samples from the source domain and the DSL loss function based on domain-specific “sanity checks” for samples from the unlabeled target domain. Without using labels from the target domain, we are able to identify vertebra centroids with an accuracy of 72.8%. By adding only ten target labels during training the accuracy increases to 89.2%, which is on par with the current state-of-the-art for full supervised learning, while using about 20 times less labels. Thus, our model can be used to extract 2D slices from 3D CT scans on arbitrary data sets fully automatically without requiring an extensive labeling effort, contributing to the clinical adoption of medical imaging by hospitals.}, number={8}, journal={Journal of Imaging}, author={Sager, Pascal and Salzmann, Sebastian and Burn, Felice and Stadelmann, Thilo}, year={2022}, month={Aug}, pages={222}, language={en} }

 @article{Ronneberger_Fischer_Brox_2015, title={U-Net: Convolutional Networks for Biomedical Image Segmentation}, url={http://arxiv.org/abs/1505.04597}, abstractNote={There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .}, note={arXiv:1505.04597 [cs]}, number={arXiv:1505.04597}, publisher={arXiv}, author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas}, year={2015}, month={May} }

 @article{Jafari_Khouzani_Soltanian_Zadeh_2005, title={Radon transform orientation estimation for rotation invariant texture analysis}, volume={27}, ISSN={0162-8828, 2160-9292}, DOI={10.1109/TPAMI.2005.126}, number={6}, journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, author={Jafari-Khouzani, K. and Soltanian-Zadeh, H.}, year={2005}, month={Jun}, pages={1004–1008} }

 @article{Ojala_Pietikainen_Maenpaa_2002, title={Multiresolution gray-scale and rotation invariant texture classification with local binary patterns}, volume={24}, ISSN={0162-8828}, DOI={10.1109/TPAMI.2002.1017623}, number={7}, journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, author={Ojala, T. and Pietikainen, M. and Maenpaa, T.}, year={2002}, month={Jul}, pages={971–987}, language={en} }

 @article{Wen_RongWu_ShiehChungWei_1996, title={Rotation and gray-scale transform-invariant texture classification using spiral resampling, subband decomposition, and hidden Markov model}, volume={5}, ISSN={1057-7149, 1941-0042}, DOI={10.1109/83.536891}, number={10}, journal={IEEE Transactions on Image Processing}, author={Wen-Rong Wu and Shieh-Chung Wei}, year={1996}, month={Oct}, pages={1423–1434} }

@inproceedings{greenspan1994rotation,
  title={Rotation invariant texture recognition using a steerable pyramid},
  author={Greenspan, Hayit and Belongie, S and Goodman, R\_ and Perona, Pietro},
  booktitle={Proceedings of the 12th IAPR International Conference on Pattern Recognition, Vol. 3-Conference C: Signal Processing (Cat. No. 94CH3440-5)},
  volume={2},
  pages={162--167},
  year={1994},
  organization={IEEE}
}

 @inproceedings{Simard_Steinkraus_Platt_2003, address={Edinburgh, UK}, title={Best practices for convolutional neural networks applied to visual document analysis}, volume={1}, ISBN={978-0-7695-1960-9}, url={http://ieeexplore.ieee.org/document/1227801/}, DOI={10.1109/ICDAR.2003.1227801}, booktitle={Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings.}, publisher={IEEE Comput. Soc}, author={Simard, P.Y. and Steinkraus, D. and Platt, J.C.}, year={2003}, pages={958–963} }

 @inproceedings{Fasel_Gatica_Perez_2006, address={Hong Kong, China}, title={Rotation-Invariant Neoperceptron}, ISBN={978-0-7695-2521-1}, url={http://ieeexplore.ieee.org/document/1699534/}, DOI={10.1109/ICPR.2006.1020}, booktitle={18th International Conference on Pattern Recognition (ICPR’06)}, publisher={IEEE}, author={Fasel, B. and Gatica-Perez, D.}, year={2006}, pages={336–339} }

 @article{Dieleman_DeFauw_Kavukcuoglu_2016, title={Exploiting Cyclic Symmetry in Convolutional Neural Networks}, url={http://arxiv.org/abs/1602.02660}, abstractNote={Many classes of images exhibit rotational symmetry. Convolutional neural networks are sometimes trained using data augmentation to exploit this, but they are still required to learn the rotation equivariance properties from the data. Encoding these properties into the network architecture, as we are already used to doing for translation equivariance by using convolutional layers, could result in a more efficient use of the parameter budget by relieving the model from learning them. We introduce four operations which can be inserted into neural network models as layers, and which can be combined to make these models partially equivariant to rotations. They also enable parameter sharing across different orientations. We evaluate the effect of these architectural modifications on three datasets which exhibit rotational symmetry and demonstrate improved performance with smaller models.}, note={arXiv:1602.02660 [cs]}, number={arXiv:1602.02660}, publisher={arXiv}, author={Dieleman, Sander and De Fauw, Jeffrey and Kavukcuoglu, Koray}, year={2016}, month={May} }

 @article{Laptev_Savinov_Buhmann_Pollefeys_2016, title={TI-POOLING: transformation-invariant pooling for feature learning in Convolutional Neural Networks}, url={http://arxiv.org/abs/1604.06318}, abstractNote={In this paper we present a deep neural network topology that incorporates a simple to implement transformation invariant pooling operator (TI-POOLING). This operator is able to efficiently handle prior knowledge on nuisance variations in the data, such as rotation or scale changes. Most current methods usually make use of dataset augmentation to address this issue, but this requires larger number of model parameters and more training data, and results in significantly increased training time and larger chance of under- or overfitting. The main reason for these drawbacks is that the learned model needs to capture adequate features for all the possible transformations of the input. On the other hand, we formulate features in convolutional neural networks to be transformation-invariant. We achieve that using parallel siamese architectures for the considered transformation set and applying the TI-POOLING operator on their outputs before the fully-connected layers. We show that this topology internally finds the most optimal “canonical” instance of the input image for training and therefore limits the redundancy in learned features. This more efficient use of training data results in better performance on popular benchmark datasets with smaller number of parameters when comparing to standard convolutional neural networks with dataset augmentation and to other baselines.}, note={arXiv:1604.06318 [cs]}, number={arXiv:1604.06318}, publisher={arXiv}, author={Laptev, Dmitry and Savinov, Nikolay and Buhmann, Joachim M. and Pollefeys, Marc}, year={2016}, month={Sep} }

 @inproceedings{Schmidt_Roth_2012, address={Providence, RI}, title={Learning rotation-aware features: From invariant priors to equivariant descriptors}, ISBN={978-1-4673-1228-8}, url={http://ieeexplore.ieee.org/document/6247909/}, DOI={10.1109/CVPR.2012.6247909}, booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, publisher={IEEE}, author={Schmidt, U. and Roth, S.}, year={2012}, month={Jun}, pages={2050–2057} }

 @inbook{Kivinen_Williams_2011, address={Berlin, Heidelberg}, series={Lecture Notes in Computer Science}, title={Transformation Equivariant Boltzmann Machines}, volume={6791}, ISBN={978-3-642-21734-0}, url={http://link.springer.com/10.1007/978-3-642-21735-7_1}, DOI={10.1007/978-3-642-21735-7_1}, booktitle={Artificial Neural Networks and Machine Learning – ICANN 2011}, publisher={Springer Berlin Heidelberg}, author={Kivinen, Jyri J. and Williams, Christopher K. I.}, editor={Honkela, Timo and Duch, Włodzisław and Girolami, Mark and Kaski, Samuel}, year={2011}, pages={1–9}, collection={Lecture Notes in Computer Science} }

 @article{Sohn_Lee_2012, title={Learning Invariant Representations with Local Transformations}, url={http://arxiv.org/abs/1206.6418}, abstractNote={Learning invariant representations is an important problem in machine learning and pattern recognition. In this paper, we present a novel framework of transformation-invariant feature learning by incorporating linear transformations into the feature learning algorithms. For example, we present the transformation-invariant restricted Boltzmann machine that compactly represents data by its weights and their transformations, which achieves invariance of the feature representation via probabilistic max pooling. In addition, we show that our transformation-invariant feature learning framework can also be extended to other unsupervised learning methods, such as autoencoders or sparse coding. We evaluate our method on several image classification benchmark datasets, such as MNIST variations, CIFAR-10, and STL-10, and show competitive or superior classification performance when compared to the state-of-the-art. Furthermore, our method achieves state-of-the-art performance on phone classification tasks with the TIMIT dataset, which demonstrates wide applicability of our proposed algorithms to other domains.}, note={arXiv:1206.6418 [cs, stat]}, number={arXiv:1206.6418}, publisher={arXiv}, author={Sohn, Kihyuk and Lee, Honglak}, year={2012}, month={Jun} }

 @inproceedings{Lee_Grosse_Ranganath_Ng_2009, address={Montreal, Quebec, Canada}, title={Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations}, ISBN={978-1-60558-516-1}, url={http://portal.acm.org/citation.cfm?doid=1553374.1553453}, DOI={10.1145/1553374.1553453}, booktitle={Proceedings of the 26th Annual International Conference on Machine Learning - ICML ’09}, publisher={ACM Press}, author={Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y.}, year={2009}, pages={1–8}, language={en} }

 @article{Teney_Hebert_2016, title={Learning to Extract Motion from Videos in Convolutional Neural Networks}, url={http://arxiv.org/abs/1601.07532}, abstractNote={This paper shows how to extract dense optical flow from videos with a convolutional neural network (CNN). The proposed model constitutes a potential building block for deeper architectures to allow using motion without resorting to an external algorithm, eg for recognition in videos. We derive our network architecture from signal processing principles to provide desired invariances to image contrast, phase and texture. We constrain weights within the network to enforce strict rotation invariance and substantially reduce the number of parameters to learn. We demonstrate end-to-end training on only 8 sequences of the Middlebury dataset, orders of magnitude less than competing CNN-based motion estimation methods, and obtain comparable performance to classical methods on the Middlebury benchmark. Importantly, our method outputs a distributed representation of motion that allows representing multiple, transparent motions, and dynamic textures. Our contributions on network design and rotation invariance offer insights nonspecific to motion estimation.}, note={arXiv:1601.07532 [cs]}, number={arXiv:1601.07532}, publisher={arXiv}, author={Teney, Damien and Hebert, Martial}, year={2016}, month={Jan} }

 @article{Wu_Hu_Kong_2015, title={Flip-Rotate-Pooling Convolution and Split Dropout on Convolution Neural Networks for Image Classification}, url={http://arxiv.org/abs/1507.08754}, abstractNote={This paper presents a new version of Dropout called Split Dropout (sDropout) and rotational convolution techniques to improve CNNs’ performance on image classification. The widely used standard Dropout has advantage of preventing deep neural networks from overfitting by randomly dropping units during training. Our sDropout randomly splits the data into two subsets and keeps both rather than discards one subset. We also introduce two rotational convolution techniques, i.e. rotate-pooling convolution (RPC) and flip-rotate-pooling convolution (FRPC) to boost CNNs’ performance on the robustness for rotation transformation. These two techniques encode rotation invariance into the network without adding extra parameters. Experimental evaluations on ImageNet2012 classification task demonstrate that sDropout not only enhances the performance but also converges faster. Additionally, RPC and FRPC make CNNs more robust for rotation transformations. Overall, FRPC together with sDropout bring $1.18%$ (model of Zeiler and Fergus~cite{zeiler2013visualizing}, 10-view, top-1) accuracy increase in ImageNet 2012 classification task compared to the original network.}, note={arXiv:1507.08754 [cs]}, number={arXiv:1507.08754}, publisher={arXiv}, author={Wu, Fa and Hu, Peijun and Kong, Dexing}, year={2015}, month={Jul} }

 @book{Costandi_2016, address={Cambridge, MA}, series={The MIT Press essential knowledge series}, title={Neuroplasticity}, ISBN={978-0-262-52933-4}, callNumber={QP364.5 .C67 2016}, publisher={The MIT Press}, author={Costandi, Moheb}, year={2016}, collection={The MIT Press essential knowledge series} }

 @article{Takagi_2000, title={Roles of ion channels in EPSP integration at neuronal dendrites}, volume={37}, ISSN={01680102}, DOI={10.1016/S0168-0102(00)00120-6}, number={3}, journal={Neuroscience Research}, author={Takagi, Hiroshi}, year={2000}, month={Jul}, pages={167–171}, language={en} }

 @article{Coombs_Eccles_Fatt_1955, title={The specific ionic conductances and the ionic movements across the motoneuronal membrane that produce the inhibitory post-synaptic potential}, volume={130}, ISSN={00223751}, DOI={10.1113/jphysiol.1955.sp005412}, number={2}, journal={The Journal of Physiology}, author={Coombs, J. S. and Eccles, J. C. and Fatt, P.}, year={1955}, month={Nov}, pages={326–373}, language={en} }

 @inbook{Niv_Joel_Meilijson_Ruppin_2001, address={Berlin, Heidelberg}, series={Lecture Notes in Computer Science}, title={Evolution of Reinforcement Learning in Uncertain Environments: Emergence of Risk-Aversion and Matching}, volume={2159}, ISBN={978-3-540-42567-0}, url={http://link.springer.com/10.1007/3-540-44811-X_27}, DOI={10.1007/3-540-44811-X_27}, booktitle={Advances in Artificial Life}, publisher={Springer Berlin Heidelberg}, author={Niv, Yael and Joel, Daphna and Meilijson, Isaac and Ruppin, Eytan}, editor={Kelemen, Jozef and Sosík, Petr}, year={2001}, pages={252–261}, collection={Lecture Notes in Computer Science} }

 @article{Salimans_Ho_Chen_Sidor_Sutskever_2017, title={Evolution Strategies as a Scalable Alternative to Reinforcement Learning}, url={http://arxiv.org/abs/1703.03864}, abstractNote={We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.}, note={arXiv:1703.03864 [cs, stat]}, number={arXiv:1703.03864}, publisher={arXiv}, author={Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya}, year={2017}, month={Sep} }

@online{MNIST,
  author = {LeCun, Yann and Cortes, Corinna},
  title = {THE MNIST DATABASE of handwritten digits},
  year = {1996},
  url = {http://yann.lecun.com/exdb/mnist/},
  urldate = {2022-09-05}
}

 @article{Mu_Gilmer_2019, title={MNIST-C: A Robustness Benchmark for Computer Vision}, url={http://arxiv.org/abs/1906.02337}, abstractNote={We introduce the MNIST-C dataset, a comprehensive suite of 15 corruptions applied to the MNIST test set, for benchmarking out-of-distribution robustness in computer vision. Through several experiments and visualizations we demonstrate that our corruptions significantly degrade performance of state-of-the-art computer vision models while preserving the semantic content of the test images. In contrast to the popular notion of adversarial robustness, our model-agnostic corruptions do not seek worst-case performance but are instead designed to be broad and diverse, capturing multiple failure modes of modern models. In fact, we find that several previously published adversarial defenses significantly degrade robustness as measured by MNIST-C. We hope that our benchmark serves as a useful tool for future work in designing systems that are able to learn robust feature representations that capture the underlying semantics of the input.}, note={arXiv:1906.02337 [cs]}, number={arXiv:1906.02337}, publisher={arXiv}, author={Mu, Norman and Gilmer, Justin}, year={2019}, month={Jun} }

 @article{Kingma_Ba_2017, title={Adam: A Method for Stochastic Optimization}, url={http://arxiv.org/abs/1412.6980}, abstractNote={We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.}, note={arXiv: 1412.6980}, journal={arXiv:1412.6980 [cs]}, author={Kingma, Diederik P. and Ba, Jimmy}, year={2017}, month={Jan} }

 @article{Liang_Glossner_Wang_Shi_Zhang_2021, title={Pruning and Quantization for Deep Neural Network Acceleration: A Survey}, url={http://arxiv.org/abs/2101.09671}, abstractNote={Deep neural networks have been applied in many applications exhibiting extraordinary abilities in the field of computer vision. However, complex network architectures challenge efficient real-time deployment and require significant computation resources and energy costs. These challenges can be overcome through optimizations such as network compression. Network compression can often be realized with little loss of accuracy. In some cases accuracy may even improve. This paper provides a survey on two types of network compression: pruning and quantization. Pruning can be categorized as static if it is performed offline or dynamic if it is performed at run-time. We compare pruning techniques and describe criteria used to remove redundant computations. We discuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise, layer-wise and even network-wise pruning. Quantization reduces computations by reducing the precision of the datatype. Weights, biases, and activations may be quantized typically to 8-bit integers although lower bit width implementations are also discussed including binary neural networks. Both pruning and quantization can be used independently or combined. We compare current techniques, analyze their strengths and weaknesses, present compressed network accuracy results on a number of frameworks, and provide practical guidance for compressing networks.}, note={arXiv:2101.09671 [cs]}, number={arXiv:2101.09671}, publisher={arXiv}, author={Liang, Tailin and Glossner, John and Wang, Lei and Shi, Shaobo and Zhang, Xiaotong}, year={2021}, month={Jun} }
 
 @article{Bartunov_Santoro_Richards_Marris_Hinton_Lillicrap_2018, title={Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures}, url={http://arxiv.org/abs/1807.04587}, abstractNote={The backpropagation of error algorithm (BP) is impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired proposals for understanding how the brain might learn across multiple layers, and hence how it might approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST, CIFAR-10, and ImageNet datasets and explore variants of target-propagation (TP) and feedback alignment (FA) algorithms, and explore performance in both fully- and locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP, especially for networks composed of locally connected units, opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.}, note={arXiv:1807.04587 [cs, stat]}, number={arXiv:1807.04587}, publisher={arXiv}, author={Bartunov, Sergey and Santoro, Adam and Richards, Blake A. and Marris, Luke and Hinton, Geoffrey E. and Lillicrap, Timothy}, year={2018}, month={Nov} }

 @article{Crick_1989, title={The recent excitement about neural networks}, volume={337}, ISSN={0028-0836, 1476-4687}, DOI={10.1038/337129a0}, number={6203}, journal={Nature}, author={Crick, Francis}, year={1989}, month={Jan}, pages={129–132}, language={en} }

 @article{Grossberg_1987, title={Competitive Learning: From Interactive Activation to Adaptive Resonance}, volume={11}, ISSN={03640213}, DOI={10.1111/j.1551-6708.1987.tb00862.x}, number={1}, journal={Cognitive Science}, author={Grossberg, Stephen}, year={1987}, month={Jan}, pages={23–63}, language={en} }

 @article{Lillicrap_Cownden_Tweed_Akerman_2014, title={Random feedback weights support learning in deep neural networks}, url={http://arxiv.org/abs/1411.0247}, abstractNote={The brain processes information through many layers of neurons. This deep architecture is representationally powerful, but it complicates learning by making it hard to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame to a neuron by computing exactly how it contributed to an error. To do this, it multiplies error signals by matrices consisting of all the synaptic weights on the neuron’s axon and farther downstream. This operation requires a precisely choreographed transport of synaptic weight information, which is thought to be impossible in the brain. Here we present a surprisingly simple algorithm for deep learning, which assigns blame by multiplying error signals by random synaptic weights. We show that a network can learn to extract useful information from signals sent through these random feedback connections. In essence, the network learns to learn. We demonstrate that this new mechanism performs as quickly and accurately as backpropagation on a variety of problems and describe the principles which underlie its function. Our demonstration provides a plausible basis for how a neuron can be adapted using error signals generated at distal locations in the brain, and thus dispels long-held assumptions about the algorithmic constraints on learning in neural circuits.}, note={arXiv:1411.0247 [cs, q-bio]}, number={arXiv:1411.0247}, publisher={arXiv}, author={Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.}, year={2014}, month={Nov} }

@article{cifar_10,
title= {CIFAR-10 (Canadian Institute for Advanced Research)},
journal= {},
author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
year= {},
url= {http://www.cs.toronto.edu/~kriz/cifar.html},
abstract= {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 
The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. },
keywords= {Dataset},
terms= {}
}

@inproceedings{deng2009imagenet, 
  title={Imagenet: A large-scale hierarchical image database}, 
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition}, 
  pages={248--255}, 
  year={2009}, 
  organization={Ieee} 
}

 @inbook{Movellan_1991, title={Contrastive Hebbian Learning in the Continuous Hopfield Model}, ISBN={978-1-4832-1448-1}, url={https://linkinghub.elsevier.com/retrieve/pii/B978148321448150007X}, DOI={10.1016/B978-1-4832-1448-1.50007-X}, booktitle={Connectionist Models}, publisher={Elsevier}, author={Movellan, Javier R.}, year={1991}, pages={10–17}, language={en} }
 
  @article{O_Reilly_1996, title={Biologically Plausible Error-Driven Learning Using Local Activation Differences: The Generalized Recirculation Algorithm}, volume={8}, ISSN={0899-7667, 1530-888X}, DOI={10.1162/neco.1996.8.5.895}, abstractNote={The error backpropagation learning algorithm (BP) is generally considered biologically implausible because it does not use locally available, activation-based variables. A version of BP that can be computed locally using bidirectional activation recirculation (Hinton and McClelland 1988) instead of backpropagated error derivatives is more biologically plausible. This paper presents a generalized version of the recirculation algorithm (GeneRec), which overcomes several limitations of the earlier algorithm by using a generic recurrent network with sigmoidal units that can learn arbitrary input/output mappings. However, the contrastive Hebbian learning algorithm (CHL, also known as DBM or mean field learning) also uses local variables to perform error-driven learning in a sigmoidal recurrent network. CHL was derived in a stochastic framework (the Boltzmann machine), but has been extended to the deterministic case in various ways, all of which rely on problematic approximations and assumptions, leading some to conclude that it is fundamentally flawed. This paper shows that CHL can be derived instead from within the BP framework via the GeneRec algorithm. CHL is a symmetry-preserving version of GeneRec that uses a simple approximation to the midpoint or second-order accurate Runge-Kutta method of numerical integration, which explains the generally faster learning speed of CHL compared to BI. Thus, all known fully general error-driven learning algorithms that use local activation-based variables in deterministic networks can be considered variations of the GeneRec algorithm (and indirectly, of the backpropagation algorithm). GeneRec therefore provides a promising framework for thinking about how the brain might perform error-driven learning. To further this goal, an explicit biological mechanism is proposed that would be capable of implementing GeneRec-style learning. This mechanism is consistent with available evidence regarding synaptic modification in neurons in the neocortex and hippocampus, and makes further predictions.}, number={5}, journal={Neural Computation}, author={O’Reilly, Randall C.}, year={1996}, month={Jul}, pages={895–938}, language={en} }

 @inbook{Le__Cun_1986, address={Berlin, Heidelberg}, title={Learning Process in an Asymmetric Threshold Network}, ISBN={978-3-642-82659-7}, url={http://link.springer.com/10.1007/978-3-642-82657-3_24}, DOI={10.1007/978-3-642-82657-3_24}, booktitle={Disordered Systems and Biological Organization}, publisher={Springer Berlin Heidelberg}, author={Le Cun, Yann}, editor={Bienenstock, E. and Soulié, F. Fogelman and Weisbuch, G.}, year={1986}, pages={233–240}, language={en} }

@inproceedings{hinton2007backpropagation,
  title={How to do backpropagation in a brain},
  author={Hinton, Geoffrey and others},
  booktitle={Invited talk at the NIPS’2007 deep learning workshop},
  volume={656},
  pages={1--16},
  year={2007}
}

 @article{Lee_Zhang_Fischer_Bengio_2015, title={Difference Target Propagation}, url={http://arxiv.org/abs/1412.7525}, abstractNote={Back-propagation has been the workhorse of recent successes of deep learning but it relies on infinitesimal effects (partial derivatives) in order to perform credit assignment. This could become a serious issue as one considers deeper and more non-linear functions, e.g., consider the extreme case of nonlinearity where the relation between parameters and cost is actually discrete. Inspired by the biological implausibility of back-propagation, a few approaches have been proposed in the past that could play a similar credit assignment role. In this spirit, we explore a novel approach to credit assignment in deep networks that we call target propagation. The main idea is to compute targets rather than gradients, at each layer. Like gradients, they are propagated backwards. In a way that is related but different from previously proposed proxies for back-propagation which rely on a backwards network with symmetric weights, target propagation relies on auto-encoders at each layer. Unlike back-propagation, it can be applied even when units exchange stochastic bits rather than real numbers. We show that a linear correction for the imperfectness of the auto-encoders, called difference target propagation, is very effective to make target propagation actually work, leading to results comparable to back-propagation for deep networks with discrete and continuous units and denoising auto-encoders and achieving state of the art for stochastic networks.}, note={arXiv:1412.7525 [cs]}, number={arXiv:1412.7525}, publisher={arXiv}, author={Lee, Dong-Hyun and Zhang, Saizheng and Fischer, Asja and Bengio, Yoshua}, year={2015}, month={Nov} }
 
@article{randazzo2020self_classifying,
  author = {Randazzo, Ettore and Mordvintsev, Alexander and Niklasson, Eyvind and Levin, Michael and Greydanus, Sam},
  title = {Self-classifying MNIST Digits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/selforg/mnist},
  doi = {10.23915/distill.00027.002}
}

 @article{Zhang_Itoh_Tanida_Ichioka_1990, title={Parallel distributed processing model with local space-invariant interconnections and its optical architecture}, volume={29}, ISSN={0003-6935, 1539-4522}, DOI={10.1364/AO.29.004790}, number={32}, journal={Applied Optics}, author={Zhang, Wei and Itoh, Kazuyoshi and Tanida, Jun and Ichioka, Yoshiki}, year={1990}, month={Nov}, pages={4790}, language={en} }

 @inbook{Mouton_Myburgh_Davel_2020, title={Stride and Translation Invariance in CNNs}, volume={1342}, url={http://arxiv.org/abs/2103.10097}, DOI={10.1007/978-3-030-66151-9_17}, abstractNote={Convolutional Neural Networks have become the standard for image classification tasks, however, these architectures are not invariant to translations of the input image. This lack of invariance is attributed to the use of stride which ignores the sampling theorem, and fully connected layers which lack spatial reasoning. We show that stride can greatly benefit translation invariance given that it is combined with sufficient similarity between neighbouring pixels, a characteristic which we refer to as local homogeneity. We also observe that this characteristic is dataset-specific and dictates the relationship between pooling kernel size and stride required for translation invariance. Furthermore we find that a trade-off exists between generalization and translation invariance in the case of pooling kernel size, as larger kernel sizes lead to better invariance but poorer generalization. Finally we explore the efficacy of other solutions proposed, namely global average pooling, anti-aliasing, and data augmentation, both empirically and through the lens of local homogeneity.}, note={arXiv:2103.10097 [cs]}, author={Mouton, Coenraad and Myburgh, Johannes C. and Davel, Marelie H.}, year={2020}, pages={267–281} }
 
  @article{Ioffe_Szegedy_2015, title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}, url={http://arxiv.org/abs/1502.03167}, abstractNote={Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.}, note={arXiv:1502.03167 [cs]}, number={arXiv:1502.03167}, publisher={arXiv}, author={Ioffe, Sergey and Szegedy, Christian}, year={2015}, month={Mar} }

 @article{Lecun_Bottou_Bengio_Haffner_1998, title={Gradient-based learning applied to document recognition}, volume={86}, ISSN={00189219}, DOI={10.1109/5.726791}, number={11}, journal={Proceedings of the IEEE}, author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.}, year={1998}, month={Nov}, pages={2278–2324} }

@inproceedings{NIPS2012_c399862d,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}

 @article{Simonyan_Zisserman_2015, title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, url={http://arxiv.org/abs/1409.1556}, abstractNote={In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.}, note={arXiv:1409.1556 [cs]}, number={arXiv:1409.1556}, publisher={arXiv}, author={Simonyan, Karen and Zisserman, Andrew}, year={2015}, month={Apr} }

 @article{Szegedy_Liu_Jia_Sermanet_Reed_Anguelov_Erhan_Vanhoucke_Rabinovich_2014, title={Going Deeper with Convolutions}, url={http://arxiv.org/abs/1409.4842}, abstractNote={We propose a deep convolutional neural network architecture codenamed “Inception”, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.}, note={arXiv:1409.4842 [cs]}, number={arXiv:1409.4842}, publisher={arXiv}, author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew}, year={2014}, month={Sep} }

 @inproceedings{He_Zhang_Ren_Sun_2016, address={Las Vegas, NV, USA}, title={Deep Residual Learning for Image Recognition}, ISBN={978-1-4673-8851-1}, url={http://ieeexplore.ieee.org/document/7780459/}, DOI={10.1109/CVPR.2016.90}, booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian}, year={2016}, month={Jun}, pages={770–778} }

 @article{Ronneberger_Fischer_Brox_2015, title={U-Net: Convolutional Networks for Biomedical Image Segmentation}, url={http://arxiv.org/abs/1505.04597}, abstractNote={There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .}, note={arXiv: 1505.04597}, journal={arXiv:1505.04597 [cs]}, author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas}, year={2015}, month={May} }

 @article{Redmon_Divvala_Girshick_Farhadi_2016, title={You Only Look Once: Unified, Real-Time Object Detection}, url={http://arxiv.org/abs/1506.02640}, abstractNote={We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.}, note={arXiv:1506.02640 [cs]}, number={arXiv:1506.02640}, publisher={arXiv}, author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali}, year={2016}, month={May} }

@inproceedings{gilbert1990lateral,
  title={Lateral interactions in visual cortex},
  author={Gilbert, Charles D and Hirsch, Joseph A and Wiesel, Torsten N},
  booktitle={Cold Spring Harbor symposia on quantitative biology},
  volume={55},
  pages={663--677},
  year={1990},
  organization={Cold Spring Harbor Laboratory Press}
}

 @article{Rochefort_Garaschuk_Milos_Narushima_Marandi_Pichler_Kovalchuk_Konnerth_2009, title={Sparsification of neuronal activity in the visual cortex at eye-opening}, volume={106}, ISSN={0027-8424, 1091-6490}, DOI={10.1073/pnas.0907660106}, abstractNote={Eye-opening represents a turning point in the function of the visual cortex. Before eye-opening, the visual cortex is largely devoid of sensory inputs and neuronal activities are generated intrinsically. After eye-opening, the cortex starts to integrate visual information. Here we used in vivo two-photon calcium imaging to explore the developmental changes of the mouse visual cortex by analyzing the ongoing spontaneous activity. We found that before eye-opening, the activity of layer 2/3 neurons consists predominantly of slow wave oscillations. These waves were first detected at postnatal day 8 (P8). Their initial very low frequency (0.01 Hz) gradually increased during development to ≈0.5 Hz in adults. Before eye-opening, a large fraction of neurons (>75%) was active during each wave. One day after eye-opening, this dense mode of recruitment changed to a sparse mode with only 36% of active neurons per wave. This was followed by a progressive decrease during the following weeks, reaching 12% of active neurons per wave in adults. The possible role of visual experience for this process of sparsification was investigated by analyzing dark-reared mice. We found that sparsification also occurred in these mice, but that the switch from a dense to a sparse activity pattern was delayed by 3–4 days as compared with normally-reared mice. These results reveal a modulatory contribution of visual experience during the first days after eye-opening, but an overall dominating role of intrinsic factors. We propose that the transformation in network activity from dense to sparse is a prerequisite for the changed cortical function at eye-opening.}, number={35}, journal={Proceedings of the National Academy of Sciences}, author={Rochefort, Nathalie L. and Garaschuk, Olga and Milos, Ruxandra-Iulia and Narushima, Madoka and Marandi, Nima and Pichler, Bruno and Kovalchuk, Yury and Konnerth, Arthur}, year={2009}, month={Sep}, pages={15049–15054}, language={en} }

 @article{Louizos_Welling_Kingma_2018, title={Learning Sparse Neural Networks through $L_0$ Regularization}, url={http://arxiv.org/abs/1712.01312}, abstractNote={We propose a practical method for $L_0$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of $L_0$ regularization. However, since the $L_0$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected $L_0$ norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the emph{hard concrete} distribution for the gates, which is obtained by “stretching” a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.}, note={arXiv:1712.01312 [cs, stat]}, number={arXiv:1712.01312}, publisher={arXiv}, author={Louizos, Christos and Welling, Max and Kingma, Diederik P.}, year={2018}, month={Jun} }
 
 @article{Hoefler_Alistarh_Ben-Nun_Dryden_Peste_2021, title={Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks}, url={http://arxiv.org/abs/2102.00554}, abstractNote={The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, if not better than, the original dense networks. Sparsity can reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks. In this paper, we survey prior work on sparsity in deep learning and provide an extensive tutorial of sparsification for both inference and training. We describe approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. Our work distills ideas from more than 300 research papers and provides guidance to practitioners who wish to utilize sparsity today, as well as to researchers whose goal is to push the frontier forward. We include the necessary background on mathematical methods in sparsification, describe phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and show techniques for achieving acceleration on real hardware. We also define a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks. We close by speculating on how sparsity can improve future workloads and outline major open problems in the field.}, note={arXiv:2102.00554 [cs]}, number={arXiv:2102.00554}, publisher={arXiv}, author={Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra}, year={2021}, month={Jan} }

 @article{Panousis_Chatzis_Theodoridis_2021, title={Stochastic Local Winner-Takes-All Networks Enable Profound Adversarial Robustness}, url={http://arxiv.org/abs/2112.02671}, abstractNote={This work explores the potency of stochastic competition-based activations, namely Stochastic Local Winner-Takes-All (LWTA), against powerful (gradient-based) white-box and black-box adversarial attacks; we especially focus on Adversarial Training settings. In our work, we replace the conventional ReLU-based nonlinearities with blocks comprising locally and stochastically competing linear units. The output of each network layer now yields a sparse output, depending on the outcome of winner sampling in each block. We rely on the Variational Bayesian framework for training and inference; we incorporate conventional PGD-based adversarial training arguments to increase the overall adversarial robustness. As we experimentally show, the arising networks yield state-of-the-art robustness against powerful adversarial attacks while retaining very high classification rate in the benign case.}, note={arXiv:2112.02671 [cs, stat]}, number={arXiv:2112.02671}, publisher={arXiv}, author={Panousis, Konstantinos P. and Chatzis, Sotirios and Theodoridis, Sergios}, year={2021}, month={Dec} }

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@article{chen2020big,
  title={Big self-supervised models are strong semi-supervised learners},
  author={Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey E},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={22243--22255},
  year={2020}
}
@article{caron2020unsupervised,
  title={Unsupervised learning of visual features by contrasting cluster assignments},
  author={Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9912--9924},
  year={2020}
}

 @article{Chung_Hsu_Tang_Glass_2019, title={An Unsupervised Autoregressive Model for Speech Representation Learning}, url={http://arxiv.org/abs/1904.03240}, abstractNote={This paper proposes a novel unsupervised autoregressive neural model for learning generic speech representations. In contrast to other speech representation learning methods that aim to remove noise or speaker variabilities, ours is designed to preserve information for a wide range of downstream tasks. In addition, the proposed model does not require any phonetic or word boundary labels, allowing the model to benefit from large quantities of unlabeled data. Speech representations learned by our model significantly improve performance on both phone classification and speaker verification over the surface features and other supervised and unsupervised approaches. Further analysis shows that different levels of speech information are captured by our model at different layers. In particular, the lower layers tend to be more discriminative for speakers, while the upper layers provide more phonetic content.}, note={arXiv:1904.03240 [cs, eess]}, number={arXiv:1904.03240}, publisher={arXiv}, author={Chung, Yu-An and Hsu, Wei-Ning and Tang, Hao and Glass, James}, year={2019}, month={Jun} }

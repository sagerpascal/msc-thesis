@article{McCulloch_Pitts_1943,
  title        = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author       = {McCulloch, Warren S. and Pitts, Walter},
  year         = 1943,
  month        = Dec,
  journal      = {The Bulletin of Mathematical Biophysics},
  publisher    = {Springer},
  volume       = 5,
  number       = 4,
  pages        = {115–133},
  doi          = {10.1007/BF02478259},
  issn         = {0007-4985, 1522-9602},
  language     = {en}
}
@article{Rosenblatt_1958,
  title        = {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
  author       = {Rosenblatt, Frank},
  year         = 1958,
  journal      = {Psychological Review},
  publisher    = {American Psychological Association},
  volume       = 65,
  number       = 6,
  pages        = {386–408},
  doi          = {10.1037/h0042519},
  issn         = {1939-1471, 0033-295X},
  language     = {en}
}
@article{Cybenko_1989,
  title        = {Approximation by Superpositions of a Sigmoidal Function},
  author       = {Cybenko, George},
  year         = 1989,
  month        = Dec,
  journal      = {Mathematics of Control, Signals, and Systems},
  publisher    = {Springer},
  volume       = 2,
  number       = 4,
  pages        = {303–314},
  doi          = {10.1007/BF02551274},
  issn         = {0932-4194, 1435-568X},
  language     = {en}
}
@article{Rumelhart_Hinton_Williams_1986,
  title        = {Learning representations by back-propagating errors},
  author       = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year         = 1986,
  month        = Oct,
  journal      = {Nature},
  publisher    = {Nature Publishing Group UK London},
  volume       = 323,
  number       = 6088,
  pages        = {533–536},
  doi          = {10.1038/323533a0},
  issn         = {0028-0836, 1476-4687},
  language     = {en}
}
@online{Coursera,
  title        = {Deep Learning Specialization},
  author       = {{Coursera Inc.}},
  year         = 2022,
  url          = {https://www.coursera.org/specializations/deep-learning},
  urldate      = {2022-08-19}
}
@online{wiki_neuron,
  title        = {Neuron},
  author       = {Wikipedia},
  year         = 2023,
  url          = {https://en.wikipedia.org/wiki/Neuron},
  urldate      = {2023-02-19}
}
@online{venturebeat_img_analysis,
  title        = {New Deep Learning Model Brings Image Segmentation to Edge Devices},
  author       = {{Venture Beat}},
  year         = 2023,
  url          = {https://venturebeat.com/ai/new-deep-learning-model-brings-image-segmentation-to-edge-devices/},
  urldate      = {2023-02-19}
}
@article{NCAs_distill,
  title        = {Growing Neural Cellular Automata},
  author       = {Mordvintsev, Alexander and Randazzo, Ettore and Niklasson, Eyvind and Levin, Michael},
  year         = 2020,
  journal      = {Distill},
  doi          = {10.23915/distill.00023},
}
@online{axios_hinton,
  title        = {Artificial Intelligence Pioneer Says We Need to Start Over},
  author       = {{Axios Media Inc.}},
  year         = 2017,
  url          = {https://www.axios.com/2017/12/15/artificial-intelligence-pioneer-says-we-need-to-start-over-1513305524},
  urldate      = {2022-09-04}
}
@article{Kingma2015AdamAM,
  title        = {Adam: A Method for Stochastic Optimization},
  author       = {Kingma, Diederik P. and Ba, Jimmy},
  year         = 2017,
  month        = {Jan},
  publisher    = {arXiv},
  number       = {arXiv:1412.6980},
  doi          = {10.48550/arXiv.1412.6980},
  url          = {http://arxiv.org/abs/1412.6980},
  abstractnote = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.}
}
@online{OpenAI_compute,
  title        = {AI and Compute},
  author       = {{Open AI}},
  year         = 2018,
  url          = {https://openai.com/blog/ai-and-compute/},
  urldate      = {2022-08-19}
}
@online{Lambda_GPT3,
  title        = {OpenAI's GPT-3 Language Model: A Technical Overview},
  author       = {{Lambda Inc.}},
  year         = 2021,
  url          = {https://lambdalabs.com/blog/demystifying-gpt-3/},
  urldate      = {2022-08-19}
}
@article{Moore_2006,
  title        = {Cramming More Components onto Integrated Circuits},
  author       = {Moore, Gordon E.},
  year         = 1965,
  month        = April,
  journal      = {Electronics},
  publisher    = {McGraw-Hill New York},
  volume       = 38,
  number       = 8,
  pages        = {114--1116}
}
@article{Kumar_2015,
  title        = {Fundamental Limits to Moore's Law},
  author       = {Kumar, Suhas},
  year         = 2015,
  month        = Nov,
  publisher    = {arXiv},
  number       = {arXiv:1511.05956},
  doi          = {10.48550/arXiv.1511.05956},
  url          = {http://arxiv.org/abs/1511.05956},
  abstractnote = {The theoretical and practical aspects of the fundamental, ultimate, physical limits to scaling, or Moore-s law, is presented.}
}
@inproceedings{Peters_Neumann_Iyyer_Gardner_Clark_Lee_Zettlemoyer_2018,
  title        = {Deep Contextualized Word Representations},
  author       = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year         = 2018,
  month        = June,
  booktitle    = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  publisher    = {Association for Computational Linguistics},
  address      = {New Orleans, Louisiana},
  volume       = 1,
  pages        = {2227–2237},
  doi          = {10.18653/v1/N18-1202},
  url          = {http://aclweb.org/anthology/N18-1202},
  language     = {en}
}
@inproceedings{NEURIPS2020_1457c0d6,
  title        = {Language Models are Few-Shot Learners},
  author       = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year         = 2020,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  location     = {Vancouver, BC, Canada},
  publisher    = {Curran Associates, Inc.},
  address      = {Red Hook, NY, USA},
  volume       = 33,
  pages        = {1877--1901},
  isbn         = 9781713829546,
  url          = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  articleno    = 159,
  numpages     = 25
}
@article{Shoeybi_Patwary_Puri_LeGresley_Casper_Catanzaro_2020,
  title        = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author       = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  year         = 2020,
  month        = Mar,
  publisher    = {arXiv},
  number       = {arXiv:1909.08053},
  doi          = {10.48550/arXiv.1909.08053},
  url          = {http://arxiv.org/abs/1909.08053},
  abstractnote = {Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).}
}
@article{Smith_Patwary_Norick_LeGresley_Rajbhandari_Casper_Liu_Prabhumoye_Zerveas_Korthikanti_etal,
  title        = {Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model},
  author       = {Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and Zhang, Elton and Child, Rewon and Aminabadi, Reza Yazdani and Bernauer, Julie and Song, Xia and Shoeybi, Mohammad and He, Yuxiong and Houston, Michael and Tiwary, Saurabh and Catanzaro, Bryan},
  year         = 2022,
  month        = {Feb},
  publisher    = {arXiv},
  number       = {arXiv:2201.11990},
  doi          = {10.48550/arXiv.2201.11990},
  url          = {http://arxiv.org/abs/2201.11990},
  abstractnote = {Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.}
}
@article{Wu_Judd_Zhang_Isaev_Micikevicius_2020,
  title        = {Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation},
  author       = {Wu, Hao and Judd, Patrick and Zhang, Xiaojie and Isaev, Mikhail and Micikevicius, Paulius},
  year         = 2020,
  month        = Apr,
  publisher    = {arXiv},
  number       = {arXiv:2004.09602},
  doi          = {10.48550/arXiv.2004.09602},
  url          = {http://arxiv.org/abs/2004.09602},
  abstractnote = {Quantization techniques can reduce the size of Deep Neural Networks and improve inference latency and throughput by taking advantage of high throughput integer instructions. In this paper we review the mathematical aspects of quantization parameters and evaluate their choices on a wide range of neural network models for different application domains, including vision, speech, and language. We focus on quantization techniques that are amenable to acceleration by processors with high-throughput integer math pipelines. We also present a workflow for 8-bit quantization that is able to maintain accuracy within 1% of the floating-point baseline on all networks studied, including models that are more difficult to quantize, such as MobileNets and BERT-large.}
}
@article{Choudhary_Mishra_Goswami_Sarangapani_2020,
  title        = {A Comprehensive Survey on Model Compression and Acceleration},
  author       = {Choudhary, Tejalal and Mishra, Vipul and Goswami, Anurag and Sarangapani, Jagannathan},
  year         = 2020,
  month        = Oct,
  journal      = {Artificial Intelligence Review},
  volume       = 53,
  number       = 7,
  pages        = {5113–5155},
  doi          = {10.1007/s10462-020-09816-7},
  issn         = {0269-2821, 1573-7462},
  language     = {en}
}
@article{Hinton_Vinyals_Dean_2015,
  title        = {Distilling the Knowledge in a Neural Network},
  author       = {Hinton, Geoffrey E. and Vinyals, Oriol and Dean, Jeff},
  year         = 2015,
  month        = Mar,
  publisher    = {arXiv},
  number       = {arXiv:1503.02531},
  doi          = {10.48550/arXiv.1503.02531},
  url          = {http://arxiv.org/abs/1503.02531},
  abstractnote = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.}
}
@article{Zhang_Yang_2021,
  title        = {A Survey on Multi-Task Learning},
  author       = {Zhang, Yu and Yang, Qiang},
  year         = 2021,
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  publisher    = {IEEE},
  volume       = 34,
  number       = 12,
  pages        = {5586--5609},
  doi          = {10.1109/TKDE.2021.3070203}
}
@article{Parisi_Kemker_Part_Kanan_Wermter_2019,
  title        = {Continual Lifelong Learning with Neural Networks: A Review},
  author       = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
  year         = 2019,
  month        = May,
  journal      = {Neural Networks},
  volume       = 113,
  pages        = {54–71},
  doi          = {10.1016/j.neunet.2019.01.012},
  issn         = {0893-6080},
  language     = {en}
}
@inproceedings{Sahoo_Pham_Lu_Hoi_2017,
  title        = {Online Deep Learning: Learning Deep Neural Networks on the Fly},
  author       = {Sahoo, Doyen and Pham, Quang and Lu, Jing and Hoi, Steven C. H.},
  year         = 2018,
  month        = {Jul},
  booktitle    = {International Joint Conference on Artificial Intelligence (ICAJ)},
  publisher    = {International Joint Conferences on Artificial Intelligence Organization},
  address      = {Stockholm, Sweden},
  pages        = {2660–2666},
  doi          = {10.24963/ijcai.2018/369},
  isbn         = {978-0-9992411-2-7},
  url          = {https://www.ijcai.org/proceedings/2018/369},
  abstractnote = {Deep Neural Networks (DNNs) are typically trained by backpropagation in a batch setting, requiring the entire training data to be made available prior to the learning task. This is not scalable for many real-world scenarios where new data arrives sequentially in a stream. We aim to address an open challenge of ``Online Deep Learning" (ODL) for learning DNNs on the fly in an online setting. Unlike traditional online learning that often optimizes some convex objective function with respect to a shallow model (e.g., a linear/kernel-based hypothesis), ODL is more challenging as the optimization objective is non-convex, and regular DNN with standard backpropagation does not work well in practice for online settings. We present a new ODL framework that attempts to tackle the challenges by learning DNN models which dynamically adapt depth from a sequence of training data in an online learning setting. Specifically, we propose a novel Hedge Backpropagation (HBP) method for online updating the parameters of DNN effectively, and validate the efficacy on large data sets (both stationary and concept drifting scenarios).},
  language     = {en}
}
@article{Madan_Henry_Dozier_Ho_Bhandari_Sasaki_Durand_Pfister_Boix_2022,
  title        = {When and how convolutional neural networks generalize to out-of-distribution category–viewpoint combinations},
  author       = {Madan, Spandan and Henry, Timothy and Dozier, Jamell and Ho, Helen and Bhandari, Nishchal and Sasaki, Tomotake and Durand, Frédo and Pfister, Hanspeter and Boix, Xavier},
  year         = 2022,
  month        = Feb,
  journal      = {Nature Machine Intelligence},
  volume       = 4,
  number       = 2,
  pages        = {146–153},
  doi          = {10.1038/s42256-021-00437-5},
  issn         = {2522-5839},
  language     = {en}
}
@article{Marcus_2018,
  title        = {Deep Learning: A Critical Appraisal},
  author       = {Marcus, Gary},
  year         = 2018,
  month        = Jan,
  publisher    = {arXiv},
  number       = {arXiv:1801.00631},
  doi          = {10.48550/arXiv.1801.00631},
  url          = {http://arxiv.org/abs/1801.00631},
  abstractnote = {Although deep learning has historical roots going back decades, neither the term “deep learning” nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton’s now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.}
}
@book{Moravec_1995,
  title        = {Mind Children: The Future of Robot and Human Intelligence},
  author       = {Moravec, Hans},
  year         = 1995,
  publisher    = {Harvard University Press},
  address      = {Cambridge},
  isbn         = {978-0-674-57618-6},
  language     = {en}
}
@article{Felleman_Van_Essen_1991,
  title        = {Distributed Hierarchical Processing in the Primate Cerebral Cortex},
  author       = {Felleman, Daniel J. and Van Essen, David C.},
  year         = 1991,
  month        = Jan,
  journal      = {Cerebral Cortex},
  volume       = 1,
  number       = 1,
  pages        = {1–47},
  doi          = {10.1093/cercor/1.1.1},
  issn         = {1047-3211, 1460-2199},
  language     = {en}
}
@article{Lillicrap_Cownden_Tweed_Akerman_2016,
  title        = {Random Synaptic Feedback Weights Support Error Backpropagation for Deep Learning},
  author       = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
  year         = 2016,
  month        = Dec,
  journal      = {Nature Communications},
  volume       = 7,
  number       = 1,
  pages        = 13276,
  doi          = {10.1038/ncomms13276},
  issn         = {2041-1723},
  language     = {en}
}
@article{Hopfield_1982,
  title        = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities},
  author       = {Hopfield, John J.},
  year         = 1982,
  month        = Apr,
  journal      = {Proceedings of the National Academy of Sciences (PNAS)},
  volume       = 79,
  number       = 8,
  pages        = {2554–2558},
  doi          = {10.1073/pnas.79.8.2554},
  issn         = {0027-8424, 1091-6490},
  abstractnote = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
  language     = {en}
}
@article{Fix_Hodges_1989,
  title        = {Discriminatory Analysis. Nonparametric Discrimination: Consistency Properties},
  author       = {Fix, Evelyn and {Hodges Jr.}, Joseph L.},
  year         = 1989,
  month        = Dec,
  journal      = {International Statistical Review},
  volume       = 57,
  number       = 3,
  pages        = {238--247},
  doi          = {10.2307/1403797},
  issn         = {03067734}
}
@article{Weston_Chopra_Bordes_2015,
  title        = {Memory Networks},
  author       = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
  year         = 2015,
  month        = Nov,
  publisher    = {arXiv},
  number       = {arXiv:1410.3916},
  doi          = {10.48550/arXiv.1410.3916},
  url          = {http://arxiv.org/abs/1410.3916},
  abstractnote = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.}
}
@article{1057328,
  title        = {The Capacity of the Hopfield Associative Memory},
  author       = {McEliece, Robert J. and Posner, Edward C. and Rodemich, Eugener and Venkatesh, Santoshs},
  year         = 1987,
  month        = {Jul},
  journal      = {IEEE Transactions on Information Theory},
  volume       = 33,
  number       = 4,
  pages        = {461--482},
  doi          = {10.1109/TIT.1987.1057328},
  issn         = {0018-9448}
}
@article{Hopfield1983,
  title        = {‘Unlearning’ Has a Stabilizing Effect in Collective Memories},
  author       = {Hopfield, John J. and Feinstein, David I. and Palmer, Richard G.},
  year         = 1983,
  month        = Jul,
  day          = {01},
  journal      = {Nature},
  volume       = 304,
  number       = 5922,
  pages        = {158--159},
  doi          = {10.1038/304158a0},
  issn         = {1476-4687},
  abstract     = {Crick and Mitchison1 have presented a hypothesis for the functional role of dream sleep involving an `unlearning' process. We have independently carried out mathematical and computer modelling of learning and `unlearning' in a collective neural network of 30--1,000 neurones. The model network has a content-addressable memory or `associative memory' which allows it to learn and store many memories. A particular memory can be evoked in its entirety when the network is stimulated by any adequate-sized subpart of the information of that memory2. But different memories of the same size are not equally easy to recall. Also, when memories are learned, spurious memories are also created and can also be evoked. Applying an `unlearning' process, similar to the learning processes but with a reversed sign and starting from a noise input, enhances the performance of the network in accessing real memories and in minimizing spurious ones. Although our model was not motivated by higher nervous function, our system displays behaviours which are strikingly parallel to those needed for the hypothesized role of `unlearning' in rapid eye movement (REM) sleep.}
}
@inproceedings{10.5555/3157096.3157228,
  title        = {Dense Associative Memory for Pattern Recognition},
  author       = {Krotov, Dmitry and Hopfield, John J.},
  year         = 2016,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  location     = {Barcelona, Spain},
  publisher    = {Curran Associates Inc.},
  address      = {Red Hook, NY, USA},
  volume       = 29,
  pages        = {1180–1188},
  isbn         = 9781510838819,
  editor       = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  abstract     = {A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions - the higher rectified polynomials which until now have not been used in deep learning. The utility of the dense memories is illustrated for two test cases: the logical gate XOR and the recognition of handwritten digits from the MNIST data set.},
  numpages     = 9
}
@article{Demircigil_Heusel_Löwe_Upgang_Vermet_2017,
  title        = {On a Model of Associative Memory with Huge Storage Capacity},
  author       = {Demircigil, Mete and Heusel, Judith and Löwe, Matthias and Upgang, Sven and Vermet, Franck},
  year         = 2017,
  month        = Jul,
  journal      = {Journal of Statistical Physics},
  volume       = 168,
  number       = 2,
  pages        = {288–299},
  doi          = {10.1007/s10955-017-1806-y},
  issn         = {0022-4715, 1572-9613},
  language     = {en}
}
@article{Ramsauer,
  title        = {Hopfield Networks is All You Need},
  author       = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
  year         = 2021,
  month        = Apr,
  publisher    = {arXiv},
  number       = {arXiv:2008.02217},
  doi          = {10.48550/arXiv.2008.02217},
  url          = {http://arxiv.org/abs/2008.02217},
  abstractnote = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers}
}
@book{Hebb_1949,
  title        = {The Organization of Behavior; A Neuropsychological Theory},
  author       = {Hebb, Donald O.},
  year         = 1949,
  publisher    = {Psychology Press},
  address      = {Oxford,  England}
}
@article{Oja_1982,
  title        = {Simplified Neuron Model as a Principal Component Analyzer},
  author       = {Oja, Erkki},
  year         = 1982,
  month        = Nov,
  journal      = {Journal of Mathematical Biology},
  volume       = 15,
  number       = 3,
  pages        = {267–273},
  doi          = {10.1007/BF00275687},
  issn         = {0303-6812, 1432-1416},
  language     = {en}
}
@article{Bienenstock_Cooper_Munro_1982,
  title        = {Theory for the Development of Neuron Selectivity: Orientation Specificity and Binocular Interaction in Visual Cortex},
  author       = {Bienenstock, Elie L. and Cooper, Leon N. and Munro, Paul W.},
  year         = 1982,
  month        = Jan,
  journal      = {The Journal of Neuroscience},
  volume       = 2,
  number       = 1,
  pages        = {32–48},
  doi          = {10.1523/JNEUROSCI.02-01-00032.1982},
  issn         = {0270-6474, 1529-2401},
  language     = {en}
}
@article{Intrator_Cooper_1992,
  title        = {Objective Function Formulation of the Bcm Theory of Visual Cortical Plasticity: Statistical Connections, Stability Conditions},
  author       = {Intrator, Nathan and Cooper, Leon N.},
  year         = 1992,
  month        = Jan,
  journal      = {Neural Networks},
  volume       = 5,
  number       = 1,
  pages        = {3–17},
  doi          = {10.1016/S0893-6080(05)80003-6},
  issn         = {08936080},
  language     = {en}
}
@article{Simoncelli_Olshausen_2001,
  title        = {Natural Image Statistics and Neural Representation},
  author       = {Simoncelli, Eero P. and Olshausen, Bruno A.},
  year         = 2001,
  month        = Mar,
  journal      = {Annual Review of Neuroscience},
  volume       = 24,
  number       = 1,
  pages        = {1193–1216},
  doi          = {10.1146/annurev.neuro.24.1.1193},
  issn         = {0147-006X, 1545-4126},
  abstractnote = {▪ Abstract  It has long been assumed that sensory neurons are adapted, through both evolutionary and developmental processes, to the statistical properties of the signals to which they are exposed. Attneave (1954) , Barlow (1961) proposed that information theory could provide a link between environmental statistics and neural responses through the concept of coding efficiency. Recent developments in statistical modeling, along with powerful computational tools, have enabled researchers to study more sophisticated statistical models for visual images, to validate these models empirically against large sets of data, and to begin experimentally testing the efficient coding hypothesis for both individual neurons and populations of neurons.},
  language     = {en}
}
@article{Vogels_Sprekeler_Zenke_Clopath_Gerstner_2011,
  title        = {Inhibitory Plasticity Balances Excitation and Inhibition in Sensory Pathways and Memory Networks},
  author       = {Vogels, Tim P. and Sprekeler, Henning and Zenke, Friedemann and Clopath, Claudia and Gerstner, Wulfram},
  year         = 2011,
  month        = Dec,
  journal      = {Science},
  volume       = 334,
  number       = 6062,
  pages        = {1569–1573},
  doi          = {10.1126/science.1211095},
  issn         = {0036-8075, 1095-9203},
  language     = {en}
}
@inproceedings{5178625,
  title        = {Rules for Information Maximization in Spiking Neurons Using Intrinsic Plasticity},
  author       = {Joshi, Prashant and Triesch, Jochen},
  year         = 2009,
  booktitle    = {International Joint Conference on Neural Networks (IJCNN)},
  address      = {Atlanta, USA},
  pages        = {1456--1461},
  doi          = {10.1109/IJCNN.2009.5178625},
  isbn         = {978-1-4244-3548-7},
  url          = {http://ieeexplore.ieee.org/document/5178625/}
}
@inproceedings{Teichmann,
  title        = {Intrinsic Plasticity: A Simple Mechanism to Stabilize Hebbian Learning in Multilayer Neural Networks},
  author       = {Teichmann, Michael and Hamker, Fred},
  year         = 2015,
  year         = 2015,
  month        = {03},
  booktitle    = {Workshop New Challenges in Neural Computation},
  pages        = {103--111},
  organization = {Citeseer}
}
@article{Grossberg_1988,
  title        = {Nonlinear Neural Networks: Principles, Mechanisms, and Architectures},
  author       = {Grossberg, Stephen},
  year         = 1988,
  month        = Jan,
  journal      = {Neural Networks},
  volume       = 1,
  number       = 1,
  pages        = {17–61},
  doi          = {10.1016/0893-6080(88)90021-4},
  issn         = {08936080},
  language     = {en}
}
@article{esn,
  title        = {The” Echo State” Approach to Analysing and Training Recurrent Neural Networks-with an Erratum Note'},
  author       = {Jäger, Herbert},
  year         = 2001,
  month        = {01},
  journal      = {German National Research Institute for Computer Science GMD: Technical Report},
  volume       = 148
}
@article{Maass_Natschlager_Markram_2002,
  title        = {Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations},
  author       = {Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
  year         = 2002,
  month        = Nov,
  journal      = {Neural Computation},
  volume       = 14,
  number       = 11,
  pages        = {2531–2560},
  doi          = {10.1162/089976602760407955},
  issn         = {0899-7667, 1530-888X},
  abstractnote = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.},
  language     = {en}
}
@inbook{Konkoli_2018,
  title        = {Reservoir Computing},
  author       = {Konkoli, Zoran},
  year         = 2018,
  booktitle    = {Unconventional Computing},
  publisher    = {Springer US},
  address      = {New York, NY},
  pages        = {619–629},
  doi          = {10.1007/978-1-4939-6883-1_683},
  isbn         = {978-1-4939-6882-4},
  url          = {http://link.springer.com/10.1007/978-1-4939-6883-1\%5F683},
  editor       = {Adamatzky, Andrew},
  language     = {en}
}
@article{Tanaka_Yamane_Héroux_Nakane_Kanazawa_Takeda_Numata_Nakano_Hirose_2019,
  title        = {Recent Advances in Physical Reservoir Computing: A Review},
  author       = {Tanaka, Gouhei and Yamane, Toshiyuki and Héroux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
  year         = 2019,
  month        = Jul,
  journal      = {Neural Networks},
  volume       = 115,
  pages        = {100–123},
  doi          = {10.1016/j.neunet.2019.03.005},
  issn         = {08936080},
  language     = {en}
}
@article{erdos59a,
  title        = {On Random Graphs I},
  author       = {Erdös, Paul and Rényi, Alfred},
  year         = 1959,
  journal      = {Publicationes Mathematicae Debrecen},
  volume       = 6,
  pages        = 290,
  keywords     = {graph sna}
}
@inbook{Lukoševičius_2012,
  title        = {A Practical Guide to Applying Echo State Networks},
  author       = {Lukoševičius, Mantas},
  year         = 2012,
  booktitle    = {Neural Networks: Tricks of the Trade},
  publisher    = {Springer Berlin Heidelberg},
  address      = {Berlin, Heidelberg},
  series       = {Lecture Notes in Computer Science},
  volume       = 7700,
  pages        = {659–686},
  doi          = {10.1007/978-3-642-35289-8_36},
  isbn         = {978-3-642-35288-1},
  url          = {http://link.springer.com/10.1007/978-3-642-35289-8\%5F36},
  editor       = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
  collection   = {Lecture Notes in Computer Science},
  language     = {en}
}
@article{Abbott1999LapicquesIO,
  title        = {Lapicque’s Introduction of the Integrate-and-Fire Model Neuron},
  author       = {Abbott, Larray F.},
  year         = 1999,
  journal      = {Brain Research Bulletin},
  volume       = 50,
  number       = 5,
  pages        = {303--304},
  doi          = {https://doi.org/10.1016/S0361-9230(99)00161-6},
  issn         = {0361-9230},
  url          = {https://www.sciencedirect.com/science/article/pii/S0361923099001616}
}
@article{Izhikevich_2003,
  title        = {Simple Model of Spiking Neurons},
  author       = {Izhikevich, Eugene M.},
  year         = 2003,
  month        = Nov,
  journal      = {IEEE Transactions on Neural Networks},
  volume       = 14,
  number       = 6,
  pages        = {1569–1572},
  doi          = {10.1109/TNN.2003.820440},
  issn         = {1045-9227},
  language     = {en}
}
@article{Brette_Gerstner_2005,
  title        = {Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity},
  author       = {Brette, Romain and Gerstner, Wulfram},
  year         = 2005,
  month        = Nov,
  journal      = {Journal of Neurophysiology},
  volume       = 94,
  number       = 5,
  pages        = {3637–3642},
  doi          = {10.1152/jn.00686.2005},
  issn         = {0022-3077, 1522-1598},
  abstractnote = {We introduce a two-dimensional integrate-and-fire model that combines an exponential spike mechanism with an adaptation equation, based on recent theoretical findings. We describe a systematic method to estimate its parameters with simple electrophysiological protocols (current-clamp injection of pulses and ramps) and apply it to a detailed conductance-based model of a regular spiking neuron. Our simple model predicts correctly the timing of 96% of the spikes (±2 ms) of the detailed model in response to injection of noisy synaptic conductances. The model is especially reliable in high-conductance states, typical of cortical activity in vivo, in which intrinsic conductances were found to have a reduced role in shaping spike trains. These results are promising because this simple model has enough expressive power to reproduce qualitatively several electrophysiological classes described in vitro.},
  language     = {en}
}
@article{Paugam_Moisy,
  title        = {Spiking Neuron Networks: A Survey},
  author       = {Paugam-Moisy, Hélène},
  year         = 2006,
  journal      = {IDIAP Research Report},
  publisher    = {IDIAP},
  volume       = 6,
  number       = 11,
  url          = {http://infoscience.epfl.ch/record/83371}
}
@article{Bi_Poo_2001,
  title        = {Synaptic Modification by Correlated Activity: Hebb’s Postulate Revisited},
  author       = {Bi, Guo-Qiang and Poo, Mu-Ming},
  year         = 2001,
  month        = Mar,
  journal      = {Annual Review of Neuroscience},
  volume       = 24,
  number       = 1,
  pages        = {139–166},
  doi          = {10.1146/annurev.neuro.24.1.139},
  issn         = {0147-006X, 1545-4126},
  language     = {en}
}
@article{KHERADPISHEH201856,
  title        = {STDP-Based Spiking Deep Convolutional Neural Networks for Object Recognition},
  author       = {Kheradpisheh, Saeed Reza and Ganjtabesh, Mohammad and Thorpe, Simon J. and Masquelier, Timothée},
  year         = 2018,
  month        = {Mar},
  journal      = {Neural Networks},
  volume       = 99,
  pages        = {56–67},
  doi          = {10.1016/j.neunet.2017.12.005},
  issn         = {08936080},
  language     = {en}
}
@article{von_der_Malsburg_Stadelmann_Grewe_2022,
  title        = {A Theory of Natural Intelligence},
  author       = {{von der Malsburg}, Christoph and Stadelmann, Thilo and Grewe, Benjamin F.},
  year         = 2022,
  month        = Apr,
  publisher    = {arXiv},
  number       = {arXiv:2205.00002},
  doi          = {10.48550/arXiv.2205.00002},
  url          = {http://arxiv.org/abs/2205.00002},
  abstractnote = {Introduction: In contrast to current AI technology, natural intelligence -- the kind of autonomous intelligence that is realized in the brains of animals and humans to attain in their natural environment goals defined by a repertoire of innate behavioral schemata -- is far superior in terms of learning speed, generalization capabilities, autonomy and creativity. How are these strengths, by what means are ideas and imagination produced in natural neural networks? Methods: Reviewing the literature, we put forward the argument that both our natural environment and the brain are of low complexity, that is, require for their generation very little information and are consequently both highly structured. We further argue that the structures of brain and natural environment are closely related. Results: We propose that the structural regularity of the brain takes the form of net fragments (self-organized network patterns) and that these serve as the powerful inductive bias that enables the brain to learn quickly, generalize from few examples and bridge the gap between abstractly defined general goals and concrete situations. Conclusions: Our results have important bearings on open problems in artificial neural network research.}
}
@mastersthesis{lehmann,
  title        = {Leveraging Neuroscience for Deep Learning Based Object Recognition},
  author       = {Lehmann, Claude},
  year         = 2022,
  school       = {Zurich University of Applied Sciences}
}
@article{Dresp2020SevenPO,
  title        = {Seven Properties of Self-Organization in the Human Brain},
  author       = {Dresp-Langley, Birgitta},
  year         = 2020,
  journal      = {Big Data and Cognitive Computing},
  publisher    = {MDPI},
  volume       = 4,
  number       = 2,
  pages        = 10
}
@book{kelso1995dynamic,
  title        = {Dynamic Patterns: The Self-Organization of Brain and Behavior},
  author       = {Kelso, Scott J. A.},
  year         = 1995,
  month        = {Jan},
  publisher    = {The MIT Press}
}
@article{hbcrd,
  title        = {A Physical Map of the Human Genome},
  author       = {McPherson, John D. and Marra, Marco and Hillier, LaDeana and Waterston, Robert H. and Chinwalla, Asif and Wallis, John and Sekhon, Mandeep and Wylie, Kristine and Mardis, Elaine R. and Wilson, Richard K. and Fulton, Robert and Kucaba, Tamara A. and Wagner-McPherson, Caryn and Barbazuk, William B. and Gregory, Simon G. and Humphray, Sean J. and French, Lisa and Evans, Richard S. and Bethel, Graeme and Whittaker, Adam and Holden, Jane L. and McCann, Owen T. and Dunham, Andrew and Soderlund, Carol and Scott, Carol E. and Bentley, David R. and Schuler, Gregory and Chen, Hsiu-Chuan and Jang, Wonhee and Green, Eric D. and Idol, Jacquelyn R. and Maduro, Valerie V. Braden and Montgomery, Kate T. and Lee, Eunice and Miller, Ashley and Emerling, Suzanne and Kucherlapati, Raju and Gibbs, Richard and Scherer, Steve and Gorrell, J. Harley and Sodergren, Erica and Clerc-Blankenburg, Kerstin and Tabor, Paul and Naylor, Susan and Garcia, Dawn and de Jong, Pieter J. and Catanese, Joseph J. and Nowak, Norma and Osoegawa, Kazutoyo and Qin, Shizhen and Rowen, Lee and Madan, Anuradha and Dors, Monica and Hood, Leroy and Trask, Barbara and Friedman, Cynthia and Massa, Hillary and Cheung, Vivian G. and Kirsch, Ilan R. and Reid, Thomas and Yonescu, Raluca and Weissenbach, Jean and Bruls, Thomas and Heilig, Roland and Branscomb, Elbert and Olsen, Anne and Doggett, Norman and Cheng, Jan-Fang and Hawkins, Trevor and Myers, Richard M. and Shang, Jin and Ramirez, Lucia and Schmutz, Jeremy and Velasquez, Olivia and Dixon, Kami and Stone, Nancy E. and Cox, David R. and Haussler, David and Kent, W. James and Furey, Terrence and Rogic, Sanja and Kennedy, Scot and Jones, Steven and Rosenthal, Andre and Wen, Gaiping and Schilhabel, Markus and Gloeckner, Gernot and Nyakatura, Gerald and Siebert, Reiner and Schlegelberger, Brigitte and Korenberg, Julie and Chen, Xiao-Ning and Fujiyama, Asao and Hattori, Masahira and Toyoda, Atsushi and Yada, Tetsushi and Park, Hong-Seok and Sakaki, Yoshiyuki and Shimizu, Nobuyoshi and Asakawa, Shuichi and Kawasaki, Kazuhiko and Sasaki, Takashi and Shintani, Ai and Shimizu, Atsushi and Shibuya, Kazunori and Kudoh, Jun and Minoshima, Shinsei and Ramser, Juliane and Seranski, Peter and Hoff, Celine and Poustka, Annemarie and Reinhardt, Richard and Lehrach, Hans},
  year         = 2001,
  month        = Feb,
  day          = {01},
  journal      = {Nature},
  volume       = 409,
  number       = 6822,
  pages        = {934--941},
  doi          = {10.1038/35057157},
  issn         = {1476-4687},
  url          = {https://doi.org/10.1038/35057157}
}
@article{Kolmogorov_1998,
  title        = {On Tables of Random Numbers},
  author       = {Kolmogorov, Andrei N.},
  year         = 1998,
  month        = Nov,
  journal      = {Theoretical Computer Science},
  volume       = 207,
  number       = 2,
  pages        = {387–395},
  doi          = {10.1016/S0304-3975(98)00075-9},
  issn         = {03043975},
  language     = {en}
}
@article{Willshaw_VonDerMalsburg_1976,
  title        = {How Patterned Neural Connections Can Be Set Up by Self-Organization},
  author       = {Willshaw, David J. and {von der Malsburg}, Christoph},
  year         = 1976,
  month        = Nov,
  journal      = {Proceedings of the Royal Society of London. Series B. Biological Sciences},
  volume       = 194,
  number       = 1117,
  pages        = {431–445},
  doi          = {10.1098/rspb.1976.0087},
  issn         = {0080-4649, 2053-9193},
  abstractnote = {An important problem in biology is to explain how patterned neural connections are set up during ontogenesis. Topographically ordered mappings, found widely in nervous systems, are those in which neighbouring elements in one sheet of cells project to neighbouring elements in a second sheet. Exploiting this neighbourhood property leads to a new theory for the establishment of topographical mappings, in which the distance between two cells is expressed in terms of their similarity with respect to certain physical properties assigned to them. This topographical code can be realized in a model employing either synchronization of nervous activity or exchange of specific molecules between neighbouring cells. By means of modifiable synapses the code is used to set up a topographical mapping between two sheets with the same internal structure. We have investigated the neural activity version. Without needing to make any elaborate assumptions about its structure or about the operations its elements are to carry out we have shown that the mappings are set up in a system-to-system rather than a cell-to-cell fashion. The pattern of connections develops in a step-by-step and orderly fashion, the orientation of the mappings being laid down in the earliest stages of development.},
  language     = {en}
}
@article{Willshaw_VonDerMalsburg_1979,
  title        = {A Marker Induction Mechanism for the Establishment of Ordered Neural Mappings: Its Application to the Retinotectal Problem},
  author       = {Willshaw, David J. and {von der Malsburg}, Christoph},
  year         = 1979,
  month        = Nov,
  journal      = {Philosophical Transactions of the Royal Society of London. B, Biological Sciences},
  volume       = 287,
  number       = 1021,
  pages        = {203–243},
  doi          = {10.1098/rstb.1979.0056},
  issn         = {0080-4622, 2054-0280},
  abstractnote = {This paper examines the idea that ordered patterns of nerve connections are set up by means of markers carried by the individual cells. The case of the ordered retinotectal projection in amphibia and fishes is discussed in great detail. It is suggested that retinotectal mappings are the result of two mechanisms acting in concert. One mechanism induces a set of retinal markers into the tectum. By this means, an initially haphazard pattern of synapses is transformed into a continuous or piece-wise continuous projection. The other mechanism places the individual pieces of the map in the correct orientation. The machinery necessary for this inductive scheme has been expressed in terms of a set of differential equations, which have been solved numerically for a number of cases. Straightforward assumptions are made as to how markers are distributed in the retina; how they are induced into the tectum; and how the induced markers bring about alterations in the pattern of synaptic contacts. A detailed physiological interpretation of the model is given. The inductive mechanism has been formulated at the level of the individual synaptic interactions. Therefore, it is possible to specify, in a given situation, not only the nature of the end state of the mapping but also how the mapping develops over time. The role of the modes of growth of retina and tectum in shaping the developing projection becomes clear. Since, on this model, the tectum is initially devoid of markers, there is an important difference between the development and the regeneration of ordered mappings. In the development of duplicate maps from various types of compound-eyes, it is suggested that the tectum, rather than the retina, contains an abnormal distribution of markers. An important parameter in these experiments, and also in the regeneration experiments where part-duplication has been found, is the range of interaction amongst the retinal cells. It is suggested that the results of many of the regeneration experiments (including apparently contradictory ones) are manifestations of a conflict between the two alternative ways of specifying the orientation of the map: through the information carried by the markers previously induced into the tectum and through the orientation mechanism itself.},
  language     = {en}
}
@article{Malsburg_1987,
  title        = {A Neural Network for the Retrieval of Superimposed Connection Patterns},
  author       = {{von der Malsburg}, Christoph and Bienenstock, Elie},
  year         = 1987,
  month        = Jun,
  journal      = {Europhysics Letters ({EPL})},
  publisher    = {{IOP} Publishing},
  volume       = 3,
  number       = 11,
  pages        = {1243--1249},
  doi          = {10.1209/0295-5075/3/11/015},
  url          = {https://doi.org/10.1209/0295-5075/3/11/015},
  abstract     = {The principle of associative memory is extended to a system with dynamical links capable of retrieval of superimposed connection patterns. The system consists of formalized neurons. Its dynamics is described by two separate Hamiltonians, one for spins and one for links. The spin part is treated in analogy to the Ising system on a 2D grid. Several such network patterns, related by permutations of neurons, are superimposed. Energy minima correspond to the activation of one connection pattern and the deactivation of all others. One important application of this system is invariant pattern recognition.}
}
@article{vonderMalsburg_2018,
  title        = {Concerning the Neuronal Code},
  author       = {{von der Malsburg}, Christoph},
  year         = 2018,
  month        = Dec,
  journal      = {Journal of Cognitive Science},
  volume       = 19,
  number       = 4,
  pages        = {511–550},
  doi          = {10.17791/JCS.2018.19.4.511}
}
@article{freeman1990representations,
  title        = {Representations: Who needs them?},
  author       = {Freeman, Walter J. and Skarda, Christine A.},
  year         = 1990,
  booktitle    = {Brain Organization and Memory},
  publisher    = {Guilford Press},
  editor       = {McGaugh, J. and Weinberger, Jerry and Lynch, G.}
}
@article{Fernandes_vonderMalsburg_2015,
  title        = {Self-Organization of Control Circuits for Invariant Fiber Projections},
  author       = {Fernandes, Tomas and {von der Malsburg}, Christoph},
  year         = 2015,
  month        = May,
  journal      = {Neural Computation},
  volume       = 27,
  number       = 5,
  pages        = {1005–1032},
  doi          = {10.1162/NECO_a_00725},
  issn         = {0899-7667, 1530-888X},
  abstractnote = {Assuming that patterns in memory are represented as two-dimensional arrays of local features, just as they are in primary visual cortices, pattern recognition can take the form of elastic graph matching (Lades et al., 1993 ). Neural implementation of this may be based on preorganized fiber projections that can be activated rapidly with the help of control units (Wolfrum, Wolff, Lücke, & von der Malsburg, 2008 ). Each control unit governs a set of projection fibers that form part of a coherent mapping. We describe a mathematical model for the ontogenesis of the underlying connectivity based on a principle of network self-organization as described by the Häussler system (Häussler & von der Malsburg, 1983 ), modified to be sensitive to pattern similarity and to support formation of multiple mappings, each under the command of a control unit. The process takes the form of a soft-winner-take-all, where units compete for the representation of maps. We show simulations for invariant point-to-point and feature-to-feature mappings.},
  language     = {en}
}
@book{Arathorn_2002,
  title        = {Map-Seeking Circuits in Visual Cognition: A Computational Mechanism for Biological and Machine Vision},
  author       = {Arathorn, David W.},
  year         = 2002,
  publisher    = {Stanford University Press},
  address      = {Stanford, Califorina},
  isbn         = {978-0-8047-4277-1},
  callnumber   = {QP383.15 .A736 2002}
}
@article{Olshausen1995,
  title        = {A Multiscale Dynamic Routing Circuit for Forming Size- And Position-Invariant Object Representations},
  author       = {Olshausen, Bruno A. and Anderson, Charles H. and Van Essen, David C.},
  year         = 1995,
  month        = Mar,
  day          = {01},
  journal      = {Journal of Computational Neuroscience},
  volume       = 2,
  number       = 1,
  pages        = {45--62},
  doi          = {10.1007/BF00962707},
  issn         = {1573-6873},
  url          = {https://doi.org/10.1007/BF00962707},
  abstract     = {We describe a neural model for forming size- and position-invariant representations of visual objects. The model is based on a previously proposed dynamic routing circuit that remaps selected portions of an input array into an object-centered reference frame. Here, we show how a multiscale representation may be incorporated at the input stage of the model, and we describe the control architecture and dynamics for a hierarchical, multistage routing circuit. Specific neurobiological substrates and mechanisms for the model are proposed, and a number of testable predictions are described.}
}
@article{dorigo1997ant,
  title        = {Ant Colony System: A Cooperative Learning Approach to the Traveling Salesman Problem},
  author       = {Dorigo, Marco and Gambardella, Luca Maria},
  year         = 1997,
  month        = {Apr},
  journal      = {IEEE Transactions on Evolutionary Computation},
  volume       = 1,
  number       = 1,
  pages        = {53–66},
  doi          = {10.1109/4235.585892},
  issn         = {1089778X}
}
@book{Hamann_2018,
  title        = {Swarm Robotics: A Formal Approach},
  author       = {Hamann, Heiko},
  year         = 2018,
  publisher    = {Springer International Publishing},
  address      = {Cham},
  doi          = {10.1007/978-3-319-74528-2},
  isbn         = {978-3-319-74526-8},
  url          = {http://link.springer.com/10.1007/978-3-319-74528-2},
  language     = {en}
}
@article{Rubenstein_Cornejo_Nagpal_2014,
  title        = {Programmable Self-Assembly in a Thousand-Robot Swarm},
  author       = {Rubenstein, Michael and Cornejo, Alejandro and Nagpal, Radhika},
  year         = 2014,
  month        = Aug,
  journal      = {Science},
  volume       = 345,
  number       = 6198,
  pages        = {795–799},
  doi          = {10.1126/science.1254295},
  issn         = {0036-8075, 1095-9203},
  language     = {en}
}
@inproceedings{Wulff1992LearningCA,
  title        = {Learning Cellular Automaton Dynamics with Neural Networks},
  author       = {Wulff, N. and Hertz, J. A.},
  year         = 1992,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  publisher    = {Morgan-Kaufmann},
  volume       = 5,
  pages        = {},
  url          = {https://proceedings.neurips.cc/paper/1992/file/d6c651ddcd97183b2e40bc464231c962-Paper.pdf},
  editor       = {S. Hanson and J. Cowan and C. Giles}
}
@inbook{Byla_Pang_2020,
  title        = {DeepSwarm: Optimising Convolutional Neural Networks Using Swarm Intelligence},
  author       = {Byla, Edvinas and Pang, Wei},
  year         = 2020,
  booktitle    = {Advances in Computational Intelligence Systems},
  publisher    = {Springer International Publishing},
  address      = {Cham},
  series       = {Advances in Intelligent Systems and Computing},
  volume       = 1043,
  pages        = {119–130},
  doi          = {10.1007/978-3-030-29933-0_10},
  isbn         = {978-3-030-29932-3},
  url          = {http://link.springer.com/10.1007/978-3-030-29933-0\%5F10},
  editor       = {Ju, Zhaojie and Yang, Longzhi and Yang, Chenguang and Gegov, Alexander and Zhou, Dalin},
  collection   = {Advances in Intelligent Systems and Computing},
  language     = {en}
}
@article{Wolfram1984,
  title        = {Cellular Automata as Models of Complexity},
  author       = {Wolfram, Stephen},
  year         = 1984,
  month        = Oct,
  day          = {01},
  journal      = {Nature},
  volume       = 311,
  number       = 5985,
  pages        = {419--424},
  doi          = {10.1038/311419a0},
  issn         = {1476-4687},
  url          = {https://doi.org/10.1038/311419a0},
  abstract     = {Natural systems from snowflakes to mollusc shells show a great diversity of complex patterns. The origins of such complexity can be investigated through mathematical models termed `cellular automata'. Cellular automata consist of many identical components, each simple., but together capable of complex behaviour. They are analysed both as discrete dynamical systems, and as information-processing systems. Here some of their universal features are discussed, and some general principles are suggested.}
}
@article{VICHNIAC198496,
  title        = {Simulating Physics With Cellular Automata},
  author       = {Vichniac, Gérard Y.},
  year         = 1984,
  journal      = {Physica D: Nonlinear Phenomena},
  volume       = 10,
  number       = 1,
  pages        = {96--116},
  doi          = {https://doi.org/10.1016/0167-2789(84)90253-7},
  issn         = {0167-2789},
  url          = {https://www.sciencedirect.com/science/article/pii/0167278984902537},
  abstract     = {Cellular automata are dynamical systems where space, time, and variables are discrete. They are shown on two-dimensional examples to be capable of non-numerical simulations of physics. They are useful for faithful parallel processing of lattice models. At another level, they exhibit behaviours and illustrate concepts that are unmistakably physical, such as non-ergodicity and order parameters, frustration, relaxation to chaos through period doublings, a conspicuous arrow of time in reversible microscopic dynamics, causality and light-cone, and non-separability. In general, they constitute exactly computable models for complex phenomena and large-scale correlations that result from very simple short-range interactions. We study their space, time, and intrinsic symmetries and the corresponding conservation laws, with an emphasis on the conservation of information obeyed by reversible cellular automata.}
}
@article{science.1372754,
  title        = {Selection of Intrinsic Horizontal Connections in the Visual Cortex by Correlated Neuronal Activity},
  author       = {Löwel, Siegrid and Singer, Wolf},
  year         = 1992,
  journal      = {Science},
  volume       = 255,
  number       = 5041,
  pages        = {209--212},
  doi          = {10.1126/science.1372754},
  url          = {https://www.science.org/doi/abs/10.1126/science.1372754},
  eprint       = {https://www.science.org/doi/pdf/10.1126/science.1372754}
}
@article{PhysRevE,
  title        = {Cellular Automata as Convolutional Neural Networks},
  author       = {Gilpin, William},
  year         = 2019,
  month        = Sep,
  journal      = {Physical Review E},
  publisher    = {American Physical Society},
  volume       = 100,
  number       = 3,
  pages        = {032402},
  doi          = {10.1103/PhysRevE.100.032402},
  issn         = {2470-0045, 2470-0053},
  url          = {https://link.aps.org/doi/10.1103/PhysRevE.100.032402},
  issue        = 3,
  numpages     = 11
}
@article{Mordvintsev_Randazzo_Fouts_2022,
  title        = {Growing Isotropic Neural Cellular Automata},
  author       = {Mordvintsev, Alexander and Randazzo, Ettore and Niklasson, Eyvind and Levin, Michael},
  year         = 2020,
  journal      = {Distill},
  doi          = {10.23915/distill.00023},
}
@article{Palm_GonDuque_Sudhakaran_Risi_2022,
  title        = {Variational Neural Cellular Automata},
  author       = {Palm, Rasmus B. and González-Duque, Miguel and Sudhakaran, Shyam and Risi, Sebastian},
  year         = 2022,
  month        = Feb,
  publisher    = {arXiv},
  number       = {arXiv:2201.12360},
  doi          = {0.48550/arXiv.2201.12360},
  url          = {http://arxiv.org/abs/2201.12360},
  abstractnote = {In nature, the process of cellular growth and differentiation has lead to an amazing diversity of organisms -- algae, starfish, giant sequoia, tardigrades, and orcas are all created by the same generative process. Inspired by the incredible diversity of this biological generative process, we propose a generative model, the Variational Neural Cellular Automata (VNCA), which is loosely inspired by the biological processes of cellular growth and differentiation. Unlike previous related works, the VNCA is a proper probabilistic generative model, and we evaluate it according to best practices. We find that the VNCA learns to reconstruct samples well and that despite its relatively few parameters and simple local-only communication, the VNCA can learn to generate a large variety of output from information encoded in a common vector format. While there is a significant gap to the current state-of-the-art in terms of generative modeling performance, we show that the VNCA can learn a purely self-organizing generative process of data. Additionally, we show that the VNCA can learn a distribution of stable attractors that can recover from significant damage.}
}
@article{Kingma_Welling_2014,
  title        = {Auto-Encoding Variational Bayes},
  author       = {Kingma, Diederik P. and Welling, Max},
  year         = 2014,
  month        = May,
  publisher    = {arXiv},
  number       = {arXiv:1312.6114},
  doi          = {10.48550/arXiv.1312.6114},
  url          = {http://arxiv.org/abs/1312.6114},
  abstractnote = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.}
}
@article{Sudhakaran_Grbic_Li_Katona_Najarro_Glanois_Risi_2021,
  title        = {Growing 3D Artefacts and Functional Machines with Neural Cellular Automata},
  author       = {Sudhakaran, Shyam and Grbic, Djordje and Li, Siyan and Katona, Adam and Najarro, Elias and Glanois, Claire and Risi, Sebastian},
  year         = 2021,
  month        = Jun,
  publisher    = {arXiv},
  number       = {arXiv:2103.08737},
  doi          = {10.48550/arXiv.2103.08737},
  url          = {http://arxiv.org/abs/2103.08737},
  abstractnote = {Neural Cellular Automata (NCAs) have been proven effective in simulating morphogenetic processes, the continuous construction of complex structures from very few starting cells. Recent developments in NCAs lie in the 2D domain, namely reconstructing target images from a single pixel or infinitely growing 2D textures. In this work, we propose an extension of NCAs to 3D, utilizing 3D convolutions in the proposed neural network architecture. Minecraft is selected as the environment for our automaton since it allows the generation of both static structures and moving machines. We show that despite their simplicity, NCAs are capable of growing complex entities such as castles, apartment blocks, and trees, some of which are composed of over 3,000 blocks. Additionally, when trained for regeneration, the system is able to regrow parts of simple functional machines, significantly expanding the capabilities of simulated morphogenetic systems. The code for the experiment in this paper can be found at: https://github.com/real-itu/3d-artefacts-nca.}
}
@inbook{Horibe_Walker_Risi_2021,
  title        = {Regenerating Soft Robots Through Neural Cellular Automata},
  author       = {Horibe, Kazuya and Walker, Kathryn and Risi, Sebastian},
  year         = 2021,
  booktitle    = {Genetic Programming},
  publisher    = {Springer International Publishing},
  address      = {Cham},
  series       = {Lecture Notes in Computer Science},
  volume       = 12691,
  pages        = {36–50},
  doi          = {10.1007/978-3-030-72812-0_3},
  isbn         = {978-3-030-72811-3},
  url          = {http://link.springer.com/10.1007/978-3-030-72812-0\%5F3},
  editor       = {Hu, Ting and Lourenço, Nuno and Medvet, Eric},
  collection   = {Lecture Notes in Computer Science},
  language     = {en}
}
@article{Grattarola_Livi_Alippi_2021,
  title        = {Learning Graph Cellular Automata},
  author       = {Grattarola, Daniele and Livi, Lorenzo and Alippi, Cesare},
  year         = 2021,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  publisher    = {Curran Associates, Inc.},
  volume       = 34,
  pages        = {20983--20994},
  url          = {https://proceedings.neurips.cc/paper/2021/file/af87f7cdcda223c41c3f3ef05a3aaeea-Paper.pdf},
  editor       = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan}
}
@article{Zhou_Cui_Hu_Zhang_Yang_Liu_Wang_Li_Sun_2021,
  title        = {Graph Neural Networks: A Review of Methods and Applications},
  author       = {Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  year         = 2020,
  journal      = {AI Open},
  volume       = 1,
  pages        = {57–81},
  doi          = {10.1016/j.aiopen.2021.01.001},
  issn         = 26666510,
  language     = {en}
}
@inproceedings{NEURIPS2020_ee23e7ad,
  title        = {Meta-Learning through Hebbian Plasticity in Random Networks},
  author       = {Najarro, Elias and Risi, Sebastian},
  year         = 2020,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  publisher    = {Curran Associates, Inc.},
  volume       = 33,
  pages        = {20719--20731},
  url          = {https://proceedings.neurips.cc/paper/2020/file/ee23e7ad9b473ad072d57aaa9b2a5222-Paper.pdf},
  editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin}
}
@inproceedings{Pedersen_Risi_2021,
  title        = {Evolving and Merging Hebbian Learning Rules: Increasing Generalization by Decreasing the Number of Rules},
  author       = {Pedersen, Joachim W. and Risi, Sebastian},
  year         = 2021,
  month        = Jun,
  booktitle    = {Proceedings of the Genetic and Evolutionary Computation Conference},
  pages        = {892–900},
  doi          = {10.1145/3449639.3459317},
  url          = {http://arxiv.org/abs/2104.07959},
  abstractnote = {Generalization to out-of-distribution (OOD) circumstances after training remains a challenge for artificial agents. To improve the robustness displayed by plastic Hebbian neural networks, we evolve a set of Hebbian learning rules, where multiple connections are assigned to a single rule. Inspired by the biological phenomenon of the genomic bottleneck, we show that by allowing multiple connections in the network to share the same local learning rule, it is possible to drastically reduce the number of trainable parameters, while obtaining a more robust agent. During evolution, by iteratively using simple K-Means clustering to combine rules, our Evolve and Merge approach is able to reduce the number of trainable parameters from 61,440 to 1,920, while at the same time improving robustness, all without increasing the number of generations used. While optimization of the agents is done on a standard quadruped robot morphology, we evaluate the agents’ performances on slight morphology modifications in a total of 30 unseen morphologies. Our results add to the discussion on generalization, overfitting and OOD adaptation. To create agents that can adapt to a wider array of unexpected situations, Hebbian learning combined with a regularising “genomic bottleneck” could be a promising research direction.}
}
@inproceedings{kirsch2021meta,
  title        = {Meta Learning Backpropagation And Improving It},
  author       = {Kirsch, Louis and Schmidhuber, Jürgen},
  year         = 2021,
  year         = 2021,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume       = 34,
  pages        = {14122--14134}
}
@article{Variengien_Nichele_Glover_Pontes_Filho_2021,
  title        = {Towards self-organized control: Using neural cellular automata to robustly control a cart-pole agent},
  author       = {Variengien, Alexandre and Nichele, Stefano and Glover, Tom and Pontes-Filho, Sidney},
  year         = 2021,
  month        = Jul,
  publisher    = {arXiv},
  number       = {arXiv:2106.15240},
  doi          = {10.48550/arXiv.2106.15240},
  url          = {http://arxiv.org/abs/2106.15240},
  abstractnote = {Neural cellular automata (Neural CA) are a recent framework used to model biological phenomena emerging from multicellular organisms. In these systems, artificial neural networks are used as update rules for cellular automata. Neural CA are end-to-end differentiable systems where the parameters of the neural network can be learned to achieve a particular task. In this work, we used neural CA to control a cart-pole agent. The observations of the environment are transmitted in input cells, while the values of output cells are used as a readout of the system. We trained the model using deep-Q learning, where the states of the output cells were used as the Q-value estimates to be optimized. We found that the computing abilities of the cellular automata were maintained over several hundreds of thousands of iterations, producing an emergent stable behavior in the environment it controls for thousands of steps. Moreover, the system demonstrated life-like phenomena such as a developmental phase, regeneration after damage, stability despite a noisy environment, and robustness to unseen disruption such as input deletion.}
}
@article{Mnih_Kavukcuoglu_Silver_Graves_Antonoglou_Wierstra_Riedmiller_2013,
  title        = {Playing Atari with Deep Reinforcement Learning},
  author       = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year         = 2013,
  month        = Dec,
  publisher    = {arXiv},
  number       = {arXiv:1312.5602},
  doi          = {10.48550/arXiv.1312.5602},
  url          = {http://arxiv.org/abs/1312.5602},
  abstractnote = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.}
}
@article{risi2021selfassemblingAI,
  title        = {The Future of Artificial Intelligence is Self-Organizing and Self-Assembling},
  author       = {Risi, Sebastian},
  year         = 2021,
  journal      = {sebastianrisi.com},
  url          = {https://sebastianrisi.com/self\%5Fassembling\%5Fai}
}
@article{8259423,
  title        = {Loihi: A Neuromorphic Manycore Processor with On-Chip Learning},
  author       = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and McCoy, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},
  year         = 2018,
  journal      = {IEEE Micro},
  volume       = 38,
  number       = 1,
  pages        = {82--99},
  doi          = {10.1109/MM.2018.112130359}
}
@article{masquelier2007unsupervised,
  title        = {Unsupervised Learning of Visual Features through Spike Timing Dependent Plasticity},
  author       = {Masquelier, Timothée and Thorpe, Simon J.},
  year         = 2007,
  month        = {Feb},
  journal      = {PLoS Computational Biology},
  volume       = 3,
  number       = 2,
  pages        = {e31},
  doi          = {10.1371/journal.pcbi.0030031},
  issn         = {1553-7358},
  editor       = {Friston, Karl J},
  language     = {en}
}
@article{6469239,
  title        = {Rapid Feedforward Computation by Temporal Encoding and Learning With Spiking Neurons},
  author       = {Yu, Qiang and Tang, Huajin and Tan, Kay Chen and Li, Haizhou},
  year         = 2013,
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  volume       = 24,
  number       = 10,
  pages        = {1539--1552},
  doi          = {10.1109/TNNLS.2013.2245677}
}
@article{mozafari2019bio,
  title        = {Bio-Inspired Digit Recognition Using Reward-Modulated Spike-Timing-Dependent Plasticity in Deep Convolutional Networks},
  author       = {Mozafari, Milad and Ganjtabesh, Mohammad and Nowzari-Dalini, Abbas and Thorpe, Simon J. and Masquelier, Timothée},
  year         = 2019,
  month        = {Oct},
  journal      = {Pattern Recognition},
  volume       = 94,
  pages        = {87–95},
  doi          = {10.1016/j.patcog.2019.05.015},
  issn         = {00313203},
  language     = {en}
}
@article{diehl2015unsupervised,
  title        = {Unsupervised Learning of Digit Recognition Using Spike-Timing-Dependent Plasticity},
  author       = {Diehl, Peter U. and Cook, Matthew},
  year         = 2015,
  month        = {Aug},
  journal      = {Frontiers in Computational Neuroscience},
  volume       = 9,
  doi          = {10.3389/fncom.2015.00099},
  issn         = {1662-5188},
  url          = {http://journal.frontiersin.org/Article/10.3389/fncom.2015.00099/abstract}
}
@article{cao2015spiking,
  title        = {Spiking Deep Convolutional Neural Networks for Energy-Efficient Object Recognition},
  author       = {Cao, Yongqiang and Chen, Yang and Khosla, Deepak},
  year         = 2015,
  month        = {May},
  journal      = {International Journal of Computer Vision},
  volume       = 113,
  number       = 1,
  pages        = {54–66},
  doi          = {10.1007/s11263-014-0788-3},
  issn         = {0920-5691, 1573-1405},
  language     = {en}
}
@article{tavanaei2016bio,
  title        = {Bio-Inspired Spiking Convolutional Neural Network using Layer-wise Sparse Coding and STDP Learning},
  author       = {Tavanaei, Amirhossein and Maida, Anthony S.},
  year         = 2017,
  month        = {Jun},
  publisher    = {arXiv},
  number       = {arXiv:1611.03000},
  doi          = {10.48550/arXiv.1611.03000},
  url          = {http://arxiv.org/abs/1611.03000},
  abstractnote = {Hierarchical feature discovery using non-spiking convolutional neural networks (CNNs) has attracted much recent interest in machine learning and computer vision. However, it is still not well understood how to create a biologically plausible network of brain-like, spiking neurons with multi-layer, unsupervised learning. This paper explores a novel bio-inspired spiking CNN that is trained in a greedy, layer-wise fashion. The proposed network consists of a spiking convolutional-pooling layer followed by a feature discovery layer extracting independent visual features. Kernels for the convolutional layer are trained using local learning. The learning is implemented using a sparse, spiking auto-encoder representing primary visual features. The feature discovery layer extracts independent features by probabilistic, leaky integrate-and-fire (LIF) neurons that are sparsely active in response to stimuli. The layer of the probabilistic, LIF neurons implicitly provides lateral inhibitions to extract sparse and independent features. Experimental results show that the convolutional layer is stack-admissible, enabling it to support a multi-layer learning. The visual features obtained from the proposed probabilistic LIF neurons in the feature discovery layer are utilized for training a classifier. Classification results contribute to the independent and informative visual features extracted in a hierarchy of convolutional and feature discovery layers. The proposed model is evaluated on the MNIST digit dataset using clean and noisy images. The recognition performance for clean images is above 98%. The performance loss for recognizing the noisy images is in the range 0.1% to 8.5% depending on noise types and densities. This level of performance loss indicates that the network is robust to additive noise.}
}
@article{ferre2018unsupervised,
  title        = {Unsupervised Feature Learning With Winner-Takes-All Based STDP},
  author       = {Ferré, Paul and Mamalet, Franck and Thorpe, Simon J.},
  year         = 2018,
  month        = {Apr},
  journal      = {Frontiers in Computational Neuroscience},
  volume       = 12,
  pages        = 24,
  doi          = {10.3389/fncom.2018.00024},
  issn         = {1662-5188}
}
@article{mozafari2018first,
  title        = {First-Spike-Based Visual Categorization Using Reward-Modulated STDP},
  author       = {Mozafari, Milad and Kheradpisheh, Saeed Reza and Masquelier, Timothee and Nowzari-Dalini, Abbas and Ganjtabesh, Mohammad},
  year         = 2018,
  month        = {Dec},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  volume       = 29,
  number       = 12,
  pages        = {6178–6190},
  doi          = {10.1109/TNNLS.2018.2826721},
  issn         = {2162-237X, 2162-2388}
}
@inproceedings{diehl2015fast,
  title        = {Fast-Classifying, High-Accuracy Spiking Deep Networks through Weight and Threshold Balancing},
  author       = {Diehl, Peter U. and Neil, Daniel and Binas, Jonathan and Cook, Matthew and Liu, Shih-Chii and Pfeiffer, Michael},
  year         = 2015,
  month        = {Jul},
  booktitle    = {2015 International Joint Conference on Neural Networks (IJCNN)},
  publisher    = {IEEE},
  address      = {Killarney, Ireland},
  pages        = {1–8},
  doi          = {10.1109/IJCNN.2015.7280696},
  isbn         = {978-1-4799-1960-4},
  url          = {http://ieeexplore.ieee.org/document/7280696/}
}
@article{zenke2018superspike,
  title        = {SuperSpike: Supervised Learning in Multilayer Spiking Neural Networks},
  author       = {Zenke, Friedemann and Ganguli, Surya},
  year         = 2018,
  month        = {Jun},
  journal      = {Neural Computation},
  volume       = 30,
  number       = 6,
  pages        = {1514–1541},
  doi          = {10.1162/neco_a_01086},
  issn         = {0899-7667, 1530-888X},
  abstractnote = {A vast majority of computation in the brain is performed by spiking neural networks. Despite the ubiquity of such spiking, we currently lack an understanding of how biological spiking neural circuits learn and compute in vivo, as well as how we can instantiate such capabilities in artificial spiking circuits in silico. Here we revisit the problem of supervised learning in temporally coding multilayer spiking neural networks. First, by using a surrogate gradient approach, we derive SuperSpike, a nonlinear voltage-based three-factor learning rule capable of training multilayer networks of deterministic integrate-and-fire neurons to perform nonlinear computations on spatiotemporal spike patterns. Second, inspired by recent results on feedback alignment, we compare the performance of our learning rule under different credit assignment strategies for propagating output errors to hidden units. Specifically, we test uniform, symmetric, and random feedback, finding that simpler tasks can be solved with any type of feedback, while more complex tasks require symmetric feedback. In summary, our results open the door to obtaining a better scientific understanding of learning and computation in spiking neural networks by advancing our ability to train them to solve nonlinear problems involving transformations between different spatiotemporal spike time patterns.},
  language     = {en}
}
@article{Raghavan_Lin_Thomson_2020,
  title        = {Self-Organization of Multi-Layer Spiking Neural Networks},
  author       = {Raghavan, Guruprasad and Lin, Cong and Thomson, Matt},
  year         = 2020,
  month        = Jun,
  publisher    = {arXiv},
  number       = {arXiv:2006.06902},
  doi          = {10.48550/arXiv.2006.06902},
  url          = {http://arxiv.org/abs/2006.06902},
  abstractnote = {Living neural networks in our brains autonomously self-organize into large, complex architectures during early development to result in an organized and functional organic computational device. A key mechanism that enables the formation of complex architecture in the developing brain is the emergence of traveling spatio-temporal waves of neuronal activity across the growing brain. Inspired by this strategy, we attempt to efficiently self-organize large neural networks with an arbitrary number of layers into a wide variety of architectures. To achieve this, we propose a modular tool-kit in the form of a dynamical system that can be seamlessly stacked to assemble multi-layer neural networks. The dynamical system encapsulates the dynamics of spiking units, their inter/intra layer interactions as well as the plasticity rules that control the flow of information between layers. The key features of our tool-kit are (1) autonomous spatio-temporal waves across multiple layers triggered by activity in the preceding layer and (2) Spike-timing dependent plasticity (STDP) learning rules that update the inter-layer connectivity based on wave activity in the connecting layers. Our framework leads to the self-organization of a wide variety of architectures, ranging from multi-layer perceptrons to autoencoders. We also demonstrate that emergent waves can self-organize spiking network architecture to perform unsupervised learning, and networks can be coupled with a linear classifier to perform classification on classic image datasets like MNIST. Broadly, our work shows that a dynamical systems framework for learning can be used to self-organize large computational devices.}
}
@article{Vaila_Chiasson_Saxena_2019,
  title        = {Deep Convolutional Spiking Neural Networks for Image Classification},
  author       = {Vaila, Ruthvik and Chiasson, John and Saxena, Vishal},
  year         = 2019,
  month        = Sep,
  publisher    = {arXiv},
  number       = {arXiv:1903.12272},
  doi          = {10.48550/arXiv.1903.12272},
  url          = {http://arxiv.org/abs/1903.12272},
  abstractnote = {Spiking neural networks are biologically plausible counterparts of the artificial neural networks, artificial neural networks are usually trained with stochastic gradient descent and spiking neural networks are trained with spike timing dependant plasticity. Training deep convolutional neural networks is a memory and power intensive job. Spiking networks could potentially help in reducing the power usage. There is a large pool of tools for one to chose to train artificial neural networks of any size, on the other hand all the available tools to simulate spiking neural networks are geared towards computational neuroscience applications and they are not suitable for real life applications. In this work we focus on implementing a spiking CNN using Tensorflow to examine behaviour of the network and empirically study the effect of various parameters on learning capabilities and also study catastrophic forgetting in the spiking CNN and weight initialization problem in R-STDP using MNIST and N-MNIST data sets.}
}
@inproceedings{Raghavan2019NeuralNG,
  title        = {Neural Networks Grown and Self-Organized by Noise},
  author       = {Raghavan, Guruprasad and Thomson, Matt},
  year         = 2019,
  journal      = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume       = 32
}
@article{Kohonen_1982,
  title        = {Self-Organized Formation of Topologically Correct Feature Maps},
  author       = {Kohonen, Teuvo},
  year         = 1982,
  journal      = {Biological Cybernetics},
  volume       = 43,
  number       = 1,
  pages        = {59–69},
  doi          = {10.1007/BF00337288},
  issn         = {0340-1200, 1432-0770},
  language     = {en}
}
@book{Kohonen_1989,
  title        = {Self-Organization and Associative Memory},
  author       = {Kohonen, Teuvo},
  year         = 1989,
  publisher    = {Springer Berlin Heidelberg},
  address      = {Berlin, Heidelberg},
  isbn         = {978-3-642-88163-3},
  edition      = {Third edition},
  abstractnote = {This monograph gives a tutorial treatment of new approaches to self-organization, adaptation, learning and memory. It is based on recent research results, both mathematical and computer simulations, and lends itself to graduate and postgraduate courses in the natural sciences. The book presents new formalisms of pattern processing: orthogonal projectors, optimal associative mappings, novelty filters, subspace methods, feature-sensitive units, and self-organization of topological maps, with all their computable algorithms. The main objective is to provide an understanding of the properties of information representations from a general point of view and of their use in pattern information processing, as well as an understanding of many functions of the brain. In the third edition two new discussions have been added and a proof has been revised. The author has developed this book from Associative Memory - A System-Theoretical Approach (Volume 17 of Springer Series in Communication and Cybernetics, 1977), the first ever monograph on distributed associative memories},
  language     = {eng}
}
@article{Marsland_Shapiro_Nehmzow_2002,
  title        = {A Self-Organising Network That Grows When Required},
  author       = {Marsland, Stephen and Shapiro, Jonathan and Nehmzow, Ulrich},
  year         = 2002,
  month        = Oct,
  journal      = {Neural Networks},
  volume       = 15,
  number       = {8–9},
  pages        = {1041–1058},
  doi          = {10.1016/S0893-6080(02)00078-3},
  issn         = {08936080},
  language     = {en}
}
@inproceedings{NIPS1994_d56b9fc4,
  title        = {A Growing Neural Gas Network Learns Topologies},
  author       = {Fritzke, Bernd},
  year         = 1994,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  publisher    = {MIT Press},
  volume       = 7,
  pages        = {},
  url          = {https://proceedings.neurips.cc/paper/1994/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf},
  editor       = {G. Tesauro and D. Touretzky and T. Leen}
}
@article{Reilly_Cooper_Elbaum_1982,
  title        = {A Neural Model for Category Learning},
  author       = {Reilly, Douglas L. and Cooper, Leon N. and Elbaum, Charles},
  year         = 1982,
  month        = Aug,
  journal      = {Biological Cybernetics},
  volume       = 45,
  number       = 1,
  pages        = {35–41},
  doi          = {10.1007/BF00387211},
  issn         = {0340-1200, 1432-0770},
  language     = {en}
}
@article{Fritzke_1994,
  title        = {Growing Cell Structures: A self-Organizing Network for Unsupervised and Supervised Learning},
  author       = {Fritzke, Bernd},
  year         = 1994,
  month        = Jan,
  journal      = {Neural Networks},
  volume       = 7,
  number       = 9,
  pages        = {1441–1460},
  doi          = {10.1016/0893-6080(94)90091-4},
  issn         = {08936080},
  language     = {en}
}
@article{Mici_Parisi_Wermter_2018,
  title        = {A Self-Organizing Neural Network Architecture for Learning Human-Object Interactions},
  author       = {Mici, Luiza and Parisi, German I. and Wermter, Stefan},
  year         = 2018,
  month        = Sep,
  journal      = {Neurocomputing},
  volume       = 307,
  pages        = {14–24},
  doi          = {10.1016/j.neucom.2018.04.015},
  issn         = {09252312},
  language     = {en}
}
@article{LeCun_AMI,
  title        = {A Path Towards Autonomous Machine Intelligence},
  author       = {LeCun, Yann},
  year         = 2022,
  month        = Jun,
  journal      = {Open Review},
  volume       = 62
}
@article{Keurti_Pan_Besserve_Grewe_Schölkopf_2022,
  title        = {Homomorphism Autoencoder -- Learning Group Structured Representations from Observed Transitions},
  author       = {Keurti, Hamza and Pan, Hsiao-Ru and Besserve, Michel and Grewe, Benjamin F. and Schölkopf, Bernhard},
  year         = 2022,
  month        = Jul,
  publisher    = {arXiv},
  number       = {arXiv:2207.12067},
  doi          = {10.48550/arXiv.2207.12067},
  url          = {http://arxiv.org/abs/2207.12067},
  abstractnote = {How can we acquire world models that veridically represent the outside world both in terms of what is there and in terms of how our actions affect it? Can we acquire such models by interacting with the world, and can we state mathematical desiderata for their relationship with a hypothetical reality existing outside our heads? As machine learning is moving towards representations containing not just observational but also interventional knowledge, we study these problems using tools from representation learning and group theory. Under the assumption that our actuators act upon the world, we propose methods to learn internal representations of not just sensory information but also of actions that modify our sensory representations in a way that is consistent with the actions and transitions in the world. We use an autoencoder equipped with a group representation linearly acting on its latent space, trained on 2-step reconstruction such as to enforce a suitable homomorphism property on the group representation. Compared to existing work, our approach makes fewer assumptions on the group representation and on which transformations the agent can sample from the group. We motivate our method theoretically, and demonstrate empirically that it can learn the correct representation of the groups and the topology of the environment. We also compare its performance in trajectory prediction with previous methods.}
}
@article{Keller_Bonhoeffer_Hübener_2012,
  title        = {Sensorimotor Mismatch Signals in Primary Visual Cortex of the Behaving Mouse},
  author       = {Keller, Georg B. and Bonhoeffer, Tobias and Hübener, Mark},
  year         = 2012,
  month        = Jun,
  journal      = {Neuron},
  volume       = 74,
  number       = 5,
  pages        = {809–815},
  doi          = {10.1016/j.neuron.2012.03.040},
  issn         = {08966273},
  language     = {en}
}
@article{Dosovitskiy_Beyer_Kolesnikov_Weissenborn_Zhai_Unterthiner_Dehghani_Minderer_Heigold_Gelly_2021,
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author       = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year         = 2021,
  journal      = {International Conference on Learning Representations (ICLR)}
}
@article{Sager_Salzmann_Burn_Stadelmann_2022,
  title        = {Unsupervised Domain Adaptation for Vertebrae Detection and Identification in 3D CT Volumes Using a Domain Sanity Loss},
  author       = {Sager, Pascal and Salzmann, Sebastian and Burn, Felice and Stadelmann, Thilo},
  year         = 2022,
  month        = Aug,
  journal      = {Journal of Imaging},
  volume       = 8,
  number       = 8,
  pages        = 222,
  doi          = {10.3390/jimaging8080222},
  issn         = {2313-433X},
  rights       = {All rights reserved},
  abstractnote = {A variety of medical computer vision applications analyze 2D slices of computed tomography (CT) scans, whereas axial slices from the body trunk region are usually identified based on their relative position to the spine. A limitation of such systems is that either the correct slices must be extracted manually or labels of the vertebrae are required for each CT scan to develop an automated extraction system. In this paper, we propose an unsupervised domain adaptation (UDA) approach for vertebrae detection and identification based on a novel Domain Sanity Loss (DSL) function. With UDA the model’s knowledge learned on a publicly available (source) data set can be transferred to the target domain without using target labels, where the target domain is defined by the specific setup (CT modality, study protocols, applied pre- and processing) at the point of use (e.g., a specific clinic with its specific CT study protocols). With our approach, a model is trained on the source and target data set in parallel. The model optimizes a supervised loss for labeled samples from the source domain and the DSL loss function based on domain-specific “sanity checks” for samples from the unlabeled target domain. Without using labels from the target domain, we are able to identify vertebra centroids with an accuracy of 72.8%. By adding only ten target labels during training the accuracy increases to 89.2%, which is on par with the current state-of-the-art for full supervised learning, while using about 20 times less labels. Thus, our model can be used to extract 2D slices from 3D CT scans on arbitrary data sets fully automatically without requiring an extensive labeling effort, contributing to the clinical adoption of medical imaging by hospitals.},
  language     = {en}
}
@inbook{Ronneberger_Fischer_Brox_2015,
  title        = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  author       = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year         = 2015,
  booktitle    = {Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015},
  publisher    = {Springer International Publishing},
  address      = {Cham},
  series       = {Lecture Notes in Computer Science},
  volume       = 9351,
  pages        = {234–241},
  doi          = {10.1007/978-3-319-24574-4_28},
  isbn         = {978-3-319-24573-7},
  url          = {http://link.springer.com/10.1007/978-3-319-24574-4\%5F28},
  editor       = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  collection   = {Lecture Notes in Computer Science},
  language     = {en}
}
@article{Jafari_Khouzani_Soltanian_Zadeh_2005,
  title        = {Radon Transform Orientation Estimation for Rotation Invariant Texture Analysis},
  author       = {Jafari-Khouzani, Kourosh and Soltanian-Zadeh, Hamid},
  year         = 2005,
  month        = Jun,
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume       = 27,
  number       = 6,
  pages        = {1004–1008},
  doi          = {10.1109/TPAMI.2005.126},
  issn         = {0162-8828, 2160-9292}
}
@article{Ojala_Pietikainen_Maenpaa_2002,
  title        = {Multiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns},
  author       = {Ojala, Timo and Pietikainen, Matti and Maenpaa, Topi},
  year         = 2002,
  month        = Jul,
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume       = 24,
  number       = 7,
  pages        = {971–987},
  doi          = {10.1109/TPAMI.2002.1017623},
  issn         = {0162-8828},
  language     = {en}
}
@article{Wen_RongWu_ShiehChungWei_1996,
  title        = {Rotation and Gray-Scale Transform-Invariant Texture Classification Using Spiral Resampling, Subband Decomposition, and Hidden Markov Model},
  author       = {Wen-Rong Wu and Shieh-Chung Wei},
  year         = 1996,
  month        = Oct,
  journal      = {IEEE Transactions on Image Processing},
  volume       = 5,
  number       = 10,
  pages        = {1423–1434},
  doi          = {10.1109/83.536891},
  issn         = {1057-7149, 1941-0042}
}
@inproceedings{Simard_Steinkraus_Platt_2003,
  title        = {Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis},
  author       = {Simard, Patrice Y. and Steinkraus, David and Platt, John C.},
  year         = 2003,
  booktitle    = {Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings.},
  publisher    = {IEEE Comput. Soc},
  address      = {Edinburgh, UK},
  volume       = 1,
  pages        = {958–963},
  doi          = {10.1109/ICDAR.2003.1227801},
  isbn         = {978-0-7695-1960-9},
  url          = {http://ieeexplore.ieee.org/document/1227801/}
}
@inproceedings{Fasel_Gatica_Perez_2006,
  title        = {Rotation-Invariant Neoperceptron},
  author       = {Fasel, Beat and Gatica-Perez, Daniel},
  year         = 2006,
  booktitle    = {18th International Conference on Pattern Recognition (ICPR’06)},
  publisher    = {IEEE},
  address      = {Hong Kong, China},
  pages        = {336–339},
  doi          = {10.1109/ICPR.2006.1020},
  isbn         = {978-0-7695-2521-1},
  url          = {http://ieeexplore.ieee.org/document/1699534/}
}
@inproceedings{Schmidt_Roth_2012,
  title        = {Learning Rotation-Aware Features: From Invariant Priors to Equivariant Descriptors},
  author       = {Schmidt, Uwe and Roth, Stefan},
  year         = 2012,
  month        = Jun,
  booktitle    = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
  publisher    = {IEEE},
  address      = {Providence, RI},
  pages        = {2050–2057},
  doi          = {10.1109/CVPR.2012.6247909},
  isbn         = {978-1-4673-1228-8},
  url          = {http://ieeexplore.ieee.org/document/6247909/}
}
@inbook{Kivinen_Williams_2011,
  title        = {Transformation Equivariant Boltzmann Machines},
  author       = {Kivinen, Jyri J. and Williams, Christopher K. I.},
  year         = 2011,
  booktitle    = {Artificial Neural Networks and Machine Learning (ICANN)},
  publisher    = {Springer Berlin Heidelberg},
  address      = {Berlin, Heidelberg},
  series       = {Lecture Notes in Computer Science},
  volume       = 6791,
  pages        = {1–9},
  doi          = {10.1007/978-3-642-21735-7_1},
  isbn         = {978-3-642-21734-0},
  url          = {http://link.springer.com/10.1007/978-3-642-21735-7\%5F1},
  editor       = {Honkela, Timo and Duch, Włodzisław and Girolami, Mark and Kaski, Samuel},
  collection   = {Lecture Notes in Computer Science}
}
@inproceedings{Lee_Grosse_Ranganath_Ng_2009,
  title        = {Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations},
  author       = {Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y.},
  year         = 2009,
  booktitle    = {International Conference on Machine Learning (ICML)},
  publisher    = {ACM Press},
  address      = {Montreal, Quebec, Canada},
  pages        = {1–8},
  doi          = {10.1145/1553374.1553453},
  isbn         = {978-1-60558-516-1},
  url          = {http://portal.acm.org/citation.cfm?doid=1553374.1553453},
  language     = {en}
}
@book{Costandi_2016,
  title        = {Neuroplasticity},
  author       = {Costandi, Moheb},
  year         = 2016,
  publisher    = {The MIT Press},
  address      = {Cambridge, MA},
  series       = {The MIT Press essential knowledge series},
  isbn         = {978-0-262-52933-4},
  callnumber   = {QP364.5 .C67 2016},
  collection   = {The MIT Press essential knowledge series}
}
@article{Takagi_2000,
  title        = {Roles of Ion Channels in EPSP Integration at Neuronal Dendrites},
  author       = {Takagi, Hiroshi},
  year         = 2000,
  month        = Jul,
  journal      = {Neuroscience Research},
  volume       = 37,
  number       = 3,
  pages        = {167–171},
  doi          = {10.1016/S0168-0102(00)00120-6},
  issn         = {01680102},
  language     = {en}
}
@article{Coombs_Eccles_Fatt_1955,
  title        = {The Specific Ionic Conductances and the Ionic Movements across the Motoneuronal Membrane That Produce the Inhibitory Post-synaptic Potential},
  author       = {Coombs, Jo S. and Eccles, John C. and Fatt, Paul},
  year         = 1955,
  month        = Nov,
  journal      = {The Journal of Physiology},
  volume       = 130,
  number       = 2,
  pages        = {326–373},
  doi          = {10.1113/jphysiol.1955.sp005412},
  issn         = {00223751},
  language     = {en}
}
@inbook{Niv_Joel_Meilijson_Ruppin_2001,
  title        = {Evolution of Reinforcement Learning in Uncertain Environments: Emergence of Risk-Aversion and Matching},
  author       = {Niv, Yael and Joel, Daphna and Meilijson, Isaac and Ruppin, Eytan},
  year         = 2001,
  booktitle    = {Advances in Artificial Life},
  publisher    = {Springer Berlin Heidelberg},
  address      = {Berlin, Heidelberg},
  series       = {Lecture Notes in Computer Science},
  volume       = 2159,
  pages        = {252–261},
  doi          = {10.1007/3-540-44811-X_27},
  isbn         = {978-3-540-42567-0},
  url          = {http://link.springer.com/10.1007/3-540-44811-X\%5F27},
  editor       = {Kelemen, Jozef and Sosík, Petr},
  collection   = {Lecture Notes in Computer Science}
}
@article{Salimans_Ho_Chen_Sidor_Sutskever_2017,
  title        = {Evolution Strategies as a Scalable Alternative to Reinforcement Learning},
  author       = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
  year         = 2017,
  month        = Sep,
  publisher    = {arXiv},
  number       = {arXiv:1703.03864},
  doi          = {10.48550/arXiv.1703.03864},
  url          = {http://arxiv.org/abs/1703.03864},
  abstractnote = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.}
}
@online{MNIST,
  title        = {The MNIST Database of Handwritten Digits},
  author       = {LeCun, Yann and Cortes, Corinna},
  year         = 1996,
  url          = {http://yann.lecun.com/exdb/mnist/},
  urldate      = {2022-09-05}
}
@article{Mu_Gilmer_2019,
  title        = {MNIST-C: A Robustness Benchmark for Computer Vision},
  author       = {Mu, Norman and Gilmer, Justin},
  year         = 2019,
  month        = Jun,
  publisher    = {arXiv},
  number       = {arXiv:1906.02337},
  doi          = {10.48550/arXiv.1906.02337},
  url          = {http://arxiv.org/abs/1906.02337},
  abstractnote = {We introduce the MNIST-C dataset, a comprehensive suite of 15 corruptions applied to the MNIST test set, for benchmarking out-of-distribution robustness in computer vision. Through several experiments and visualizations we demonstrate that our corruptions significantly degrade performance of state-of-the-art computer vision models while preserving the semantic content of the test images. In contrast to the popular notion of adversarial robustness, our model-agnostic corruptions do not seek worst-case performance but are instead designed to be broad and diverse, capturing multiple failure modes of modern models. In fact, we find that several previously published adversarial defenses significantly degrade robustness as measured by MNIST-C. We hope that our benchmark serves as a useful tool for future work in designing systems that are able to learn robust feature representations that capture the underlying semantics of the input.}
}
%@article{Kingma_Ba_2017,  -> Kingma2015AdamAM
%  title        = {Adam: A Method for Stochastic Optimization},
%  author       = {Kingma, Diederik P. and Ba, Jimmy},
%  year         = 2017,
%  month        = Jan,
%  publisher    = {arXiv},
%  number       = {arXiv:1412.6980},
%  doi          = {10.48550/arXiv.1412.6980},
%  url          = {http://arxiv.org/abs/1412.6980},
%  abstractnote = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.}
%}
@article{Liang_Glossner_Wang_Shi_Zhang_2021,
  title        = {Pruning and Quantization for Deep Neural Network Acceleration: A Survey},
  author       = {Liang, Tailin and Glossner, John and Wang, Lei and Shi, Shaobo and Zhang, Xiaotong},
  year         = 2021,
  month        = {Oct},
  journal      = {Neurocomputing},
  volume       = 461,
  pages        = {370–403},
  doi          = {10.1016/j.neucom.2021.07.045},
  issn         = {09252312},
  language     = {en}
}
@article{Bartunov_Santoro_Richards_Marris_Hinton_Lillicrap_2018,
  title        = {Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures},
  author       = {Bartunov, Sergey and Santoro, Adam and Richards, Blake A. and Marris, Luke and Hinton, Geoffrey E. and Lillicrap, Timothy},
  year         = 2018,
  journal      = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume       = 31
}
@article{Crick_1989,
  title        = {The Recent Excitement about Neural Networks},
  author       = {Crick, Francis},
  year         = 1989,
  month        = Jan,
  journal      = {Nature},
  volume       = 337,
  number       = 6203,
  pages        = {129–132},
  doi          = {10.1038/337129a0},
  issn         = {0028-0836, 1476-4687},
  language     = {en}
}
@article{Grossberg_1987,
  title        = {Competitive Learning: From Interactive Activation to Adaptive Resonance},
  author       = {Grossberg, Stephen},
  year         = 1987,
  month        = Jan,
  journal      = {Cognitive Science},
  volume       = 11,
  number       = 1,
  pages        = {23–63},
  doi          = {10.1111/j.1551-6708.1987.tb00862.x},
  issn         = {03640213},
  language     = {en}
}
@techreport{cifar_10,
  title        = {Learning Multiple Layers of Features from Tiny Images},
  author       = {Krizhevsky, Alex},
  year         = 2009,
  institution  = {Canadian Institute for Advanced Research}
}
@inproceedings{deng2009imagenet,
  title        = {ImageNet: A Large-Scale Hierarchical Image Database},
  author       = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  year         = 2009,
  booktitle    = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {248--255},
  organization = {IEEE}
}
@inbook{Movellan_1991,
  title        = {Contrastive Hebbian Learning in the Continuous Hopfield Model},
  author       = {Movellan, Javier R.},
  year         = 1991,
  booktitle    = {Connectionist Models},
  publisher    = {Elsevier},
  pages        = {10–17},
  doi          = {10.1016/B978-1-4832-1448-1.50007-X},
  isbn         = {978-1-4832-1448-1},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/B978148321448150007X},
  language     = {en}
}
@article{O_Reilly_1996,
  title        = {Biologically Plausible Error-Driven Learning Using Local Activation Differences: The Generalized Recirculation Algorithm},
  author       = {O’Reilly, Randall C.},
  year         = 1996,
  month        = Jul,
  journal      = {Neural Computation},
  volume       = 8,
  number       = 5,
  pages        = {895–938},
  doi          = {10.1162/neco.1996.8.5.895},
  issn         = {0899-7667, 1530-888X},
  abstractnote = {The error backpropagation learning algorithm (BP) is generally considered biologically implausible because it does not use locally available, activation-based variables. A version of BP that can be computed locally using bidirectional activation recirculation (Hinton and McClelland 1988) instead of backpropagated error derivatives is more biologically plausible. This paper presents a generalized version of the recirculation algorithm (GeneRec), which overcomes several limitations of the earlier algorithm by using a generic recurrent network with sigmoidal units that can learn arbitrary input/output mappings. However, the contrastive Hebbian learning algorithm (CHL, also known as DBM or mean field learning) also uses local variables to perform error-driven learning in a sigmoidal recurrent network. CHL was derived in a stochastic framework (the Boltzmann machine), but has been extended to the deterministic case in various ways, all of which rely on problematic approximations and assumptions, leading some to conclude that it is fundamentally flawed. This paper shows that CHL can be derived instead from within the BP framework via the GeneRec algorithm. CHL is a symmetry-preserving version of GeneRec that uses a simple approximation to the midpoint or second-order accurate Runge-Kutta method of numerical integration, which explains the generally faster learning speed of CHL compared to BI. Thus, all known fully general error-driven learning algorithms that use local activation-based variables in deterministic networks can be considered variations of the GeneRec algorithm (and indirectly, of the backpropagation algorithm). GeneRec therefore provides a promising framework for thinking about how the brain might perform error-driven learning. To further this goal, an explicit biological mechanism is proposed that would be capable of implementing GeneRec-style learning. This mechanism is consistent with available evidence regarding synaptic modification in neurons in the neocortex and hippocampus, and makes further predictions.},
  language     = {en}
}
@inbook{Le__Cun_1986,
  title        = {Learning Process in an Asymmetric Threshold Network},
  author       = {LeCun, Yann},
  year         = 1986,
  booktitle    = {Disordered Systems and Biological Organization},
  publisher    = {Springer Berlin Heidelberg},
  address      = {Berlin, Heidelberg},
  pages        = {233–240},
  doi          = {10.1007/978-3-642-82657-3_24},
  isbn         = {978-3-642-82659-7},
  url          = {http://link.springer.com/10.1007/978-3-642-82657-3\%5F24},
  editor       = {Bienenstock, E. and Soulié, F. Fogelman and Weisbuch, G.},
  language     = {en}
}
@inbook{Lee_Zhang_Fischer_Bengio_2015,
  title        = {Difference Target Propagation},
  author       = {Lee, Dong-Hyun and Zhang, Saizheng and Fischer, Asja and Bengio, Yoshua},
  year         = 2015,
  booktitle    = {Machine Learning and Knowledge Discovery in Databases},
  publisher    = {Springer International Publishing},
  address      = {Cham},
  series       = {Lecture Notes in Computer Science},
  volume       = 9284,
  pages        = {498–515},
  doi          = {10.1007/978-3-319-23528-8_31},
  isbn         = {978-3-319-23527-1},
  url          = {http://link.springer.com/10.1007/978-3-319-23528-8\%5F31},
  editor       = {Appice, Annalisa and Rodrigues, Pedro Pereira and Santos Costa, Vítor and Soares, Carlos and Gama, João and Jorge, Alípio},
  collection   = {Lecture Notes in Computer Science},
  language     = {en}
}
@article{randazzo2020self_classifying,
  title        = {Self-Classifying MNIST Digits},
  author       = {Randazzo, Ettore and Mordvintsev, Alexander and Niklasson, Eyvind and Levin, Michael and Greydanus, Sam},
  year         = 2020,
  journal      = {Distill},
  doi          = {10.23915/distill.00027.002}
}
@article{Zhang_Itoh_Tanida_Ichioka_1990,
  title        = {Parallel Distributed Processing Model with Local Space-Invariant Interconnections and Its Optical Architecture},
  author       = {Zhang, Wei and Itoh, Kazuyoshi and Tanida, Jun and Ichioka, Yoshiki},
  year         = 1990,
  month        = Nov,
  journal      = {Applied Optics},
  volume       = 29,
  number       = 32,
  pages        = 4790,
  doi          = {10.1364/AO.29.004790},
  issn         = {0003-6935, 1539-4522},
  language     = {en}
}
@inbook{Mouton_Myburgh_Davel_2020,
  title        = {Stride and Translation Invariance in CNNs},
  author       = {Mouton, Coenraad and Myburgh, Johannes C. and Davel, Marelie H.},
  year         = 2020,
  booktitle    = {Artificial Intelligence Research},
  publisher    = {Springer International Publishing},
  address      = {Cham},
  series       = {Communications in Computer and Information Science},
  volume       = 1342,
  pages        = {267–281},
  doi          = {10.1007/978-3-030-66151-9_17},
  isbn         = {978-3-030-66150-2},
  url          = {https://link.springer.com/10.1007/978-3-030-66151-9\%5F17},
  editor       = {Gerber, Aurona},
  collection   = {Communications in Computer and Information Science},
  language     = {en}
}
@inproceedings{Ioffe_Szegedy_2015,
  title        = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author       = {Ioffe, Sergey and Szegedy, Christian},
  year         = 2015,
  booktitle    = {International Conference on Machine Learning (ICML)},
  pages        = {448--456},
  organization = {PMLR}
}
@article{Lecun_Bottou_Bengio_Haffner_1998,
  title        = {Gradient-Based Learning Applied to Document Recognition},
  author       = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  year         = 1998,
  month        = Nov,
  journal      = {Proceedings of the IEEE},
  volume       = 86,
  number       = 11,
  pages        = {2278–2324},
  doi          = {10.1109/5.726791},
  issn         = {00189219}
}
@inproceedings{NIPS2012_c399862d,
  title        = {ImageNet Classification with Deep Convolutional Neural Networks},
  author       = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year         = 2012,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  publisher    = {Curran Associates, Inc.},
  volume       = 25,
  pages        = {},
  url          = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
  editor       = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger}
}
@inproceedings{Simonyan_Zisserman_2015,
  title        = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author       = {Simonyan, Karen and Zisserman, Andrew},
  year         = 2015,
  booktitle    = {International Conference on Learning Representations (ICLR)},
  pages        = {1--14},
  organization = {Computational and Biological Learning Society}
}
@inproceedings{Szegedy_Liu_Jia_Sermanet_Reed_Anguelov_Erhan_Vanhoucke_Rabinovich_2014,
  title        = {Going Deeper with Convolutions},
  author       = {Szegedy, Christian and Wei Liu and Yangqing Jia and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year         = 2015,
  month        = {Jun},
  booktitle    = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher    = {IEEE},
  address      = {Boston, MA, USA},
  pages        = {1–9},
  doi          = {10.1109/CVPR.2015.7298594},
  isbn         = {978-1-4673-6964-0},
  url          = {http://ieeexplore.ieee.org/document/7298594/}
}
@inproceedings{He_Zhang_Ren_Sun_2016,
  title        = {Deep Residual Learning for Image Recognition},
  author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year         = 2016,
  month        = Jun,
  booktitle    = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher    = {IEEE},
  address      = {Las Vegas, NV, USA},
  pages        = {770–778},
  doi          = {10.1109/CVPR.2016.90},
  isbn         = {978-1-4673-8851-1},
  url          = {http://ieeexplore.ieee.org/document/7780459/}
}
@inproceedings{Redmon_Divvala_Girshick_Farhadi_2016,
  title        = {You Only Look Once: Unified, Real-Time Object Detection},
  author       = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year         = 2016,
  month        = {Jun},
  booktitle    = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher    = {IEEE},
  address      = {Las Vegas, NV, USA},
  pages        = {779–788},
  doi          = {10.1109/CVPR.2016.91},
  isbn         = {978-1-4673-8851-1},
  url          = {http://ieeexplore.ieee.org/document/7780460/}
}
@inproceedings{gilbert1990lateral,
  title        = {Lateral Interactions in Visual Cortex},
  author       = {Gilbert, Charles D. and Hirsch, Joseph A. and Wiesel, Torsten N.},
  year         = 1990,
  booktitle    = {Cold Spring Harbor: Symposia on Quantitative Biology},
  volume       = 55,
  pages        = {663--677},
  organization = {Cold Spring Harbor Laboratory Press}
}
@article{Rochefort_Garaschuk_Milos_Narushima_Marandi_Pichler_Kovalchuk_Konnerth_2009,
  title        = {Sparsification of Neuronal Activity in the Visual Cortex at Eye-Opening},
  author       = {Rochefort, Nathalie L. and Garaschuk, Olga and Milos, Ruxandra-Iulia and Narushima, Madoka and Marandi, Nima and Pichler, Bruno and Kovalchuk, Yury and Konnerth, Arthur},
  year         = 2009,
  month        = Sep,
  journal      = {Proceedings of the National Academy of Sciences},
  volume       = 106,
  number       = 35,
  pages        = {15049–15054},
  doi          = {10.1073/pnas.0907660106},
  issn         = {0027-8424, 1091-6490},
  abstractnote = {Eye-opening represents a turning point in the function of the visual cortex. Before eye-opening, the visual cortex is largely devoid of sensory inputs and neuronal activities are generated intrinsically. After eye-opening, the cortex starts to integrate visual information. Here we used in vivo two-photon calcium imaging to explore the developmental changes of the mouse visual cortex by analyzing the ongoing spontaneous activity. We found that before eye-opening, the activity of layer 2/3 neurons consists predominantly of slow wave oscillations. These waves were first detected at postnatal day 8 (P8). Their initial very low frequency (0.01 Hz) gradually increased during development to ≈0.5 Hz in adults. Before eye-opening, a large fraction of neurons (>75%) was active during each wave. One day after eye-opening, this dense mode of recruitment changed to a sparse mode with only 36% of active neurons per wave. This was followed by a progressive decrease during the following weeks, reaching 12% of active neurons per wave in adults. The possible role of visual experience for this process of sparsification was investigated by analyzing dark-reared mice. We found that sparsification also occurred in these mice, but that the switch from a dense to a sparse activity pattern was delayed by 3–4 days as compared with normally-reared mice. These results reveal a modulatory contribution of visual experience during the first days after eye-opening, but an overall dominating role of intrinsic factors. We propose that the transformation in network activity from dense to sparse is a prerequisite for the changed cortical function at eye-opening.},
  language     = {en}
}
@article{Louizos_Welling_Kingma_2018,
  title        = {Learning Sparse Neural Networks through $L_0$ Regularization},
  author       = {Louizos, Christos and Welling, Max and Kingma, Diederik P.},
  year         = 2018,
  month        = Jun,
  publisher    = {arXiv},
  number       = {arXiv:1712.01312},
  doi          = {10.48550/arXiv.1712.01312},
  url          = {http://arxiv.org/abs/1712.01312},
  abstractnote = {We propose a practical method for $L_0$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of $L_0$ regularization. However, since the $L_0$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected $L_0$ norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the emph{hard concrete} distribution for the gates, which is obtained by “stretching” a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.}
}
@article{Hoefler_Alistarh_Ben_Nun_Dryden_Peste_2021,
  title        = {Sparsity in Deep Learning: Pruning and Growth for Efficient Inference and Training in Neural Networks},
  author       = {Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
  year         = 2021,
  month        = {jan},
  journal      = {Journal of Machine Learning Research},
  publisher    = {JMLR.org},
  volume       = 22,
  number       = 1,
  pages        = {10882--11005},
  issn         = {1532-4435},
  issue_date   = {January 2021},
  eid          = 241,
  numpages     = 124
}
@article{Panousis_Chatzis_Theodoridis_2021,
  title        = {Stochastic Local Winner-Takes-All Networks Enable Profound Adversarial Robustness},
  author       = {Panousis, Konstantinos P. and Chatzis, Sotirios and Theodoridis, Sergios},
  year         = 2021,
  month        = Dec,
  publisher    = {arXiv},
  number       = {arXiv:2112.02671},
  doi          = {10.48550/arXiv.2112.02671},
  url          = {http://arxiv.org/abs/2112.02671},
  abstractnote = {This work explores the potency of stochastic competition-based activations, namely Stochastic Local Winner-Takes-All (LWTA), against powerful (gradient-based) white-box and black-box adversarial attacks; we especially focus on Adversarial Training settings. In our work, we replace the conventional ReLU-based nonlinearities with blocks comprising locally and stochastically competing linear units. The output of each network layer now yields a sparse output, depending on the outcome of winner sampling in each block. We rely on the Variational Bayesian framework for training and inference; we incorporate conventional PGD-based adversarial training arguments to increase the overall adversarial robustness. As we experimentally show, the arising networks yield state-of-the-art robustness against powerful adversarial attacks while retaining very high classification rate in the benign case.}
}
@inproceedings{chen2020simple,
  title        = {A Simple Framework for Contrastive Learning of Visual Representations},
  author       = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey E.},
  year         = 2020,
  booktitle    = {International Conference on Machine Learning (ICML)},
  publisher    = {JMLR.org},
  series       = {ICML'20},
  pages        = {1597--1607},
  eid          = 149,
  numpages     = 11
}
@inproceedings{chen2020big,
  title        = {Big Self-Supervised Models Are Strong Semi-Supervised Learners},
  author       = {Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey E.},
  year         = 2020,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  location     = {Vancouver, BC, Canada},
  publisher    = {Curran Associates Inc.},
  address      = {Red Hook, NY, USA},
  series       = {NeurIPS'20},
  isbn         = 9781713829546,
  articleno    = 1865,
  numpages     = 13
}
@inproceedings{chung19_interspeech,
  title        = {An Unsupervised Autoregressive Model for Speech Representation Learning},
  author       = {Chung, Yu-An and Hsu, Wei-Ning and Tang, Hao and Glass, James},
  year         = 2019,
  booktitle    = {Interspeech},
  pages        = {146--150},
  doi          = {10.21437/Interspeech.2019-1473}
}
@inproceedings{Simmler_Sager_Andermatt_Chavarriaga_Schilling_Rosenthal_Stadelmann_2021,
  title        = {A Survey of Un-, Weakly-, and Semi-Supervised Learning Methods for Noisy, Missing and Partial Labels in Industrial Vision Applications},
  author       = {Simmler, Niclas and Sager, Pascal and Andermatt, Philipp and Chavarriaga, Ricardo and Schilling, Frank-Peter and Rosenthal, Matthias and Stadelmann, Thilo},
  year         = 2021,
  month        = Jun,
  booktitle    = {2021 8th Swiss Conference on Data Science (SDS)},
  publisher    = {IEEE},
  address      = {Lucerne, Switzerland},
  pages        = {26–31},
  doi          = {10.1109/SDS51136.2021.00012},
  isbn         = {978-1-66543-874-2},
  url          = {https://ieeexplore.ieee.org/document/9474624/}
}
@inbook{Liu_Anguelov_Erhan_Szegedy_Reed_Fu_Berg_2016,
  title        = {SSD: Single Shot MultiBox Detector},
  author       = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  year         = 2016,
  booktitle    = {Computer Vision – ECCV 2016},
  publisher    = {Springer International Publishing},
  address      = {Cham},
  series       = {Lecture Notes in Computer Science},
  volume       = 9905,
  pages        = {21–37},
  doi          = {10.1007/978-3-319-46448-0_2},
  isbn         = {978-3-319-46447-3},
  url          = {http://link.springer.com/10.1007/978-3-319-46448-0\%5F2},
  editor       = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  collection   = {Lecture Notes in Computer Science},
  language     = {en}
}
@inproceedings{He_Gkioxari_Dollar_Girshick_2017,
  title        = {Mask R-CNN},
  author       = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
  year         = 2017,
  month        = Oct,
  booktitle    = {2017 IEEE International Conference on Computer Vision (ICCV)},
  publisher    = {IEEE},
  address      = {Venice},
  pages        = {2980–2988},
  doi          = {10.1109/ICCV.2017.322},
  isbn         = {978-1-5386-1032-9},
  url          = {http://ieeexplore.ieee.org/document/8237584/}
}
@article{Wu_Zhang_Huang_Liang_Yu_2019,
  title        = {FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation},
  author       = {Wu, Huikai and Zhang, Junge and Huang, Kaiqi and Liang, Kongming and Yu, Yizhou},
  year         = 2019,
  month        = Mar,
  publisher    = {arXiv},
  number       = {arXiv:1903.11816},
  doi          = {10.48550/arXiv.1903.11816},
  url          = {http://arxiv.org/abs/1903.11816},
  abstractnote = {Modern approaches for semantic segmentation usually employ dilated convolutions in the backbone to extract high-resolution feature maps, which brings heavy computation complexity and memory footprint. To replace the time and memory consuming dilated convolutions, we propose a novel joint upsampling module named Joint Pyramid Upsampling (JPU) by formulating the task of extracting high-resolution feature maps into a joint upsampling problem. With the proposed JPU, our method reduces the computation complexity by more than three times without performance loss. Experiments show that JPU is superior to other upsampling modules, which can be plugged into many existing approaches to reduce computation complexity and improve performance. By replacing dilated convolutions with the proposed JPU module, our method achieves the state-of-the-art performance in Pascal Context dataset (mIoU of 53.13%) and ADE20K dataset (final score of 0.5584) while running 3 times faster.}
}
@inbook{rumelhart1985learning,
  title        = {Learning Internal Representations by Error Propagation},
  author       = {Rumelhart, David E. and McClelland, James L.},
  year         = 1987,
  booktitle    = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations},
  volume       = {},
  number       = {},
  pages        = {318--362},
  doi          = {}
}
@inproceedings{pmlrv27baldi12a,
  title        = {Autoencoders, Unsupervised Learning, and Deep Architectures},
  author       = {Baldi, Pierre},
  year         = 2012,
  month        = Jul,
  booktitle    = {ICML Workshop on Unsupervised and Transfer Learning},
  publisher    = {PMLR},
  address      = {Bellevue, Washington, USA},
  series       = {Proceedings of Machine Learning Research},
  volume       = 27,
  pages        = {37--49},
  url          = {https://proceedings.mlr.press/v27/baldi12a.html},
  editor       = {Guyon, Isabelle and Dror, Gideon and Lemaire, Vincent and Taylor, Graham and Silver, Daniel},
  pdf          = {http://proceedings.mlr.press/v27/baldi12a/baldi12a.pdf},
  abstract     = {Autoencoders play a fundamental role in unsupervised learning and in deep architectures for transfer learning and other tasks. In spite of their fundamental role, only linear autoencoders over the real numbers have been solved analytically. Here we present a general mathematical framework for the study of both linear and non-linear autoencoders. The framework allows one to derive an analytical treatment for the most non-linear autoencoder, the Boolean autoencoder. Learning in the Boolean autoencoder is equivalent to a clustering problem that can be solved in polynomial time when the number of clusters is small and becomes NP complete when the number of clusters is large. The framework sheds light on the different kinds of autoencoders, their learning complexity, their horizontal and vertical composability in deep architectures, their critical points, and their fundamental connections to clustering, Hebbian learning, and information theory.}
}
@inproceedings{Ranzato_Huang_Boureau_LeCun_2007,
  title        = {Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition},
  author       = {Ranzato, Marc’Aurelio and Huang, Fu Jie and Boureau, Y-Lan and LeCun, Yann},
  year         = 2007,
  month        = Jun,
  booktitle    = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher    = {IEEE},
  address      = {Minneapolis, MN, USA},
  pages        = {1–8},
  doi          = {10.1109/CVPR.2007.383157},
  isbn         = {978-1-4244-1179-5},
  url          = {http://ieeexplore.ieee.org/document/4270182/}
}
@inproceedings{10-5555-3042573-3042641,
  title        = {Building High-Level Features Using Large Scale Unsupervised Learning},
  author       = {Le, Quoc V. and Ranzato, Marc Aurelio and Monga, Rajat and Devin, Matthieu and Chen, Kai and Corrado, Greg S. and Dean, Jeff and Ng, Andrew Y.},
  year         = 2012,
  booktitle    = {International Coference on International Conference on Machine Learning (ICML)},
  location     = {Edinburgh, Scotland},
  publisher    = {Omnipress},
  address      = {Madison, WI, USA},
  series       = {ICML'12},
  pages        = {507–514},
  isbn         = 9781450312851,
  numpages     = 8
}
@inproceedings{10-1145-1390156-1390294,
  title        = {Extracting and Composing Robust Features with Denoising Autoencoders},
  author       = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  year         = 2008,
  booktitle    = {International Conference on Machine Learning (ICML)},
  location     = {Helsinki, Finland},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  series       = {ICML '08},
  pages        = {1096–1103},
  doi          = {10.1145/1390156.1390294},
  isbn         = 9781605582054,
  url          = {10.1145/1390156.1390294},
  abstract     = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
  numpages     = 8
}
@inproceedings{10-5555-3104482-3104587,
  title        = {Contractive Auto-Encoders: Explicit Invariance during Feature Extraction},
  author       = {Rifai, Salah and Vincent, Pascal and Muller, Xavier and Glorot, Xavier and Bengio, Yoshua},
  year         = 2011,
  booktitle    = {International Conference on International Conference on Machine Learning (ICML)},
  location     = {Bellevue, Washington, USA},
  publisher    = {Omnipress},
  address      = {Madison, WI, USA},
  series       = {ICML'11},
  pages        = {833–840},
  isbn         = 9781450306195,
  abstract     = {We present in this paper a novel approach for training deterministic auto-encoders. We show that by adding a well chosen penalty term to the classical reconstruction cost function, we can achieve results that equal or surpass those attained by other regularized auto-encoders as well as denoising auto-encoders on a range of datasets. This penalty term corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. We show that this penalty term results in a localized space contraction which in turn yields robust features on the activation layer. Furthermore, we show how this penalty term is related to both regularized auto-encoders and denoising auto-encoders and how it can be seen as a link between deterministic and non-deterministic auto-encoders. We find empirically that this penalty helps to carve a representation that better captures the local directions of variation dictated by the data, corresponding to a lower-dimensional non-linear manifold, while being more invariant to the vast majority of directions orthogonal to the manifold. Finally, we show that by using the learned features to initialize a MLP, we achieve state of the art classification error on a range of datasets, surpassing other methods of pretraining.},
  numpages     = 8
}
@article{Elharrouss_Almaadeed_Al-Maadeed_Akbari_2020,
  title        = {Image Inpainting: A Review},
  author       = {Elharrouss, Omar and Almaadeed, Noor and Al-Maadeed, Somaya and Akbari, Younes},
  year         = 2020,
  month        = Apr,
  journal      = {Neural Processing Letters},
  volume       = 51,
  number       = 2,
  pages        = {2007–2028},
  doi          = {10.1007/s11063-019-10163-0},
  issn         = {1370-4621, 1573-773X},
  language     = {en}
}
@inproceedings{Pathak_Krahenbuhl_Donahue_Darrell_Efros_2016,
  title        = {Context Encoders: Feature Learning by Inpainting},
  author       = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
  year         = 2016,
  month        = Jun,
  booktitle    = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher    = {IEEE},
  address      = {Las Vegas, NV, USA},
  pages        = {2536–2544},
  doi          = {10.1109/CVPR.2016.278},
  isbn         = {978-1-4673-8851-1},
  url          = {http://ieeexplore.ieee.org/document/7780647/}
}
@inproceedings{he2022masked,
  title        = {Masked Autoencoders Are Scalable Vision Learners},
  author       = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollar, Piotr and Girshick, Ross},
  year         = 2022,
  month        = {Jun},
  booktitle    = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher    = {IEEE},
  address      = {New Orleans, LA, USA},
  pages        = {15979–15988},
  doi          = {10.1109/CVPR52688.2022.01553},
  isbn         = {978-1-66546-946-3},
  url          = {https://ieeexplore.ieee.org/document/9879206/}
}
@inproceedings{shi2022adversarial,
  title        = {Adversarial Masking for Self-Supervised Learning},
  author       = {Shi, Yuge and Siddharth, N and Torr, Philip and Kosiorek, Adam R.},
  year         = 2022,
  booktitle    = {International Conference on Machine Learning (ICML)},
  pages        = {20026--20040},
  organization = {PMLR}
}
@inproceedings{komodakis2018unsupervised,
  title        = {Unsupervised Representation Learning by Predicting Image Rotations},
  author       = {Komodakis, Nikos and Gidaris, Spyros},
  year         = 2018,
  booktitle    = {International Conference on Learning Representations (ICLR)}
}
@inproceedings{Zhai_Oliver_Kolesnikov_Beyer_2019,
  title        = {S4L: Self-Supervised Semi-Supervised Learning},
  author       = {Zhai, Xiaohua and Oliver, Avital and Kolesnikov, Alexander and Beyer, Lucas},
  year         = 2019,
  month        = {Oct},
  booktitle    = {IEEE/CVF International Conference on Computer Vision (ICCV)},
  publisher    = {IEEE},
  address      = {Seoul, Korea (South)},
  pages        = {1476–1485},
  doi          = {10.1109/ICCV.2019.00156},
  isbn         = {978-1-72814-803-8},
  url          = {https://ieeexplore.ieee.org/document/9010283/}
}
@article{khosla2020supervised,
  title        = {Supervised Contrastive Learning},
  author       = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
  year         = 2020,
  journal      = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume       = 33,
  pages        = {18661--18673}
}
@inproceedings{he2020momentum,
  title        = {Momentum Contrast for Unsupervised Visual Representation Learning},
  author       = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  year         = 2020,
  booktitle    = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {9729--9738}
}
@article{caron2020unsupervised,
  title        = {Unsupervised Learning of Visual Features by Contrasting Cluster Assignments},
  author       = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  year         = 2020,
  journal      = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume       = 33,
  pages        = {9912--9924}
}
@article{chen2022semi,
  title        = {Semi-Supervised and Unsupervised Deep Visual Learning: A Survey},
  author       = {Chen, Yanbei and Mancini, Massimiliano and Zhu, Xiatian and Akata, Zeynep},
  year         = 2022,
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  publisher    = {IEEE}
}
@inbook{Widrow_Kim_Park_Perin_2019,
  title        = {Nature’s Learning Rule},
  author       = {Widrow, Bernard and Kim, Youngsik and Park, Dookun and Perin, Jose Krause},
  year         = 2019,
  booktitle    = {Artificial Intelligence in the Age of Neural Networks and Brain Computing},
  publisher    = {Elsevier},
  pages        = {1–30},
  doi          = {10.1016/B978-0-12-815480-9.00001-3},
  isbn         = {978-0-12-815480-9},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/B9780128154809000013},
  language     = {en}
}
@inbook{Anderson_1998,
  title        = {Biased Random-Walk Learning: A Neurobiological Correlate to Trial-and-Error},
  author       = {Anderson, Russell W.},
  year         = 1998,
  booktitle    = {Neural Networks and Pattern Recognition},
  publisher    = {Elsevier},
  pages        = {221–244},
  doi          = {10.1016/B978-012526420-4/50008-2},
  isbn         = {978-0-12-526420-4},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/B9780125264204500082},
  language     = {en}
}
@article{Grajski_Merzenich_1990,
  title        = {Hebb-Type Dynamics is Sufficient to Account for the Inverse Magnification Rule in Cortical Somatotopy},
  author       = {Grajski, Kamil A. and Merzenich, Michael M.},
  year         = 1990,
  month        = Mar,
  journal      = {Neural Computation},
  volume       = 2,
  number       = 1,
  pages        = {71–84},
  doi          = {10.1162/neco.1990.2.1.71},
  issn         = {0899-7667, 1530-888X},
  abstractnote = {The inverse magnification rule in cortical somatotopy is the experimentally derived inverse relationship between cortical magnification (area of somatotopic map representing a unit area of skin surface) and receptive field size (area of restricted skin surface driving a cortical neuron). We show by computer simulation of a simple, multilayer model that Hebb-type synaptic modification subject to competitive constraints is sufficient to account for the inverse magnification rule.},
  language     = {en}
}
@article{Montague_Gally_Edelman_1991,
  title        = {Spatial Signaling in the Development and Function of Neural Connections},
  author       = {Montague, Read P. and Gally, Joseph A. and Edelman, Gerald M.},
  year         = 1991,
  journal      = {Cerebral Cortex},
  volume       = 1,
  number       = 3,
  pages        = {199–220},
  doi          = {10.1093/cercor/1.3.199},
  issn         = {1047-3211, 1460-2199},
  language     = {en}
}
@article{Mel_1992,
  title        = {NMDA-Based Pattern Discrimination in a Modeled Cortical Neuron},
  author       = {Mel, Bartlett W.},
  year         = 1992,
  month        = Jul,
  journal      = {Neural Computation},
  volume       = 4,
  number       = 4,
  pages        = {502–517},
  doi          = {10.1162/neco.1992.4.4.502},
  issn         = {0899-7667, 1530-888X},
  abstractnote = {Compartmental simulations of an anatomically characterized cortical pyramidal cell were carried out to study the integrative behavior of a complex dendritic tree. Previous theoretical (Feldman and Ballard 1982; Durbin and Rumelhart 1989; Mel 1990; Mel and Koch 1990; Poggio and Girosi 1990) and compartmental modeling (Koch et al. 1983; Shepherd et al. 1985; Koch and Poggio 1987; Rall and Segev 1987; Shepherd and Brayton 1987; Shepherd et al. 1989; Brown et al. 1991) work had suggested that multiplicative interactions among groups of neighboring synapses could greatly enhance the processing power of a neuron relative to a unit with only a single global firing threshold. This issue was investigated here, with a particular focus on the role of voltage-dependent N-methyl-D-asparate (NMDA) channels in the generation of cell responses. First, it was found that when a large proportion of the excitatory synaptic input to dendritic spines is carried by NMDA channels, the pyramidal cell responds preferentially to spatially clustered, rather than random, distributions of activated synapses. Second, based on this mechanism, the NMDA-rich neuron is shown to be capable of solving a nonlinear pattern discrimination task. We propose that manipulation of the spatial ordering of afferent synaptic connections onto the dendritic arbor is a possible biological strategy for pattern information storage during learning.},
  language     = {en}
}
@inbook{6302929,
  title        = {Learning Internal Representations by Error Propagation},
  author       = {Rumelhart, David E. and McClelland, James L.},
  year         = 1987,
  booktitle    = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations},
  volume       = {},
  number       = {},
  pages        = {318--362},
  doi          = {}
}
@article{Grossberg_Schmajuk_1989,
  title        = {Neural Dynamics of Adaptive Timing and Temporal Discrimination during Associative Learning},
  author       = {Grossberg, Stephen and Schmajuk, Nestor A.},
  year         = 1989,
  month        = Jan,
  journal      = {Neural Networks},
  volume       = 2,
  number       = 2,
  pages        = {79–102},
  doi          = {10.1016/0893-6080(89)90026-9},
  issn         = {08936080},
  language     = {en}
}
@article{Zhang_He_Sra_Jadbabaie_2020,
  title        = {Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},
  author       = {Zhang, Jingzhao and He, Tianxing and Sra, Suvrit and Jadbabaie, Ali},
  year         = 2020,
  month        = Feb,
  publisher    = {arXiv},
  number       = {arXiv:1905.11881},
  doi          = {10.48550/arXiv.1905.11881},
  url          = {http://arxiv.org/abs/1905.11881},
  abstractnote = {We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, emph{gradient clipping} and emph{normalized gradient}, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.}
}
@inproceedings{pmlr-v38-lee15a,
  title        = {{Deeply-Supervised Nets}},
  author       = {Lee, Chen-Yu and Xie, Saining and Gallagher, Patrick and Zhang, Zhengyou and Tu, Zhuowen},
  year         = 2015,
  month        = May,
  booktitle    = {International Conference on Artificial Intelligence and Statistics},
  publisher    = {PMLR},
  address      = {San Diego, California, USA},
  series       = {Proceedings of Machine Learning Research},
  volume       = 38,
  pages        = {562--570},
  url          = {https://proceedings.mlr.press/v38/lee15a.html},
  editor       = {Lebanon, Guy and Vishwanathan, S. V. N.},
  pdf          = {http://proceedings.mlr.press/v38/lee15a.pdf},
  abstract     = {We propose deeply-supervised nets (DSN), a method that simultaneously minimizes classification error and improves the directness and transparency of the hidden layer learning process. We focus our attention on three aspects of traditional convolutional-neural-network-type (CNN-type) architectures:  (1) transparency in the effect intermediate layers have on overall classification;  (2) discriminativeness and robustness of learned features, especially in early layers;  (3) training effectiveness in the face of “vanishing” gradients.  To combat these issues, we introduce “companion” objective functions at each hidden layer, in addition to the overall objective function at the output layer (an integrated strategy distinct from layer-wise pre-training). We also analyze our algorithm using techniques extended from stochastic gradient methods. The advantages provided by our method are evident in our experimental results, showing state-of-the-art performance on MNIST, CIFAR-10, CIFAR-100, and SVHN.}
}
@article{Duan_Yu_Principe_2022,
  title        = {Modularizing Deep Learning via Pairwise Learning With Kernels},
  author       = {Duan, Shiyu and Yu, Shujian and Principe, Jose C.},
  year         = 2022,
  month        = Apr,
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  volume       = 33,
  number       = 4,
  pages        = {1441–1451},
  doi          = {10.1109/TNNLS.2020.3042346},
  issn         = {2162-237X, 2162-2388}
}
@article{Duan_Yu_Chen_Principe_2020,
  title        = {On Kernel Method–Based Connectionist Models and Supervised Deep Learning Without Backpropagation},
  author       = {Duan, Shiyu and Yu, Shujian and Chen, Yunmei and Principe, Jose C.},
  year         = 2020,
  month        = Jan,
  journal      = {Neural Computation},
  volume       = 32,
  number       = 1,
  pages        = {97–135},
  doi          = {10.1162/neco_a_01250},
  issn         = {0899-7667, 1530-888X},
  abstractnote = {We propose a novel family of connectionist models based on kernel machines and consider the problem of learning layer by layer a compositional hypothesis class (i.e., a feedforward, multilayer architecture) in a supervised setting. In terms of the models, we present a principled method to “kernelize” (partly or completely) any neural network (NN). With this method, we obtain a counterpart of any given NN that is powered by kernel machines instead of neurons. In terms of learning, when learning a feedforward deep architecture in a supervised setting, one needs to train all the components simultaneously using backpropagation (BP) since there are no explicit targets for the hidden layers (Rumelhart, Hinton, & Williams, 1986 ). We consider without loss of generality the two-layer case and present a general framework that explicitly characterizes a target for the hidden layer that is optimal for minimizing the objective function of the network. This characterization then makes possible a purely greedy training scheme that learns one layer at a time, starting from the input layer. We provide instantiations of the abstract framework under certain architectures and objective functions. Based on these instantiations, we present a layer-wise training algorithm for an [Formula: see text]-layer feedforward network for classification, where [Formula: see text] can be arbitrary. This algorithm can be given an intuitive geometric interpretation that makes the learning dynamics transparent. Empirical results are provided to complement our theory. We show that the kernelized networks, trained layer-wise, compare favorably with classical kernel machines as well as other connectionist models trained by BP. We also visualize the inner workings of the greedy kernelized models to validate our claim on the transparency of the layer-wise algorithm.},
  language     = {en}
}
@article{Wang_Ni_Song_Yang_Huang_2021,
  title        = {Revisiting Locally Supervised Learning: an Alternative to End-to-end Training},
  author       = {Wang, Yulin and Ni, Zanlin and Song, Shiji and Yang, Le and Huang, Gao},
  year         = 2021,
  month        = Jan,
  publisher    = {arXiv},
  number       = {arXiv:2101.10832},
  doi          = {10.48550/arXiv.2101.10832},
  url          = {http://arxiv.org/abs/2101.10832},
  abstractnote = {Due to the need to store the intermediate activations for back-propagation, end-to-end (E2E) training of deep networks usually suffers from high GPUs memory footprint. This paper aims to address this problem by revisiting the locally supervised learning, where a network is split into gradient-isolated modules and trained with local supervision. We experimentally show that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, we propose an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. As InfoPro loss is difficult to compute in its original form, we derive a feasible upper bound as a surrogate optimization objective, yielding a simple but effective algorithm. In fact, we show that the proposed method boils down to minimizing the combination of a reconstruction loss and a normal cross-entropy/contrastive term. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate that InfoPro is capable of achieving competitive performance with less than 40% memory footprint compared to E2E training, while allowing using training data with higher-resolution or larger batch sizes under the same GPU memory constraint. Our method also enables training local modules asynchronously for potential training acceleration. Code is available at: https://github.com/blackfeather-wang/InfoPro-Pytorch.}
}
@inproceedings{belilovsky2019greedy,
  title        = {Greedy Layerwise Learning Can Scale to ImageNet},
  author       = {Belilovsky, Eugene and Eickenberg, Michael and Oyallon, Edouard},
  year         = 2019,
  booktitle    = {International Conference on Machine Learning (ICML)},
  pages        = {583--593},
  organization = {PMLR}
}
@article{Mostafa_Ramesh_Cauwenberghs_2018,
  title        = {Deep Supervised Learning Using Local Errors},
  author       = {Mostafa, Hesham and Ramesh, Vishwajith and Cauwenberghs, Gert},
  year         = 2018,
  month        = Aug,
  journal      = {Frontiers in Neuroscience},
  volume       = 12,
  pages        = 608,
  doi          = {10.3389/fnins.2018.00608},
  issn         = {1662-453X}
}
@article{Marquez_Hare_Niranjan_2018,
  title        = {Deep Cascade Learning},
  author       = {Marquez, Enrique S. and Hare, Jonathon S. and Niranjan, Mahesan},
  year         = 2018,
  month        = Nov,
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  volume       = 29,
  number       = 11,
  pages        = {5475–5485},
  doi          = {10.1109/TNNLS.2018.2805098},
  issn         = {2162-237X, 2162-2388}
}
@inproceedings{pmlr-v97-nokland19a,
  title        = {Training Neural Networks with Local Error Signals},
  author       = {Nokland, Arild and Eidnes, Lars Hiller},
  year         = 2019,
  month        = Jun,
  booktitle    = {International Conference on Machine Learning (ICML)},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 97,
  pages        = {4839--4850},
  url          = {https://proceedings.mlr.press/v97/nokland19a.html},
  editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  pdf          = {http://proceedings.mlr.press/v97/nokland19a/nokland19a.pdf},
  abstract     = {Supervised training of neural networks for classification is typically performed with a global loss function. The loss function provides a gradient for the output layer, and this gradient is back-propagated to hidden layers to dictate an update direction for the weights. An alternative approach is to train the network with layer-wise loss functions. In this paper we demonstrate, for the first time, that layer-wise training can approach the state-of-the-art on a variety of image datasets. We use single-layer sub-networks and two different supervised loss functions to generate local error signals for the hidden layers, and we show that the combination of these losses help with optimization in the context of local learning. Using local errors could be a step towards more biologically plausible deep learning because the global error does not have to be transported back to hidden layers. A completely backprop free variant outperforms previously reported results among methods aiming for higher biological plausibility.}
}
@inproceedings{10.5555/3524938.3525007,
  title        = {Decoupled Greedy Learning of CNNs},
  author       = {Belilovsky, Eugene and Eickenberg, Michael and Oyallon, Edouard},
  year         = 2020,
  booktitle    = {International Conference on Machine Learning (ICML)},
  publisher    = {JMLR.org},
  series       = {ICML'20},
  pages        = {736--745},
  abstract     = {A commonly cited inefficiency of neural network training by back-propagation is the update locking problem: each layer must wait for the signal to propagate through the full network before updating. Several alternatives that can alleviate this issue have been proposed. In this context, we consider a simpler, but more effective, substitute that uses minimal feedback, which we call Decoupled Greedy Learning (DGL). It is based on a greedy relaxation of the joint training objective, recently shown to be effective in the context of Convolutional Neural Networks (CNNs) on large-scale image classification. We consider an optimization of this objective that permits us to decouple the layer training, allowing for layers or modules in networks to be trained with a potentially linear parallelization in layers. With the use of a replay buffer we show this approach can be extended to asynchronous settings, where modules can operate with possibly large communication delays. We show theoretically and empirically that this approach converges. Then, we empirically find that it can lead to better generalization than sequential greedy optimization. We demonstrate the effectiveness of DGL against alternative approaches on the CIFAR-10 dataset and on the large-scale ImageNet dataset.},
  articleno    = 69,
  numpages     = 10
}
@article{Bengio_2014,
  title        = {How Auto-Encoders Could Provide Credit Assignment in Deep Networks via Target Propagation},
  author       = {Bengio, Yoshua},
  year         = 2014,
  month        = Sep,
  publisher    = {arXiv},
  number       = {arXiv:1407.7906},
  doi          = {10.48550/arXiv.1407.7906},
  url          = {http://arxiv.org/abs/1407.7906},
  abstractnote = {We propose to exploit {em reconstruction} as a layer-local training signal for deep learning. Reconstructions can be propagated in a form of target propagation playing a role similar to back-propagation but helping to reduce the reliance on derivatives in order to perform credit assignment across many levels of possibly strong non-linearities (which is difficult for back-propagation). A regularized auto-encoder tends produce a reconstruction that is a more likely version of its input, i.e., a small move in the direction of higher likelihood. By generalizing gradients, target propagation may also allow to train deep networks with discrete hidden units. If the auto-encoder takes both a representation of input and target (or of any side information) in input, then its reconstruction of input representation provides a target towards a representation that is more likely, conditioned on all the side information. A deep auto-encoder decoding path generalizes gradient propagation in a learned way that can could thus handle not just infinitesimal changes but larger, discrete changes, hopefully allowing credit assignment through a long chain of non-linear operations. In addition to each layer being a good auto-encoder, the encoder also learns to please the upper layers by transforming the data into a space where it is easier to model by them, flattening manifolds and disentangling factors. The motivations and theoretical justifications for this approach are laid down in this paper, along with conjectures that will have to be verified either mathematically or experimentally, including a hypothesis stating that such auto-encoder mediated target propagation could play in brains the role of credit assignment through many non-linear, noisy and discrete transformations.}
}
@inproceedings{10.5555/3495724.3497405,
  title        = {A Theoretical Framework for Target Propagation},
  author       = {Meulemans, Alexander and Carzaniga, Francesco S. and Suykens, Johan A.K. and Sacramento, Jo\~{a}o and Grewe, Benjamin F.},
  year         = 2020,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  location     = {Vancouver, BC, Canada},
  publisher    = {Curran Associates Inc.},
  address      = {Red Hook, NY, USA},
  series       = {NeurIPS'20},
  isbn         = 9781713829546,
  abstract     = {The success of deep learning, a brain-inspired form of AI, has sparked interest in understanding how the brain could similarly learn across multiple layers of neurons. However, the majority of biologically-plausible learning algorithms have not yet reached the performance of backpropagation (BP), nor are they built on strong theoretical foundations. Here, we analyze target propagation (TP), a popular but not yet fully understood alternative to BP, from the standpoint of mathematical optimization. Our theory shows that TP is closely related to Gauss-Newton optimization and thus substantially differs from BP. Furthermore, our analysis reveals a fundamental limitation of difference target propagation (DTP), a well-known variant of TP, in the realistic scenario of non-invertible neural networks. We provide a first solution to this problem through a novel reconstruction loss that improves feedback weight training, while simultaneously introducing architectural flexibility by allowing for direct feedback connections from the output to each hidden layer. Our theory is corroborated by experimental results that show significant improvements in performance and in the alignment of forward weight updates with loss gradients, compared to DTP.},
  articleno    = 1681,
  numpages     = 13
}
@inproceedings{pmlr-v70-jaderberg17a,
  title        = {Decoupled Neural Interfaces using Synthetic Gradients},
  author       = {Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
  year         = 2017,
  month        = Aug,
  booktitle    = {International Conference on Machine Learning (ICML)},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 70,
  pages        = {1627--1635},
  url          = {https://proceedings.mlr.press/v70/jaderberg17a.html},
  editor       = {Precup, Doina and Teh, Yee Whye},
  pdf          = {http://proceedings.mlr.press/v70/jaderberg17a/jaderberg17a.pdf},
  abstract     = {Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled <em>synthetic gradient</em> in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise <em>decoupled neural interfaces</em>. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one’s future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass – amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.}
}
@inproceedings{10.5555/3305381.3305475,
  title        = {Understanding Synthetic Gradients and Decoupled Neural Interfaces},
  author       = {Czarnecki, Wojciech Marian and Swirszcz, Grzegorz and Jaderberg, Max and Osindero, Simon and Vinyals, Oriol and Kavukcuoglu, Koray},
  year         = 2017,
  booktitle    = {International Conference on Machine Learning (ICML)},
  location     = {Sydney, NSW, Australia},
  publisher    = {JMLR.org},
  series       = {ICML'17},
  pages        = {904–912},
  abstract     = {When training neural networks, the use of Synthetic Gradients (SG) allows layers or modules to be trained without update locking - without waiting for a true error gradient to be backpropagated - resulting in Decoupled Neural Interfaces (DNIs). This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al. (2016). However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view. In this paper, we study DNIs through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation. We show that the incorporation of SGs does not affect the representational strength of the learning system for a neural network, and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approx-imate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations. Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison.},
  numpages     = 9
}
@article{Lansdell_Prakash_Kording_2020,
  title        = {Learning to Solve the Credit Assignment Problem},
  author       = {Lansdell, Benjamin James and Prakash, Prashanth Ravi and Kording, Konrad Paul},
  year         = 2020,
  month        = Apr,
  publisher    = {arXiv},
  number       = {arXiv:1906.00889},
  doi          = {10.48550/arXiv.1906.00889},
  url          = {http://arxiv.org/abs/1906.00889},
  abstractnote = {Backpropagation is driving today’s artificial neural networks (ANNs). However, despite extensive research, it remains unclear if the brain implements this algorithm. Among neuroscientists, reinforcement learning (RL) algorithms are often seen as a realistic alternative: neurons can randomly introduce change, and use unspecific feedback signals to observe their effect on the cost and thus approximate their gradient. However, the convergence rate of such learning scales poorly with the number of involved neurons. Here we propose a hybrid learning approach. Each neuron uses an RL-type strategy to learn how to approximate the gradients that backpropagation would provide. We provide proof that our approach converges to the true gradient for certain classes of networks. In both feedforward and convolutional networks, we empirically show that our approach learns to approximate the gradient, and can match or the performance of exact gradient-based learning. Learning feedback weights provides a biologically plausible mechanism of achieving good performance, without the need for precise, pre-specified learning rules.}
}
@inproceedings{10.5555.3157096.3157213,
  title        = {Direct Feedback Alignment Provides Learning in Deep Neural Networks},
  author       = {Nokland, Arild},
  year         = 2016,
  booktitle    = {International Conference on Neural Information Processing Systems (NeurIPS)},
  location     = {Barcelona, Spain},
  publisher    = {Curran Associates Inc.},
  series       = {NeurIPS'16},
  pages        = {1045–1053},
  isbn         = 9781510838819,
  numpages     = 9
}
@inproceedings{Xiao_Chen_Liao_Poggio_2019,
  title        = {Biologically-Plausible Learning Algorithms Can Scale to Large Datasets},
  author       = {Xiao, Will and Chen, Honglin and Liao, Qianli and Poggio, Tomaso},
  year         = 2019,
  booktitle    = {International Conference on Learning Representations, (ICLR)},
  abstractnote = {<p>The backpropagation (BP) algorithm is often thought to be biologically implau- sible in the brain. One of the main reasons is that BP requires symmetric weight matrices in the feedforward and feedback pathways. To address this “weight transport problem” (Grossberg, 1987), two biologically-plausible algorithms, pro- posed by Liao et al. (2016) and Lillicrap et al. (2016), relax BP’s weight sym- metry requirements and demonstrate comparable learning capabilities to that of BP on small datasets. However, a recent study by Bartunov et al. (2018) finds that although feedback alignment (FA) and some variants of target-propagation (TP) perform well on MNIST and CIFAR, they perform significantly worse than BP on ImageNet. Here, we additionally evaluate the sign-symmetry (SS) algo- rithm (Liao et al., 2016), which differs from both BP and FA in that the feedback and feedforward weights do not share magnitudes but share signs. We examined the performance of sign-symmetry and feedback alignment on ImageNet and MS COCO datasets using different network architectures (ResNet-18 and AlexNet for ImageNet; RetinaNet for MS COCO). Surprisingly, networks trained with sign- symmetry can attain classification performance approaching that of BP-trained networks. These results complement the study by Bartunov et al. (2018) and es- tablish a new benchmark for future biologically-plausible learning algorithms on more difficult datasets and more complex architectures.</p>}
}
@inproceedings{aaai.BalduzziVB15,
  title        = {Kickback Cuts Backprop's Red-Tape: Biologically Plausible Credit Assignment in Neural Networks},
  author       = {Balduzzi, David and Vanchinathan, Hastagiri and Buhmann, Joachim},
  year         = 2015,
  booktitle    = {AAAI Conference on Artificial Intelligence},
  publisher    = {{AAAI} Press},
  pages        = {485--491},
  url          = {http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9467},
  editor       = {Blai Bonet and Sven Koenig},
  timestamp    = {Wed, 10 Feb 2021 08:43:24 +0100},
  biburl       = {https://dblp.org/rec/conf/aaai/BalduzziVB15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{10.5555/3016100.3016156,
  title        = {How Important is Weight Symmetry in Backpropagation?},
  author       = {Liao, Qianli and Leibo, Joel Z. and Poggio, Tomaso},
  year         = 2016,
  booktitle    = {AAAI Conference on Artificial Intelligence},
  location     = {Phoenix, Arizona},
  publisher    = {AAAI Press},
  series       = {AAAI'16},
  pages        = {1837–1844},
  numpages     = 8
}
@inproceedings{pmlr-v33-carreira-perpinan14,
  title        = {{Distributed Optimization of Deeply Nested Systems}},
  author       = {Carreira-Perpinan, Miguel and Wang, Weiran},
  year         = 2014,
  month        = Apr,
  booktitle    = {International Conference on Artificial Intelligence and Statistics},
  publisher    = {PMLR},
  address      = {Reykjavik, Iceland},
  series       = {Proceedings of Machine Learning Research},
  volume       = 33,
  pages        = {10--19},
  url          = {https://proceedings.mlr.press/v33/carreira-perpinan14.html},
  editor       = {Kaski, Samuel and Corander, Jukka},
  pdf          = {http://proceedings.mlr.press/v33/carreira-perpinan14.pdf},
  abstract     = {Intelligent processing of complex signals such as images is often performed by a hierarchy of nonlinear processing layers, such as a deep net or an object recognition cascade. Joint estimation of the parameters of all the layers is a difficult nonconvex optimization. We describe a general strategy to learn the parameters and, to some extent, the architecture of nested systems, which we call the method of auxiliary coordinates (MAC). This replaces the original problem involving a deeply nested function with a constrained problem involving a different function in an augmented space without nesting. The constrained problem may be solved with penalty-based methods using alternating optimization over the parameters and the auxiliary coordinates. MAC has provable convergence, is easy to implement reusing existing algorithms for single layers, can be parallelized trivially and massively, applies even when parameter derivatives are not available or not desirable, can perform some model selection on the fly, and is competitive with state-of-the-art nonlinear optimizers even in the serial computation setting, often providing reasonable models within a few iterations.}
}
@inproceedings{10.5555/3045390.3045677,
  title        = {Training Neural Networks without Gradients: A Scalable ADMM Approach},
  author       = {Taylor, Gavin and Burmeister, Ryan and Xu, Zheng and Singh, Bharat and Patel, Ankit and Goldstein, Tom},
  year         = 2016,
  booktitle    = {International Conference on Machine Learning (ICML)},
  location     = {New York, NY, USA},
  publisher    = {JMLR.org},
  series       = {ICML'16},
  pages        = {2722–2731},
  abstract     = {With the growing importance of large network models and enormous training datasets, GPUs have become increasingly necessary to train neural networks. This is largely because conventional optimization algorithms rely on stochastic gradient methods that don't scale well to large numbers of cores in a cluster setting. Furthermore, the convergence of all gradient methods, including batch methods, suffers from common problems like saturation effects, poor conditioning, and saddle points. This paper explores an unconventional training method that uses alternating direction methods and Bregman iteration to train networks without gradient descent steps. The proposed method reduces the network training problem to a sequence of minimization substeps that can each be solved globally in closed form. The proposed method is advantageous because it avoids many of the caveats that make gradient methods slow on highly nonconvex problems. The method exhibits strong scaling in the distributed setting, yielding linear speedups even when split over thousands of cores.},
  numpages     = 10
}
@inproceedings{10.5555/3294771.3294935,
  title        = {Convergent Block Coordinate Descent for Training Tikhonov Regularized Deep Neural Networks},
  author       = {Zhang, Ziming and Brand, Matthew},
  year         = 2017,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  location     = {Long Beach, California, USA},
  publisher    = {Curran Associates Inc.},
  address      = {Red Hook, NY, USA},
  series       = {NIPS'17},
  pages        = {1719–1728},
  isbn         = 9781510860964,
  abstract     = {By lifting the ReLU function into a higher dimensional space, we develop a smooth multi-convex formulation for training feed-forward deep neural networks (DNNs). This allows us to develop a block coordinate descent (BCD) training algorithm consisting of a sequence of numerically well-behaved convex optimizations. Using ideas from proximal point methods in convex analysis, we prove that this BCD algorithm will converge globally to a stationary point with R-linear convergence rate of order one. In experiments with the MNIST database, DNNs trained with this BCD algorithm consistently yielded better test-set error rates than identical DNN architectures trained via all the stochastic gradient descent (SGD) variants in the Caffe toolbox.},
  numpages     = 10
}
@article{Lau_Zeng_Wu_Yao_2018,
  title        = {A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training},
  author       = {Lau, Tim Tsz-Kit and Zeng, Jinshan and Wu, Baoyuan and Yao, Yuan},
  year         = 2018,
  month        = Mar,
  publisher    = {arXiv},
  number       = {arXiv:1803.09082},
  doi          = {10.48550/arXiv.1803.09082},
  url          = {http://arxiv.org/abs/1803.09082},
  abstractnote = {Training deep neural networks (DNNs) efficiently is a challenge due to the associated highly nonconvex optimization. The backpropagation (backprop) algorithm has long been the most widely used algorithm for gradient computation of parameters of DNNs and is used along with gradient descent-type algorithms for this optimization task. Recent work have shown the efficiency of block coordinate descent (BCD) type methods empirically for training DNNs. In view of this, we propose a novel algorithm based on the BCD method for training DNNs and provide its global convergence results built upon the powerful framework of the Kurdyka-Lojasiewicz (KL) property. Numerical experiments on standard datasets demonstrate its competitive efficiency against standard optimizers with backprop.}
}
@article{Duan_Principe_2022,
  title        = {Training Deep Architectures Without End-to-End Backpropagation: A Survey on the Provably Optimal Methods},
  author       = {Duan, Shiyu and Principe, Jose C.},
  year         = 2022,
  month        = Aug,
  publisher    = {arXiv},
  number       = {arXiv:2101.03419},
  doi          = {10.48550/arXiv.2101.03419},
  url          = {http://arxiv.org/abs/2101.03419},
  abstractnote = {This tutorial paper surveys provably optimal alternatives to end-to-end backpropagation (E2EBP) -- the de facto standard for training deep architectures. Modular training refers to strictly local training without both the forward and the backward pass, i.e., dividing a deep architecture into several nonoverlapping modules and training them separately without any end-to-end operation. Between the fully global E2EBP and the strictly local modular training, there are weakly modular hybrids performing training without the backward pass only. These alternatives can match or surpass the performance of E2EBP on challenging datasets such as ImageNet, and are gaining increasing attention primarily because they offer practical advantages over E2EBP, which will be enumerated herein. In particular, they allow for greater modularity and transparency in deep learning workflows, aligning deep learning with the mainstream computer science engineering that heavily exploits modularization for scalability. Modular training has also revealed novel insights about learning and has further implications on other important research domains. Specifically, it induces natural and effective solutions to some important practical problems such as data efficiency and transferability estimation.}
}
@online{ff_algo,
  title        = {The Forward-Forward Algorithm: Some Preliminary Investigations},
  author       = {Hinton, Geoffrey},
  year         = 2022,
  url          = {https://www.cs.toronto.edu/~hinton/FFA13.pdf},
  urldate      = {2022-12-17}
}
@article{Hinton_2021,
  title        = {How to Represent Part-Whole Hierarchies in a Neural Network},
  author       = {Hinton, Geoffrey},
  year         = 2021,
  month        = Feb,
  publisher    = {arXiv},
  number       = {arXiv:2102.12627},
  doi          = {10.48550/arXiv.2102.12627},
  url          = {http://arxiv.org/abs/2102.12627},
  abstractnote = {This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language}
}
@inbook{Thrun_Pratt_1998,
  title        = {Learning to Learn: Introduction and Overview},
  author       = {Thrun, Sebastian and Pratt, Lorien},
  year         = 1998,
  booktitle    = {Learning to Learn},
  publisher    = {Springer US},
  address      = {Boston, MA},
  pages        = {3–17},
  doi          = {10.1007/978-1-4615-5529-2_1},
  isbn         = {978-1-4613-7527-2},
  url          = {http://link.springer.com/10.1007/978-1-4615-5529-2\%5F1},
  editor       = {Thrun, Sebastian and Pratt, Lorien},
  language     = {en}
}
@article{Hospedales_Antoniou_Micaelli_Storkey_2021,
  title        = {Meta-Learning in Neural Networks: A Survey},
  author       = {Hospedales, Timothy M. and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos J.},
  year         = 2021,
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages        = {1–1},
  doi          = {10.1109/TPAMI.2021.3079209},
  issn         = {0162-8828, 2160-9292, 1939-3539}
}
@article{Schrier_1984,
  title        = {Learning How to Learn: The Significance and Current Status of Learning Set Formation},
  author       = {Schrier, Allan M.},
  year         = 1984,
  month        = Jan,
  journal      = {Primates},
  volume       = 25,
  number       = 1,
  pages        = {95–102},
  doi          = {10.1007/BF02382299},
  issn         = {0032-8332, 1610-7365},
  language     = {en}
}
@inproceedings{10-5555-3305381-3305498,
  title        = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author       = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  year         = 2017,
  booktitle    = {International Conference on Machine Learning (ICML)},
  location     = {Sydney, NSW, Australia},
  publisher    = {JMLR.org},
  series       = {ICML'17},
  pages        = {1126–1135},
  abstract     = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  numpages     = 10
}
@inproceedings{10-5555-3327546-3327622,
  title        = {Probabilistic Model-Agnostic Meta-Learning},
  author       = {Finn, Chelsea and Xu, Kelvin and Levine, Sergey},
  year         = 2018,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  location     = {Montreal, Canada},
  publisher    = {Curran Associates Inc.},
  address      = {Red Hook, NY, USA},
  series       = {NeurIPS'18},
  pages        = {9537–9548},
  abstract     = {Meta-learning for few-shot learning entails acquiring a prior over previous tasks and experiences, such that new tasks be learned from small amounts of data. However, a critical challenge in few-shot learning is task ambiguity: even when a powerful prior can be meta-learned from a large number of prior tasks, a small dataset for a new task can simply be too ambiguous to acquire a single model (e.g., a classifier) for that task that is accurate. In this paper, we propose a probabilistic meta-learning algorithm that can sample models for a new task from a model distribution. Our approach extends model-agnostic meta-learning, which adapts to new tasks via gradient descent, to incorporate a parameter distribution that is trained via a variational lower bound. At meta-test time, our algorithm adapts via a simple procedure that injects noise into gradient descent, and at meta-training time, the model is trained such that this stochastic adaptation procedure produces samples from the approximate model posterior. Our experimental results show that our method can sample plausible classifiers and regressors in ambiguous few-shot learning problems. We also show how reasoning about ambiguity can also be used for downstream active learning problems.},
  numpages     = 12
}
@inproceedings{lee2018gradient,
  title        = {Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace},
  author       = {Lee, Yoonho and Choi, Seungjin},
  year         = 2018,
  booktitle    = {International Conference on Machine Learning (ICML)},
  pages        = {2927--2936},
  organization = {PMLR}
}
@article{rusu2018meta,
  title        = {Meta-Learning with Latent Embedding Optimization},
  author       = {Rusu, Andrei A. and Rao, Dushyant and Sygnowski, Jakub and Vinyals, Oriol and Pascanu, Razvan and Osindero, Simon and Hadsell, Raia},
  year         = 2019,
  month        = {Mar},
  publisher    = {arXiv},
  number       = {arXiv:1807.05960},
  doi          = {10.48550/arXiv.1807.05960},
  url          = {http://arxiv.org/abs/1807.05960},
  abstractnote = {Gradient-based meta-learning techniques are both widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they have practical difficulties when operating on high-dimensional parameter spaces in extreme low-data regimes. We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this low-dimensional latent space. The resulting approach, latent embedding optimization (LEO), decouples the gradient-based adaptation procedure from the underlying high-dimensional space of model parameters. Our evaluation shows that LEO can achieve state-of-the-art performance on the competitive miniImageNet and tieredImageNet few-shot classification tasks. Further analysis indicates LEO is able to capture uncertainty in the data, and can perform adaptation more effectively by optimizing in latent space.}
}
@inproceedings{ravi2017optimization,
  title        = {Optimization as a Model for Few-Shot Learning},
  author       = {Ravi, Sachin and Larochelle, Hugo},
  year         = 2017,
  booktitle    = {International Conference on Learning Representations (ICLR)}
}
@article{Li_Malik_2016,
  title        = {Learning to Optimize},
  author       = {Li, Ke and Malik, Jitendra},
  year         = 2016,
  month        = Jun,
  publisher    = {arXiv},
  number       = {arXiv:1606.01885},
  doi          = {10.48550/arXiv.1606.01885},
  url          = {http://arxiv.org/abs/1606.01885},
  abstractnote = {Algorithm design is a laborious process and often requires many iterations of ideation and validation. In this paper, we explore automating algorithm design and present a method to learn an optimization algorithm, which we believe to be the first method that can automatically discover a better algorithm. We approach this problem from a reinforcement learning perspective and represent any particular optimization algorithm as a policy. We learn an optimization algorithm using guided policy search and demonstrate that the resulting algorithm outperforms existing hand-engineered algorithms in terms of convergence speed and/or the final objective value.}
}
@article{Li_Zhou_Chen_Li_2017,
  title        = {Meta-SGD: Learning to Learn Quickly for Few-Shot Learning},
  author       = {Li, Zhenguo and Zhou, Fengwei and Chen, Fei and Li, Hang},
  year         = 2017,
  month        = Sep,
  publisher    = {arXiv},
  number       = {arXiv:1707.09835},
  doi          = {10.48550/arXiv.1707.09835},
  url          = {http://arxiv.org/abs/1707.09835},
  abstractnote = {Few-shot learning is challenging for learning algorithms that learn each task in isolation and from scratch. In contrast, meta-learning learns from many related tasks a meta-learner that can learn a new task more accurately and faster with fewer examples, where the choice of meta-learners is crucial. In this paper, we develop Meta-SGD, an SGD-like, easily trainable meta-learner that can initialize and adapt any differentiable learner in just one step, on both supervised learning and reinforcement learning. Compared to the popular meta-learner LSTM, Meta-SGD is conceptually simpler, easier to implement, and can be learned more efficiently. Compared to the latest meta-learner MAML, Meta-SGD has a much higher capacity by learning to learn not just the learner initialization, but also the learner update direction and learning rate, all in a single meta-learning process. Meta-SGD shows highly competitive performance for few-shot learning on regression, classification, and reinforcement learning.}
}
@article{heskes2000empirical,
  title        = {Empirical Bayes for Learning to Learn},
  author       = {Heskes, Tom M.},
  year         = 2000,
  publisher    = {San Francisco: Morgan Kaufmann}
}
@inbook{10-5555-3454287-3455002,
  title        = {Fast and Flexible Multi-Task Classification Using Conditional Neural Adaptive Processes},
  author       = {Requeima, James and Gordon, Jonathan and Bronskill, John and Nowozin, Sebastian and Turner, Richard E.},
  year         = 2019,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  publisher    = {Curran Associates Inc.},
  address      = {Red Hook, NY, USA},
  abstract     = {The goal of this paper is to design image classification systems that, after an initial multi-task training phase, can automatically adapt to new tasks encountered at test time. We introduce a conditional neural process based approach to the multi-task classification setting for this purpose, and establish connections to the meta-learning and few-shot learning literature. The resulting approach, called CNAPS, comprises a classifier whose parameters are modulated by an adaptation network that takes the current task's dataset as input. We demonstrate that CNAPS achieves state-of-the-art results on the challenging META-DATASET benchmark indicating high-quality transfer-learning. We show that the approach is robust, avoiding both over-fitting in low-shot regimes and under-fitting in high-shot regimes. Timing experiments reveal that CNAPS is computationally efficient at test-time as it does not involve gradient based adaptation. Finally, we show that trained models are immediately deployable to continual learning and active learning where they can outperform existing approaches that do not leverage transfer learning.},
  articleno    = 715,
  numpages     = 12
}
@article{Ha_Dai_Le_2016,
  title        = {HyperNetworks},
  author       = {Ha, David and Dai, Andrew and Le, Quoc V.},
  year         = 2016,
  month        = Dec,
  publisher    = {arXiv},
  number       = {arXiv:1609.09106},
  doi          = {10.48550/arXiv.1609.09106},
  url          = {http://arxiv.org/abs/1609.09106},
  abstractnote = {This work explores hypernetworks: an approach of using a one network, also known as a hypernetwork, to generate the weights for another network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype - the hypernetwork - and a phenotype - the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve near state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters.}
}
@article{Park_Oliva_2020,
  title        = {Meta-Curvature},
  author       = {Park, Eunbyung and Oliva, Junier B.},
  year         = 2020,
  month        = Jan,
  publisher    = {arXiv},
  number       = {arXiv:1902.03356},
  doi          = {10.48550/arXiv.1902.03356},
  url          = {http://arxiv.org/abs/1902.03356},
  abstractnote = {We propose meta-curvature (MC), a framework to learn curvature information for better generalization and fast model adaptation. MC expands on the model-agnostic meta-learner (MAML) by learning to transform the gradients in the inner optimization such that the transformed gradients achieve better generalization performance to a new task. For training large scale neural networks, we decompose the curvature matrix into smaller matrices in a novel scheme where we capture the dependencies of the model’s parameters with a series of tensor products. We demonstrate the effects of our proposed method on several few-shot learning tasks and datasets. Without any task specific techniques and architectures, the proposed method achieves substantial improvement upon previous MAML variants and outperforms the recent state-of-the-art methods. Furthermore, we observe faster convergence rates of the meta-training process. Finally, we present an analysis that explains better generalization performance with the meta-trained curvature.}
}
@inproceedings{10-5555-3294996-3295163,
  title        = {Prototypical Networks for Few-Shot Learning},
  author       = {Snell, Jake and Swersky, Kevin and Zemel, Richard},
  year         = 2017,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  location     = {Long Beach, California, USA},
  publisher    = {Curran Associates Inc.},
  address      = {Red Hook, NY, USA},
  series       = {NeurIPS'17},
  pages        = {4080–4090},
  isbn         = 9781510860964,
  abstract     = {We propose Prototypical Networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical Networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend Prototypical Networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.},
  numpages     = 11
}
@inproceedings{qiao2018few,
  title        = {Few-Shot Image Recognition by Predicting Parameters from Activations},
  author       = {Qiao, Siyuan and Liu, Chenxi and Shen, Wei and Yuille, Alan L.},
  year         = 2018,
  booktitle    = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {7229--7238}
}
@article{Chen_Liu_Kira_Wang_Huang_2020,
  title        = {A Closer Look at Few-shot Classification},
  author       = {Chen, Wei-Yu and Liu, Yen-Cheng and Kira, Zsolt and Wang, Yu-Chiang Frank and Huang, Jia-Bin},
  year         = 2020,
  month        = Jan,
  publisher    = {arXiv},
  number       = {arXiv:1904.04232},
  doi          = {10.48550/arXiv.1904.04232},
  url          = {http://arxiv.org/abs/1904.04232},
  abstractnote = {Few-shot classification aims to learn a classifier to recognize unseen classes during training with limited labeled examples. While significant progress has been made, the growing complexity of network designs, meta-learning algorithms, and differences in implementation details make a fair comparison difficult. In this paper, we present 1) a consistent comparative analysis of several representative few-shot classification algorithms, with results showing that deeper backbones significantly reduce the performance differences among methods on datasets with limited domain differences, 2) a modified baseline method that surprisingly achieves competitive performance when compared with the state-of-the-art on both the miniI and the CUB datasets, and 3) a new experimental setting for evaluating the cross-domain generalization ability for few-shot classification algorithms. Our results reveal that reducing intra-class variation is an important factor when the feature backbone is shallow, but not as critical when using deeper backbones. In a realistic cross-domain evaluation setting, we show that a baseline method with a standard fine-tuning practice compares favorably against other state-of-the-art few-shot learning algorithms.}
}
@inproceedings{NEURIPS2018_7876acb6,
  title        = {Evolved Policy Gradients},
  author       = {Houthooft, Rein and Chen, Richard Y. and Isola, Phillip and Stadie, Bradly C. and Wolski, Filip and Ho, Jonathan and Abbeel, Pieter},
  year         = 2018,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  publisher    = {Curran Associates, Inc.},
  volume       = 31,
  pages        = {},
  url          = {https://proceedings.neurips.cc/paper/2018/file/7876acb66640bad41f1e1371ef30c180-Paper.pdf},
  editor       = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett}
}
@article{Sung_Zhang_Xiang_Hospedales_Yang_2017,
  title        = {Learning to Learn: Meta-Critic Networks for Sample Efficient Learning},
  author       = {Sung, Flood and Zhang, Li and Xiang, Tao and Hospedales, Timothy and Yang, Yongxin},
  year         = 2017,
  month        = Jun,
  publisher    = {arXiv},
  number       = {arXiv:1706.09529},
  doi          = {10.48550/arXiv.1706.09529},
  url          = {http://arxiv.org/abs/1706.09529},
  abstractnote = {We propose a novel and flexible approach to meta-learning for learning-to-learn from only a few examples. Our framework is motivated by actor-critic reinforcement learning, but can be applied to both reinforcement and supervised learning. The key idea is to learn a meta-critic: an action-value function neural network that learns to criticise any actor trying to solve any specified task. For supervised learning, this corresponds to the novel idea of a trainable task-parametrised loss generator. This meta-critic approach provides a route to knowledge transfer that can flexibly deal with few-shot and semi-supervised conditions for both reinforcement and supervised learning. Promising results are shown on both reinforcement and supervised learning problems.}
}
@inproceedings{NEURIPS2019_e0e2b58d,
  title        = {Online-Within-Online Meta-Learning},
  author       = {Denevi, Giulia and Stamos, Dimitris and Ciliberto, Carlo and Pontil, Massimiliano},
  year         = 2019,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  publisher    = {Curran Associates, Inc.},
  volume       = 32,
  pages        = {},
  url          = {https://proceedings.neurips.cc/paper/2019/file/e0e2b58d64fb37a2527329a5ce093d80-Paper.pdf},
  editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@inproceedings{NEURIPS2018_b9a25e42,
  title        = {Learning To Learn Around A Common Mean},
  author       = {Denevi, Giulia and Ciliberto, Carlo and Stamos, Dimitris and Pontil, Massimiliano},
  year         = 2018,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  publisher    = {Curran Associates, Inc.},
  volume       = 31,
  pages        = {},
  url          = {https://proceedings.neurips.cc/paper/2018/file/b9a25e422ba96f7572089a00b838c3f8-Paper.pdf},
  editor       = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett}
}
@inproceedings{gonzalez2020improved,
  title        = {Improved Training Speed, Accuracy, and Data Utilization through Loss Function Optimization},
  author       = {Gonzalez, Santiago and Miikkulainen, Risto},
  year         = 2020,
  booktitle    = {2020 IEEE Congress on Evolutionary Computation (CEC)},
  pages        = {1--8},
  organization = {IEEE}
}
@inproceedings{li2019feature,
  title        = {Feature-Critic Networks for Heterogeneous Domain Generalization},
  author       = {Li, Yiying and Yang, Yongxin and Zhou, Wei and Hospedales, Timothy},
  year         = 2019,
  booktitle    = {International Conference on Machine Learning (ICML)},
  pages        = {3915--3924},
  organization = {PMLR}
}
@inproceedings{NEURIPS2019_6018df18,
  title        = {Learning to Learn By Self-Critique},
  author       = {Antoniou, Antreas and Storkey, Amos J.},
  year         = 2019,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  publisher    = {Curran Associates, Inc.},
  volume       = 32,
  pages        = {},
  url          = {https://proceedings.neurips.cc/paper/2019/file/6018df1842f7130f1b85a6f8e911b96b-Paper.pdf},
  editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@inproceedings{Boney2018SemiSupervisedFL,
  title        = {Semi-Supervised Few-Shot Learning with MAML},
  author       = {Rinu Boney and Alexander Ilin},
  year         = 2018,
  booktitle    = {International Conference on Learning Representations (ICLR)}
}
@article{zoph2017neural,
  title        = {Neural Architecture Search with Reinforcement Learning},
  author       = {Zoph, Barret and Le, Quoc V.},
  year         = 2017,
  month        = {Feb},
  publisher    = {arXiv},
  number       = {arXiv:1611.01578},
  doi          = {10.48550/arXiv.1611.01578},
  url          = {http://arxiv.org/abs/1611.01578},
  abstractnote = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.}
}
@inbook{Bayer_Wierstra_Togelius_Schmidhuber_2009,
  title        = {Evolving Memory Cell Structures for Sequence Learning},
  author       = {Bayer, Justin and Wierstra, Daan and Togelius, Julian and Schmidhuber, Jürgen},
  year         = 2009,
  booktitle    = {International Conference on Artificial Neural Networks (ICANN)},
  publisher    = {Springer Berlin Heidelberg},
  address      = {Berlin, Heidelberg},
  series       = {Lecture Notes in Computer Science},
  volume       = 5769,
  pages        = {755–764},
  doi          = {10.1007/978-3-642-04277-5_76},
  isbn         = {978-3-642-04276-8},
  url          = {http://link.springer.com/10.1007/978-3-642-04277-5\%5F76},
  editor       = {Alippi, Cesare and Polycarpou, Marios and Panayiotou, Christos and Ellinas, Georgios},
  collection   = {Lecture Notes in Computer Science}
}
@inproceedings{10-1609-aaai-v33i01-33014780,
  title        = {Regularized Evolution for Image Classifier Architecture Search},
  author       = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.},
  year         = 2019,
  booktitle    = {Conference on Artificial Intelligence (AAAI)},
  location     = {Honolulu, Hawaii, USA},
  publisher    = {AAAI Press},
  series       = {AAAI'19/IAAI'19/EAAI'19},
  doi          = {10.1609/aaai.v33i01.33014780},
  isbn         = {978-1-57735-809-1},
  url          = {10.1609/aaai.v33i01.33014780},
  articleno    = 587,
  numpages     = 10
}
@inproceedings{pmlr-v80-franceschi18a,
  title        = {Bilevel Programming for Hyperparameter Optimization and Meta-Learning},
  author       = {Franceschi, Luca and Frasconi, Paolo and Salzo, Saverio and Grazzi, Riccardo and Pontil, Massimiliano},
  year         = 2018,
  month        = Jul,
  booktitle    = {International Conference on Machine Learning (ICML)},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 80,
  pages        = {1568--1577},
  url          = {https://proceedings.mlr.press/v80/franceschi18a.html},
  editor       = {Dy, Jennifer and Krause, Andreas},
  pdf          = {http://proceedings.mlr.press/v80/franceschi18a/franceschi18a.pdf},
  abstract     = {We introduce a framework based on bilevel programming that unifies gradient-based hyperparameter optimization and meta-learning. We show that an approximate version of the bilevel problem can be solved by taking into explicit account the optimization dynamics for the inner objective. Depending on the specific setting, the outer variables take either the meaning of hyperparameters in a supervised learning problem or parameters of a meta-learner. We provide sufficient conditions under which solutions of the approximate problem converge to those of the exact problem. We instantiate our approach for meta-learning in the case of deep learning where representation layers are treated as hyperparameters shared across a set of training episodes. In experiments, we confirm our theoretical findings, present encouraging results for few-shot learning and contrast the bilevel approach against classical approaches for learning-to-learn.}
}
@article{Micaelli_Storkey_2021,
  title        = {Gradient-based Hyperparameter Optimization Over Long Horizons},
  author       = {Micaelli, Paul and Storkey, Amos},
  year         = 2021,
  month        = Sep,
  publisher    = {arXiv},
  number       = {arXiv:2007.07869},
  doi          = {10.48550/arXiv.2007.07869},
  url          = {http://arxiv.org/abs/2007.07869},
  abstractnote = {Gradient-based hyperparameter optimization has earned a widespread popularity in the context of few-shot meta-learning, but remains broadly impractical for tasks with long horizons (many gradient steps), due to memory scaling and gradient degradation issues. A common workaround is to learn hyperparameters online, but this introduces greediness which comes with a significant performance drop. We propose forward-mode differentiation with sharing (FDS), a simple and efficient algorithm which tackles memory scaling issues with forward-mode differentiation, and gradient degradation issues by sharing hyperparameters that are contiguous in time. We provide theoretical guarantees about the noise reduction properties of our algorithm, and demonstrate its efficiency empirically by differentiating through $sim 10^4$ gradient steps of unrolled optimization. We consider large hyperparameter search ranges on CIFAR-10 where we significantly outperform greedy gradient-based alternatives, while achieving $times 20$ speedups compared to the state-of-the-art black-box methods. Code is available at: url{https://github.com/polo5/FDS}}
}
@inproceedings{10-5555-3305381-3305502,
  title        = {Forward and Reverse Gradient-Based Hyperparameter Optimization},
  author       = {Franceschi, Luca and Donini, Michele and Frasconi, Paolo and Pontil, Massimiliano},
  year         = 2017,
  booktitle    = {International Conference on Machine Learning (ICML)},
  location     = {Sydney, NSW, Australia},
  publisher    = {JMLR.org},
  series       = {ICML'17},
  pages        = {1165–1173},
  abstract     = {We study two procedures (reverse-mode and forward-mode) for computing the gradient of the validation error with respect to the hyperparameters of any iterative learning algorithm such as stochastic gradient descent. These procedures mirror two methods of computing gradients for recurrent neural networks and have different trade-offs in terms of running time and space requirements. Our formulation of the reverse-mode procedure is linked to previous work by Maclaurin et al. (2015) but does not require reversible dynamics. The forward-mode procedure is suitable for real-time hyperparameter updates, which may significantly speed up hyperparameter optimization on large datasets. We present experiments on data cleaning and on learning task interactions. We also present one large-scale experiment where the use of previous gradient-based methods would be prohibitive.},
  numpages     = 9
}
@inproceedings{Cubuk_2019_CVPR,
  title        = {AutoAugment: Learning Augmentation Strategies From Data},
  author       = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
  year         = 2019,
  month        = Jun,
  booktitle    = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {113--123}
}
@inbook{Li_Hu_Wang_Hospedales_Robertson_Yang_2020,
  title        = {Differentiable Automatic Data Augmentation},
  author       = {Li, Yonggang and Hu, Guosheng and Wang, Yongtao and Hospedales, Timothy and Robertson, Neil M. and Yang, Yongxin},
  year         = 2020,
  booktitle    = {Computer Vision – ECCV 2020},
  publisher    = {Springer International Publishing},
  address      = {Cham},
  series       = {Lecture Notes in Computer Science},
  volume       = 12367,
  pages        = {580–595},
  doi          = {10.1007/978-3-030-58542-6_35},
  isbn         = {978-3-030-58541-9},
  url          = {https://link.springer.com/10.1007/978-3-030-58542-6\%5F35},
  editor       = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  collection   = {Lecture Notes in Computer Science},
  language     = {en}
}
@inproceedings{10-1609-aaai-v33i01-33015741,
  title        = {Active Mini-Batch Sampling Using Repulsive Point Processes},
  author       = {Zhang, Cheng and \"{O}ztireli, Cengiz and Mandt, Stephan and Salvi, Giampiero},
  year         = 2019,
  booktitle    = {Conference on Artificial Intelligence (AAAI)},
  location     = {Honolulu, Hawaii, USA},
  publisher    = {AAAI Press},
  series       = {AAAI'19/IAAI'19/EAAI'19},
  doi          = {10.1609/aaai.v33i01.33015741},
  isbn         = {978-1-57735-809-1},
  url          = {https://doi.org/10.1609/aaai.v33i01.33015741},
  abstract     = {The convergence speed of stochastic gradient descent (SGD) can be improved by actively selecting mini-batches. We explore sampling schemes where similar data points are less likely to be selected in the same mini-batch. In particular, we prove that such repulsive sampling schemes lower the variance of the gradient estimator. This generalizes recent work on using Determinantal Point Processes (DPPs) for mini-batch diversification (Zhang et al., 2017) to the broader class of repulsive point processes. We first show that the phenomenon of variance reduction by diversified sampling generalizes in particular to non-stationary point processes. We then show that other point processes may be computationally much more efficient than DPPs. In particular, we propose and investigate Poisson Disk sampling—frequently encountered in the computer graphics community—for this task. We show empirically that our approach improves over standard SGD both in terms of convergence speed as well as final model performance.},
  articleno    = 704,
  numpages     = 8
}
@article{fan2018learning,
  title        = {Learning to Teach},
  author       = {Fan, Yang and Tian, Fei and Qin, Tao and Li, Xiang-Yang and Liu, Tie-Yan},
  year         = 2018,
  month        = {May},
  publisher    = {arXiv},
  number       = {arXiv:1805.03643},
  doi          = {10.48550/arXiv.1805.03643},
  url          = {http://arxiv.org/abs/1805.03643},
  abstractnote = {Teaching plays a very important role in our society, by spreading human knowledge and educating our next generations. A good teacher will select appropriate teaching materials, impact suitable methodologies, and set up targeted examinations, according to the learning behaviors of the students. In the field of artificial intelligence, however, one has not fully explored the role of teaching, and pays most attention to machine emph{learning}. In this paper, we argue that equal attention, if not more, should be paid to teaching, and furthermore, an optimization framework (instead of heuristics) should be used to obtain good teaching strategies. We call this approach `learning to teach’. In the approach, two intelligent agents interact with each other: a student model (which corresponds to the learner in traditional machine learning algorithms), and a teacher model (which determines the appropriate data, loss function, and hypothesis space to facilitate the training of the student model). The teacher model leverages the feedback from the student model to optimize its own teaching strategies by means of reinforcement learning, so as to achieve teacher-student co-evolution. To demonstrate the practical value of our proposed approach, we take the training of deep neural networks (DNN) as an example, and show that by using the learning to teach techniques, we are able to use much less training data and fewer iterations to achieve almost the same accuracy for different kinds of DNN models (e.g., multi-layer perceptron, convolutional neural networks and recurrent neural networks) under various machine learning tasks (e.g., image classification and text understanding).}
}
@inproceedings{pmlr-v108-lorraine20a,
  title        = {Optimizing Millions of Hyperparameters by Implicit Differentiation},
  author       = {Lorraine, Jonathan and Vicol, Paul and Duvenaud, David},
  year         = 2020,
  month        = Aug,
  booktitle    = {International Conference on Artificial Intelligence and Statistics},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 108,
  pages        = {1540--1552},
  url          = {https://proceedings.mlr.press/v108/lorraine20a.html},
  editor       = {Chiappa, Silvia and Calandra, Roberto},
  pdf          = {http://proceedings.mlr.press/v108/lorraine20a/lorraine20a.pdf},
  abstract     = {We propose an algorithm for inexpensive gradient-based hyperparameter optimization that combines the implicit function theorem (IFT) with efficient inverse Hessian approximations. We present results about the relationship between the IFT and differentiating through optimization, motivating our algorithm.  We use the proposed approach to train modern network architectures with millions of weights and millions of hyper-parameters. For example, we learn a data-augmentation network—where every weight is a hyperparameter tuned for validation performance—outputting augmented training examples. Jointly tuning weights and hyper-parameters is only a few times more costly in memory and compute than standard training.}
}
@article{Wang_Zhu_Torralba_Efros_2020,
  title        = {Dataset Distillation},
  author       = {Wang, Tongzhou and Zhu, Jun-Yan and Torralba, Antonio and Efros, Alexei A.},
  year         = 2020,
  month        = Feb,
  publisher    = {arXiv},
  number       = {arXiv:1811.10959},
  doi          = {10.48550/arXiv.1811.10959},
  url          = {http://arxiv.org/abs/1811.10959},
  abstractnote = {Model distillation aims to distill the knowledge of a complex model into a simpler one. In this paper, we consider an alternative formulation called dataset distillation: we keep the model fixed and instead attempt to distill the knowledge from a large training dataset into a small one. The idea is to synthesize a small number of data points that do not need to come from the correct data distribution, but will, when given to the learning algorithm as training data, approximate the model trained on the original data. For example, we show that it is possible to compress 60,000 MNIST training images into just 10 synthetic distilled images (one per class) and achieve close to original performance with only a few gradient descent steps, given a fixed network initialization. We evaluate our method in various initialization settings and with different learning objectives. Experiments on multiple datasets show the advantage of our approach compared to alternative methods.}
}
@article{Andrychowicz_Baker_Chociej_Józefowicz_McGrew_Pachocki_Petron_Plappert_Powell_Ray_2020,
  title        = {Learning Dexterous In-Hand Manipulation},
  author       = {Andrychowicz, Marcin and Baker, Bowen and Chociej, Maciek and Józefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and Schneider, Jonas and Sidor, Szymon and Tobin, Josh and Welinder, Peter and Weng, Lilian and Zaremba, Wojciech},
  year         = 2020,
  month        = Jan,
  journal      = {The International Journal of Robotics Research},
  volume       = 39,
  number       = 1,
  pages        = {3–20},
  doi          = {10.1177/0278364919887447},
  issn         = {0278-3649, 1741-3176},
  abstractnote = {We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies that can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system such as friction coefficients and an object’s appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM .},
  language     = {en}
}
@article{ruiz2018learning,
  title        = {Learning To Simulate},
  author       = {Ruiz, Nataniel and Schulter, Samuel and Chandraker, Manmohan},
  year         = 2019,
  month        = {May},
  publisher    = {arXiv},
  number       = {arXiv:1810.02513},
  doi          = {10.48550/arXiv.1810.02513},
  url          = {http://arxiv.org/abs/1810.02513},
  abstractnote = {Simulation is a useful tool in situations where training data for machine learning models is costly to annotate or even hard to acquire. In this work, we propose a reinforcement learning-based method for automatically adjusting the parameters of any (non-differentiable) simulator, thereby controlling the distribution of synthesized data in order to maximize the accuracy of a model trained on that data. In contrast to prior art that hand-crafts these simulation parameters or adjusts only parts of the available parameters, our approach fully controls the simulator with the actual underlying goal of maximizing accuracy, rather than mimicking the real data distribution or randomly generating a large volume of data. We find that our approach (i) quickly converges to the optimal simulation parameters in controlled experiments and (ii) can indeed discover good sets of parameters for an image rendering simulator in actual computer vision applications.}
}
@article{Vuong_Vikram_Su_Gao_Christensen_2019,
  title        = {How to Pick the Domain Randomization Parameters for Sim-To-Real Transfer of Reinforcement Learning Policies?},
  author       = {Vuong, Quan and Vikram, Sharad and Su, Hao and Gao, Sicun and Christensen, Henrik I.},
  year         = 2019,
  month        = Mar,
  publisher    = {arXiv},
  number       = {arXiv:1903.11774},
  doi          = {10.48550/arXiv.1903.11774},
  url          = {http://arxiv.org/abs/1903.11774},
  abstractnote = {Recently, reinforcement learning (RL) algorithms have demonstrated remarkable success in learning complicated behaviors from minimally processed input. However, most of this success is limited to simulation. While there are promising successes in applying RL algorithms directly on real systems, their performance on more complex systems remains bottle-necked by the relative data inefficiency of RL algorithms. Domain randomization is a promising direction of research that has demonstrated impressive results using RL algorithms to control real robots. At a high level, domain randomization works by training a policy on a distribution of environmental conditions in simulation. If the environments are diverse enough, then the policy trained on this distribution will plausibly generalize to the real world. A human-specified design choice in domain randomization is the form and parameters of the distribution of simulated environments. It is unclear how to the best pick the form and parameters of this distribution and prior work uses hand-tuned distributions. This extended abstract demonstrates that the choice of the distribution plays a major role in the performance of the trained policies in the real world and that the parameter of this distribution can be optimized to maximize the performance of the trained policies in the real world}
}
@inproceedings{huang2019addressing,
  title        = {Addressing the Loss-Metric Mismatch with Adaptive Loss Alignment},
  author       = {Huang, Chen and Zhai, Shuangfei and Talbott, Walter and Martin, Miguel Bautista and Sun, Shih-Yu and Guestrin, Carlos and Susskind, Josh},
  year         = 2019,
  booktitle    = {International Conference on Machine Learning},
  pages        = {2891--2900},
  organization = {PMLR}
}
@article{Duan_Schulman_Chen_Bartlett_Sutskever_Abbeel_2016,
  title        = {RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning},
  author       = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
  year         = 2016,
  month        = Nov,
  publisher    = {arXiv},
  number       = {arXiv:1611.02779},
  doi          = {10.48550/arXiv.1611.02779},
  url          = {http://arxiv.org/abs/1611.02779},
  abstractnote = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a “fast” reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL$^2$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose (“slow”) RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the “fast” RL algorithm on the current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL$^2$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL$^2$ on a vision-based navigation task and show that it scales up to high-dimensional problems.}
}
@mastersthesis{schmidhuber-1987,
  title        = {Evolutionary Principles in Self-Referential Learning. On Learning now to Learn: The Meta-Meta-Meta...-Hook},
  author       = {Schmidhuber, Jürgen},
  year         = 1987,
  month        = May,
  url          = {http://www.idsia.ch/~juergen/diploma.html},
  school       = {Technische Universitat Munchen, Germany},
  size         = {62 pages},
  type         = {Diploma Thesis}
}
@article{Stanley_Clune_Lehman_Miikkulainen_2019,
  title        = {Designing Neural Networks through Neuroevolution},
  author       = {Stanley, Kenneth O. and Clune, Jeff and Lehman, Joel and Miikkulainen, Risto},
  year         = 2019,
  month        = Jan,
  journal      = {Nature Machine Intelligence},
  volume       = 1,
  number       = 1,
  pages        = {24–35},
  doi          = {10.1038/s42256-018-0006-z},
  issn         = {2522-5839},
  language     = {en}
}
@inproceedings{10-5555-3305890-3306069,
  title        = {Learned Optimizers That Scale and Generalize},
  author       = {Wichrowska, Olga and Maheswaranathan, Niru and Hoffman, Matthew W. and Colmenarejo, Sergio Gomez and Denil, Misha and de Freitas, Nando and Sohl-Dickstein, Jascha},
  year         = 2017,
  booktitle    = {International Conference on Machine Learning (ICML)},
  location     = {Sydney, NSW, Australia},
  publisher    = {JMLR.org},
  series       = {ICML'17},
  pages        = {3751–3760},
  abstract     = {Learning to learn has emerged as an important direction for achieving artificial intelligence. Two of the primary barriers to its adoption are an inability to scale to larger problems and a limited ability to generalize to new tasks. We introduce a learned gradient descent optimizer that generalizes well to new tasks, and which has significantly reduced memory and computation overhead. We achieve this by introducing a novel hierarchical RNN architecture, with minimal per-parameter overhead, augmented with additional architectural features that mirror the known structure of optimization tasks. We also develop a meta-training ensemble of small, diverse, optimization tasks capturing common properties of loss landscapes. The optimizer learns to outperform RMSProp/ADAM on problems in this corpus. More importantly, it performs comparably or better when applied to small convolutional neural networks, despite seeing no neural networks in its meta-training set. Finally, it generalizes to train Inception V3 and ResNet V2 architectures on the ImageNet dataset for thousands of steps, optimization problems that are of a vastly different scale than those it was trained on.},
  numpages     = 10
}
@article{antoniou2018how,
  title        = {How to train your MAML},
  author       = {Antoniou, Antreas and Edwards, Harrison and Storkey, Amos},
  year         = 2019,
  month        = {Mar},
  publisher    = {arXiv},
  number       = {arXiv:1810.09502},
  doi          = {10.48550/arXiv.1810.09502},
  url          = {http://arxiv.org/abs/1810.09502},
  abstractnote = {The field of few-shot learning has recently seen substantial advancements. Most of these advancements came from casting few-shot learning as a meta-learning problem. Model Agnostic Meta Learning or MAML is currently one of the best approaches for few-shot learning via meta-learning. MAML is simple, elegant and very powerful, however, it has a variety of issues, such as being very sensitive to neural network architectures, often leading to instability during training, requiring arduous hyperparameter searches to stabilize training and achieve high generalization and being very computationally expensive at both training and inference times. In this paper, we propose various modifications to MAML that not only stabilize the system, but also substantially improve the generalization performance, convergence speed and computational overhead of MAML, which we call MAML++.}
}
@article{Hochreiter_Schmidhuber_1997,
  title        = {Long Short-Term Memory},
  author       = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  year         = 1997,
  month        = {Nov},
  journal      = {Neural Computation},
  volume       = 9,
  number       = 8,
  pages        = {1735–1780},
  doi          = {10.1162/neco.1997.9.8.1735},
  issn         = {0899-7667, 1530-888X},
  abstractnote = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter’s (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  language     = {en}
}
@inproceedings{10.5555/2969033.2969073,
  title        = {Recurrent Models of Visual Attention},
  author       = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and Kavukcuoglu, Koray},
  year         = 2014,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  location     = {Montreal, Canada},
  publisher    = {MIT Press},
  address      = {Cambridge, MA, USA},
  series       = {NeurIPS'14},
  pages        = {2204–2212},
  numpages     = 9
}
@article{Cortes_Vapnik_1995,
  title        = {Support-Vector Networks},
  author       = {Cortes, Corinna and Vapnik, Vladimir},
  year         = 1995,
  month        = {Sep},
  journal      = {Machine Learning},
  volume       = 20,
  number       = 3,
  pages        = {273–297},
  doi          = {10.1007/BF00994018},
  issn         = {0885-6125, 1573-0565},
  language     = {en}
}
@article{Cover_Hart_1967,
  title        = {Nearest Neighbor Pattern Classification},
  author       = {Cover, Thomas and Hart, Peter},
  year         = 1967,
  month        = {Jan},
  journal      = {IEEE Transactions on Information Theory},
  volume       = 13,
  number       = 1,
  pages        = {21–27},
  doi          = {10.1109/TIT.1967.1053964},
  issn         = {0018-9448, 1557-9654}
}
@article{xiao2017/online,
  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  author       = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  year         = 2017,
  month        = {Sep},
  publisher    = {arXiv},
  number       = {arXiv:1708.07747},
  doi          = {10.48550/arXiv.1708.07747},
  url          = {http://arxiv.org/abs/1708.07747},
  abstractnote = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist}
}
@article{Lecun_Bottou_Bengio_Haffner_1998,
  title        = {Gradient-Based Learning Applied to Document Recognition},
  author       = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  year         = 1998,
  month        = {Nov},
  journal      = {Proceedings of the IEEE},
  volume       = 86,
  number       = 11,
  pages        = {2278–2324},
  doi          = {10.1109/5.726791},
  issn         = {00189219}
}
@inproceedings{storck1995reinforcement,
  title        = {Reinforcement driven information acquisition in non-deterministic environments},
  author       = {Storck, Jan and Hochreiter, Sepp and Schmidhuber, J{\"u}rgen and others},
  year         = 1995,
  booktitle    = {International Conference on Artificial Neural Networks (ICANN)},
  volume       = 2,
  pages        = {159--164}
}
@inbook{Klapper_Rybicka_Schraudolph_Schmidhuber_2001,
  title        = {Unsupervised Learning in LSTM Recurrent Neural Networks},
  author       = {Klapper-Rybicka, Magdalena and Schraudolph, Nicol N. and Schmidhuber, Jürgen},
  year         = 2001,
  booktitle    = {International Conference on Artificial Neural Networks (ICANN)},
  publisher    = {Springer Berlin Heidelberg},
  address      = {Berlin, Heidelberg},
  series       = {Lecture Notes in Computer Science},
  volume       = 2130,
  pages        = {684–691},
  doi          = {10.1007/3-540-44668-0_95},
  isbn         = {978-3-540-42486-4},
  url          = {http://link.springer.com/10.1007/3-540-44668-0\%5F95},
  editor       = {Dorffner, Georg and Bischof, Horst and Hornik, Kurt},
  collection   = {Lecture Notes in Computer Science}
}
@inproceedings{Balntas_Riba_Ponsa_Mikolajczyk_2016,
  title        = {Learning Local Feature Descriptors with Triplets and Shallow Convolutional Neural Networks},
  author       = {Balntas, Vassileios and Riba, Edgar and Ponsa, Daniel and Mikolajczyk, Krystian},
  year         = 2016,
  booktitle    = {Procedings of the British Machine Vision Conference},
  publisher    = {British Machine Vision Association},
  address      = {York, UK},
  pages        = {119.1--119.11},
  doi          = {10.5244/C.30.119},
  isbn         = {978-1-901725-59-9},
  url          = {http://www.bmva.org/bmvc/2016/papers/paper119/index.html},
  language     = {en}
}
@inproceedings{bowman2016generating,
  title        = {Generating Sentences from a Continuous Space},
  author       = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M and Jozefowicz, Rafal and Bengio, Samy},
  year         = 2016,
  booktitle    = {SIGNLL Conference on Computational Natural Language Learning (CoNLL)},
  pages        = {10--21},
  organization = {Association for Computational Linguistics (ACL)}
}
@inproceedings{Fu_Li_Liu_Gao_Celikyilmaz_Carin_2019,
  title        = {Cyclical Annealing Schedule: A Simple Approach to Mitigating},
  author       = {Fu, Hao and Li, Chunyuan and Liu, Xiaodong and Gao, Jianfeng and Celikyilmaz, Asli and Carin, Lawrence},
  year         = 2019,
  booktitle    = {Proceedings of the 2019 Conference of the North},
  publisher    = {Association for Computational Linguistics},
  address      = {Minneapolis, Minnesota},
  pages        = {240–250},
  doi          = {10.18653/v1/N19-1021},
  url          = {http://aclweb.org/anthology/N19-1021},
  language     = {en}
}
@inproceedings{NIPS2017_7a98af17,
  title        = {Neural Discrete Representation Learning},
  author       = {{van den Oord}, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
  year         = 2017,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  publisher    = {Curran Associates, Inc.},
  volume       = 30,
  pages        = {},
  url          = {https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf},
  editor       = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett}
}
@article{Piaget_1964,
  title        = {Part I: Cognitive Development in Children: Development and Learning},
  author       = {Piaget, Jean},
  year         = 1964,
  month        = {Sep},
  journal      = {Journal of Research in Science Teaching},
  volume       = 2,
  number       = 3,
  pages        = {176–186},
  doi          = {10.1002/tea.3660020306},
  issn         = {0022-4308, 1098-2736},
  language     = {en}
}
@inproceedings{DBLP:conf/bmvc/JungLO0SP22,
  title        = {Unified Negative Pair Generation toward Well-Discriminative Feature Space for Face Recognition},
  author       = {Jung, Junuk and Lee, Seonhoon and Oh, Heung{-}Seon and Park, Yongjun and Son, Sungbin and Park, Joochan},
  year         = 2022,
  booktitle    = {British Machine Vision Conference 2022 (BMVC)},
  publisher    = {{BMVA} Press},
  pages        = 421,
  url          = {https://bmvc2022.mpi-inf.mpg.de/421/},
  timestamp    = {Thu, 16 Feb 2023 16:15:43 +0100},
  biburl       = {https://dblp.org/rec/conf/bmvc/JungLO0SP22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{NEURIPS2021_e614f646,
  title        = {Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation},
  author       = {Bengio, Emmanuel and Jain, Moksh and Korablyov, Maksym and Precup, Doina and Bengio, Yoshua},
  year         = 2021,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  publisher    = {Curran Associates, Inc.},
  volume       = 34,
  pages        = {27381--27394},
  url          = {https://proceedings.neurips.cc/paper/2021/file/e614f646836aaed9f89ce58e837e2310-Paper.pdf},
  editor       = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan}
}
@article{Wolfrum_Wolff_Lücke_vonderMalsburg_2008,
  title        = {A Recurrent Dynamic Model for Correspondence-Based Face Recognition},
  author       = {Wolfrum, Philipp and Wolff, Christian and Lücke, Jörg and {von der Malsburg}, Christoph},
  year         = 2008,
  month        = {Dec},
  journal      = {Journal of Vision},
  volume       = 8,
  number       = 7,
  pages        = 34,
  doi          = {10.1167/8.7.34},
  issn         = {1534-7362},
  language     = {en}
}
@article{Willshaw_VonDerMalsburg_1979,
  title        = {A Marker Induction Mechanism for the Establishment of Ordered Neural Mappings: Its Application to the Retinotectal Problem},
  author       = {Willshaw, David J. and {von der Malsburg}, Christoph},
  year         = 1979,
  month        = {Nov},
  journal      = {Philosophical Transactions of the Royal Society of London. B, Biological Sciences},
  volume       = 287,
  number       = 1021,
  pages        = {203–243},
  doi          = {10.1098/rstb.1979.0056},
  issn         = {0080-4622, 2054-0280},
  abstractnote = {This paper examines the idea that ordered patterns of nerve connections are set up by means of markers carried by the individual cells. The case of the ordered retinotectal projection in amphibia and fishes is discussed in great detail. It is suggested that retinotectal mappings are the result of two mechanisms acting in concert. One mechanism induces a set of retinal markers into the tectum. By this means, an initially haphazard pattern of synapses is transformed into a continuous or piece-wise continuous projection. The other mechanism places the individual pieces of the map in the correct orientation. The machinery necessary for this inductive scheme has been expressed in terms of a set of differential equations, which have been solved numerically for a number of cases. Straightforward assumptions are made as to how markers are distributed in the retina; how they are induced into the tectum; and how the induced markers bring about alterations in the pattern of synaptic contacts. A detailed physiological interpretation of the model is given. The inductive mechanism has been formulated at the level of the individual synaptic interactions. Therefore, it is possible to specify, in a given situation, not only the nature of the end state of the mapping but also how the mapping develops over time. The role of the modes of growth of retina and tectum in shaping the developing projection becomes clear. Since, on this model, the tectum is initially devoid of markers, there is an important difference between the development and the regeneration of ordered mappings. In the development of duplicate maps from various types of compound-eyes, it is suggested that the tectum, rather than the retina, contains an abnormal distribution of markers. An important parameter in these experiments, and also in the regeneration experiments where part-duplication has been found, is the range of interaction amongst the retinal cells. It is suggested that the results of many of the regeneration experiments (including apparently contradictory ones) are manifestations of a conflict between the two alternative ways of specifying the orientation of the map: through the information carried by the markers previously induced into the tectum and through the orientation mechanism itself.},
  language     = {en}
}
@book{wiskott1996face,
  title        = {Face Recognition by Dynamic Link Matching},
  author       = {Wiskott, Laurenz and {von der Malsburg}, Christoph},
  year         = 1996,
  publisher    = {Ruhr-Univ., Inst. für Neuroinformatik}
}
@article{Fernandes_vonderMalsburg_2015,
  title        = {Self-Organization of Control Circuits for Invariant Fiber Projections},
  author       = {Fernandes, Tomas and {von der Malsburg}, Christoph},
  year         = 2015,
  month        = {May},
  journal      = {Neural Computation},
  volume       = 27,
  number       = 5,
  pages        = {1005–1032},
  doi          = {10.1162/NECO_a_00725},
  issn         = {0899-7667, 1530-888X},
  abstractnote = {Assuming that patterns in memory are represented as two-dimensional arrays of local features, just as they are in primary visual cortices, pattern recognition can take the form of elastic graph matching (Lades et al., 1993 ). Neural implementation of this may be based on preorganized fiber projections that can be activated rapidly with the help of control units (Wolfrum, Wolff, Lücke, & von der Malsburg, 2008 ). Each control unit governs a set of projection fibers that form part of a coherent mapping. We describe a mathematical model for the ontogenesis of the underlying connectivity based on a principle of network self-organization as described by the Häussler system (Häussler & von der Malsburg, 1983 ), modified to be sensitive to pattern similarity and to support formation of multiple mappings, each under the command of a control unit. The process takes the form of a soft-winner-take-all, where units compete for the representation of maps. We show simulations for invariant point-to-point and feature-to-feature mappings.},
  language     = {en}
}
@book{Marr_2010,
  title        = {Vision: A Computational Investigation into the Human Representation and Processing of Visual Information},
  author       = {Marr, David},
  year         = 2010,
  publisher    = {The MIT Press},
  doi          = {10.7551/mitpress/9780262514620.001.0001},
  isbn         = {978-0-262-28961-0},
  url          = {https://direct.mit.edu/books/book/3299},
  language     = {en}
}
@article{kohler1929gestalt,
  title        = {Gestalt Psychology},
  author       = {Köhler, Wolfgang},
  year         = 1929,
  publisher    = {Liveright}
}
@article{Long_yan_chen_2021,
  title        = {A Graph Neural Network for Superpixel Image Classification},
  author       = {Long, Jianwu and Yan, Zeran and Chen, Hongfa},
  year         = 2021,
  month        = {Apr},
  journal      = {Journal of Physics: Conference Series},
  volume       = 1871,
  number       = 1,
  pages        = {012071},
  doi          = {10.1088/1742-6596/1871/1/012071},
  issn         = {1742-6588, 1742-6596},
  abstractnote = {Abstract The classification of superpixel images by graph neural networks has gradually become a research hotspot. It is a crucial issue to embed super-pixel images from lowdimensional to high-dimensional so as to turn complex image information into graph signals. This paper proposes a method for image classification using a graph neural network (GNN) model. We convert the input image into a region adjacency graph (RAG) composed of superpixels as nodes, and use residual and concat structure to extract deep features. Finally, the loss function that increases the distance between classes and compactness within classes is used as supervision. Experiments have been tested with different numbers of superpixels on multiple datasets, and the results show that our method has a great performance in superpixel images classification.}
}
@inproceedings{10.5555/3327345.3327389,
  title        = {Hierarchical Graph Representation Learning with Differentiable Pooling},
  author       = {Ying, Rex and You, Jiaxuan and Morris, Christopher and Ren, Xiang and Hamilton, William L. and Leskovec, Jure},
  year         = 2018,
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  location     = {Montreal, Canada},
  publisher    = {Curran Associates Inc.},
  address      = {Red Hook, NY, USA},
  series       = {NeurIPS'18},
  pages        = {4805–4815},
  numpages     = 11
}
@article{Vasudevan_Bassenne_Islam_Xing_2023,
  title        = {Image Classification Using Graph Neural Network and Multiscale Wavelet Superpixels},
  author       = {Vasudevan, Varun and Bassenne, Maxime and Islam, Md Tauhidul and Xing, Lei},
  year         = 2023,
  month        = {Feb},
  journal      = {Pattern Recognition Letters},
  volume       = 166,
  pages        = {89–96},
  doi          = {10.1016/j.patrec.2023.01.003},
  issn         = {01678655},
  language     = {en}
}
@inproceedings{li2019graph,
  title        = {Graph Matching Networks for Learning the Similarity of Graph Structured Objects},
  author       = {Li, Yujia and Gu, Chenjie and Dullien, Thomas and Vinyals, Oriol and Kohli, Pushmeet},
  year         = 2019,
  booktitle    = {International conference on machine learning},
  pages        = {3835--3845},
  organization = {PMLR}
}
@book{Xu_Nikolentzos_Vazirgiannis_Boström_2022,
  title        = {Image Keypoint Matching Using Graph Neural Networks},
  author       = {Xu, Nancy and Nikolentzos, Giannis and Vazirgiannis, Michalis and Boström, Henrik},
  year         = 2022,
  booktitle    = {Complex Networks & Their Applications X},
  publisher    = {Springer International Publishing},
  address      = {Cham},
  series       = {Studies in Computational Intelligence},
  volume       = 1016,
  pages        = {441–451},
  doi          = {10.1007/978-3-030-93413-2_37},
  isbn         = {978-3-030-93412-5},
  url          = {https://link.springer.com/10.1007/978-3-030-93413-2\%5F37},
  editor       = {Benito, Rosa Maria and Cherifi, Chantal and Cherifi, Hocine and Moro, Esteban and Rocha, Luis M. and Sales-Pardo, Marta},
  collection   = {Studies in Computational Intelligence},
  language     = {en}
}
@article{tolstikhin2021mlp,
  title        = {MLP-Mixer: An All-MLP Architecture for Vision},
  author       = {Tolstikhin, Ilya O. and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and others},
  year         = 2021,
  journal      = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume       = 34,
  pages        = {24261--24272}
}
@article{van2008visualizing,
  title        = {Visualizing Data using t-SNE},
  author       = {{van der Maaten}, Laurens and Hinton, Geoffrey},
  year         = 2008,
  journal      = {Journal of Machine Learning Research},
  volume       = 9,
  number       = 11
}
@article{van2008visualizing,
  title        = {Visualizing Data using t-SNE},
  author       = {{von der Malsburg}, Christoph and Bienenstock, Elie L.},
  year         = 1987,
  journal      = {Europhysics Letters},
  volume       = 3,
  number       = 11
}
 @article{Lades_Vorbruggen_Buhmann_Lange_vonderMalsburg_Wurtz_Konen_1993, title={Distortion Invariant Object Recognition in the Dynamic Link Architecture}, volume={42}, ISSN={00189340}, DOI={10.1109/12.210173}, number={3}, journal={IEEE Transactions on Computers}, author={Lades, M. and Vorbruggen, J.C. and Buhmann, J. and Lange, J. and {von der Malsburg}, C. and Wurtz, R.P. and Konen, W.}, year={1993}, month={Mar}, pages={300–311} }

 @article{Bienenstock_vonderMalsburg_1987, title={A Neural Network for Invariant Pattern Recognition}, volume={4}, ISSN={0295-5075, 1286-4854}, DOI={10.1209/0295-5075/4/1/020}, number={1}, journal={Europhysics Letters (EPL)}, author={Bienenstock, Elie and {von der Malsburg}, Christoph}, year={1987}, month={Jul}, pages={121–126} }

 @inbook{Mohebali_Tahmassebi_Meyer_Baese_Gandomi_2020, title={Probabilistic Neural Networks}, ISBN={978-0-12-816514-0}, url={https://linkinghub.elsevier.com/retrieve/pii/B978012816514000014X}, DOI={10.1016/B978-0-12-816514-0.00014-X}, booktitle={Handbook of Probabilistic Models}, publisher={Elsevier}, author={Mohebali, Behshad and Tahmassebi, Amirhessam and Meyer-Baese, Anke and Gandomi, Amir H.}, year={2020}, pages={347–367}, language={en} }

 @article{Specht_1967, title={Generation of Polynomial Discriminant Functions for Pattern Recognition}, volume={EC-16}, ISSN={0367-7508}, DOI={10.1109/PGEC.1967.264667}, number={3}, journal={IEEE Transactions on Electronic Computers}, author={Specht, Donald F.}, year={1967}, month={Jun}, pages={308–319} }

 @article{Specht_1990, title={Probabilistic Neural Networks}, volume={3}, ISSN={08936080}, DOI={10.1016/0893-6080(90)90049-Q}, number={1}, journal={Neural Networks}, author={Specht, Donald F.}, year={1990}, month={Jan}, pages={109–118}, language={en} }

 @article{Zeinali_Story_2017, title={Competitive Probabilistic Nneural Network}, volume={24}, ISSN={10692509, 18758835}, DOI={10.3233/ICA-170540}, number={2}, journal={Integrated Computer-Aided Engineering}, author={Zeinali, Yasha and Story, Brett A.}, year={2017}, month={Mar}, pages={105–118} }

@online{sdp_mountain,
  title        = {Sandro del Prete - Mountain Spirit in Winter},
  author       = {Impossible World},
  year         = 2023,
  url          = {https://im-possible.info/english/art/delprete/the-master-of-illusions/008.html},
  urldate      = {2023-06-10}
}

 @article{Ahmad_Hawkins_2015, title={Properties of Sparse Distributed Representations and their Application to Hierarchical Temporal Memory}, url={http://arxiv.org/abs/1503.07469}, abstractNote={Empirical evidence demonstrates that every region of the neocortex represents information using sparse activity patterns. This paper examines Sparse Distributed Representations (SDRs), the primary information representation strategy in Hierarchical Temporal Memory (HTM) systems and the neocortex. We derive a number of properties that are core to scaling, robustness, and generalization. We use the theory to provide practical guidelines and illustrate the power of SDRs as the basis of HTM. Our goal is to help create a unified mathematical and practical framework for SDRs as it relates to cortical function.}, note={arXiv:1503.07469 [cs, q-bio]}, number={arXiv:1503.07469}, publisher={arXiv}, author={Ahmad, Subutai and Hawkins, Jeff}, year={2015}, month={Mar} }


 @article{Akhtar_Mian_2018, title={Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey}, volume={6}, ISSN={2169-3536}, DOI={10.1109/ACCESS.2018.2807385}, journal={IEEE Access}, author={Akhtar, Naveed and Mian, Ajmal}, year={2018}, pages={14410–14430} }

 @article{Gabor_1946, title={Theory of Communication. Part 1: The Analysis of Information}, volume={93}, ISSN={2054-0604}, DOI={10.1049/ji-3-2.1946.0074}, number={26}, journal={Journal of the Institution of Electrical Engineers - Part III: Radio and Communication Engineering}, author={Gabor, D.}, year={1946}, month={Nov}, pages={429–441}, language={en} }

 @article{Granlund_1978, title={In Search of a General Picture Processing Operator}, volume={8}, ISSN={0146664X}, DOI={10.1016/0146-664X(78)90047-3}, number={2}, journal={Computer Graphics and Image Processing}, author={Granlund, Goesta H.}, year={1978}, month={Oct}, pages={155–173}, language={en} }

 @article{Miconi_2021, title={Hebbian learning with gradients: Hebbian convolutional neural networks with modern deep learning frameworks}, url={http://arxiv.org/abs/2107.01729}, abstractNote={Deep learning networks generally use non-biological learning methods. By contrast, networks based on more biologically plausible learning, such as Hebbian learning, show comparatively poor performance and difficulties of implementation. Here we show that Hebbian learning in hierarchical, convolutional neural networks can be implemented almost trivially with modern deep learning frameworks, by using specific losses whose gradients produce exactly the desired Hebbian updates. We provide expressions whose gradients exactly implement a plain Hebbian rule (dw ~= xy), Grossberg’s instar rule (dw ~= y(x-w)), and Oja’s rule (dw ~= y(x-yw)). As an application, we build Hebbian convolutional multi-layer networks for object recognition. We observe that higher layers of such networks tend to learn large, simple features (Gabor-like filters and blobs), explaining the previously reported decrease in decoding performance over successive layers. To combat this tendency, we introduce interventions (denser activations with sparse plasticity, pruning of connections between layers) which result in sparser learned features, massively increase performance, and allow information to increase over successive layers. We hypothesize that more advanced techniques (dynamic stimuli, trace learning, feedback connections, etc.), together with the massive computational boost offered by modern deep learning frameworks, could greatly improve the performance and biological relevance of multi-layer Hebbian networks.}, note={arXiv:2107.01729 [cs]}, number={arXiv:2107.01729}, publisher={arXiv}, author={Miconi, Thomas}, year={2021}, month={Nov} }

 @article{Revonsuo_Newman_1999, title={Binding and Consciousness}, volume={8}, ISSN={10538100}, DOI={10.1006/ccog.1999.0393}, number={2}, journal={Consciousness and Cognition}, author={Revonsuo, Antti and Newman, James}, year={1999}, month={Jun}, pages={123–127}, language={en} }

 @article{Feldman_2013, title={The Neural Binding Problem(s)}, volume={7}, ISSN={1871-4080, 1871-4099}, DOI={10.1007/s11571-012-9219-8}, number={1}, journal={Cognitive Neurodynamics}, author={Feldman, Jerome}, year={2013}, month={Feb}, pages={1–11}, language={en} }

 @article{Wiskott_Fellous_Kuiger_VonDerMalsburg_1997, title={Face Recognition by Elastic Bunch Graph Matching}, volume={19}, ISSN={0162-8828, 2160-9292}, DOI={10.1109/34.598235}, number={7}, journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, author={Wiskott, L. and Fellous, J.-M. and Kuiger, N. and Von Der Malsburg, C.}, year={1997}, month={Jul}, pages={775–779} }

 @article{Olshausen_Field_1996, title={Emergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code for Natural Images}, volume={381}, ISSN={0028-0836, 1476-4687}, DOI={10.1038/381607a0}, number={6583}, journal={Nature}, author={Olshausen, Bruno A. and Field, David J.}, year={1996}, month={Jun}, pages={607–609}, language={en} }

 @article{Bao_She_McGill_Tsao_2020, title={A Map of Object Space in Primate Inferotemporal Cortex}, volume={583}, ISSN={0028-0836, 1476-4687}, DOI={10.1038/s41586-020-2350-5}, number={7814}, journal={Nature}, author={Bao, Pinglei and She, Liang and McGill, Mason and Tsao, Doris Y.}, year={2020}, month={Jul}, pages={103–108}, language={en} }

 @article{Josselyn_Tonegawa_2020, title={Memory Engrams: Recalling the Past and Imagining the Future}, volume={367}, ISSN={0036-8075, 1095-9203}, DOI={10.1126/science.aaw4325}, number={6473}, journal={Science}, author={Josselyn, Sheena A. and Tonegawa, Susumu}, year={2020}, month={Jan}, pages={eaaw4325}, language={en} }
 
 @article{Ma_Tsao_Shum_2022, title={On the Principles of Parsimony and Self-consistency for the Emergence of Intelligence}, volume={23}, ISSN={2095-9184, 2095-9230}, DOI={10.1631/FITEE.2200297}, number={9}, journal={Frontiers of Information Technology & Electronic Engineering}, author={Ma, Yi and Tsao, Doris and Shum, Heung-Yeung}, year={2022}, month={Sep}, pages={1298–1323}, language={en} }

 @article{Touvron_Lavril_Izacard_Martinet_Lachaux_Lacroix_Rozière_Goyal_Hambro_Azhar, title={LLaMA: Open and Efficient Foundation Language Models}, url={http://arxiv.org/abs/2302.13971}, abstractNote={We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.}, note={arXiv:2302.13971 [cs]}, number={arXiv:2302.13971}, publisher={arXiv}, author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume}, year={2023}, month={Feb} }

 @article{Ouyang_Wu_Jiang_Almeida_Wainwright_Mishkin_Zhang_Agarwal_Slama_Ray, title={Training Language Models to Follow Instructions with Human Feedback}, url={http://arxiv.org/abs/2203.02155}, abstractNote={Making language models bigger does not inherently make them better at following a user’s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.}, note={arXiv:2203.02155 [cs]}, number={arXiv:2203.02155}, publisher={arXiv}, author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan}, year={2022}, month={Mar} }

 @article{Feldman_Foulds_Pan_2023, title={Trapping LLM Hallucinations Using Tagged Context Prompts}, url={http://arxiv.org/abs/2306.06085}, abstractNote={Recent advances in large language models (LLMs), such as ChatGPT, have led to highly sophisticated conversation agents. However, these models suffer from “hallucinations,” where the model generates false or fabricated information. Addressing this challenge is crucial, particularly with AI-driven platforms being adopted across various sectors. In this paper, we propose a novel method to recognize and flag instances when LLMs perform outside their domain knowledge, and ensuring users receive accurate information. We find that the use of context combined with embedded tags can successfully combat hallucinations within generative language models. To do this, we baseline hallucination frequency in no-context prompt-response pairs using generated URLs as easily-tested indicators of fabricated data. We observed a significant reduction in overall hallucination when context was supplied along with question prompts for tested generative engines. Lastly, we evaluated how placing tags within contexts impacted model responses and were able to eliminate hallucinations in responses with 98.88% effectiveness.}, note={arXiv:2306.06085 [cs]}, number={arXiv:2306.06085}, publisher={arXiv}, author={Feldman, Philip and Foulds, James R. and Pan, Shimei}, year={2023}, month={Jun} }

 @article{Manakul_Liusie_Gales_2023, title={SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models}, url={http://arxiv.org/abs/2303.08896}, abstractNote={Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose “SelfCheckGPT”, a simple sampling-based approach that can be used to fact-check black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if a LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that in sentence hallucination detection, our approach has AUC-PR scores comparable to or better than grey-box methods, while SelfCheckGPT is best at passage factuality assessment.}, note={arXiv:2303.08896 [cs]}, number={arXiv:2303.08896}, publisher={arXiv}, author={Manakul, Potsawee and Liusie, Adian and Gales, Mark J. F.}, year={2023}, month={May} }

 @article{Hoi_Sahoo_Lu_Zhao_2021, title={Online learning: A Comprehensive Survey}, volume={459}, ISSN={09252312}, DOI={10.1016/j.neucom.2021.04.112}, journal={Neurocomputing}, author={Hoi, Steven C.H. and Sahoo, Doyen and Lu, Jing and Zhao, Peilin}, year={2021}, month={Oct}, pages={249–289}, language={en} }

 @article{Zhuang_Qi_Duan_Xi_Zhu_Zhu_Xiong_He_2021, title={A Comprehensive Survey on Transfer Learning}, volume={109}, ISSN={0018-9219, 1558-2256}, DOI={10.1109/JPROC.2020.3004555}, number={1}, journal={Proceedings of the IEEE}, author={Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing}, year={2021}, month={Jan}, pages={43–76} }

 @article{Richards_Lillicrap_Beaudoin_Bengio_Bogacz_Christensen_Clopath_Costa_De, title={A Deep Learning Framework for Neuroscience}, volume={22}, ISSN={1097-6256, 1546-1726}, DOI={10.1038/s41593-019-0520-2}, number={11}, journal={Nature Neuroscience}, author={Richards, Blake A. and Lillicrap, Timothy P. and Beaudoin, Philippe and Bengio, Yoshua and Bogacz, Rafal and Christensen, Amelia and Clopath, Claudia and Costa, Rui Ponte and De Berker, Archy and Ganguli, Surya and Gillon, Colleen J. and Hafner, Danijar and Kepecs, Adam and Kriegeskorte, Nikolaus and Latham, Peter and Lindsay, Grace W. and Miller, Kenneth D. and Naud, Richard and Pack, Christopher C. and Poirazi, Panayiota and Roelfsema, Pieter and Sacramento, João and Saxe, Andrew and Scellier, Benjamin and Schapiro, Anna C. and Senn, Walter and Wayne, Greg and Yamins, Daniel and Zenke, Friedemann and Zylberberg, Joel and Therien, Denis and Kording, Konrad P.}, year={2019}, month={Nov}, pages={1761–1770}, language={en} }

 @article{Hawkins_Lewis_Klukas_Purdy_Ahmad_2019, title={A Framework for Intelligence and Cortical Function Based on Grid Cells in the Neocortex}, volume={12}, ISSN={1662-5110}, DOI={10.3389/fncir.2018.00121}, journal={Frontiers in Neural Circuits}, author={Hawkins, Jeff and Lewis, Marcus and Klukas, Mirko and Purdy, Scott and Ahmad, Subutai}, year={2019}, month={Jan}, pages={121} }

 @inbook{Mountcastle_1978, address={Cambridge, MA}, title={An Organizing Principle for Cerebral Function: The Unit Model and the Distributed System}, booktitle={The Mindful Brain}, publisher={MIT Press}, author={Mountcastle, Vernon}, year={1978}, pages={7–50}, language={en} }

 @article{Mountcastle_1997, title={The Columnar Organization of the Neocortex}, volume={120}, ISSN={14602156}, DOI={10.1093/brain/120.4.701}, number={4}, journal={Brain}, author={Mountcastle, Vernon}, year={1997}, month={Apr}, pages={701–722} }

 @article{Lewis_Purdy_Ahmad_Hawkins_2019, title={Locations in the Neocortex: A Theory of Sensorimotor Object Recognition Using Cortical Grid Cells}, volume={13}, ISSN={1662-5110}, DOI={10.3389/fncir.2019.00022}, journal={Frontiers in Neural Circuits}, author={Lewis, Marcus and Purdy, Scott and Ahmad, Subutai and Hawkins, Jeff}, year={2019}, month={Apr}, pages={22} }

 @inproceedings{TinKamHo_1995, address={Montreal, Que., Canada}, title={Random Decision Forests}, volume={1}, ISBN={978-0-8186-7128-9}, url={http://ieeexplore.ieee.org/document/598994/}, DOI={10.1109/ICDAR.1995.598994}, booktitle={Proceedings of 3rd International Conference on Document Analysis and Recognition}, publisher={IEEE Comput. Soc. Press}, author={Tin Kam Ho}, year={1995}, pages={278–282} }

 @article{Yang_Lv_Chen_2023, title={A Survey on Ensemble Learning Under the Era of Deep Learning}, volume={56}, ISSN={0269-2821, 1573-7462}, DOI={10.1007/s10462-022-10283-5}, number={6}, journal={Artificial Intelligence Review}, author={Yang, Yongquan and Lv, Haijun and Chen, Ning}, year={2023}, month={Jun}, pages={5545–5589}, language={en} }

 @article{Ferrier_2014, title={Toward a Universal Cortical Algorithm: Examining Hierarchical Temporal Memory in Light of Frontal Cortical Function}, url={http://arxiv.org/abs/1411.4702}, abstractNote={A wide range of evidence points toward the existence of a common algorithm underlying the processing of information throughout the cerebral cortex. Several hypothesized features of this cortical algorithm are reviewed, including sparse distributed representation, Bayesian inference, hierarchical organization composed of alternating template matching and pooling layers, temporal slowness and predictive coding. Hierarchical Temporal Memory (HTM) is a family of learning algorithms and corresponding theories of cortical function that embodies these principles. HTM has previously been applied mainly to perceptual tasks typical of posterior cortex. In order to evaluate HTM as a candidate model of cortical function, it is necessary also to investigate its compatibility with the requirements of frontal cortical function. To this end, a variety of models of frontal cortical function are reviewed and integrated, to arrive at the hypothesis that frontal functions including attention, working memory and action selection depend largely upon the same basic algorithms as do posterior functions, with the notable additions of a mechanism for the active maintenance of representations and of multiple cortico-striato-thalamo-cortical loops that allow communication between regions of frontal cortex to be gated in an adaptive manner. Computational models of this system are reviewed. Finally, there is a discussion of how HTM can contribute to the understanding of frontal cortical function, and of what the requirements of frontal cortical function mean for the future development of HTM.}, note={arXiv:1411.4702 [cs, q-bio]}, number={arXiv:1411.4702}, publisher={arXiv}, author={Ferrier, Michael R.}, year={2014}, month={Nov} }

 @article{George_Lehrach_Kansky_Gredilla_Laan_Marthi_Lou_Meng_Liu_Wang_etal, title={A Generative Vision Model that Trains with High Data Efficiency and Breaks Text-Based CAPTCHAs}, volume={358}, ISSN={0036-8075, 1095-9203}, DOI={10.1126/science.aag2612}, number={6368}, journal={Science}, author={George, Dileep and Lehrach, Wolfgang and Kansky, Ken and Lázaro-Gredilla, Miguel and Laan, Christopher and Marthi, Bhaskara and Lou, Xinghua and Meng, Zhaoshi and Liu, Yi and Wang, Huayan and Lavin, Alex and Phoenix, D. Scott}, year={2017}, month={Dec}, pages={eaag2612}, language={en} }

@inproceedings{10.5555/3294996.3295142, author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.}, title = {Dynamic Routing between Capsules}, year = {2017}, isbn = {9781510860964}, publisher = {Curran Associates Inc.}, address = {Red Hook, NY, USA}, abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.}, booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems}, pages = {3859–3869}, numpages = {11}, location = {Long Beach, California, USA}, series = {NIPS'17} }

 @book{Parr_Pezzulo_Friston_2022, title={Active Inference: The Free Energy Principle in Mind, Brain, and Behavior}, ISBN={978-0-262-36997-8}, url={https://direct.mit.edu/books/book/5299/Active-InferenceThe-Free-Energy-Principle-in-Mind}, DOI={10.7551/mitpress/12441.001.0001}, abstractNote={The first comprehensive treatment of active inference, an integrative perspective on brain, cognition, and behavior used across multiple disciplines.
            Active inference is a way of understanding sentient behavior—a theory that characterizes perception, planning, and action in terms of probabilistic inference. Developed by theoretical neuroscientist Karl Friston over years of groundbreaking research, active inference provides an integrated perspective on brain, cognition, and behavior that is increasingly used across multiple disciplines including neuroscience, psychology, and philosophy. Active inference puts the action into perception. This book offers the first comprehensive treatment of active inference, covering theory, applications, and cognitive domains.
            Active inference is a “first principles” approach to understanding behavior and the brain, framed in terms of a single imperative to minimize free energy. The book emphasizes the implications of the free energy principle for understanding how the brain works. It first introduces active inference both conceptually and formally, contextualizing it within current theories of cognition. It then provides specific examples of computational models that use active inference to explain such cognitive phenomena as perception, attention, memory, and planning.}, publisher={The MIT Press}, author={Parr, Thomas and Pezzulo, Giovanni and Friston, Karl J.}, year={2022}, month={Mar}, language={en} }

 @article{Friston_FitzGerald_Rigoli_Schwartenbeck_ODoherty_Pezzulo_2016, title={Active inference and learning}, volume={68}, ISSN={01497634}, DOI={10.1016/j.neubiorev.2016.06.022}, journal={Neuroscience & Biobehavioral Reviews}, author={Friston, Karl and FitzGerald, Thomas and Rigoli, Francesco and Schwartenbeck, Philipp and O'Doherty, John and Pezzulo, Giovanni}, year={2016}, month={Sep}, pages={862–879}, language={en} }

@inproceedings{NEURIPS2021_e614f646,
 author = {Bengio, Emmanuel and Jain, Moksh and Korablyov, Maksym and Precup, Doina and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {27381--27394},
 publisher = {Curran Associates, Inc.},
 title = {Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/e614f646836aaed9f89ce58e837e2310-Paper.pdf},
 volume = {34},
 year = {2021}
}

 @article{Bengio_Lahlou_Deleu_Hu_Tiwari_Bengio_2022, title={GFlowNet Foundations}, url={http://arxiv.org/abs/2111.09266}, abstractNote={Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of GFlowNets. They can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. GFlowNets amortize the work typically done by computationally expensive MCMC methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entropy and mutual information, sampling from a Pareto frontier, connections to reward-maximizing policies, and extensions to stochastic environments, continuous actions and modular energy functions.}, note={arXiv:2111.09266 [cs, stat]}, number={arXiv:2111.09266}, publisher={arXiv}, author={Bengio, Yoshua and Lahlou, Salem and Deleu, Tristan and Hu, Edward J. and Tiwari, Mo and Bengio, Emmanuel}, year={2022}, month={Aug} }

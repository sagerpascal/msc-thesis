@article{McCulloch_Pitts_1943,
	title={A logical calculus of the ideas immanent in nervous activity},
	volume={5},
	ISSN={0007-4985, 1522-9602},
	DOI={10.1007/BF02478259},
	number={4},
	journal={The Bulletin of Mathematical Biophysics},
	author={McCulloch, Warren S. and Pitts, Walter},
	year={1943},
	month={Dec},
	pages={115–133},
	language={en}
}

 @article{Rosenblatt_1958,
 title={The perceptron: A probabilistic model for information storage and organization in the brain.},
 volume={65},
 ISSN={1939-1471, 0033-295X},
 DOI={10.1037/h0042519},
 number={6},
 journal={Psychological Review},
 author={Rosenblatt, F.},
 year={1958},
 pages={386–408},
 language={en}
}

@article{Cybenko_1989,
 title={Approximation by superpositions of a sigmoidal function},
 volume={2},
 ISSN={0932-4194, 1435-568X},
 DOI={10.1007/BF02551274},
 number={4},
 journal={Mathematics of Control, Signals, and Systems},
 author={Cybenko, G.},
 year={1989},
 month={Dec},
 pages={303–314},
 language={en}
}
 
@article{Rumelhart_Hinton_Williams_1986,
	title={Learning representations by back-propagating errors},
	volume={323},
	ISSN={0028-0836, 1476-4687},
	DOI={10.1038/323533a0},
	number={6088},
	journal={Nature},
	author={Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	year={1986},
	month={Oct},
	pages={533–536},
	language={en}
}

@online{Coursera,
  author = {Coursera Inc.},
  title = {Deep Learning Specialization},
  year = 2022,
  url = {https://www.coursera.org/specializations/deep-learning},
  urldate = {2022-08-19}
}

@article{Kingma2015AdamAM,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2015},
  volume={abs/1412.6980}
}

@online{OpenAI_compute,
  author = {Open AI},
  title = {AI and Compute},
  year = 2018,
  url = {https://openai.com/blog/ai-and-compute/},
  urldate = {2022-08-19}
}

@online{Lambda_GPT3,
  author = {Lambda},
  title = {OpenAI's GPT-3 Language Model: A Technical Overview},
  year = 2021,
  url = {https://lambdalabs.com/blog/demystifying-gpt-3/},
  urldate = {2022-08-19}
}

 @article{Moore_2006, title={Cramming more components onto integrated circuits, Reprinted from Electronics, volume 38, number 8, April 19, 1965, pp.114 ff.}, volume={11}, ISSN={1098-4232}, DOI={10.1109/N-SSC.2006.4785860}, number={3}, journal={IEEE Solid-State Circuits Society Newsletter}, author={Moore, Gordon E.}, year={2006}, month={Sep}, pages={33–35} }

 @article{Kumar_2015, title={Fundamental Limits to Moore’s Law}, url={http://arxiv.org/abs/1511.05956}, abstractNote={The theoretical and practical aspects of the fundamental, ultimate, physical limits to scaling, or Moore-s law, is presented.}, note={arXiv:1511.05956 [cond-mat]}, number={arXiv:1511.05956}, publisher={arXiv}, author={Kumar, Suhas}, year={2015}, month={Nov} }

 @inproceedings{Peters_Neumann_Iyyer_Gardner_Clark_Lee_Zettlemoyer_2018, address={New Orleans, Louisiana}, title={Deep Contextualized Word Representations}, url={http://aclweb.org/anthology/N18-1202}, DOI={10.18653/v1/N18-1202}, booktitle={Proceedings of the 2018 Conference of the North American Chapter of           the Association for Computational Linguistics: Human Language           Technologies, Volume 1 (Long Papers)}, publisher={Association for Computational Linguistics}, author={Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke}, year={2018}, pages={2227–2237}, language={en} }
 
@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

 @article{Shoeybi_Patwary_Puri_LeGresley_Casper_Catanzaro_2020, title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}, url={http://arxiv.org/abs/1909.08053}, abstractNote={Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).}, note={arXiv:1909.08053 [cs]}, number={arXiv:1909.08053}, publisher={arXiv}, author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan}, year={2020}, month={Mar} }
 
 @article{Wu_Judd_Zhang_Isaev_Micikevicius_2020, title={Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation}, url={http://arxiv.org/abs/2004.09602}, abstractNote={Quantization techniques can reduce the size of Deep Neural Networks and improve inference latency and throughput by taking advantage of high throughput integer instructions. In this paper we review the mathematical aspects of quantization parameters and evaluate their choices on a wide range of neural network models for different application domains, including vision, speech, and language. We focus on quantization techniques that are amenable to acceleration by processors with high-throughput integer math pipelines. We also present a workflow for 8-bit quantization that is able to maintain accuracy within 1% of the floating-point baseline on all networks studied, including models that are more difficult to quantize, such as MobileNets and BERT-large.}, note={arXiv:2004.09602 [cs, stat]}, number={arXiv:2004.09602}, publisher={arXiv}, author={Wu, Hao and Judd, Patrick and Zhang, Xiaojie and Isaev, Mikhail and Micikevicius, Paulius}, year={2020}, month={Apr} }
 
@article{Choudhary_Mishra_Goswami_Sarangapani_2020, title={A comprehensive survey on model compression and acceleration}, volume={53}, ISSN={0269-2821, 1573-7462}, DOI={10.1007/s10462-020-09816-7}, number={7}, journal={Artificial Intelligence Review}, author={Choudhary, Tejalal and Mishra, Vipul and Goswami, Anurag and Sarangapani, Jagannathan}, year={2020}, month={Oct}, pages={5113–5155}, language={en} }

  @article{Hinton_Vinyals_Dean_2015, title={Distilling the Knowledge in a Neural Network}, url={http://arxiv.org/abs/1503.02531}, abstractNote={A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.}, note={arXiv:1503.02531 [cs, stat]}, number={arXiv:1503.02531}, publisher={arXiv}, author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff}, year={2015}, month={Mar} }

  @article{Zhang_Yang_2021, title={A Survey on Multi-Task Learning}, url={http://arxiv.org/abs/1707.08114}, abstractNote={Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a definition of MTL and then classify different MTL algorithms into five categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL.}, note={arXiv:1707.08114 [cs]}, number={arXiv:1707.08114}, publisher={arXiv}, author={Zhang, Yu and Yang, Qiang}, year={2021}, month={Mar} }

  @article{Parisi_Kemker_Part_Kanan_Wermter_2019, title={Continual lifelong learning with neural networks: A review}, volume={113}, ISSN={08936080}, DOI={10.1016/j.neunet.2019.01.012}, journal={Neural Networks}, author={Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan}, year={2019}, month={May}, pages={54–71}, language={en} }

 @article{Sahoo_Pham_Lu_Hoi_2017, title={Online Deep Learning: Learning Deep Neural Networks on the Fly}, url={http://arxiv.org/abs/1711.03705}, abstractNote={Deep Neural Networks (DNNs) are typically trained by backpropagation in a batch learning setting, which requires the entire training data to be made available prior to the learning task. This is not scalable for many real-world scenarios where new data arrives sequentially in a stream form. We aim to address an open challenge of “Online Deep Learning” (ODL) for learning DNNs on the fly in an online setting. Unlike traditional online learning that often optimizes some convex objective function with respect to a shallow model (e.g., a linear/kernel-based hypothesis), ODL is significantly more challenging since the optimization of the DNN objective function is non-convex, and regular backpropagation does not work well in practice, especially for online learning settings. In this paper, we present a new online deep learning framework that attempts to tackle the challenges by learning DNN models of adaptive depth from a sequence of training data in an online learning setting. In particular, we propose a novel Hedge Backpropagation (HBP) method for online updating the parameters of DNN effectively, and validate the efficacy of our method on large-scale data sets, including both stationary and concept drifting scenarios.}, note={arXiv:1711.03705 [cs]}, number={arXiv:1711.03705}, publisher={arXiv}, author={Sahoo, Doyen and Pham, Quang and Lu, Jing and Hoi, Steven C. H.}, year={2017}, month={Nov} }

 @article{Madan_Henry_Dozier_Ho_Bhandari_Sasaki_Durand_Pfister_Boix_2022, title={When and how convolutional neural networks generalize to out-of-distribution category–viewpoint combinations}, volume={4}, ISSN={2522-5839}, DOI={10.1038/s42256-021-00437-5}, number={2}, journal={Nature Machine Intelligence}, author={Madan, Spandan and Henry, Timothy and Dozier, Jamell and Ho, Helen and Bhandari, Nishchal and Sasaki, Tomotake and Durand, Frédo and Pfister, Hanspeter and Boix, Xavier}, year={2022}, month={Feb}, pages={146–153}, language={en} }

 @article{Marcus_2018, title={Deep Learning: A Critical Appraisal}, url={http://arxiv.org/abs/1801.00631}, abstractNote={Although deep learning has historical roots going back decades, neither the term “deep learning” nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton’s now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.}, note={arXiv:1801.00631 [cs, stat]}, number={arXiv:1801.00631}, publisher={arXiv}, author={Marcus, Gary}, year={2018}, month={Jan} }

 @book{Moravec_1995, address={Cambridge}, edition={4. print}, title={Mind children: the future of robot and human intelligence}, ISBN={978-0-674-57618-6}, publisher={Harvard Univ. Press}, author={Moravec, Hans}, year={1995}, language={eng} }


 @article{Felleman_Van_Essen_1991, title={Distributed Hierarchical Processing in the Primate Cerebral Cortex}, volume={1}, ISSN={1047-3211, 1460-2199}, DOI={10.1093/cercor/1.1.1}, number={1}, journal={Cerebral Cortex}, author={Felleman, D. J. and Van Essen, D. C.}, year={1991}, month={Jan}, pages={1–47}, language={en} }

 @article{Lillicrap_Cownden_Tweed_Akerman_2016, title={Random synaptic feedback weights support error backpropagation for deep learning}, volume={7}, ISSN={2041-1723}, DOI={10.1038/ncomms13276}, number={1}, journal={Nature Communications}, author={Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.}, year={2016}, month={Dec}, pages={13276}, language={en} }

 @article{Hopfield_1982, title={Neural networks and physical systems with emergent collective computational abilities.}, volume={79}, ISSN={0027-8424, 1091-6490}, DOI={10.1073/pnas.79.8.2554}, abstractNote={Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.}, number={8}, journal={Proceedings of the National Academy of Sciences}, author={Hopfield, J J}, year={1982}, month={Apr}, pages={2554–2558}, language={en} }

 @article{Fix_Hodges_1989, title={Discriminatory Analysis. Nonparametric Discrimination: Consistency Properties}, volume={57}, ISSN={03067734}, DOI={10.2307/1403797}, number={3}, journal={International Statistical Review / Revue Internationale de Statistique}, author={Fix, Evelyn and Hodges, J. L.}, year={1989}, month={Dec}, pages={238} }

 @article{Weston_Chopra_Bordes_2015, title={Memory Networks}, url={http://arxiv.org/abs/1410.3916}, abstractNote={We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.}, note={arXiv:1410.3916 [cs, stat]}, number={arXiv:1410.3916}, publisher={arXiv}, author={Weston, Jason and Chopra, Sumit and Bordes, Antoine}, year={2015}, month={Nov} }

@ARTICLE{1057328,
  author={McEliece, R. and Posner, E. and Rodemich, E. and Venkatesh, S.},
  journal={IEEE Transactions on Information Theory}, 
  title={The capacity of the Hopfield associative memory}, 
  year={1987},
  volume={33},
  number={4},
  pages={461-482},
  doi={10.1109/TIT.1987.1057328}}

@Article{Hopfield1983,
author={Hopfield, J. J.
and Feinstein, D. I.
and Palmer, R. G.},
title={`Unlearning' has a stabilizing effect in collective memories},
journal={Nature},
year={1983},
month={Jul},
day={01},
volume={304},
number={5922},
pages={158-159},
abstract={Crick and Mitchison1 have presented a hypothesis for the functional role of dream sleep involving an `unlearning' process. We have independently carried out mathematical and computer modelling of learning and `unlearning' in a collective neural network of 30--1,000 neurones. The model network has a content-addressable memory or `associative memory' which allows it to learn and store many memories. A particular memory can be evoked in its entirety when the network is stimulated by any adequate-sized subpart of the information of that memory2. But different memories of the same size are not equally easy to recall. Also, when memories are learned, spurious memories are also created and can also be evoked. Applying an `unlearning' process, similar to the learning processes but with a reversed sign and starting from a noise input, enhances the performance of the network in accessing real memories and in minimizing spurious ones. Although our model was not motivated by higher nervous function, our system displays behaviours which are strikingly parallel to those needed for the hypothesized role of `unlearning' in rapid eye movement (REM) sleep.},
issn={1476-4687},
doi={10.1038/304158a0},
url={https://doi.org/10.1038/304158a0}
}

@inproceedings{10.5555/3157096.3157228, author = {Krotov, Dmitry and Hopfield, John J.}, title = {Dense Associative Memory for Pattern Recognition}, year = {2016}, isbn = {9781510838819}, publisher = {Curran Associates Inc.}, address = {Red Hook, NY, USA}, abstract = {A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions - the higher rectified polynomials which until now have not been used in deep learning. The utility of the dense memories is illustrated for two test cases: the logical gate XOR and the recognition of handwritten digits from the MNIST data set.}, booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems}, pages = {1180–1188}, numpages = {9}, location = {Barcelona, Spain}, series = {NIPS'16} }

 @article{Demircigil_Heusel_Löwe_Upgang_Vermet_2017, title={On a Model of Associative Memory with Huge Storage Capacity}, volume={168}, ISSN={0022-4715, 1572-9613}, DOI={10.1007/s10955-017-1806-y}, number={2}, journal={Journal of Statistical Physics}, author={Demircigil, Mete and Heusel, Judith and Löwe, Matthias and Upgang, Sven and Vermet, Franck}, year={2017}, month={Jul}, pages={288–299}, language={en} }

 @article{Ramsauer, title={Hopfield Networks is All You Need}, url={http://arxiv.org/abs/2008.02217}, abstractNote={We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers}, note={arXiv:2008.02217 [cs, stat]}, number={arXiv:2008.02217}, publisher={arXiv}, author={Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp}, year={2021}, month={Apr} }

 @book{Hebb_1949, address={Oxford,  England}, series={The organization of behavior; a neuropsychological theory.}, title={The organization of behavior; a neuropsychological theory.}, abstractNote={“This book presents a theory of behavior that is based as far as possible on the physiology of the nervous system, and makes a sedulous attempt to find some community of neurological and psychological conceptions.” Using the concept of the reverbatory circuit and the assumption that “some growth process or metabolic change” in neurones takes place as a result of repeated transmission across synapses, perceptual integration is described in terms of “cell-assemblies.” Of 11 chapters, 4 are devoted to perceptual problems, 2 to learning, 2 to motivation, and 1 each to emotional disturbances and intelligence. 14-page bibliography. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}, publisher={Wiley}, author={Hebb, D. O.}, year={1949}, pages={xix, 335}, collection={The organization of behavior; a neuropsychological theory.} }

 @article{Oja_1982, title={Simplified neuron model as a principal component analyzer}, volume={15}, ISSN={0303-6812, 1432-1416}, DOI={10.1007/BF00275687}, number={3}, journal={Journal of Mathematical Biology}, author={Oja, Erkki}, year={1982}, month={Nov}, pages={267–273}, language={en} }

 @article{Bienenstock_Cooper_Munro_1982, title={Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex}, volume={2}, ISSN={0270-6474, 1529-2401}, DOI={10.1523/JNEUROSCI.02-01-00032.1982}, number={1}, journal={The Journal of Neuroscience}, author={Bienenstock, El and Cooper, Ln and Munro, Pw}, year={1982}, month={Jan}, pages={32–48}, language={en} }

 @article{Intrator_Cooper_1992, title={Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions}, volume={5}, ISSN={08936080}, DOI={10.1016/S0893-6080(05)80003-6}, number={1}, journal={Neural Networks}, author={Intrator, Nathan and Cooper, Leon N}, year={1992}, month={Jan}, pages={3–17}, language={en} }

 @article{Simoncelli_Olshausen_2001, title={Natural Image Statistics and Neural Representation}, volume={24}, ISSN={0147-006X, 1545-4126}, DOI={10.1146/annurev.neuro.24.1.1193}, abstractNote={▪ Abstract  It has long been assumed that sensory neurons are adapted, through both evolutionary and developmental processes, to the statistical properties of the signals to which they are exposed. Attneave (1954) , Barlow (1961) proposed that information theory could provide a link between environmental statistics and neural responses through the concept of coding efficiency. Recent developments in statistical modeling, along with powerful computational tools, have enabled researchers to study more sophisticated statistical models for visual images, to validate these models empirically against large sets of data, and to begin experimentally testing the efficient coding hypothesis for both individual neurons and populations of neurons.}, number={1}, journal={Annual Review of Neuroscience}, author={Simoncelli, Eero P and Olshausen, Bruno A}, year={2001}, month={Mar}, pages={1193–1216}, language={en} }

 @article{Vogels_Sprekeler_Zenke_Clopath_Gerstner_2011, title={Inhibitory Plasticity Balances Excitation and Inhibition in Sensory Pathways and Memory Networks}, volume={334}, ISSN={0036-8075, 1095-9203}, DOI={10.1126/science.1211095}, number={6062}, journal={Science}, author={Vogels, T. P. and Sprekeler, H. and Zenke, F. and Clopath, C. and Gerstner, W.}, year={2011}, month={Dec}, pages={1569–1573}, language={en} }

@INPROCEEDINGS{5178625,
  author={Joshi, Prashant and Triesch, Jochen},
  booktitle={2009 International Joint Conference on Neural Networks}, 
  title={Rules for information maximization in spiking neurons using intrinsic plasticity}, 
  year={2009},
  volume={},
  number={},
  pages={1456-1461},
  doi={10.1109/IJCNN.2009.5178625}}

@inproceedings{Teichmann,
author = {Teichmann, Michael and Hamker, Fred},
year = {2015},
month = {03},
pages = {},
title = {Intrinsic plasticity: A simple mechanism to stabilize Hebbian learning in multilayer neural networks.}
}

 @article{Grossberg_1988, title={Nonlinear neural networks: Principles, mechanisms, and architectures}, volume={1}, ISSN={08936080}, DOI={10.1016/0893-6080(88)90021-4}, number={1}, journal={Neural Networks}, author={Grossberg, Stephen}, year={1988}, month={Jan}, pages={17–61}, language={en} }

@article{esn,
author = {Jaeger, Herbert},
year = {2001},
month = {01},
pages = {},
title = {The" echo state" approach to analysing and training recurrent neural networks-with an erratum note'},
volume = {148},
journal = {Bonn, Germany: German National Research Center for Information Technology GMD Technical Report}
}

 @article{Maass_Natschlager_Markram_2002, title={Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations}, volume={14}, ISSN={0899-7667, 1530-888X}, DOI={10.1162/089976602760407955}, abstractNote={A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.}, number={11}, journal={Neural Computation}, author={Maass, Wolfgang and Natschläger, Thomas and Markram, Henry}, year={2002}, month={Nov}, pages={2531–2560}, language={en} }

 @inbook{Konkoli_2018, address={New York, NY}, title={Reservoir Computing}, ISBN={978-1-4939-6882-4}, url={http://link.springer.com/10.1007/978-1-4939-6883-1_683}, DOI={10.1007/978-1-4939-6883-1_683}, booktitle={Unconventional Computing}, publisher={Springer US}, author={Konkoli, Zoran}, editor={Adamatzky, Andrew}, year={2018}, pages={619–629}, language={en} }

 @article{Tanaka_Yamane_Héroux_Nakane_Kanazawa_Takeda_Numata_Nakano_Hirose_2019, title={Recent advances in physical reservoir computing: A review}, volume={115}, ISSN={08936080}, DOI={10.1016/j.neunet.2019.03.005}, journal={Neural Networks}, author={Tanaka, Gouhei and Yamane, Toshiyuki and Héroux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira}, year={2019}, month={Jul}, pages={100–123}, language={en} }


@article{erdos59a,
  author = {Erdös, P. and Rényi, A.},
  journal = {Publicationes Mathematicae Debrecen},
  keywords = {graph sna},
  pages = 290,
  title = {On Random Graphs I},
  volume = 6,
  year = 1959
}

 @inbook{Lukoševičius_2012, address={Berlin, Heidelberg}, series={Lecture Notes in Computer Science}, title={A Practical Guide to Applying Echo State Networks}, volume={7700}, ISBN={978-3-642-35288-1}, url={http://link.springer.com/10.1007/978-3-642-35289-8_36}, DOI={10.1007/978-3-642-35289-8_36}, booktitle={Neural Networks: Tricks of the Trade}, publisher={Springer Berlin Heidelberg}, author={Lukoševičius, Mantas}, editor={Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert}, year={2012}, pages={659–686}, collection={Lecture Notes in Computer Science}, language={en} }



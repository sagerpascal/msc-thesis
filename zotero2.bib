
@collection{byrne_oxford_2019,
	location = {New York, {NY}},
	title = {The Oxford handbook of invertebrate neurobiology},
	isbn = {978-0-19-045675-7},
	pagetotal = {752},
	publisher = {Oxford University Press},
	editor = {Byrne, John H.},
	date = {2019},
	keywords = {Handbooks, manuals, etc, Invertebrates, Neurobiology, Physiology},
}

@article{pessoa_understanding_2014,
	title = {Understanding brain networks and brain organization},
	volume = {11},
	issn = {15710645},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1571064514000451},
	doi = {10.1016/j.plrev.2014.03.005},
	pages = {400--435},
	number = {3},
	journaltitle = {Physics of Life Reviews},
	shortjournal = {Physics of Life Reviews},
	author = {Pessoa, Luiz},
	urldate = {2023-08-03},
	date = {2014-09},
	langid = {english},
}

@online{open_ai_ai_2018,
	title = {{AI} and Compute},
	url = {https://openai.com/blog/ai-and-compute/},
	author = {{Open AI}},
	urldate = {2022-08-19},
	date = {2018},
}

@misc{open_ai_gpt-4_2023,
	title = {{GPT}-4 Technical Report},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of {GPT}-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, {GPT}-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. {GPT}-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of {GPT}-4's performance based on models trained with no more than 1/1,000th the compute of {GPT}-4.},
	number = {{arXiv}:2303.08774},
	publisher = {{arXiv}},
	author = {Open {AI}},
	urldate = {2023-06-24},
	date = {2023-03-27},
	eprinttype = {arxiv},
	eprint = {2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{zhou_distilling_2021,
	location = {Montreal, Canada},
	title = {Distilling Holistic Knowledge With Graph Neural Networks},
	series = {{ICCV}'21},
	pages = {10387--10396},
	booktitle = {Proceedings of the {IEEE}/{CVF} International Conference on Computer Vision},
	publisher = {{IEEE}},
	author = {Zhou, Sheng and Wang, Yucheng and Chen, Defang and Chen, Jiawei and Wang, Xin and Wang, Can and Bu, Jiajun},
	date = {2021-10},
}

@inproceedings{ho_random_1995,
	location = {Montreal, Canada},
	title = {Random decision forests},
	volume = {1},
	isbn = {978-0-8186-7128-9},
	url = {http://ieeexplore.ieee.org/document/598994/},
	doi = {10.1109/ICDAR.1995.598994},
	eventtitle = {3rd International Conference on Document Analysis and Recognition},
	pages = {278--282},
	booktitle = {Proceedings of 3rd International Conference on Document Analysis and Recognition},
	publisher = {{IEEE}},
	author = {Ho, Tin Kam},
	urldate = {2023-07-01},
	date = {1995},
}

@inproceedings{sahoo_online_2018,
	location = {Stockholm, Sweden},
	title = {Online Deep Learning: Learning Deep Neural Networks on the Fly},
	isbn = {978-0-9992411-2-7},
	url = {https://www.ijcai.org/proceedings/2018/369},
	doi = {10.24963/ijcai.2018/369},
	series = {{IJCAI}'18},
	shorttitle = {Online Deep Learning},
	abstract = {Deep Neural Networks ({DNNs}) are typically trained by backpropagation in a batch setting, requiring the entire training data to be made available prior to the learning task. This is not scalable for many real-world scenarios where new data arrives sequentially in a stream. We aim to address an open challenge of ``Online Deep Learning" ({ODL}) for learning {DNNs} on the fly in an online setting. Unlike traditional online learning that often optimizes some convex objective function with respect to a shallow model (e.g., a linear/kernel-based hypothesis), {ODL} is more challenging as the optimization objective is non-convex, and regular {DNN} with standard backpropagation does not work well in practice for online settings. We present a new {ODL} framework that attempts to tackle the challenges by learning {DNN} models which dynamically adapt depth from a sequence of training data in an online learning setting. Specifically, we propose a novel Hedge Backpropagation ({HBP}) method for online updating the parameters of {DNN} effectively, and validate the efficacy on large data sets (both stationary and concept drifting scenarios).},
	eventtitle = {27th International Joint Conference on Artificial Intelligence},
	pages = {2660--2666},
	booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
	publisher = {{AAAI} Press},
	author = {Sahoo, Doyen and Pham, Quang and Lu, Jing and Hoi, Steven C. H.},
	urldate = {2023-06-28},
	date = {2018-07},
	langid = {english},
}

@inproceedings{czarnecki_understanding_2017,
	location = {Sydney, Australia},
	title = {Understanding Synthetic Gradients and Decoupled Neural Interfaces},
	volume = {70},
	series = {{ICML}'17},
	abstract = {When training neural networks, the use of Synthetic Gradients ({SG}) allows layers or modules to be trained without update locking - without waiting for a true error gradient to be backpropagated - resulting in Decoupled Neural Interfaces ({DNIs}). This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al. (2016). However, there has been very little demonstration of what changes {DNIs} and {SGs} impose from a functional, representational, and learning dynamics point of view. In this paper, we study {DNIs} through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation. We show that the incorporation of {SGs} does not affect the representational strength of the learning system for a neural network, and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approx-imate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations. Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison.},
	pages = {904--912},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Czarnecki, Wojciech Marian and Swirszcz, Grzegorz and Jaderberg, Max and Osindero, Simon and Vinyals, Oriol and Kavukcuoglu, Koray},
	editor = {Precup, Doina and Teh, Yee Whye},
	date = {2017},
	note = {Place: Sydney, {NSW}, Australia},
}

@inproceedings{taylor_training_2016,
	location = {New York, {NY}, {USA}},
	title = {Training Neural Networks without Gradients: A Scalable {ADMM} Approach},
	volume = {48},
	series = {{ICML}'16},
	abstract = {With the growing importance of large network models and enormous training datasets, {GPUs} have become increasingly necessary to train neural networks. This is largely because conventional optimization algorithms rely on stochastic gradient methods that don't scale well to large numbers of cores in a cluster setting. Furthermore, the convergence of all gradient methods, including batch methods, suffers from common problems like saturation effects, poor conditioning, and saddle points. This paper explores an unconventional training method that uses alternating direction methods and Bregman iteration to train networks without gradient descent steps. The proposed method reduces the network training problem to a sequence of minimization substeps that can each be solved globally in closed form. The proposed method is advantageous because it avoids many of the caveats that make gradient methods slow on highly nonconvex problems. The method exhibits strong scaling in the distributed setting, yielding linear speedups even when split over thousands of cores.},
	pages = {2722--2731},
	booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
	publisher = {{PMLR}},
	author = {Taylor, Gavin and Burmeister, Ryan and Xu, Zheng and Singh, Bharat and Patel, Ankit and Goldstein, Tom},
	date = {2016},
	note = {Place: New York, {NY}, {USA}},
}

@inproceedings{ioffe_batch_2015,
	location = {Lille, France},
	title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
	volume = {37},
	series = {{ICML}'15},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on {ImageNet} classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
	pages = {448--456},
	booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Ioffe, Sergey and Szegedy, Christian},
	editor = {Bach, Francis and Blei, David},
	date = {2015},
	note = {Place: Lille, France},
}

@inproceedings{ngiam_multimodal_2011,
	location = {Bellevue, {DC}, {USA}},
	title = {Multimodal Deep Learning},
	isbn = {978-1-4503-0619-5},
	series = {{ICML}'11},
	abstract = {Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the {CUAVE} and {AVLetters} datasets on audio-visual speech classification, demonstrating best published visual speech classification on {AVLetters} and effective shared representation learning.},
	pages = {689--696},
	booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y.},
	editor = {Getoor, Lise and Scheffer, Tobias},
	date = {2011},
	note = {event-place: Bellevue, Washington, {USA}},
}

@article{russakovsky_imagenet_2015,
	title = {{ImageNet} Large Scale Visual Recognition Challenge},
	volume = {115},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-015-0816-y},
	doi = {10.1007/s11263-015-0816-y},
	series = {{IJCV}'15},
	pages = {211--252},
	number = {3},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vis},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	urldate = {2023-07-31},
	date = {2015-12},
	langid = {english},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {00189219},
	url = {http://ieeexplore.ieee.org/document/726791/},
	doi = {10.1109/5.726791},
	pages = {2278--2324},
	number = {11},
	journaltitle = {Proceedings of the {IEEE}},
	shortjournal = {Proc. {IEEE}},
	author = {{LeCun}, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
	urldate = {2023-06-30},
	date = {1998-11},
}

@online{hui_machine_2017,
	title = {Machine learning - Restricted Boltzmann Machines},
	url = {https://jhui.github.io/2017/01/15/Machine-learning-Boltzmann-machines/},
	titleaddon = {Jonathan Hui blog},
	type = {Blog},
	author = {Hui, Jonathan},
	urldate = {2023-07-25},
	date = {2017-01-15},
}

@article{hinton_training_2002,
	title = {Training Products of Experts by Minimizing Contrastive Divergence},
	volume = {14},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/14/8/1771-1800/6687},
	doi = {10.1162/089976602760128018},
	abstract = {It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual “expert” models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts ({PoE}) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a {PoE} by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a {PoE} can be trained using a different objective function called “contrastive divergence” whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.},
	pages = {1771--1800},
	number = {8},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Hinton, Geoffrey E.},
	urldate = {2023-07-24},
	date = {2002-08-01},
	langid = {english},
}

@incollection{smolensky_chapter_1986,
	location = {Cambridge, {MA}, {USA}},
	title = {Chapter 6: Information Processing in Dynamical Systems: Foundations of Harmony Theory},
	isbn = {0-262-68053-X},
	pages = {194--281},
	booktitle = {Parallel Distributed Processing, Volume 1: Explorations in the Microstructure of Cognition: Foundations},
	publisher = {{MIT} Press},
	author = {Smolensky, Paul},
	date = {1986},
}

@article{gross_genealogy_2002,
	title = {Genealogy of the “Grandmother Cell”},
	volume = {8},
	issn = {1073-8584, 1089-4098},
	url = {http://journals.sagepub.com/doi/10.1177/107385802237175},
	doi = {10.1177/107385802237175},
	abstract = {A “grandmother cell” is a hypothetical neuron that responds only to a highly complex, specific, and meaningful stimulus, such as the image of one’s grandmother. The term originated in a parable Jerry Lettvin told in 1967. A similar concept had been systematically developed a few years earlier by Jerzy Konorski who called such cells “gnostic” units. This essay discusses the origin, influence, and current status of these terms and of the alternative view that complex stimuli are represented by the pattern of firing across ensembles of neurons.},
	pages = {512--518},
	number = {5},
	journaltitle = {The Neuroscientist},
	shortjournal = {Neuroscientist},
	author = {Gross, Charles G.},
	urldate = {2023-07-24},
	date = {2002-10},
	langid = {english},
}

@article{tomita_top-down_1999,
	title = {Top-down signal from prefrontal cortex in executive control of memory retrieval},
	volume = {401},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/44372},
	doi = {10.1038/44372},
	pages = {699--703},
	number = {6754},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Tomita, Hyoe and Ohbayashi, Machiko and Nakahara, Kiyoshi and Hasegawa, Isao and Miyashita, Yasushi},
	urldate = {2023-07-24},
	date = {1999-10},
	langid = {english},
}

@article{han_image-based_2021,
	title = {Image-Based 3D Object Reconstruction: State-of-the-Art and Trends in the Deep Learning Era},
	volume = {43},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/8908779/},
	doi = {10.1109/TPAMI.2019.2954885},
	shorttitle = {Image-Based 3D Object Reconstruction},
	pages = {1578--1604},
	number = {5},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Han, Xian-Feng and Laga, Hamid and Bennamoun, Mohammed},
	urldate = {2023-07-24},
	date = {2021-05-01},
}

@article{qu_transmef_2022,
	title = {{TransMEF}: A Transformer-Based Multi-Exposure Image Fusion Framework Using Self-Supervised Multi-Task Learning},
	volume = {36},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20109},
	doi = {10.1609/aaai.v36i2.20109},
	shorttitle = {{TransMEF}},
	abstract = {In this paper, we propose {TransMEF}, a transformer-based multi-exposure image fusion framework that uses self-supervised multi-task learning. The framework is based on an encoder-decoder network, which can be trained on large natural image datasets and does not require ground truth fusion images. We design three self-supervised reconstruction tasks according to the characteristics of multi-exposure images and conduct these tasks simultaneously using multi-task learning; through this process, the network can learn the characteristics of multi-exposure images and extract more generalized features. In addition, to compensate for the defect in establishing long-range dependencies in {CNN}-based architectures, we design an encoder that combines a {CNN} module with a transformer module. This combination enables the network to focus on both local and global information. We evaluated our method and compared it to 11 competitive traditional and deep learning-based methods on the latest released multi-exposure image fusion benchmark dataset, and our method achieved the best performance in both subjective and objective evaluations. Code will be available at https://github.com/miccaiif/{TransMEF}.},
	pages = {2126--2134},
	number = {2},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	shortjournal = {{AAAI}},
	author = {Qu, Linhao and Liu, Shaolei and Wang, Manning and Song, Zhijian},
	urldate = {2023-07-24},
	date = {2022-06-28},
}

@misc{ramsauer_hopfield_2021,
	title = {Hopfield Networks is All You Need},
	url = {http://arxiv.org/abs/2008.02217},
	doi = {10.48550/arXiv.2008.02217},
	abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the {UCI} benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers},
	number = {{arXiv}:2008.02217},
	publisher = {{arXiv}},
	author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
	urldate = {2023-07-24},
	date = {2021-04-28},
	eprinttype = {arxiv},
	eprint = {2008.02217 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{anderson_shifter_1987,
	title = {Shifter circuits: a computational strategy for dynamic aspects of visual processing.},
	volume = {84},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.84.17.6297},
	doi = {10.1073/pnas.84.17.6297},
	shorttitle = {Shifter circuits},
	abstract = {We propose a general strategy for dynamic control of information flow between arrays of neurons at different levels of the visual pathway, starting in the lateral geniculate nucleus and the geniculorecipient layers of cortical area V1. This strategy can be used for resolving computational problems arising in the domains of stereopsis, directed visual attention, and the perception of moving images. In each of these situations, some means of dynamically controlling how retinal outputs map onto higher-level targets is desirable--in order to achieve binocular fusion, to allow shifts of the focus of attention, and to prevent blurring of moving images. The proposed solution involves what we term "shifter circuits," which allow for dynamic shifts in the relative alignment of input and output arrays without loss of local spatial relationships. The shifts are produced in increments along a succession of relay stages that are linked by diverging excitatory inputs. The direction of shift is controlled at each stage by inhibitory neurons that selectively suppress appropriate sets of ascending inputs. The shifter hypothesis is consistent with available anatomical and physiological evidence on the organization of the primate visual pathway, and it offers a sensible explanation for a variety of otherwise puzzling facts, such as the plethora of cells in the geniculorecipient layers of V1.},
	pages = {6297--6301},
	number = {17},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Anderson, Charles H. and van Essen, David C.},
	urldate = {2023-07-14},
	date = {1987-09},
	langid = {english},
}

@article{wiskott_role_1999,
	title = {The role of topographical constraints in face recognition},
	volume = {20},
	issn = {01678655},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167865598001226},
	doi = {10.1016/S0167-8655(98)00122-6},
	pages = {89--96},
	number = {1},
	journaltitle = {Pattern Recognition Letters},
	shortjournal = {Pattern Recognition Letters},
	author = {Wiskott, Laurenz},
	urldate = {2023-07-21},
	date = {1999-01},
	langid = {english},
}

@article{simion_face_2015,
	title = {Face perception and processing in early infancy: inborn predispositions and developmental changes},
	volume = {6},
	issn = {1664-1078},
	url = {http://journal.frontiersin.org/Article/10.3389/fpsyg.2015.00969/abstract},
	doi = {10.3389/fpsyg.2015.00969},
	shorttitle = {Face perception and processing in early infancy},
	pages = {969},
	journaltitle = {Frontiers in Psychology},
	shortjournal = {Front. Psychol.},
	author = {Simion, Francesca and Di Giorgio, Elisa},
	urldate = {2023-07-21},
	date = {2015-07-09},
}

@article{womelsdorf_dynamic_2006,
	title = {Dynamic shifts of visual receptive fields in cortical area {MT} by spatial attention},
	volume = {9},
	issn = {1097-6256, 1546-1726},
	url = {https://www.nature.com/articles/nn1748},
	doi = {10.1038/nn1748},
	pages = {1156--1160},
	number = {9},
	journaltitle = {Nature Neuroscience},
	shortjournal = {Nat Neurosci},
	author = {Womelsdorf, Thilo and Anton-Erxleben, Katharina and Pieper, Florian and Treue, Stefan},
	urldate = {2023-07-21},
	date = {2006-09},
	langid = {english},
}

@article{kusunoki_time_2003,
	title = {The Time Course of Perisaccadic Receptive Field Shifts in the Lateral Intraparietal Area of the Monkey},
	volume = {89},
	issn = {0022-3077, 1522-1598},
	url = {https://www.physiology.org/doi/10.1152/jn.00519.2002},
	doi = {10.1152/jn.00519.2002},
	abstract = {Neurons in the lateral intraparietal area of the monkey ({LIP}) have visual receptive fields in retinotopic coordinates when studied in a fixation task. However, in the period immediately surrounding a saccade these receptive fields often shift, so that a briefly flashed stimulus outside the receptive field will drive the neurons if the eye movement will bring the spatial location of that vanished stimulus into the receptive field. This is equivalent to a transient shift of the retinal receptive field. The process enables the monkey brain to process a stimulus in a spatially accurate manner after a saccade, even though the stimulus appeared only before the saccade. We studied the time course of this receptive field shift by flashing a task-irrelevant stimulus for 100 ms before, during, or after a saccade. The stimulus could appear in receptive field as defined by the fixation before the saccade (the current receptive field) or the receptive field as defined by the fixation after the saccade (the future receptive field). We recorded the activity of 48 visually responsive neurons in {LIP} of three hemispheres of two rhesus monkeys. We studied 45 neurons in the current receptive field task, in which the saccade removed the stimulus from the receptive field. Of these neurons 29/45 (64\%) showed a significant decrement of response when the stimulus appeared 250 ms or less before the saccade, as compared with their activity during fixation. The average response decrement was 38\% for those cells showing a significant ( P {\textless} 0.05 by t-test) decrement. We studied 39 neurons in the future receptive field task, in which the saccade brought the spatial location of a recently vanished stimulus into the receptive field. Of these 32/39 (82\%) had a significant response to stimuli flashed for 100 ms in the future receptive field, even 400 ms before the saccade. Neurons never responded to stimuli moved by the saccade from a point outside the receptive field to another point outside the receptive field. Neurons did not necessarily show any saccadic suppression for stimuli moved from one part of the receptive field to another by the saccade. Stimuli flashed {\textless}250 ms before the saccade-evoked responses in both the presaccadic and the postsaccadic receptive fields, resulting in an increase in the effective receptive field size, an effect that we suggest is responsible for perisaccadic perceptual inaccuracies.},
	pages = {1519--1527},
	number = {3},
	journaltitle = {Journal of Neurophysiology},
	shortjournal = {Journal of Neurophysiology},
	author = {Kusunoki, Makoto and Goldberg, Michael E.},
	urldate = {2023-07-21},
	date = {2003-03-01},
	langid = {english},
}

@article{bundesen_visual_1975,
	title = {Visual transformation of size.},
	volume = {1},
	issn = {1939-1277, 0096-1523},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-1523.1.3.214},
	doi = {10.1037/0096-1523.1.3.214},
	pages = {214--220},
	number = {3},
	journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
	shortjournal = {Journal of Experimental Psychology: Human Perception and Performance},
	author = {Bundesen, Claus and Larsen, Axel},
	urldate = {2023-07-21},
	date = {1975},
	langid = {english},
}

@article{lawson_effect_1999,
	title = {The effect of prior experience on recognition thresholds for plane-disoriented pictures of familiar objects},
	volume = {27},
	issn = {0090-502X, 1532-5946},
	url = {http://link.springer.com/10.3758/BF03211567},
	doi = {10.3758/BF03211567},
	pages = {751--758},
	number = {4},
	journaltitle = {Memory \& Cognition},
	shortjournal = {Memory \& Cognition},
	author = {Lawson, Rebecca and Jolicoeur, Pierre},
	urldate = {2023-07-21},
	date = {1999-07},
	langid = {english},
}

@article{jolicoeur_time_1985,
	title = {The time to name disoriented natural objects},
	volume = {13},
	issn = {0090-502X, 1532-5946},
	url = {http://link.springer.com/10.3758/BF03202498},
	doi = {10.3758/BF03202498},
	pages = {289--303},
	number = {4},
	journaltitle = {Memory \& Cognition},
	shortjournal = {Mem Cogn},
	author = {Jolicoeur, Pierre},
	urldate = {2023-07-21},
	date = {1985-07},
	langid = {english},
}

@article{olshausen_neurobiological_1993,
	title = {A neurobiological model of visual attention and invariant pattern recognition based on dynamic routing of information.},
	volume = {13},
	issn = {1529-2401(Electronic),0270-6474(Print)},
	abstract = {Proposes a neurobiological mechanism for routing retinal information so that an object becomes represented within an object-based reference frame in higher cortical areas. The model presented allows both shifting and scaling between input and output arrays, and it provides a solution for controlling the shift and scale in an autonomous fashion. The article begins with a description of the basic model (the dynamic routing circuit) and its autonomous control. Subsequent sections describe the proposed neurobiological substrates and mechanisms, predictions of the model, and a comparison with other models that have been proposed for visual attention and recognition. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {4700--4719},
	number = {11},
	journaltitle = {The Journal of Neuroscience},
	author = {Olshausen, Bruno A. and Anderson, Charles H. and Van Essen, David C.},
	date = {1993},
	note = {Place: {US}
Publisher: Society for Neuroscience},
	keywords = {*Attention, *Neurobiology, *Retina, *Visual Cortex, *Visual Perception, Models, Visual Attention},
}

@article{abbott_lapicques_1999,
	title = {Lapicque’s introduction of the integrate-and-fire model neuron},
	volume = {50},
	issn = {03619230},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0361923099001616},
	doi = {10.1016/S0361-9230(99)00161-6},
	pages = {303--304},
	number = {5},
	journaltitle = {Brain Research Bulletin},
	shortjournal = {Brain Research Bulletin},
	author = {Abbott, Larry F.},
	urldate = {2023-06-29},
	date = {1999-11},
	langid = {english},
}

@inproceedings{zhang_convergent_2017,
	location = {Long Beach, {CA}, {USA}},
	title = {Convergent Block Coordinate Descent for Training Tikhonov Regularized Deep Neural Networks},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6a2feef8ed6a9fe76d6b3f30f02150b4-Paper.pdf},
	series = {{NIPS}'17},
	pages = {1719--1728},
	booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Ziming and Brand, Matthew},
	editor = {Guyon, Isabelle and von Luxburg, Ulrike and Bengio, Sammy and Wallach, Hannah and Fergus, Rob and Vishwanathan, Vishy and Garnett, Roman},
	date = {2017},
}

@inproceedings{wulff_learning_1992,
	location = {Denver, {CO}, {USA}},
	title = {Learning Cellular Automaton Dynamics with Neural Networks},
	volume = {5},
	url = {https://proceedings.neurips.cc/paper_files/paper/1992/file/d6c651ddcd97183b2e40bc464231c962-Paper.pdf},
	series = {{NIPS}'92},
	pages = {631--638},
	booktitle = {Proceedings of the 5th International Conference on Neural Information Processing Systems},
	publisher = {Morgan-Kaufmann},
	author = {Wulff, Niels H. and Hertz, John A.},
	editor = {Hanson, Stephen J. and Cowan, Jack D. and Giles, Clyde L.},
	date = {1992},
}

@inproceedings{waibel_phoneme_1987,
	location = {Tokyo, Japan},
	title = {Phoneme Recognition Using Time-Delay Neural Networks},
	volume = {37},
	series = {{IEICE}'87},
	eventtitle = {Information and Communication Engineers},
	pages = {329--339},
	booktitle = {Meeting of the Institute of Electrical},
	author = {Waibel, Alex and Hanazawa, Toshiyuki and Hinton, Geoffrey E. and Shikano, Kiyohiro and Lang, Kevin J.},
	date = {1987},
	langid = {english},
}

@inproceedings{van_den_oord_neural_2017,
	location = {Long Beach, {CA}, {USA}},
	title = {Neural Discrete Representation Learning},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf},
	series = {{NIPS}'17},
	pages = {6309--6318},
	booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {van den Oord, Aaron and Vinyals, Oriol and kavukcuoglu, koray},
	editor = {Guyon, Isabelle and von Luxburg, Ulrike and Bengio, Sammy and Wallach, Hannah and Fergus, Rob and Vishwanathan, Vishy and Garnett, Roman},
	date = {2017},
}

@inproceedings{tolstikhin_mlp-mixer_2021,
	location = {Online},
	title = {{MLP}-Mixer: An all-{MLP} Architecture for Vision},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Paper.pdf},
	series = {{NeurIPS}'21},
	pages = {24261--24272},
	booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
	editor = {Ranzato, Marc'Aurelio and Beygelzimer, Alina and Dauphin, Yann and Liang, Percy S. and Vaughan, Jenn W.},
	date = {2021},
}

@inproceedings{teichmann_intrinsic_2015,
	title = {Intrinsic Plasticity: A Simple Mechanism to Stabilize Hebbian Learning in Multilayer Neural Networks},
	series = {{NC}2},
	pages = {103--111},
	booktitle = {Proceedings Workshop New Challenges in Neural Computation},
	publisher = {Machine Learning Reports},
	author = {Teichmann, Michael and Hamker, Fred},
	editor = {Hammer, Barbara and Martinetz, Thomas and Villmann, Thomas},
	date = {2015-03},
}

@inproceedings{sabour_dynamic_2017,
	location = {Red Hook, {NY}, {USA}},
	title = {Dynamic Routing between Capsules},
	volume = {30},
	isbn = {978-1-5108-6096-4},
	series = {{NIPS}'17},
	abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on {MNIST} and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
	pages = {3859--3869},
	booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates Inc.},
	author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
	editor = {Guyon, Isabelle and von Luxburg, Ulrike and Bengio, Sammy and Wallach, Hannah and Fergus, Rob and Vishwanathan, Vishy and Garnett, Roman},
	date = {2017},
	note = {event-place: Long Beach, California, {USA}},
}

@inproceedings{raghavan_neural_2019,
	location = {Vancouver, Canada},
	title = {Neural networks grown and self-organized by noise},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf},
	series = {{NeurIPS}'19},
	pages = {1897--1907},
	booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Raghavan, Guruprasad and Thomson, Matt},
	editor = {Wallach, Hannah and Larochelle, Hugo and Beygelzimer, Alina and d'Alché-Buc, Florence and Fox, Emily and Garnett, Roman},
	date = {2019},
}

@inproceedings{poulenard_effective_2019,
	location = {Québec, Canada},
	title = {Effective Rotation-Invariant Point {CNN} with Spherical Harmonics Kernels},
	isbn = {978-1-72813-131-3},
	url = {https://ieeexplore.ieee.org/document/8886010/},
	doi = {10.1109/3DV.2019.00015},
	series = {3DV'19},
	eventtitle = {International Conference on 3D Vision},
	pages = {47--56},
	booktitle = {International Conference on 3D Vision},
	publisher = {{IEEE}},
	author = {Poulenard, Adrien and Rakotosaona, Marie-Julie and Ponty, Yann and Ovsjanikov, Maks},
	urldate = {2023-07-02},
	date = {2019-09},
}

@inproceedings{pedersen_evolving_2021,
	location = {Lille, France},
	title = {Evolving and Merging Hebbian Learning Rules: Increasing Generalization by Decreasing the Number of Rules},
	isbn = {978-1-4503-8350-9},
	url = {http://arxiv.org/abs/2104.07959},
	doi = {10.1145/3449639.3459317},
	series = {{GECCO}'21},
	shorttitle = {Evolving and Merging Hebbian Learning Rules},
	abstract = {Generalization to out-of-distribution ({OOD}) circumstances after training remains a challenge for artificial agents. To improve the robustness displayed by plastic Hebbian neural networks, we evolve a set of Hebbian learning rules, where multiple connections are assigned to a single rule. Inspired by the biological phenomenon of the genomic bottleneck, we show that by allowing multiple connections in the network to share the same local learning rule, it is possible to drastically reduce the number of trainable parameters, while obtaining a more robust agent. During evolution, by iteratively using simple K-Means clustering to combine rules, our Evolve and Merge approach is able to reduce the number of trainable parameters from 61,440 to 1,920, while at the same time improving robustness, all without increasing the number of generations used. While optimization of the agents is done on a standard quadruped robot morphology, we evaluate the agents' performances on slight morphology modifications in a total of 30 unseen morphologies. Our results add to the discussion on generalization, overfitting and {OOD} adaptation. To create agents that can adapt to a wider array of unexpected situations, Hebbian learning combined with a regularising "genomic bottleneck" could be a promising research direction.},
	pages = {892--900},
	booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
	publisher = {{ACM}},
	author = {Pedersen, Joachim Winther and Risi, Sebastian},
	urldate = {2023-07-03},
	date = {2021-06-26},
	eprinttype = {arxiv},
	eprint = {2104.07959 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{oberlaender_beyond_2012,
	location = {Munich, Germany},
	title = {Beyond the Cortical Column: Structural Organization Principles in Rat Vibrissal Cortex},
	series = {{INCF}'12},
	booktitle = {5th {INCF} Congress of Neuroinformatics},
	publisher = {Frontiers Research Foundation},
	author = {Oberlaender, Marcel and Narayanan, Rajeev T. and Egger, Robert and Meyer, Hanno and Baltruschat, Lothar and Dercksen, Vincent and Bruno, Randy and De Kock, Christiaan P. J. and Sakmann, Bert},
	date = {2012},
}

@inproceedings{nokland_training_2019,
	location = {Long Beach, {CA}, {USA}},
	title = {Training Neural Networks with Local Error Signals},
	volume = {97},
	url = {https://proceedings.mlr.press/v97/nokland19a.html},
	series = {{ICML}'19},
	abstract = {Supervised training of neural networks for classification is typically performed with a global loss function. The loss function provides a gradient for the output layer, and this gradient is back-propagated to hidden layers to dictate an update direction for the weights. An alternative approach is to train the network with layer-wise loss functions. In this paper we demonstrate, for the first time, that layer-wise training can approach the state-of-the-art on a variety of image datasets. We use single-layer sub-networks and two different supervised loss functions to generate local error signals for the hidden layers, and we show that the combination of these losses help with optimization in the context of local learning. Using local errors could be a step towards more biologically plausible deep learning because the global error does not have to be transported back to hidden layers. A completely backprop free variant outperforms previously reported results among methods aiming for higher biological plausibility.},
	pages = {4839--4850},
	booktitle = {Proceedings of the 36th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Nøkland, Arild and Eidnes, Lars Hiller},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	date = {2019-06-09},
}

@inproceedings{najarro_meta-learning_2020,
	location = {Vancouver, Canada},
	title = {Meta-Learning through Hebbian Plasticity in Random Networks},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/ee23e7ad9b473ad072d57aaa9b2a5222-Paper.pdf},
	series = {{NeurIPS}'20},
	pages = {20719--20731},
	booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Najarro, Elias and Risi, Sebastian},
	editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Marina F. and Lin, Hsuan-Tien},
	date = {2020},
}

@inproceedings{mordvintsev_growing_2022,
	location = {Online},
	title = {Growing Isotropic Neural Cellular Automata},
	url = {https://direct.mit.edu/isal/article-abstract/doi/10.1162/isal_a_00552},
	doi = {10.1162/isal_a_00552},
	series = {{ALIFE}'22},
	eventtitle = {The 2022 Conference on Artificial Life},
	pages = {65},
	booktitle = {The 2022 Conference on Artificial Life},
	publisher = {{MIT} Press},
	author = {Mordvintsev, Alexander and Randazzo, Ettore and Fouts, Craig},
	urldate = {2023-07-03},
	date = {2022},
	langid = {english},
}

@inproceedings{meulemans_theoretical_2020,
	location = {Vancouver, Canada},
	title = {A Theoretical Framework for Target Propagation},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/e7a425c6ece20cbc9056f98699b53c6f-Paper.pdf},
	series = {{NeurIPS}'20},
	pages = {20024--20036},
	booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Meulemans, Alexander and Carzaniga, Francesco and Suykens, Johan and Sacramento, João and Grewe, Benjamin F.},
	editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Marina F. and Lin, Hsuan-Tien},
	date = {2020},
}

@inproceedings{liao_how_2016,
	location = {Phoenix, {AZ}, {USA}},
	title = {How Important is Weight Symmetry in Backpropagation?},
	volume = {30},
	doi = {10.1609/aaai.v30i1.10279},
	series = {{AAAI}'16},
	abstract = {Gradient backpropagation ({BP}) requires symmetric feedforward and feedback connections—the same weights must be used for forward and backward passes. This "weight transport problem" (Grossberg 1987) is thought to be one of the main reasons to doubt {BP}'s biologically plausibility. Using 15 different classification datasets, we systematically investigate to what extent {BP} really depends on weight symmetry. In a study that turned out to be surprisingly similar in spirit to Lillicrap et al.'s demonstration (Lillicrap et al. 2014) but orthogonal in its results, our experiments indicate that: (1) the magnitudes of feedback weights do not matter to performance (2) the signs of feedback weights do matter—the more concordant signs between feedforward and their corresponding feedback connections, the better (3) with feedback weights having random magnitudes and 100\% concordant signs, we were able to achieve the same or even better performance than {SGD}. (4) some normalizations/stabilizations are indispensable for such asymmetric {BP} to work, namely Batch Normalization ({BN}) (Ioffe and Szegedy 2015) and/or a "Batch Manhattan" ({BM}) update rule.},
	pages = {1837--1844},
	booktitle = {Proceedings of the 30th {AAAI} Conference on Artificial Intelligence},
	publisher = {{AAAI} Press},
	author = {Liao, Qianli and Leibo, Joel Z. and Poggio, Tomaso},
	date = {2016},
	note = {Place: Phoenix, Arizona},
}

@inproceedings{lee_deeply-supervised_2015,
	location = {San Diego, {CA}, {USA}},
	title = {Deeply-Supervised Nets},
	volume = {38},
	url = {https://proceedings.mlr.press/v38/lee15a.html},
	series = {{AIStats}'15},
	abstract = {We propose deeply-supervised nets ({DSN}), a method that simultaneously minimizes classification error and improves the directness and transparency of the hidden layer learning process. We focus our attention on three aspects of traditional convolutional-neural-network-type ({CNN}-type) architectures: (1) transparency in the effect intermediate layers have on overall classification; (2) discriminativeness and robustness of learned features, especially in early layers; (3) training effectiveness in the face of “vanishing” gradients. To combat these issues, we introduce “companion” objective functions at each hidden layer, in addition to the overall objective function at the output layer (an integrated strategy distinct from layer-wise pre-training). We also analyze our algorithm using techniques extended from stochastic gradient methods. The advantages provided by our method are evident in our experimental results, showing state-of-the-art performance on {MNIST}, {CIFAR}-10, {CIFAR}-100, and {SVHN}.},
	pages = {562--570},
	booktitle = {Proceedings of the 18th International Conference on Artificial Intelligence and Statistics},
	publisher = {{PMLR}},
	author = {Lee, Chen-Yu and Xie, Saining and Gallagher, Patrick and Zhang, Zhengyou and Tu, Zhuowen},
	editor = {Lebanon, Guy and Vishwanathan, Vishy},
	date = {2015-05-09},
}

@inproceedings{krotov_dense_2016,
	location = {Red Hook, {NY}, {USA}},
	title = {Dense Associative Memory for Pattern Recognition},
	volume = {29},
	isbn = {978-1-5108-3881-9},
	series = {{NIPS}'16},
	abstract = {A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions - the higher rectified polynomials which until now have not been used in deep learning. The utility of the dense memories is illustrated for two test cases: the logical gate {XOR} and the recognition of handwritten digits from the {MNIST} data set.},
	pages = {1180--1188},
	booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates Inc.},
	author = {Krotov, Dmitry and Hopfield, John J.},
	editor = {Lee, Daniel D. and Sugiyama, Masashi and von Luxburg, Ulrike and Guyon, Isabelle and Garnett, Roman},
	date = {2016},
	note = {event-place: Barcelona, Spain},
}

@inproceedings{kolesnikov_image_2021,
	location = {Vienna, Austria},
	title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	series = {{ICLR}'21},
	booktitle = {Proceedings of the 9th International Conference on Learning Representations},
	author = {Kolesnikov, Alexander and Dosovitskiy, Alexey and Weissenborn, Dirk and Heigold, Georg and Uszkoreit, Jakob and Beyer, Lucas and Minderer, Matthias and Dehghani, Mostafa and Houlsby, Neil and Gelly, Sylvain and Unterthiner, Thomas and Zhai, Xiaohua},
	date = {2021},
}

@inproceedings{dosovitskiy_image_2021,
	location = {Vienna, Austria},
	title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	series = {{ICLR}'21},
	booktitle = {Proceedings of the 9th International Conference on Learning Representations},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	date = {2021},
}

@inproceedings{kirsch_meta_2021,
	location = {Online},
	title = {Meta Learning Backpropagation And Improving It},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/7608de7a475c0c878f60960d72a92654-Paper.pdf},
	series = {{NeurIPS}'21},
	pages = {14122--14134},
	booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Kirsch, Louis and Schmidhuber, Jürgen},
	editor = {Ranzato, Marc'Aurelio and Beygelzimer, Alina and Dauphin, Yann and Liang, Percy S. and Vaughan, Jenn W.},
	date = {2021},
}

@inproceedings{jaderberg_decoupled_2017,
	location = {Sydney, Australia},
	title = {Decoupled Neural Interfaces using Synthetic Gradients},
	volume = {70},
	url = {https://proceedings.mlr.press/v70/jaderberg17a.html},
	series = {{ICML}'17},
	abstract = {Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled {\textless}em{\textgreater}synthetic gradient{\textless}/em{\textgreater} in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise {\textless}em{\textgreater}decoupled neural interfaces{\textless}/em{\textgreater}. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks ({RNNs}) where predicting one’s future gradient extends the time over which the {RNN} can effectively model, and also a hierarchical {RNN} system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass – amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.},
	pages = {1627--1635},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
	editor = {Precup, Doina and Teh, Yee Whye},
	date = {2017-08-06},
}

@inproceedings{grattarola_learning_2021,
	location = {Online},
	title = {Learning Graph Cellular Automata},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/af87f7cdcda223c41c3f3ef05a3aaeea-Paper.pdf},
	series = {{NeurIPS}'21},
	pages = {20983--20994},
	booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Grattarola, Daniele and Livi, Lorenzo and Alippi, Cesare},
	editor = {Ranzato, Marc'Aurelio and Beygelzimer, Alina and Dauphin, Yann and Liang, Percy S. and Vaughan, J. Wortman},
	date = {2021},
}

@inproceedings{gerasimou_importance-driven_2020,
	location = {Seoul, South Korea},
	title = {Importance-driven deep learning system testing},
	isbn = {978-1-4503-7121-6},
	url = {https://dl.acm.org/doi/10.1145/3377811.3380391},
	doi = {10.1145/3377811.3380391},
	series = {{ICSE}'20},
	eventtitle = {{ICSE} '20: 42nd International Conference on Software Engineering},
	pages = {702--713},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Gerasimou, Simos and Eniser, Hasan Ferit and Sen, Alper and Cakan, Alper},
	editor = {Pasquale, Liliana},
	urldate = {2023-07-08},
	date = {2020-06-27},
	langid = {english},
}

@inproceedings{fritzke_growing_1994,
	location = {Denver, {CO}, {USA}},
	title = {A Growing Neural Gas Network Learns Topologies},
	volume = {7},
	url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf},
	series = {{NIPS}'94},
	pages = {625--632},
	booktitle = {Proceedings of the 7th International Conference on Neural Information Processing Systems},
	publisher = {{MIT} Press},
	author = {Fritzke, Bernd},
	editor = {Tesauro, Gerald and Touretzky, David S. and Leen, Todd K.},
	date = {1994},
}

@inproceedings{bengio_flow_2021,
	location = {Online},
	title = {Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/e614f646836aaed9f89ce58e837e2310-Paper.pdf},
	series = {{NeurIPS}'21},
	pages = {27381--27394},
	booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Bengio, Emmanuel and Jain, Moksh and Korablyov, Maksym and Precup, Doina and Bengio, Yoshua},
	editor = {Ranzato, Marc'Aurelio and Beygelzimer, Alina and Dauphin, Yann and Liang, Percy S. and Vaughan, Jenn W.},
	date = {2021},
}

@inproceedings{belilovsky_decoupled_2020,
	location = {Vienna, Austria},
	title = {Decoupled Greedy Learning of {CNNs}},
	volume = {119},
	url = {https://proceedings.mlr.press/v119/belilovsky20a.html},
	series = {{ICML}'20},
	abstract = {A commonly cited inefficiency of neural network training by back-propagation is the update locking problem: each layer must wait for the signal to propagate through the full network before updating. Several alternatives that can alleviate this issue have been proposed. In this context, we consider a simpler, but more effective, substitute that uses minimal feedback, which we call Decoupled Greedy Learning ({DGL}). It is based on a greedy relaxation of the joint training objective, recently shown to be effective in the context of Convolutional Neural Networks ({CNNs}) on large-scale image classification. We consider an optimization of this objective that permits us to decouple the layer training, allowing for layers or modules in networks to be trained with a potentially linear parallelization in layers. With the use of a replay buffer we show this approach can be extended to asynchronous settings, where modules can operate with possibly large communication delays. We show theoretically and empirically that this approach converges. Then, we empirically find that it can lead to better generalization than sequential greedy optimization. We demonstrate the effectiveness of {DGL} against alternative approaches on the {CIFAR}-10 dataset and on the large-scale {ImageNet} dataset.},
	pages = {736--745},
	booktitle = {Proceedings of the 37th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Belilovsky, Eugene and Eickenberg, Michael and Oyallon, Edouard},
	editor = {Daumé {III}, Hal and Singh, Aarti},
	date = {2020},
}

@inproceedings{belilovsky_greedy_2019,
	location = {Long Beach, {CA}, {USA}},
	title = {Greedy Layerwise Learning Can Scale to {ImageNet}},
	volume = {97},
	url = {https://proceedings.mlr.press/v97/belilovsky19a.html},
	series = {{ICML}'19},
	pages = {583--593},
	booktitle = {Proceedings of the 36th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Belilovsky, Eugene and Eickenberg, Michael and Oyallon, Edouard},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan R.},
	date = {2019},
}

@inproceedings{ciresan_flexible_2011,
	location = {Barcelona, Spain},
	title = {Flexible, High Performance Convolutional Neural Networks for Image Classification},
	volume = {2},
	isbn = {978-1-57735-514-4},
	series = {{IJCAI}'11},
	abstract = {We present a fast, fully parameterizable {GPU} implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification ({NORB}, {CIFAR}10) and handwritten digit recognition ({MNIST}), with error rates of 2.53\%, 19.51\%, 0.35\%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. {NORB} is completely trained within five epochs. Test error rates on {MNIST} drop to 2.42\%, 0.97\% and 0.48\% after 1, 3 and 17 epochs, respectively.},
	pages = {1237--1242},
	booktitle = {Proceedings of the 22nd International Joint Conference on Artificial Intelligence},
	publisher = {{AAAI} Press},
	author = {Cireşan, Dan C. and Meier, Ueli and Masci, Jonathan and Gambardella, Luca M. and Schmidhuber, Jürgen},
	editor = {Walsh, Toby},
	date = {2011},
}

@inproceedings{brown_language_2020,
	location = {Vancouver, Canada},
	title = {Language Models are Few-Shot Learners},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	series = {{NeurIPS}'20},
	pages = {1877--1901},
	booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D. and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Marina F. and Lin, Hsuan-Tien},
	date = {2020},
}

@inproceedings{bartunov_assessing_2018,
	location = {Montreal, Canada},
	title = {Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/63c3ddcc7b23daa1e42dc41f9a44a873-Paper.pdf},
	series = {{NeurIPS}'18},
	pages = {9390--9400},
	booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Bartunov, Sergey and Santoro, Adam and Richards, Blake and Marris, Luke and Hinton, Geoffrey E. and Lillicrap, Timothy},
	editor = {Bengio, Sammy and Wallach, Hanna and Larochelle, Hugo and Grauman, Kristen and Cesa-Bianchi, Nicolò and Garnett, Roman},
	date = {2018},
}

@article{lillicrap_backpropagation_2020,
	title = {Backpropagation and the brain},
	volume = {21},
	issn = {1471-003X, 1471-0048},
	url = {https://www.nature.com/articles/s41583-020-0277-3},
	doi = {10.1038/s41583-020-0277-3},
	pages = {335--346},
	number = {6},
	journaltitle = {Nature Reviews Neuroscience},
	shortjournal = {Nat Rev Neurosci},
	author = {Lillicrap, Timothy P. and Santoro, Adam and Marris, Luke and Akerman, Colin J. and Hinton, Geoffrey E.},
	urldate = {2023-06-28},
	date = {2020-06},
	langid = {english},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	pages = {436--444},
	number = {7553},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {{LeCun}, Yann and Bengio, Yoshua and Hinton, Geoffrey E.},
	urldate = {2023-06-26},
	date = {2015-05-28},
	langid = {english},
}

@book{marr_vision_2010,
	title = {Vision: A Computational Investigation into the Human Representation and Processing of Visual Information},
	isbn = {978-0-262-28961-0},
	url = {https://direct.mit.edu/books/book/3299},
	publisher = {{MIT} Press},
	author = {Marr, David},
	urldate = {2023-07-02},
	date = {2010},
	langid = {english},
	doi = {10.7551/mitpress/9780262514620.001.0001},
}

@inproceedings{szegedy_going_2015,
	location = {Boston, {MA}, {USA}},
	title = {Going deeper with convolutions},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298594/},
	doi = {10.1109/CVPR.2015.7298594},
	series = {{CVPR}'15},
	eventtitle = {{IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {1--9},
	booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition},
	publisher = {{IEEE}},
	author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	urldate = {2023-06-27},
	date = {2015-06},
}

@inproceedings{simmler_survey_2021,
	location = {Lucerne, Switzerland},
	title = {A Survey of Un-, Weakly-, and Semi-Supervised Learning Methods for Noisy, Missing and Partial Labels in Industrial Vision Applications},
	isbn = {978-1-66543-874-2},
	url = {https://ieeexplore.ieee.org/document/9474624/},
	doi = {10.1109/SDS51136.2021.00012},
	series = {{SDS}'21},
	eventtitle = {8th Swiss Conference on Data Science},
	pages = {26--31},
	booktitle = {8th Swiss Conference on Data Science},
	publisher = {{IEEE}},
	author = {Simmler, Niclas and Sager, Pascal and Andermatt, Philipp and Chavarriaga, Ricardo and Schilling, Frank-Peter and Rosenthal, Matthias and Stadelmann, Thilo},
	urldate = {2023-06-27},
	date = {2021-06},
}

@inproceedings{rennie_self-critical_2017,
	location = {Honolulu, {HI}, {USA}},
	title = {Self-Critical Sequence Training for Image Captioning},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099614/},
	doi = {10.1109/CVPR.2017.131},
	series = {{CVPR}'17},
	eventtitle = {{IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {1179--1195},
	booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition},
	publisher = {{IEEE}},
	author = {Rennie, Steven J. and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
	urldate = {2023-07-20},
	date = {2017-07},
}

@inproceedings{rhu_compressing_2018,
	location = {Vienna, Austria},
	title = {Compressing {DMA} Engine: Leveraging Activation Sparsity for Training Deep Neural Networks},
	isbn = {978-1-5386-3659-6},
	url = {http://ieeexplore.ieee.org/document/8327000/},
	doi = {10.1109/HPCA.2018.00017},
	series = {{HPCA}'18},
	shorttitle = {Compressing {DMA} Engine},
	eventtitle = {{IEEE} International Symposium on High Performance Computer Architecture},
	pages = {78--91},
	booktitle = {{IEEE} International Symposium on High Performance Computer Architecture},
	publisher = {{IEEE}},
	author = {Rhu, Minsoo and O'Connor, Mike and Chatterjee, Niladrish and Pool, Jeff and Kwon, Youngeun and Keckler, Stephen W.},
	urldate = {2023-07-08},
	date = {2018-02},
}

@inproceedings{joshi_rules_2009,
	location = {Atlanta, {GA}, {USA}},
	title = {Rules for information maximization in spiking neurons using intrinsic plasticity},
	isbn = {978-1-4244-3548-7},
	url = {http://ieeexplore.ieee.org/document/5178625/},
	doi = {10.1109/IJCNN.2009.5178625},
	series = {{IJCNN}'09},
	eventtitle = {International Joint Conference on Neural Networks},
	pages = {1456--1461},
	booktitle = {International Joint Conference on Neural Networks},
	publisher = {{IEEE}},
	author = {Joshi, Prashant and Triesch, Jochen},
	urldate = {2023-06-29},
	date = {2009-06},
}

@inproceedings{ide_improvement_2017,
	location = {Anchorage, {AK}, {USA}},
	title = {Improvement of learning for {CNN} with {ReLU} activation by sparse regularization},
	isbn = {978-1-5090-6182-2},
	url = {http://ieeexplore.ieee.org/document/7966185/},
	doi = {10.1109/IJCNN.2017.7966185},
	series = {{IJCNN}'17},
	eventtitle = {International Joint Conference on Neural Networks},
	pages = {2684--2691},
	booktitle = {International Joint Conference on Neural Networks},
	publisher = {{IEEE}},
	author = {Ide, Hidenori and Kurita, Takio},
	urldate = {2023-07-08},
	date = {2017-05},
}

@inproceedings{he_deep_2016,
	location = {Las Vegas, {NV}, {USA}},
	title = {Deep Residual Learning for Image Recognition},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	doi = {10.1109/CVPR.2016.90},
	series = {{CVPR}'16},
	eventtitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {770--778},
	booktitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
	publisher = {{IEEE}},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2023-06-27},
	date = {2016-06},
}

@inproceedings{glasmachers_limits_2017,
	location = {Seoul, South Korea},
	title = {Limits of End-to-End Learning},
	volume = {77},
	url = {https://proceedings.mlr.press/v77/glasmachers17a.html},
	series = {{ACML}'17},
	abstract = {End-to-end learning refers to training a possibly complex learning system by applying gradient-based learning to the system as a whole. End-to-end learning systems are specifically designed so that all modules are differentiable. In effect, not only a central learning machine, but also all “peripheral” modules like representation learning and memory formation are covered by a holistic learning process. The power of end-to-end learning has been demonstrated on many tasks, like playing a whole array of Atari video games with a single architecture. While pushing for solutions to more challenging tasks, network architectures keep growing more and more complex. In this paper we ask the question whether and to what extent end-to-end learning is a future-proof technique in the sense of {\textbackslash}emphscaling to complex and diverse data processing architectures. We point out potential inefficiencies, and we argue in particular that end-to-end learning does not make optimal use of the modular design of present neural networks. Our surprisingly simple experiments demonstrate these inefficiencies, up to the complete breakdown of learning.},
	pages = {17--32},
	booktitle = {Proceedings of the 9th Asian Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Glasmachers, Tobias},
	editor = {Zhang, Min-Ling and Noh, Yung-Kyun},
	date = {2017-11-15},
}

@article{hopfield_neural_1982,
	title = {Neural networks and physical systems with emergent collective computational abilities.},
	volume = {79},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.79.8.2554},
	doi = {10.1073/pnas.79.8.2554},
	abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
	pages = {2554--2558},
	number = {8},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Hopfield, John J.},
	urldate = {2023-06-26},
	date = {1982-04},
	langid = {english},
}

@misc{hinton_how_2021,
	title = {How to represent part-whole hierarchies in a neural network},
	url = {http://arxiv.org/abs/2102.12627},
	doi = {10.48550/arXiv.2102.12627},
	abstract = {This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called {GLOM}. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. {GLOM} answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If {GLOM} can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language},
	number = {{arXiv}:2102.12627},
	publisher = {{arXiv}},
	author = {Hinton, Geoffrey E.},
	urldate = {2023-07-14},
	date = {2021-02-24},
	eprinttype = {arxiv},
	eprint = {2102.12627 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.2.6, I.4.8},
}

@inproceedings{peters_deep_2018,
	location = {New Orleans, {LA}, {USA}},
	title = {Deep Contextualized Word Representations},
	volume = {1},
	url = {http://aclweb.org/anthology/N18-1202},
	doi = {10.18653/v1/N18-1202},
	series = {{NAACL}'18},
	eventtitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics},
	pages = {2227--2237},
	booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	urldate = {2023-06-27},
	date = {2018},
	langid = {english},
}

@inproceedings{carreira-perpinan_distributed_2014,
	location = {Reykjavik, Iceland},
	title = {Distributed optimization of deeply nested systems},
	volume = {33},
	url = {https://proceedings.mlr.press/v33/carreira-perpinan14.html},
	series = {{AIStats}'14},
	abstract = {Intelligent processing of complex signals such as images is often performed by a hierarchy of nonlinear processing layers, such as a deep net or an object recognition cascade. Joint estimation of the parameters of all the layers is a difficult nonconvex optimization. We describe a general strategy to learn the parameters and, to some extent, the architecture of nested systems, which we call the method of auxiliary coordinates ({MAC}). This replaces the original problem involving a deeply nested function with a constrained problem involving a different function in an augmented space without nesting. The constrained problem may be solved with penalty-based methods using alternating optimization over the parameters and the auxiliary coordinates. {MAC} has provable convergence, is easy to implement reusing existing algorithms for single layers, can be parallelized trivially and massively, applies even when parameter derivatives are not available or not desirable, can perform some model selection on the fly, and is competitive with state-of-the-art nonlinear optimizers even in the serial computation setting, often providing reasonable models within a few iterations.},
	pages = {10--19},
	booktitle = {Proceedings of the 17th International Conference on Artificial Intelligence and Statistics},
	publisher = {{PMLR}},
	author = {Carreira-Perpinan, Miguel and Wang, Weiran},
	editor = {Kaski, Samuel and Corander, Jukka},
	date = {2014-04-22},
}

@inproceedings{bengio_deep_2012,
	location = {Bellevue, {DC}, {USA}},
	title = {Deep Learning of Representations for Unsupervised and Transfer Learning},
	volume = {27},
	url = {https://proceedings.mlr.press/v27/bengio12a.html},
	series = {{ICML}'12},
	abstract = {Deep learning algorithms seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels, with higher-level learned features defined in terms of lower-level features. The objective is to make these higher-level representations more abstract, with their individual features more invariant to most of the variations that are typically present in the training distribution, while collectively preserving as much as possible of the information in the input. Ideally, we would like these representations to disentangle the unknown factors of variation that underlie the training distribution. Such unsupervised learning of representations can be exploited usefully under the hypothesis that the input distribution P(x) is structurally related to some task of interest, say predicting P(y{\textbar}x). This paper focuses on the context of the Unsupervised and Transfer Learning Challenge, on why unsupervised pre-training of representations can be useful, and how it can be exploited in the transfer learning scenario, where we care about predictions on examples that are not from the same distribution as the training distribution.},
	pages = {17--36},
	booktitle = {Proceedings of {ICML} Workshop on Unsupervised and Transfer Learning},
	publisher = {{PMLR}},
	author = {Bengio, Yoshua},
	editor = {Guyon, Isabelle and Dror, Gideon and Lemaire, Vincent and Taylor, Graham and Silver, Daniel},
	date = {2012-07-02},
}

@incollection{wagner_robustness_2013,
	title = {Robustness in Natural Systems and Self-Organization},
	isbn = {978-1-4008-4938-3},
	url = {https://www.degruyter.com/document/doi/10.1515/9781400849383.297/html},
	pages = {297--309},
	booktitle = {Robustness and Evolvability in Living Systems},
	publisher = {Princeton University Press},
	author = {Wagner, Andreas},
	urldate = {2023-07-03},
	date = {2013-12-31},
	doi = {10.1515/9781400849383.297},
}

@book{kowalski_information_1997,
	location = {Boston, {MA}, {USA}},
	title = {Information Retrieval Systems},
	volume = {1},
	isbn = {978-0-7923-9926-1},
	url = {http://link.springer.com/10.1007/b102478},
	series = {The Information Retrieval Series},
	publisher = {Springer {US}},
	author = {Kowalski, Gerald},
	urldate = {2023-06-29},
	date = {1997},
	langid = {english},
	doi = {10.1007/b102478},
}

@incollection{adamatzky_reservoir_2018,
	location = {New York, {NY}, {USA}},
	title = {Reservoir Computing},
	isbn = {978-1-4939-6882-4 978-1-4939-6883-1},
	url = {http://link.springer.com/10.1007/978-1-4939-6883-1_683},
	pages = {619--629},
	booktitle = {Unconventional Computing},
	publisher = {Springer {US}},
	author = {Konkoli, Zoran},
	editor = {Adamatzky, Andrew},
	urldate = {2023-06-29},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-1-4939-6883-1_683},
}

@book{russell_artificial_2021,
	location = {Hoboken, {NY}, {USA}},
	edition = {4},
	title = {Artificial intelligence: a modern approach},
	isbn = {978-0-13-461099-3},
	series = {Pearson series in artificial intelligence},
	shorttitle = {Artificial intelligence},
	abstract = {"Updated edition of popular textbook on Artificial Intelligence. This edition specific looks at ways of keeping artificial intelligence under control"--},
	publisher = {Pearson},
	author = {Russell, Stuart J. and Norvig, Peter},
	date = {2021},
	keywords = {Artificial intelligence},
}

@collection{newman_current_1985,
	location = {Edinburgh, {NY}, {USA}},
	title = {Current perspectives in dysphasia},
	isbn = {978-0-443-03039-0},
	pagetotal = {237},
	publisher = {Churchill Livingstone},
	editor = {Newman, Simon P. and Epstein, Ruth},
	date = {1985},
	keywords = {Aphasia},
}

@incollection{gerber_stride_2020,
	location = {Cham, Switzerland},
	title = {Stride and Translation Invariance in {CNNs}},
	volume = {1342},
	isbn = {978-3-030-66150-2 978-3-030-66151-9},
	url = {https://link.springer.com/10.1007/978-3-030-66151-9_17},
	pages = {267--281},
	booktitle = {Artificial Intelligence Research},
	publisher = {Springer International Publishing},
	author = {Mouton, Coenraad and Myburgh, Johannes C. and Davel, Marelie H.},
	editor = {Gerber, Aurona},
	urldate = {2023-06-27},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-66151-9_17},
	note = {Series Title: Communications in Computer and Information Science},
}

@incollection{appice_difference_2015,
	location = {Cham, Switzerland},
	title = {Difference Target Propagation},
	volume = {9284},
	isbn = {978-3-319-23527-1 978-3-319-23528-8},
	url = {http://link.springer.com/10.1007/978-3-319-23528-8_31},
	pages = {498--515},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	publisher = {Springer International Publishing},
	author = {Lee, Dong-Hyun and Zhang, Saizheng and Fischer, Asja and Bengio, Yoshua},
	editor = {Appice, Annalisa and Rodrigues, Pedro Pereira and Santos Costa, Vítor and Soares, Carlos and Gama, João and Jorge, Alípio},
	urldate = {2023-07-04},
	date = {2015},
	langid = {english},
	doi = {10.1007/978-3-319-23528-8_31},
	note = {Series Title: Lecture Notes in Computer Science},
}

@incollection{hu_regenerating_2021,
	location = {Cham, Switzerland},
	title = {Regenerating Soft Robots Through Neural Cellular Automata},
	volume = {12691},
	isbn = {978-3-030-72811-3 978-3-030-72812-0},
	url = {http://link.springer.com/10.1007/978-3-030-72812-0_3},
	pages = {36--50},
	booktitle = {Genetic Programming},
	publisher = {Springer International Publishing},
	author = {Horibe, Kazuya and Walker, Kathryn and Risi, Sebastian},
	editor = {Hu, Ting and Lourenço, Nuno and Medvet, Eric},
	urldate = {2023-07-03},
	date = {2021},
	langid = {english},
	doi = {10.1007/978-3-030-72812-0_3},
	note = {Series Title: Lecture Notes in Computer Science},
}

@incollection{freeman_representations_1990,
	location = {Oxford, {NY}, {USA}},
	title = {Representations: Who needs them?},
	pages = {375--380},
	booktitle = {Brain Organization and Memory: Cells, Systems and Circuits.},
	publisher = {Oxford Guilford Press},
	author = {Freeman, Walter J. and Skarda, Christine A.},
	editor = {{McGaugh}, J. and Weinberger, Jerry and Lynch, G.},
	date = {1990},
	note = {Publisher: Guilford Press},
}

@book{prince_understanding_2023,
	location = {Cambridge, {MA}, {USA}},
	title = {Understanding Deep Learning},
	isbn = {978-0-262-04864-4},
	publisher = {{MIT} Press},
	author = {Prince, Simon J. D.},
	date = {2023},
	note = {{OCLC}: 1372277824},
}

@book{parr_active_2022,
	location = {Cambridge, {MA}, {USA}},
	title = {Active Inference: The Free Energy Principle in Mind, Brain, and Behavior},
	isbn = {978-0-262-36997-8},
	url = {https://direct.mit.edu/books/book/5299/Active-InferenceThe-Free-Energy-Principle-in-Mind},
	shorttitle = {Active Inference},
	abstract = {The first comprehensive treatment of active inference, an integrative perspective on brain, cognition, and behavior used across multiple disciplines.
            Active inference is a way of understanding sentient behavior—a theory that characterizes perception, planning, and action in terms of probabilistic inference. Developed by theoretical neuroscientist Karl Friston over years of groundbreaking research, active inference provides an integrated perspective on brain, cognition, and behavior that is increasingly used across multiple disciplines including neuroscience, psychology, and philosophy. Active inference puts the action into perception. This book offers the first comprehensive treatment of active inference, covering theory, applications, and cognitive domains.
            Active inference is a “first principles” approach to understanding behavior and the brain, framed in terms of a single imperative to minimize free energy. The book emphasizes the implications of the free energy principle for understanding how the brain works. It first introduces active inference both conceptually and formally, contextualizing it within current theories of cognition. It then provides specific examples of computational models that use active inference to explain such cognitive phenomena as perception, attention, memory, and planning.},
	publisher = {{MIT} Press},
	author = {Parr, Thomas and Pezzulo, Giovanni and Friston, Karl J.},
	urldate = {2023-07-01},
	date = {2022-03-29},
	langid = {english},
	doi = {10.7551/mitpress/12441.001.0001},
}

@incollection{mountcastle_organizing_1978,
	location = {Cambridge, {MA}, {USA}},
	title = {An Organizing Principle for Cerebral Function: The Unit Model and the Distributed System},
	pages = {7--50},
	booktitle = {The Mindful Brain},
	publisher = {{MIT} Press},
	author = {Mountcastle, Vernon},
	date = {1978},
	langid = {english},
}

@book{moravec_mind_1995,
	location = {Cambridge, {MA}, {USA}},
	title = {Mind Children: The Future of Robot and Human Intelligence},
	isbn = {978-0-674-57618-6},
	publisher = {Harvard University Press},
	author = {Moravec, Hans},
	date = {1995},
	langid = {english},
}

@incollection{thrun_learning_1998,
	location = {Boston, {MA}, {USA}},
	title = {Learning to Learn: Introduction and Overview},
	isbn = {978-1-4613-7527-2 978-1-4615-5529-2},
	url = {http://link.springer.com/10.1007/978-1-4615-5529-2_1},
	shorttitle = {Learning to Learn},
	pages = {3--17},
	booktitle = {Learning to Learn},
	publisher = {Springer {US}},
	author = {Thrun, Sebastian and Pratt, Lorien},
	editor = {Thrun, Sebastian and Pratt, Lorien},
	urldate = {2023-07-01},
	date = {1998},
	langid = {english},
	doi = {10.1007/978-1-4615-5529-2_1},
}

@incollection{montavon_practical_2012,
	location = {Heidelberg, Germany},
	title = {A Practical Guide to Applying Echo State Networks},
	volume = {7700},
	isbn = {978-3-642-35288-1 978-3-642-35289-8},
	url = {http://link.springer.com/10.1007/978-3-642-35289-8_36},
	pages = {659--686},
	booktitle = {Neural Networks: Tricks of the Trade},
	publisher = {Springer Berlin Heidelberg},
	author = {Lukoševičius, Mantas},
	editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
	urldate = {2023-06-29},
	date = {2012},
	langid = {english},
	doi = {10.1007/978-3-642-35289-8_36},
	note = {Series Title: Lecture Notes in Computer Science},
}

@incollection{cord_supervised_2008,
	location = {Heidelberg, Germany},
	title = {Supervised Learning},
	isbn = {978-3-540-75170-0 978-3-540-75171-7},
	url = {http://link.springer.com/10.1007/978-3-540-75171-7_2},
	pages = {21--49},
	booktitle = {Machine Learning Techniques for Multimedia},
	publisher = {Springer Berlin Heidelberg},
	author = {Cunningham, Pádraig and Cord, Matthieu and Delany, Sarah Jane},
	editor = {Cord, Matthieu and Cunningham, Pádraig},
	urldate = {2023-06-26},
	date = {2008},
	langid = {english},
	doi = {10.1007/978-3-540-75171-7_2},
	note = {Series Title: Cognitive Technologies},
}

@incollection{eckmiller_spike_1990,
	location = {New York, {NY}, {USA}},
	title = {Spike arrival times: A highly efficient coding scheme for neural networks},
	isbn = {978-0-444-88390-2},
	pages = {91--94},
	booktitle = {Parallel processing in neural systems and computers},
	publisher = {North-Holland ; Distributors for the U.S. and Canada, Elsevier Science Pub. Co},
	author = {Thorpe, Simon J.},
	editor = {Eckmiller, Rolf and Hartmann, Georg and Hauske, Gert},
	date = {1990},
	note = {Meeting Name: International Conference on Parallel Processing in Neural Systems and Computers},
	keywords = {Congresses, Neural computers, Parallel processing (Electronic computers)},
}

@book{mitchell_machine_1997,
	location = {New York, {NY}, {USA}},
	title = {Machine Learning},
	isbn = {978-0-07-042807-2},
	series = {{McGraw}-Hill series in computer science},
	pagetotal = {414},
	publisher = {{McGraw}-Hill},
	author = {Mitchell, Tom M.},
	date = {1997},
	keywords = {Computer algorithms, Machine learning},
}

@book{kohonen_self-organization_1989,
	location = {Heidelberg, Germany},
	edition = {3rd},
	title = {Self-Organization and Associative Memory},
	isbn = {978-3-642-88163-3},
	abstract = {This monograph gives a tutorial treatment of new approaches to self-organization, adaptation, learning and memory. It is based on recent research results, both mathematical and computer simulations, and lends itself to graduate and postgraduate courses in the natural sciences. The book presents new formalisms of pattern processing: orthogonal projectors, optimal associative mappings, novelty filters, subspace methods, feature-sensitive units, and self-organization of topological maps, with all their computable algorithms. The main objective is to provide an understanding of the properties of information representations from a general point of view and of their use in pattern information processing, as well as an understanding of many functions of the brain. In the third edition two new discussions have been added and a proof has been revised. The author has developed this book from Associative Memory - A System-Theoretical Approach (Volume 17 of Springer Series in Communication and Cybernetics, 1977), the first ever monograph on distributed associative memories},
	publisher = {Springer Berlin Heidelberg},
	author = {Kohonen, Teuvo},
	date = {1989},
	note = {{OCLC}: 851372105},
}

@collection{kandel_principles_2013,
	location = {New York, {NY}, {USA}},
	edition = {5th},
	title = {Principles of neural science},
	isbn = {978-0-07-139011-8},
	abstract = {"The field's definitive work from a Nobel Prize-winning author 900 full-color illustrations Principles of Neural Science, 5e describes our current understanding of how the nerves, brain, and mind function. From molecules to anatomic structures and systems to cognitive function, this comprehensive reference covers all aspects of neuroscience. Widely regarded as the field's cornerstone reference, the fifth edition is highlighted by more than 900 full-color illustrations. The fifth edition has been completely updated to reflect the tremendous amount of new research and development in neuroscience in the last decade. Lead author Eric Kandel was awarded the Nobel Prize in Physiology or Medicine in 2000"--Provided by publisher},
	pagetotal = {1709},
	publisher = {{McGraw}-Hill},
	editor = {Kandel, Eric R.},
	date = {2013},
	keywords = {Central Nervous System, Mental Processes, Nervous System Diseases, Neuropsychology, physiology},
}

@collection{lee_advances_2017,
	location = {Red Hook, {NY}, {USA}},
	title = {Advances in neural information processing systems 29: 30th Annual Conference on Neural Information Processing Systems 2016: Barcelona, Spain, 5-10 December 2016},
	isbn = {978-1-5108-3881-9},
	shorttitle = {Advances in neural information processing systems 29},
	publisher = {Curran Associates, Inc},
	editor = {Lee, Daniel D. and Luxburg, Ulrike von and Garnett, Roman and Sugiyama, Masashi and Guyon, Isabelle and Neural Information Processing Systems Foundation},
	date = {2017},
	note = {Meeting Name: Annual Conference on Neural Information Processing Systems},
}

@book{koller_probabilistic_2009,
	location = {Cambridge, {MA}, {USA}},
	title = {Probabilistic graphical models: principles and techniques},
	isbn = {978-0-262-01319-2},
	publisher = {{MIT} Press},
	author = {Koller, Daphne and Friedman, Nir},
	date = {2009},
}

@book{kohler_gestalt_1992,
	location = {New York, {NY}, {USA}},
	title = {Gestalt psychology: an introduction to new concepts in modern psychology},
	isbn = {978-0-87140-218-9},
	shorttitle = {Gestalt psychology},
	pagetotal = {369},
	publisher = {Liveright},
	author = {Köhler, Wolfgang},
	date = {1992},
	keywords = {Gestalt psychology},
}

@book{kelso_dynamic_1999,
	location = {Cambridge, {MA}, {USA}},
	edition = {3rd},
	title = {Dynamic patterns: the self-organization of brain and behavior},
	isbn = {978-0-262-61131-2 978-0-262-11200-0},
	series = {A Bradford book},
	shorttitle = {Dynamic patterns},
	pagetotal = {334},
	publisher = {{MIT} Press},
	author = {Kelso, J. A. Scott},
	date = {1999},
}

@book{ivakhnenko_cybernetic_1965,
	location = {New York, {NY}, {USA}},
	title = {Cybernetic Predicting Devices},
	publisher = {{CCM} Information Corporation},
	author = {Ivakhnenko, Aleksei G. and Lapa, Valentin G.},
	date = {1965},
}

@collection{ellis_source_1938,
	location = {London, England},
	title = {A source book of Gestalt psychology.},
	url = {http://content.apa.org/books/11496-000},
	publisher = {Kegan Paul, Trench, Trubner \& Company},
	editor = {Ellis, Willis D.},
	urldate = {2023-07-07},
	date = {1938},
	langid = {english},
	doi = {10.1037/11496-000},
}

@book{goodfellow_deep_2016,
	location = {Cambridge, {MA}, {USA}},
	title = {Deep Learning},
	url = {http://www.deeplearningbook.org},
	publisher = {{MIT} Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	urldate = {2023-06-27},
	date = {2016},
}

@collection{dong_deep_2020,
	location = {Singapore, Republic of Singapore},
	edition = {1},
	title = {Deep Reinforcement Learning: Fundamentals, Research and Applications},
	isbn = {9789811540943 9789811540950},
	url = {http://link.springer.com/10.1007/978-981-15-4095-0},
	shorttitle = {Deep Reinforcement Learning},
	pagetotal = {514},
	publisher = {Springer Singapore},
	editor = {Dong, Hao and Ding, Zihan and Zhang, Shanghang},
	urldate = {2023-06-28},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-981-15-4095-0},
}

@book{costandi_neuroplasticity_2016,
	location = {Cambridge, {MA}, {USA}},
	title = {Neuroplasticity},
	isbn = {978-0-262-52933-4},
	series = {The {MIT} Press essential knowledge series},
	pagetotal = {181},
	publisher = {{MIT} Press},
	author = {Costandi, Moheb},
	date = {2016},
	keywords = {Nervous system, Neural transmission, Neuroplasticity, Physiology},
}

@book{bain_mind_1873,
	location = {New York, {NY}, {USA}},
	title = {Mind and body: The theories of their relation},
	volume = {1},
	publisher = {D. Appleton and Company},
	author = {Bain, Alexander},
	date = {1873},
}

@book{arathorn_map-seeking_2002,
	location = {Stanford, {CA}, {USA}},
	title = {Map-seeking circuits in visual cognition: a computational mechanism for biological and machine vision},
	isbn = {978-0-8047-4277-1},
	shorttitle = {Map-seeking circuits in visual cognition},
	pagetotal = {221},
	publisher = {Stanford University Press},
	author = {Arathorn, David W.},
	date = {2002},
	keywords = {Cognitive maps (Psychology), Computer vision, Neural circuitry, Vision, Visual cortex},
}

@book{ackerman_discovering_1992,
	location = {Washington, {DC}, {USA}},
	title = {Discovering the brain},
	isbn = {978-0-309-04529-2},
	pagetotal = {180},
	publisher = {National Academy Press},
	author = {Ackerman, Sandra},
	date = {1992},
	keywords = {Brain, Congresses, Neurobiology, Neurology, United States},
}

@article{grill-spector_human_2004,
	title = {The human visual cortex},
	volume = {27},
	issn = {0147-006X, 1545-4126},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.neuro.27.070203.144220},
	doi = {10.1146/annurev.neuro.27.070203.144220},
	abstract = {▪ Abstract  The discovery and analysis of cortical visual areas is a major accomplishment of visual neuroscience. In the past decade the use of noninvasive functional imaging, particularly functional magnetic resonance imaging ({fMRI}), has dramatically increased our detailed knowledge of the functional organization of the human visual cortex and its relation to visual perception. The {fMRI} method offers a major advantage over other techniques applied in neuroscience by providing a large-scale neuroanatomical perspective that stems from its ability to image the entire brain essentially at once. This bird's eye view has the potential to reveal large-scale principles within the very complex plethora of visual areas. Thus, it could arrange the entire constellation of human visual areas in a unified functional organizational framework. Here we review recent findings and methods employed to uncover the functional properties of the human visual cortex focusing on two themes: functional specialization and hierarchical processing.},
	pages = {649--677},
	number = {1},
	journaltitle = {Annual Review of Neuroscience},
	shortjournal = {Annu. Rev. Neurosci.},
	author = {Grill-Spector, Kalanit and Malach, Rafael},
	urldate = {2023-07-08},
	date = {2004-07-21},
	langid = {english},
}

@article{colby_space_1999,
	title = {Space and attention in parietal cortex},
	volume = {22},
	issn = {0147-006X, 1545-4126},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.neuro.22.1.319},
	doi = {10.1146/annurev.neuro.22.1.319},
	abstract = {▪ Abstract  The space around us is represented not once but many times in parietal cortex. These multiple representations encode locations and objects of interest in several egocentric reference frames. Stimulus representations are transformed from the coordinates of receptor surfaces, such as the retina or the cochlea, into the coordinates of effectors, such as the eye, head, or hand. The transformation is accomplished by dynamic updating of spatial representations in conjunction with voluntary movements. This direct sensory-to-motor coordinate transformation obviates the need for a single representation of space in environmental coordinates. In addition to representing object locations in motoric coordinates, parietal neurons exhibit strong modulation by attention. Both top-down and bottom-up mechanisms of attention contribute to the enhancement of visual responses. The saliance of a stimulus is the primary factor in determining the neural response to it. Although parietal neurons represent objects in motor coordinates, visual responses are independent of the intention to perform specific motor acts.},
	pages = {319--349},
	number = {1},
	journaltitle = {Annual Review of Neuroscience},
	shortjournal = {Annu. Rev. Neurosci.},
	author = {Colby, Carol L. and Goldberg, Michael E.},
	urldate = {2023-07-15},
	date = {1999-03},
	langid = {english},
}

@artwork{del_prete_mountain_1982,
	title = {Mountain Spirit in Winter},
	url = {https://im-possible.info/english/art/delprete/the-master-of-illusions/008.html},
	author = {del Prete, Sandro},
	urldate = {2023-07-14},
	date = {1982},
}

@article{goodale_separate_1992,
	title = {Separate visual pathways for perception and action},
	volume = {15},
	issn = {01662236},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0166223692903448},
	doi = {10.1016/0166-2236(92)90344-8},
	pages = {20--25},
	number = {1},
	journaltitle = {Trends in Neurosciences},
	shortjournal = {Trends in Neurosciences},
	author = {Goodale, Melvyn A. and Milner, A.David},
	urldate = {2023-07-14},
	date = {1992-01},
	langid = {english},
}

@online{fasoli_human_2023,
	title = {The human visual system},
	url = {https://www.neuroinformatics.it/ai/},
	titleaddon = {Artificial Intelligence},
	author = {Fasoli, Diego},
	urldate = {2023-07-14},
	date = {2023},
	langid = {english},
}

@article{feldman_neural_2013,
	title = {The neural binding problem(s)},
	volume = {7},
	issn = {1871-4080, 1871-4099},
	url = {http://link.springer.com/10.1007/s11571-012-9219-8},
	doi = {10.1007/s11571-012-9219-8},
	pages = {1--11},
	number = {1},
	journaltitle = {Cognitive Neurodynamics},
	shortjournal = {Cogn Neurodyn},
	author = {Feldman, Jerome},
	urldate = {2023-07-14},
	date = {2013-02},
	langid = {english},
}

@article{revonsuo_binding_1999,
	title = {Binding and Consciousness},
	volume = {8},
	issn = {10538100},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053810099903938},
	doi = {10.1006/ccog.1999.0393},
	pages = {123--127},
	number = {2},
	journaltitle = {Consciousness and Cognition},
	shortjournal = {Consciousness and Cognition},
	author = {Revonsuo, Antti and Newman, James},
	urldate = {2023-07-14},
	date = {1999-06},
	langid = {english},
}

@article{akhtar_threat_2018,
	title = {Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey},
	volume = {6},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8294186/},
	doi = {10.1109/ACCESS.2018.2807385},
	shorttitle = {Threat of Adversarial Attacks on Deep Learning in Computer Vision},
	pages = {14410--14430},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Akhtar, Naveed and Mian, Ajmal},
	urldate = {2023-07-14},
	date = {2018},
}

@misc{mnih_playing_2013,
	title = {Playing Atari with Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1312.5602},
	doi = {10.48550/arXiv.1312.5602},
	abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	number = {{arXiv}:1312.5602},
	publisher = {{arXiv}},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	urldate = {2023-07-14},
	date = {2013-12-19},
	eprinttype = {arxiv},
	eprint = {1312.5602 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{zhu_maplets_2004,
	title = {Maplets for correspondence-based object recognition},
	volume = {17},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608004001509},
	doi = {10.1016/j.neunet.2004.06.010},
	pages = {1311--1326},
	number = {8},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Zhu, Junmei and von der Malsburg, Christoph},
	urldate = {2023-07-13},
	date = {2004-10},
	langid = {english},
}

@article{willshaw_how_1976,
	title = {How patterned neural connections can be set up by self-organization},
	volume = {194},
	issn = {0080-4649, 2053-9193},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspb.1976.0087},
	doi = {10.1098/rspb.1976.0087},
	abstract = {An important problem in biology is to explain how patterned neural connections are set up during ontogenesis. Topographically ordered mappings, found widely in nervous systems, are those in which neighbouring elements in one sheet of cells project to neighbouring elements in a second sheet. Exploiting this neighbourhood property leads to a new theory for the establishment of topographical mappings, in which the distance between two cells is expressed in terms of their similarity with respect to certain physical properties assigned to them. This topographical code can be realized in a model employing either synchronization of nervous activity or exchange of specific molecules between neighbouring cells. By means of modifiable synapses the code is used to set up a topographical mapping between two sheets with the same internal structure. We have investigated the neural activity version. Without needing to make any elaborate assumptions about its structure or about the operations its elements are to carry out we have shown that the mappings are set up in a system-to-system rather than a cell-to-cell fashion. The pattern of connections develops in a step-by-step and orderly fashion, the orientation of the mappings being laid down in the earliest stages of development.},
	pages = {431--445},
	number = {1117},
	journaltitle = {Proceedings of the Royal Society of London. Series B. Biological Sciences},
	shortjournal = {Proc. R. Soc. Lond. B.},
	author = {Willshaw, David J. and von der Malsburg, Christoph},
	urldate = {2023-06-30},
	date = {1976-11-12},
	langid = {english},
}

@article{wiskott_face_1997,
	title = {Face recognition by elastic bunch graph matching},
	volume = {19},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/598235/},
	doi = {10.1109/34.598235},
	pages = {775--779},
	number = {7},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Wiskott, Laurenz and Fellous, Jean-Marc and Kuiger, Norbert and von der Malsburg, Christoph},
	urldate = {2023-06-14},
	date = {1997-07},
}

@article{wolfrum_recurrent_2008,
	title = {A recurrent dynamic model for correspondence-based face recognition},
	volume = {8},
	issn = {1534-7362},
	url = {http://jov.arvojournals.org/article.aspx?doi=10.1167/8.7.34},
	doi = {10.1167/8.7.34},
	pages = {34},
	number = {7},
	journaltitle = {Journal of Vision},
	shortjournal = {Journal of Vision},
	author = {Wolfrum, Philipp and Wolff, Christian and Lücke, Jörg and von der Malsburg, Christoph},
	urldate = {2023-07-03},
	date = {2008-12-29},
	langid = {english},
}

@article{von_der_malsburg_neural_1987,
	title = {A Neural Network for the Retrieval of Superimposed Connection Patterns},
	volume = {3},
	issn = {0295-5075, 1286-4854},
	url = {https://iopscience.iop.org/article/10.1209/0295-5075/3/11/015},
	doi = {10.1209/0295-5075/3/11/015},
	pages = {1243--1249},
	number = {11},
	journaltitle = {Europhysics Letters ({EPL})},
	shortjournal = {Europhys. Lett.},
	author = {von der Malsburg, Christoph and Bienenstock, Elie L.},
	urldate = {2023-06-30},
	date = {1987-06-01},
}

@article{willshaw_marker_1979,
	title = {A marker induction mechanism for the establishment of ordered neural mappings: its application to the retinotectal problem},
	volume = {287},
	issn = {0080-4622, 2054-0280},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.1979.0056},
	doi = {10.1098/rstb.1979.0056},
	shorttitle = {A marker induction mechanism for the establishment of ordered neural mappings},
	abstract = {This paper examines the idea that ordered patterns of nerve connections are set up by means of markers carried by the individual cells. The case of the ordered retinotectal projection in amphibia and fishes is discussed in great detail. It is suggested that retinotectal mappings are the result of two mechanisms acting in concert. One mechanism induces a set of retinal markers into the tectum. By this means, an initially haphazard pattern of synapses is transformed into a continuous or piece-wise continuous projection. The other mechanism places the individual pieces of the map in the correct orientation. The machinery necessary for this inductive scheme has been expressed in terms of a set of differential equations, which have been solved numerically for a number of cases. Straightforward assumptions are made as to how markers are distributed in the retina; how they are induced into the tectum; and how the induced markers bring about alterations in the pattern of synaptic contacts. A detailed physiological interpretation of the model is given. The inductive mechanism has been formulated at the level of the individual synaptic interactions. Therefore, it is possible to specify, in a given situation, not only the nature of the end state of the mapping but also how the mapping develops over time. The role of the modes of growth of retina and tectum in shaping the developing projection becomes clear. Since, on this model, the tectum is initially devoid of markers, there is an important difference between the development and the regeneration of ordered mappings. In the development of duplicate maps from various types of compound-eyes, it is suggested that the tectum, rather than the retina, contains an abnormal distribution of markers. An important parameter in these experiments, and also in the regeneration experiments where part-duplication has been found, is the range of interaction amongst the retinal cells. It is suggested that the results of many of the regeneration experiments (including apparently contradictory ones) are manifestations of a conflict between the two alternative ways of specifying the orientation of the map: through the information carried by the markers previously induced into the tectum and through the orientation mechanism itself.},
	pages = {203--243},
	number = {1021},
	journaltitle = {Philosophical Transactions of the Royal Society of London. B, Biological Sciences},
	shortjournal = {Phil. Trans. R. Soc. Lond. B},
	author = {Willshaw, David J. and von der Malsburg, Christoph},
	urldate = {2023-06-30},
	date = {1979-11},
	langid = {english},
}

@article{rosenblatt_perceptron_1958,
	title = {The perceptron: A probabilistic model for information storage and organization in the brain.},
	volume = {65},
	issn = {1939-1471, 0033-295X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0042519},
	doi = {10.1037/h0042519},
	shorttitle = {The perceptron},
	pages = {386--408},
	number = {6},
	journaltitle = {Psychological Review},
	shortjournal = {Psychological Review},
	author = {Rosenblatt, F.},
	urldate = {2023-07-14},
	date = {1958},
	langid = {english},
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {0007-4985, 1522-9602},
	url = {http://link.springer.com/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	pages = {115--133},
	number = {4},
	journaltitle = {The Bulletin of Mathematical Biophysics},
	shortjournal = {Bulletin of Mathematical Biophysics},
	author = {{McCulloch}, Warren S. and Pitts, Walter},
	urldate = {2023-07-14},
	date = {1943-12},
	langid = {english},
}

@article{greig_molecular_2013,
	title = {Molecular logic of neocortical projection neuron specification, development and diversity},
	volume = {14},
	issn = {1471-003X, 1471-0048},
	url = {https://www.nature.com/articles/nrn3586},
	doi = {10.1038/nrn3586},
	pages = {755--769},
	number = {11},
	journaltitle = {Nature Reviews Neuroscience},
	shortjournal = {Nat Rev Neurosci},
	author = {Greig, Luciano Custo and Woodworth, Mollie B. and Galazo, Maria J. and Padmanabhan, Hari and Macklis, Jeffrey D.},
	urldate = {2023-07-13},
	date = {2013-11},
	langid = {english},
}

@article{narr_relationships_2007,
	title = {Relationships between {IQ} and Regional Cortical Gray Matter Thickness in Healthy Adults},
	volume = {17},
	issn = {1047-3211, 1460-2199},
	url = {https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/bhl125},
	doi = {10.1093/cercor/bhl125},
	pages = {2163--2171},
	number = {9},
	journaltitle = {Cerebral Cortex},
	shortjournal = {Cerebral Cortex},
	author = {Narr, Katherine L. and Woods, Roger P. and Thompson, Paul M. and Szeszko, Philip and Robinson, Delbert and Dimtcheva, Teodora and Gurbani, Mala and Toga, Arthur W. and Bilder, Robert M.},
	urldate = {2023-07-13},
	date = {2007-09-01},
	langid = {english},
}

@article{miconi_hebbian_2021,
	title = {Hebbian learning with gradients: Hebbian convolutional neural networks with modern deep learning frameworks},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2107.01729},
	doi = {10.48550/arXiv.2107.01729},
	shorttitle = {Hebbian learning with gradients},
	abstract = {Deep learning networks generally use non-biological learning methods. By contrast, networks based on more biologically plausible learning, such as Hebbian learning, show comparatively poor performance and difficulties of implementation. Here we show that Hebbian learning in hierarchical, convolutional neural networks can be implemented almost trivially with modern deep learning frameworks, by using specific losses whose gradients produce exactly the desired Hebbian updates. We provide expressions whose gradients exactly implement a plain Hebbian rule (dw {\textasciitilde}= xy), Grossberg's instar rule (dw {\textasciitilde}= y(x-w)), and Oja's rule (dw {\textasciitilde}= y(x-yw)). As an application, we build Hebbian convolutional multi-layer networks for object recognition. We observe that higher layers of such networks tend to learn large, simple features (Gabor-like filters and blobs), explaining the previously reported decrease in decoding performance over successive layers. To combat this tendency, we introduce interventions (denser activations with sparse plasticity, pruning of connections between layers) which result in sparser learned features, massively increase performance, and allow information to increase over successive layers. We hypothesize that more advanced techniques (dynamic stimuli, trace learning, feedback connections, etc.), together with the massive computational boost offered by modern deep learning frameworks, could greatly improve the performance and biological relevance of multi-layer Hebbian networks.},
	author = {Miconi, Thomas},
	urldate = {2023-07-11},
	date = {2021},
	note = {Publisher: {arXiv}
Version Number: 2},
	keywords = {{FOS}: Computer and information sciences, Neural and Evolutionary Computing (cs.{NE})},
}

@misc{miconi_hebbian_2021-1,
	title = {Hebbian learning with gradients: Hebbian convolutional neural networks with modern deep learning frameworks},
	url = {http://arxiv.org/abs/2107.01729},
	shorttitle = {Hebbian learning with gradients},
	abstract = {Deep learning networks generally use non-biological learning methods. By contrast, networks based on more biologically plausible learning, such as Hebbian learning, show comparatively poor performance and difficulties of implementation. Here we show that Hebbian learning in hierarchical, convolutional neural networks can be implemented almost trivially with modern deep learning frameworks, by using specific losses whose gradients produce exactly the desired Hebbian updates. We provide expressions whose gradients exactly implement a plain Hebbian rule (dw {\textasciitilde}= xy), Grossberg's instar rule (dw {\textasciitilde}= y(x-w)), and Oja's rule (dw {\textasciitilde}= y(x-yw)). As an application, we build Hebbian convolutional multi-layer networks for object recognition. We observe that higher layers of such networks tend to learn large, simple features (Gabor-like filters and blobs), explaining the previously reported decrease in decoding performance over successive layers. To combat this tendency, we introduce interventions (denser activations with sparse plasticity, pruning of connections between layers) which result in sparser learned features, massively increase performance, and allow information to increase over successive layers. We hypothesize that more advanced techniques (dynamic stimuli, trace learning, feedback connections, etc.), together with the massive computational boost offered by modern deep learning frameworks, could greatly improve the performance and biological relevance of multi-layer Hebbian networks.},
	number = {{arXiv}:2107.01729},
	publisher = {{arXiv}},
	author = {Miconi, Thomas},
	urldate = {2023-07-11},
	date = {2021-11-01},
	eprinttype = {arxiv},
	eprint = {2107.01729 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@article{granlund_search_1978,
	title = {In search of a general picture processing operator},
	volume = {8},
	issn = {0146664X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0146664X78900473},
	doi = {10.1016/0146-664X(78)90047-3},
	pages = {155--173},
	number = {2},
	journaltitle = {Computer Graphics and Image Processing},
	shortjournal = {Computer Graphics and Image Processing},
	author = {Granlund, Goesta H.},
	urldate = {2023-07-11},
	date = {1978-10},
	langid = {english},
}

@article{gabor_theory_1946,
	title = {Theory of communication. Part 1: The analysis of information},
	volume = {93},
	issn = {2054-0604},
	url = {https://digital-library.theiet.org/content/journals/10.1049/ji-3-2.1946.0074},
	doi = {10.1049/ji-3-2.1946.0074},
	shorttitle = {Theory of communication. Part 1},
	pages = {429--441},
	number = {26},
	journaltitle = {Journal of the Institution of Electrical Engineers - Part {III}: Radio and Communication Engineering},
	author = {Gabor, D.},
	urldate = {2023-07-11},
	date = {1946-11},
	langid = {english},
}

@misc{hinton_improving_2012,
	title = {Improving neural networks by preventing co-adaptation of feature detectors},
	url = {http://arxiv.org/abs/1207.0580},
	doi = {10.48550/arXiv.1207.0580},
	abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
	number = {{arXiv}:1207.0580},
	publisher = {{arXiv}},
	author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
	urldate = {2023-07-10},
	date = {2012-07-03},
	eprinttype = {arxiv},
	eprint = {1207.0580 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{gallie_experimental_1976,
	title = {Experimental immunisation of six-month old calves against infection with the cysticercus stage of Taenia saginata},
	volume = {8},
	issn = {0049-4747},
	abstract = {Three groups of calves aged 6 months were completely protected against oral challenge with Taenia saginata eggs following immunisation by three different methods. These were hyperimmunisation with six serial inoculations of a homogenate of T. saginata strobila in Freund's complete adjuvant, a single intramuscular inoculation with hatched T. saginata eggs or a single oral dose of unhatched T. saginata eggs. The calves immunised with tapeworm homogenate developed the strongest haemagglutinating and precipitating antibody response to the complex of antigens in an extract of tapeworm strobila, cysticercus tissue or cysticercus fluid. The orally infected calves developed a moderate antibody response to these antigens but the calves inoculated with hatched eggs showed only a very weak antibody response. The calves infected orally with eggs developed a peripheral eosinophilia but the other two methods of immunisation did not evoke this response. After challenge infection all groups showed an increase in peripheral eosinophil counts except the group immunised with tapeworm homogenate.},
	pages = {233--242},
	number = {4},
	journaltitle = {Tropical Animal Health and Production},
	shortjournal = {Trop Anim Health Prod},
	author = {Gallie, G. J. and Sewell, M. M.},
	date = {1976-11},
	pmid = {996937},
	keywords = {Animals, Antibody Formation, Cattle, Cattle Diseases, Cysticercosis, Immunization, Leukocyte Count, Male, Taenia},
}

@article{niu_review_2021,
	title = {A review on the attention mechanism of deep learning},
	volume = {452},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S092523122100477X},
	doi = {10.1016/j.neucom.2021.03.091},
	pages = {48--62},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Niu, Zhaoyang and Zhong, Guoqiang and Yu, Hui},
	urldate = {2023-07-08},
	date = {2021-09},
	langid = {english},
}

@article{gilbert_lateral_1990,
	title = {Lateral Interactions in Visual Cortex},
	volume = {55},
	issn = {0091-7451, 1943-4456},
	url = {http://symposium.cshlp.org/cgi/doi/10.1101/SQB.1990.055.01.063},
	doi = {10.1101/SQB.1990.055.01.063},
	pages = {663--677},
	number = {0},
	journaltitle = {Cold Spring Harbor Symposia on Quantitative Biology},
	shortjournal = {Cold Spring Harbor Symposia on Quantitative Biology},
	author = {Gilbert, C.D. and Hirsch, J.A. and Wiesel, T.N.},
	urldate = {2023-07-08},
	date = {1990-01-01},
	langid = {english},
}

@article{conway_organization_2018,
	title = {The Organization and Operation of Inferior Temporal Cortex},
	volume = {4},
	issn = {2374-4642, 2374-4650},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-vision-091517-034202},
	doi = {10.1146/annurev-vision-091517-034202},
	abstract = {Inferior temporal cortex ({IT}) is a key part of the ventral visual pathway implicated in object, face, and scene perception. But how does {IT} work? Here, I describe an organizational scheme that marries form and function and provides a framework for future research. The scheme consists of a series of stages arranged along the posterior-anterior axis of {IT}, defined by anatomical connections and functional responses. Each stage comprises a complement of subregions that have a systematic spatial relationship. The organization of each stage is governed by an eccentricity template, and corresponding eccentricity representations across stages are interconnected. Foveal representations take on a role in high-acuity object vision (including face recognition); intermediate representations compute other aspects of object vision such as behavioral valence (using color and surface cues); and peripheral representations encode information about scenes. This multistage, parallel-processing model invokes an innately determined organization refined by visual experience that is consistent with principles of cortical development. The model is also consistent with principles of evolution, which suggest that visual cortex expanded through replication of retinotopic areas. Finally, the model predicts that the most extensively studied network within {IT}—the face patches—is not unique but rather one manifestation of a canonical set of operations that reveal general principles of how {IT} works.},
	pages = {381--402},
	number = {1},
	journaltitle = {Annual Review of Vision Science},
	shortjournal = {Annu. Rev. Vis. Sci.},
	author = {Conway, Bevil R.},
	urldate = {2023-07-08},
	date = {2018-09-15},
	langid = {english},
}

@article{miyashita_inferior_1993,
	title = {Inferior Temporal Cortex: Where Visual Perception Meets Memory},
	volume = {16},
	issn = {0147-006X, 1545-4126},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.ne.16.030193.001333},
	doi = {10.1146/annurev.ne.16.030193.001333},
	shorttitle = {Inferior Temporal Cortex},
	pages = {245--263},
	number = {1},
	journaltitle = {Annual Review of Neuroscience},
	shortjournal = {Annu. Rev. Neurosci.},
	author = {Miyashita, Y},
	urldate = {2023-07-08},
	date = {1993-03},
	langid = {english},
}

@article{tong_primary_2003,
	title = {Primary visual cortex and visual awareness},
	volume = {4},
	issn = {1471-003X, 1471-0048},
	url = {https://www.nature.com/articles/nrn1055},
	doi = {10.1038/nrn1055},
	pages = {219--229},
	number = {3},
	journaltitle = {Nature Reviews Neuroscience},
	shortjournal = {Nat Rev Neurosci},
	author = {Tong, Frank},
	urldate = {2023-07-08},
	date = {2003-03},
	langid = {english},
}

@article{li_interpretable_2022,
	title = {Interpretable deep learning: interpretation, interpretability, trustworthiness, and beyond},
	volume = {64},
	issn = {0219-1377, 0219-3116},
	url = {https://link.springer.com/10.1007/s10115-022-01756-8},
	doi = {10.1007/s10115-022-01756-8},
	shorttitle = {Interpretable deep learning},
	pages = {3197--3234},
	number = {12},
	journaltitle = {Knowledge and Information Systems},
	shortjournal = {Knowl Inf Syst},
	author = {Li, Xuhong and Xiong, Haoyi and Li, Xingjian and Wu, Xuanyu and Zhang, Xiao and Liu, Ji and Bian, Jiang and Dou, Dejing},
	urldate = {2023-07-08},
	date = {2022-12},
	langid = {english},
}

@article{baltrusaitis_multimodal_2019,
	title = {Multimodal Machine Learning: A Survey and Taxonomy},
	volume = {41},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/8269806/},
	doi = {10.1109/TPAMI.2018.2798607},
	shorttitle = {Multimodal Machine Learning},
	pages = {423--443},
	number = {2},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Baltrusaitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
	urldate = {2023-07-07},
	date = {2019-02-01},
}

@misc{liu_learn_2018,
	title = {Learn to Combine Modalities in Multimodal Deep Learning},
	url = {http://arxiv.org/abs/1805.11730},
	doi = {10.48550/arXiv.1805.11730},
	abstract = {Combining complementary information from multiple modalities is intuitively appealing for improving the performance of learning-based approaches. However, it is challenging to fully leverage different modalities due to practical challenges such as varying levels of noise and conflicts between modalities. Existing methods do not adopt a joint approach to capturing synergies between the modalities while simultaneously filtering noise and resolving conflicts on a per sample basis. In this work we propose a novel deep neural network based technique that multiplicatively combines information from different source modalities. Thus the model training process automatically focuses on information from more reliable modalities while reducing emphasis on the less reliable modalities. Furthermore, we propose an extension that multiplicatively combines not only the single-source modalities, but a set of mixtured source modalities to better capture cross-modal signal correlations. We demonstrate the effectiveness of our proposed technique by presenting empirical results on three multimodal classification tasks from different domains. The results show consistent accuracy improvements on all three tasks.},
	number = {{arXiv}:1805.11730},
	publisher = {{arXiv}},
	author = {Liu, Kuan and Li, Yanen and Xu, Ning and Natarajan, Prem},
	urldate = {2023-07-07},
	date = {2018-05-29},
	eprinttype = {arxiv},
	eprint = {1805.11730 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{payeur_burst-dependent_2021,
	title = {Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits},
	volume = {24},
	issn = {1097-6256, 1546-1726},
	url = {https://www.nature.com/articles/s41593-021-00857-x},
	doi = {10.1038/s41593-021-00857-x},
	pages = {1010--1019},
	number = {7},
	journaltitle = {Nature Neuroscience},
	shortjournal = {Nat Neurosci},
	author = {Payeur, Alexandre and Guerguiev, Jordan and Zenke, Friedemann and Richards, Blake A. and Naud, Richard},
	urldate = {2023-07-07},
	date = {2021-07},
	langid = {english},
}

@article{vogels_inhibitory_2011,
	title = {Inhibitory Plasticity Balances Excitation and Inhibition in Sensory Pathways and Memory Networks},
	volume = {334},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.1211095},
	doi = {10.1126/science.1211095},
	abstract = {Plasticity at inhibitory synapses maintains balanced excitatory and inhibitory synaptic inputs at cortical neurons.
          , 
            Cortical neurons receive balanced excitatory and inhibitory synaptic currents. Such a balance could be established and maintained in an experience-dependent manner by synaptic plasticity at inhibitory synapses. We show that this mechanism provides an explanation for the sparse firing patterns observed in response to natural stimuli and fits well with a recently observed interaction of excitatory and inhibitory receptive field plasticity. The introduction of inhibitory plasticity in suitable recurrent networks provides a homeostatic mechanism that leads to asynchronous irregular network states. Further, it can accommodate synaptic memories with activity patterns that become indiscernible from the background state but can be reactivated by external stimuli. Our results suggest an essential role of inhibitory plasticity in the formation and maintenance of functional cortical circuitry.},
	pages = {1569--1573},
	number = {6062},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Vogels, T. P. and Sprekeler, H. and Zenke, F. and Clopath, C. and Gerstner, W.},
	urldate = {2023-07-07},
	date = {2011-12-16},
	langid = {english},
}

@article{vogels_inhibitory_2011-1,
	title = {Inhibitory Plasticity Balances Excitation and Inhibition in Sensory Pathways and Memory Networks},
	volume = {334},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.1211095},
	doi = {10.1126/science.1211095},
	abstract = {Plasticity at inhibitory synapses maintains balanced excitatory and inhibitory synaptic inputs at cortical neurons.
          , 
            Cortical neurons receive balanced excitatory and inhibitory synaptic currents. Such a balance could be established and maintained in an experience-dependent manner by synaptic plasticity at inhibitory synapses. We show that this mechanism provides an explanation for the sparse firing patterns observed in response to natural stimuli and fits well with a recently observed interaction of excitatory and inhibitory receptive field plasticity. The introduction of inhibitory plasticity in suitable recurrent networks provides a homeostatic mechanism that leads to asynchronous irregular network states. Further, it can accommodate synaptic memories with activity patterns that become indiscernible from the background state but can be reactivated by external stimuli. Our results suggest an essential role of inhibitory plasticity in the formation and maintenance of functional cortical circuitry.},
	pages = {1569--1573},
	number = {6062},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Vogels, T. P. and Sprekeler, H. and Zenke, F. and Clopath, C. and Gerstner, W.},
	urldate = {2023-07-07},
	date = {2011-12-16},
	langid = {english},
}

@article{olshausen_emergence_1996,
	title = {Emergence of simple-cell receptive field properties by learning a sparse code for natural images},
	volume = {381},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/381607a0},
	doi = {10.1038/381607a0},
	pages = {607--609},
	number = {6583},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Olshausen, Bruno A. and Field, David J.},
	urldate = {2023-07-07},
	date = {1996-06},
	langid = {english},
}

@article{wagemans_century_2012,
	title = {A century of Gestalt psychology in visual perception: {II}. Conceptual and theoretical foundations.},
	volume = {138},
	issn = {1939-1455, 0033-2909},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0029334},
	doi = {10.1037/a0029334},
	shorttitle = {A century of Gestalt psychology in visual perception},
	pages = {1218--1252},
	number = {6},
	journaltitle = {Psychological Bulletin},
	shortjournal = {Psychological Bulletin},
	author = {Wagemans, Johan and Feldman, Jacob and Gepshtein, Sergei and Kimchi, Ruth and Pomerantz, James R. and Van Der Helm, Peter A. and Van Leeuwen, Cees},
	urldate = {2023-07-05},
	date = {2012},
	langid = {english},
}

@book{hamlyn_psychology_2017,
	edition = {0},
	title = {The Psychology of Perception: A Philosophical Examination of Gestalt Theory and Derivative Theories of Perception},
	isbn = {978-1-315-47329-1},
	url = {https://www.taylorfrancis.com/books/9781315473284},
	shorttitle = {The Psychology of Perception},
	publisher = {Routledge},
	author = {Hamlyn, D. W.},
	urldate = {2023-07-05},
	date = {2017-03-27},
	langid = {english},
	doi = {10.4324/9781315473291},
}

@misc{lau_proximal_2018,
	title = {A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training},
	url = {http://arxiv.org/abs/1803.09082},
	doi = {10.48550/arXiv.1803.09082},
	abstract = {Training deep neural networks ({DNNs}) efficiently is a challenge due to the associated highly nonconvex optimization. The backpropagation (backprop) algorithm has long been the most widely used algorithm for gradient computation of parameters of {DNNs} and is used along with gradient descent-type algorithms for this optimization task. Recent work have shown the efficiency of block coordinate descent ({BCD}) type methods empirically for training {DNNs}. In view of this, we propose a novel algorithm based on the {BCD} method for training {DNNs} and provide its global convergence results built upon the powerful framework of the Kurdyka-Lojasiewicz ({KL}) property. Numerical experiments on standard datasets demonstrate its competitive efficiency against standard optimizers with backprop.},
	number = {{arXiv}:1803.09082},
	publisher = {{arXiv}},
	author = {Lau, Tim Tsz-Kit and Zeng, Jinshan and Wu, Baoyuan and Yao, Yuan},
	urldate = {2023-07-04},
	date = {2018-03-24},
	eprinttype = {arxiv},
	eprint = {1803.09082 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{lillicrap_random_2016,
	title = {Random synaptic feedback weights support error backpropagation for deep learning},
	volume = {7},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/ncomms13276},
	doi = {10.1038/ncomms13276},
	abstract = {Abstract
            The brain processes information through multiple layers of neurons. This deep architecture is representationally powerful, but complicates learning because it is difficult to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame by multiplying error signals with all the synaptic weights on each neuron’s axon and further downstream. However, this involves a precise, symmetric backward connectivity pattern, which is thought to be impossible in the brain. Here we demonstrate that this strong architectural constraint is not required for effective error propagation. We present a surprisingly simple mechanism that assigns blame by multiplying errors by even random synaptic weights. This mechanism can transmit teaching signals across multiple layers of neurons and performs as effectively as backpropagation on a variety of tasks. Our results help reopen questions about how the brain could use error signals and dispel long-held assumptions about algorithmic constraints on learning.},
	pages = {13276},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
	urldate = {2023-07-04},
	date = {2016-11-08},
	langid = {english},
}

@misc{lansdell_learning_2020,
	title = {Learning to solve the credit assignment problem},
	url = {http://arxiv.org/abs/1906.00889},
	doi = {10.48550/arXiv.1906.00889},
	abstract = {Backpropagation is driving today's artificial neural networks ({ANNs}). However, despite extensive research, it remains unclear if the brain implements this algorithm. Among neuroscientists, reinforcement learning ({RL}) algorithms are often seen as a realistic alternative: neurons can randomly introduce change, and use unspecific feedback signals to observe their effect on the cost and thus approximate their gradient. However, the convergence rate of such learning scales poorly with the number of involved neurons. Here we propose a hybrid learning approach. Each neuron uses an {RL}-type strategy to learn how to approximate the gradients that backpropagation would provide. We provide proof that our approach converges to the true gradient for certain classes of networks. In both feedforward and convolutional networks, we empirically show that our approach learns to approximate the gradient, and can match or the performance of exact gradient-based learning. Learning feedback weights provides a biologically plausible mechanism of achieving good performance, without the need for precise, pre-specified learning rules.},
	number = {{arXiv}:1906.00889},
	publisher = {{arXiv}},
	author = {Lansdell, Benjamin James and Prakash, Prashanth Ravi and Kording, Konrad Paul},
	urldate = {2023-07-04},
	date = {2020-04-22},
	eprinttype = {arxiv},
	eprint = {1906.00889 [cs, q-bio]},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
}

@misc{bengio_how_2014,
	title = {How Auto-Encoders Could Provide Credit Assignment in Deep Networks via Target Propagation},
	url = {http://arxiv.org/abs/1407.7906},
	doi = {10.48550/arXiv.1407.7906},
	abstract = {We propose to exploit \{{\textbackslash}em reconstruction\} as a layer-local training signal for deep learning. Reconstructions can be propagated in a form of target propagation playing a role similar to back-propagation but helping to reduce the reliance on derivatives in order to perform credit assignment across many levels of possibly strong non-linearities (which is difficult for back-propagation). A regularized auto-encoder tends produce a reconstruction that is a more likely version of its input, i.e., a small move in the direction of higher likelihood. By generalizing gradients, target propagation may also allow to train deep networks with discrete hidden units. If the auto-encoder takes both a representation of input and target (or of any side information) in input, then its reconstruction of input representation provides a target towards a representation that is more likely, conditioned on all the side information. A deep auto-encoder decoding path generalizes gradient propagation in a learned way that can could thus handle not just infinitesimal changes but larger, discrete changes, hopefully allowing credit assignment through a long chain of non-linear operations. In addition to each layer being a good auto-encoder, the encoder also learns to please the upper layers by transforming the data into a space where it is easier to model by them, flattening manifolds and disentangling factors. The motivations and theoretical justifications for this approach are laid down in this paper, along with conjectures that will have to be verified either mathematically or experimentally, including a hypothesis stating that such auto-encoder mediated target propagation could play in brains the role of credit assignment through many non-linear, noisy and discrete transformations.},
	number = {{arXiv}:1407.7906},
	publisher = {{arXiv}},
	author = {Bengio, Yoshua},
	urldate = {2023-07-04},
	date = {2014-09-18},
	eprinttype = {arxiv},
	eprint = {1407.7906 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@online{hinton_forward-forward_2022,
	title = {The Forward-Forward Algorithm: Some Preliminary Investigations},
	url = {https://www.cs.toronto.edu/~hinton/FFA13.pdf},
	author = {Hinton, Geoffrey E.},
	urldate = {2022-12-17},
	date = {2022},
}

@article{marquez_deep_2018,
	title = {Deep Cascade Learning},
	volume = {29},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/8307262/},
	doi = {10.1109/TNNLS.2018.2805098},
	pages = {5475--5485},
	number = {11},
	journaltitle = {{IEEE} Transactions on Neural Networks and Learning Systems},
	shortjournal = {{IEEE} Trans. Neural Netw. Learning Syst.},
	author = {Marquez, Enrique S. and Hare, Jonathon S. and Niranjan, Mahesan},
	urldate = {2023-07-04},
	date = {2018-11},
}

@article{mostafa_deep_2018,
	title = {Deep Supervised Learning Using Local Errors},
	volume = {12},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/article/10.3389/fnins.2018.00608/full},
	doi = {10.3389/fnins.2018.00608},
	pages = {608},
	journaltitle = {Frontiers in Neuroscience},
	shortjournal = {Front. Neurosci.},
	author = {Mostafa, Hesham and Ramesh, Vishwajith and Cauwenberghs, Gert},
	urldate = {2023-07-04},
	date = {2018-08-31},
}

@misc{wang_revisiting_2021,
	title = {Revisiting Locally Supervised Learning: an Alternative to End-to-end Training},
	url = {http://arxiv.org/abs/2101.10832},
	doi = {10.48550/arXiv.2101.10832},
	shorttitle = {Revisiting Locally Supervised Learning},
	abstract = {Due to the need to store the intermediate activations for back-propagation, end-to-end (E2E) training of deep networks usually suffers from high {GPUs} memory footprint. This paper aims to address this problem by revisiting the locally supervised learning, where a network is split into gradient-isolated modules and trained with local supervision. We experimentally show that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, we propose an information propagation ({InfoPro}) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. As {InfoPro} loss is difficult to compute in its original form, we derive a feasible upper bound as a surrogate optimization objective, yielding a simple but effective algorithm. In fact, we show that the proposed method boils down to minimizing the combination of a reconstruction loss and a normal cross-entropy/contrastive term. Extensive empirical results on five datasets (i.e., {CIFAR}, {SVHN}, {STL}-10, {ImageNet} and Cityscapes) validate that {InfoPro} is capable of achieving competitive performance with less than 40\% memory footprint compared to E2E training, while allowing using training data with higher-resolution or larger batch sizes under the same {GPU} memory constraint. Our method also enables training local modules asynchronously for potential training acceleration. Code is available at: https://github.com/blackfeather-wang/{InfoPro}-Pytorch.},
	number = {{arXiv}:2101.10832},
	publisher = {{arXiv}},
	author = {Wang, Yulin and Ni, Zanlin and Song, Shiji and Yang, Le and Huang, Gao},
	urldate = {2023-07-04},
	date = {2021-01-26},
	eprinttype = {arxiv},
	eprint = {2101.10832 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{duan_kernel_2020,
	title = {On Kernel Method–Based Connectionist Models and Supervised Deep Learning Without Backpropagation},
	volume = {32},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/32/1/97-135/95570},
	doi = {10.1162/neco_a_01250},
	abstract = {We propose a novel family of connectionist models based on kernel machines and consider the problem of learning layer by layer a compositional hypothesis class (i.e., a feedforward, multilayer architecture) in a supervised setting. In terms of the models, we present a principled method to “kernelize” (partly or completely) any neural network ({NN}). With this method, we obtain a counterpart of any given {NN} that is powered by kernel machines instead of neurons. In terms of learning, when learning a feedforward deep architecture in a supervised setting, one needs to train all the components simultaneously using backpropagation ({BP}) since there are no explicit targets for the hidden layers (Rumelhart, Hinton, \& Williams, 1986 ). We consider without loss of generality the two-layer case and present a general framework that explicitly characterizes a target for the hidden layer that is optimal for minimizing the objective function of the network. This characterization then makes possible a purely greedy training scheme that learns one layer at a time, starting from the input layer. We provide instantiations of the abstract framework under certain architectures and objective functions. Based on these instantiations, we present a layer-wise training algorithm for an [Formula: see text]-layer feedforward network for classification, where [Formula: see text] can be arbitrary. This algorithm can be given an intuitive geometric interpretation that makes the learning dynamics transparent. Empirical results are provided to complement our theory. We show that the kernelized networks, trained layer-wise, compare favorably with classical kernel machines as well as other connectionist models trained by {BP}. We also visualize the inner workings of the greedy kernelized models to validate our claim on the transparency of the layer-wise algorithm.},
	pages = {97--135},
	number = {1},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Duan, Shiyu and Yu, Shujian and Chen, Yunmei and Principe, Jose C.},
	urldate = {2023-07-04},
	date = {2020-01},
	langid = {english},
}

@article{duan_modularizing_2022,
	title = {Modularizing Deep Learning via Pairwise Learning With Kernels},
	volume = {33},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/9314071/},
	doi = {10.1109/TNNLS.2020.3042346},
	pages = {1441--1451},
	number = {4},
	journaltitle = {{IEEE} Transactions on Neural Networks and Learning Systems},
	shortjournal = {{IEEE} Trans. Neural Netw. Learning Syst.},
	author = {Duan, Shiyu and Yu, Shujian and Principe, Jose C.},
	urldate = {2023-07-04},
	date = {2022-04},
}

@article{poljak_experimental_1927,
	title = {An experimental study of the association callosal, and projection fibers of the cerebral cortex of the cat},
	volume = {44},
	issn = {0021-9967, 1096-9861},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/cne.900440202},
	doi = {10.1002/cne.900440202},
	pages = {197--258},
	number = {2},
	journaltitle = {The Journal of Comparative Neurology},
	shortjournal = {J. Comp. Neurol.},
	author = {Poljak, Stjepan},
	urldate = {2023-07-02},
	date = {1927-12},
	langid = {english},
}

@article{kolmogorov_tables_1998,
	title = {On tables of random numbers},
	volume = {207},
	issn = {03043975},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304397598000759},
	doi = {10.1016/S0304-3975(98)00075-9},
	pages = {387--395},
	number = {2},
	journaltitle = {Theoretical Computer Science},
	shortjournal = {Theoretical Computer Science},
	author = {Kolmogorov, Andrei N.},
	urldate = {2023-06-30},
	date = {1998-11},
	langid = {english},
}

@article{friston_active_2016,
	title = {Active inference and learning},
	volume = {68},
	issn = {01497634},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0149763416301336},
	doi = {10.1016/j.neubiorev.2016.06.022},
	pages = {862--879},
	journaltitle = {Neuroscience \& Biobehavioral Reviews},
	shortjournal = {Neuroscience \& Biobehavioral Reviews},
	author = {Friston, Karl and {FitzGerald}, Thomas and Rigoli, Francesco and Schwartenbeck, Philipp and O'Doherty, John and Pezzulo, Giovanni},
	urldate = {2023-07-01},
	date = {2016-09},
	langid = {english},
}

@article{duan_kernel_2020-1,
	title = {On Kernel Method–Based Connectionist Models and Supervised Deep Learning Without Backpropagation},
	volume = {32},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/32/1/97-135/95570},
	doi = {10.1162/neco_a_01250},
	abstract = {We propose a novel family of connectionist models based on kernel machines and consider the problem of learning layer by layer a compositional hypothesis class (i.e., a feedforward, multilayer architecture) in a supervised setting. In terms of the models, we present a principled method to “kernelize” (partly or completely) any neural network ({NN}). With this method, we obtain a counterpart of any given {NN} that is powered by kernel machines instead of neurons. In terms of learning, when learning a feedforward deep architecture in a supervised setting, one needs to train all the components simultaneously using backpropagation ({BP}) since there are no explicit targets for the hidden layers (Rumelhart, Hinton, \& Williams, 1986 ). We consider without loss of generality the two-layer case and present a general framework that explicitly characterizes a target for the hidden layer that is optimal for minimizing the objective function of the network. This characterization then makes possible a purely greedy training scheme that learns one layer at a time, starting from the input layer. We provide instantiations of the abstract framework under certain architectures and objective functions. Based on these instantiations, we present a layer-wise training algorithm for an [Formula: see text]-layer feedforward network for classification, where [Formula: see text] can be arbitrary. This algorithm can be given an intuitive geometric interpretation that makes the learning dynamics transparent. Empirical results are provided to complement our theory. We show that the kernelized networks, trained layer-wise, compare favorably with classical kernel machines as well as other connectionist models trained by {BP}. We also visualize the inner workings of the greedy kernelized models to validate our claim on the transparency of the layer-wise algorithm.},
	pages = {97--135},
	number = {1},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Duan, Shiyu and Yu, Shujian and Chen, Yunmei and Principe, Jose C.},
	urldate = {2023-07-04},
	date = {2020-01},
	langid = {english},
}

@article{grossberg_neural_1989,
	title = {Neural dynamics of adaptive timing and temporal discrimination during associative learning},
	volume = {2},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0893608089900269},
	doi = {10.1016/0893-6080(89)90026-9},
	pages = {79--102},
	number = {2},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Grossberg, Stephen and Schmajuk, Nestor A.},
	urldate = {2023-07-03},
	date = {1989-01},
	langid = {english},
}

@incollection{rumelhart_learning_1986,
	location = {Cambridge, {MA}, {USA}},
	title = {Learning Internal Representations by Error Propagation},
	isbn = {0-262-68053-X},
	pages = {318--362},
	booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations},
	publisher = {{MIT} Press},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	date = {1986},
}

@incollection{anderson_biased_1998,
	title = {Biased Random-Walk Learning: A Neurobiological Correlate to Trial-and-Error},
	isbn = {978-0-12-526420-4},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780125264204500082},
	shorttitle = {Biased Random-Walk Learning},
	pages = {221--244},
	booktitle = {Neural Networks and Pattern Recognition},
	publisher = {Elsevier},
	author = {Anderson, Russell W.},
	urldate = {2023-07-03},
	date = {1998},
	langid = {english},
	doi = {10.1016/B978-012526420-4/50008-2},
}

@article{mel_nmda-based_1992,
	title = {{NMDA}-Based Pattern Discrimination in a Modeled Cortical Neuron},
	volume = {4},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/4/4/502-517/5650},
	doi = {10.1162/neco.1992.4.4.502},
	abstract = {Compartmental simulations of an anatomically characterized cortical pyramidal cell were carried out to study the integrative behavior of a complex dendritic tree. Previous theoretical (Feldman and Ballard 1982; Durbin and Rumelhart 1989; Mel 1990; Mel and Koch 1990; Poggio and Girosi 1990) and compartmental modeling (Koch et al. 1983; Shepherd et al. 1985; Koch and Poggio 1987; Rall and Segev 1987; Shepherd and Brayton 1987; Shepherd et al. 1989; Brown et al. 1991) work had suggested that multiplicative interactions among groups of neighboring synapses could greatly enhance the processing power of a neuron relative to a unit with only a single global firing threshold. This issue was investigated here, with a particular focus on the role of voltage-dependent N-methyl-D-asparate ({NMDA}) channels in the generation of cell responses. First, it was found that when a large proportion of the excitatory synaptic input to dendritic spines is carried by {NMDA} channels, the pyramidal cell responds preferentially to spatially clustered, rather than random, distributions of activated synapses. Second, based on this mechanism, the {NMDA}-rich neuron is shown to be capable of solving a nonlinear pattern discrimination task. We propose that manipulation of the spatial ordering of afferent synaptic connections onto the dendritic arbor is a possible biological strategy for pattern information storage during learning.},
	pages = {502--517},
	number = {4},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Mel, Bartlett W.},
	urldate = {2023-07-03},
	date = {1992-07},
	langid = {english},
}

@article{montague_spatial_1991,
	title = {Spatial Signaling in the Development and Function of Neural Connections},
	volume = {1},
	issn = {1047-3211, 1460-2199},
	url = {https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/1.3.199},
	doi = {10.1093/cercor/1.3.199},
	pages = {199--220},
	number = {3},
	journaltitle = {Cerebral Cortex},
	shortjournal = {Cereb Cortex},
	author = {Montague, P. Read and Gally, Joseph A. and Edelman, Gerald M.},
	urldate = {2023-07-03},
	date = {1991},
	langid = {english},
}

@article{grajski_hebb-type_1990,
	title = {Hebb-Type Dynamics is Sufficient to Account for the Inverse Magnification Rule in Cortical Somatotopy},
	volume = {2},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/2/1/71-84/5512},
	doi = {10.1162/neco.1990.2.1.71},
	abstract = {The inverse magnification rule in cortical somatotopy is the experimentally derived inverse relationship between cortical magnification (area of somatotopic map representing a unit area of skin surface) and receptive field size (area of restricted skin surface driving a cortical neuron). We show by computer simulation of a simple, multilayer model that Hebb-type synaptic modification subject to competitive constraints is sufficient to account for the inverse magnification rule.},
	pages = {71--84},
	number = {1},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Grajski, Kamil A. and Merzenich, Michael M.},
	urldate = {2023-07-03},
	date = {1990-03},
	langid = {english},
}

@incollection{widrow_natures_2019,
	title = {Nature's Learning Rule},
	isbn = {978-0-12-815480-9},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780128154809000013},
	pages = {1--30},
	booktitle = {Artificial Intelligence in the Age of Neural Networks and Brain Computing},
	publisher = {Elsevier},
	author = {Widrow, Bernard and Kim, Youngsik and Park, Dookun and Perin, Jose Krause},
	urldate = {2023-07-03},
	date = {2019},
	langid = {english},
	doi = {10.1016/B978-0-12-815480-9.00001-3},
}

@misc{zhang_why_2020,
	title = {Why gradient clipping accelerates training: A theoretical justification for adaptivity},
	url = {http://arxiv.org/abs/1905.11881},
	doi = {10.48550/arXiv.1905.11881},
	shorttitle = {Why gradient clipping accelerates training},
	abstract = {We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, {\textbackslash}emph\{gradient clipping\} and {\textbackslash}emph\{normalized gradient\}, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.},
	number = {{arXiv}:1905.11881},
	publisher = {{arXiv}},
	author = {Zhang, Jingzhao and He, Tianxing and Sra, Suvrit and Jadbabaie, Ali},
	urldate = {2023-07-03},
	date = {2020-02-10},
	eprinttype = {arxiv},
	eprint = {1905.11881 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@article{grossberg_competitive_1987,
	title = {Competitive Learning: From Interactive Activation to Adaptive Resonance},
	volume = {11},
	issn = {03640213},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1551-6708.1987.tb00862.x},
	doi = {10.1111/j.1551-6708.1987.tb00862.x},
	shorttitle = {Competitive Learning},
	pages = {23--63},
	number = {1},
	journaltitle = {Cognitive Science},
	author = {Grossberg, Stephen},
	urldate = {2023-07-03},
	date = {1987-01-03},
	langid = {english},
}

@article{crick_recent_1989,
	title = {The recent excitement about neural networks},
	volume = {337},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/337129a0},
	doi = {10.1038/337129a0},
	pages = {129--132},
	number = {6203},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Crick, Francis},
	urldate = {2023-07-03},
	date = {1989-01},
	langid = {english},
}

@misc{raghavan_self-organization_2020,
	title = {Self-organization of multi-layer spiking neural networks},
	url = {http://arxiv.org/abs/2006.06902},
	doi = {10.48550/arXiv.2006.06902},
	abstract = {Living neural networks in our brains autonomously self-organize into large, complex architectures during early development to result in an organized and functional organic computational device. A key mechanism that enables the formation of complex architecture in the developing brain is the emergence of traveling spatio-temporal waves of neuronal activity across the growing brain. Inspired by this strategy, we attempt to efficiently self-organize large neural networks with an arbitrary number of layers into a wide variety of architectures. To achieve this, we propose a modular tool-kit in the form of a dynamical system that can be seamlessly stacked to assemble multi-layer neural networks. The dynamical system encapsulates the dynamics of spiking units, their inter/intra layer interactions as well as the plasticity rules that control the flow of information between layers. The key features of our tool-kit are (1) autonomous spatio-temporal waves across multiple layers triggered by activity in the preceding layer and (2) Spike-timing dependent plasticity ({STDP}) learning rules that update the inter-layer connectivity based on wave activity in the connecting layers. Our framework leads to the self-organization of a wide variety of architectures, ranging from multi-layer perceptrons to autoencoders. We also demonstrate that emergent waves can self-organize spiking network architecture to perform unsupervised learning, and networks can be coupled with a linear classifier to perform classification on classic image datasets like {MNIST}. Broadly, our work shows that a dynamical systems framework for learning can be used to self-organize large computational devices.},
	number = {{arXiv}:2006.06902},
	publisher = {{arXiv}},
	author = {Raghavan, Guruprasad and Lin, Cong and Thomson, Matt},
	urldate = {2023-07-03},
	date = {2020-06-11},
	eprinttype = {arxiv},
	eprint = {2006.06902 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
}

@article{marsland_self-organising_2002,
	title = {A self-organising network that grows when required},
	volume = {15},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608002000783},
	doi = {10.1016/S0893-6080(02)00078-3},
	pages = {1041--1058},
	number = {8},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Marsland, Stephen and Shapiro, Jonathan and Nehmzow, Ulrich},
	urldate = {2023-07-03},
	date = {2002-10},
	langid = {english},
}

@article{fritzke_growing_1994,
	title = {Growing cell structures—A self-organizing network for unsupervised and supervised learning},
	volume = {7},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0893608094900914},
	doi = {10.1016/0893-6080(94)90091-4},
	pages = {1441--1460},
	number = {9},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Fritzke, Bernd},
	urldate = {2023-07-03},
	date = {1994-01},
	langid = {english},
}

@article{reilly_neural_1982,
	title = {A neural model for category learning},
	volume = {45},
	issn = {0340-1200, 1432-0770},
	url = {http://link.springer.com/10.1007/BF00387211},
	doi = {10.1007/BF00387211},
	pages = {35--41},
	number = {1},
	journaltitle = {Biological Cybernetics},
	shortjournal = {Biol. Cybern.},
	author = {Reilly, Douglas L. and Cooper, Leon N. and Elbaum, Charles},
	urldate = {2023-07-03},
	date = {1982-08},
	langid = {english},
}

@article{kohonen_self-organized_1982,
	title = {Self-organized formation of topologically correct feature maps},
	volume = {43},
	issn = {0340-1200, 1432-0770},
	url = {http://link.springer.com/10.1007/BF00337288},
	doi = {10.1007/BF00337288},
	pages = {59--69},
	number = {1},
	journaltitle = {Biological Cybernetics},
	shortjournal = {Biol. Cybern.},
	author = {Kohonen, Teuvo},
	urldate = {2023-07-03},
	date = {1982},
	langid = {english},
}

@online{risi_future_2021,
	title = {The Future of Artificial Intelligence is Self-Organizing and Self-Assembling},
	url = {https://sebastianrisi.com/self\%5Fassembling\%5Fai},
	titleaddon = {sebastianrisi.com},
	author = {Risi, Sebastian},
	date = {2021},
}

@misc{variengien_towards_2021,
	title = {Towards self-organized control: Using neural cellular automata to robustly control a cart-pole agent},
	url = {http://arxiv.org/abs/2106.15240},
	doi = {10.48550/arXiv.2106.15240},
	shorttitle = {Towards self-organized control},
	abstract = {Neural cellular automata (Neural {CA}) are a recent framework used to model biological phenomena emerging from multicellular organisms. In these systems, artificial neural networks are used as update rules for cellular automata. Neural {CA} are end-to-end differentiable systems where the parameters of the neural network can be learned to achieve a particular task. In this work, we used neural {CA} to control a cart-pole agent. The observations of the environment are transmitted in input cells, while the values of output cells are used as a readout of the system. We trained the model using deep-Q learning, where the states of the output cells were used as the Q-value estimates to be optimized. We found that the computing abilities of the cellular automata were maintained over several hundreds of thousands of iterations, producing an emergent stable behavior in the environment it controls for thousands of steps. Moreover, the system demonstrated life-like phenomena such as a developmental phase, regeneration after damage, stability despite a noisy environment, and robustness to unseen disruption such as input deletion.},
	number = {{arXiv}:2106.15240},
	publisher = {{arXiv}},
	author = {Variengien, Alexandre and Nichele, Stefano and Glover, Tom and Pontes-Filho, Sidney},
	urldate = {2023-07-03},
	date = {2021-07-12},
	eprinttype = {arxiv},
	eprint = {2106.15240 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@article{randazzo_self-classifying_2020,
	title = {Self-classifying {MNIST} Digits},
	volume = {5},
	issn = {2476-0757},
	url = {https://distill.pub/2020/selforg/mnist},
	doi = {10.23915/distill.00027.002},
	pages = {10.23915/distill.00027.002},
	number = {8},
	journaltitle = {Distill},
	shortjournal = {Distill},
	author = {Randazzo, Ettore and Mordvintsev, Alexander and Niklasson, Eyvind and Levin, Michael and Greydanus, Sam},
	urldate = {2023-07-03},
	date = {2020-08-27},
}

@misc{sudhakaran_growing_2021,
	title = {Growing 3D Artefacts and Functional Machines with Neural Cellular Automata},
	url = {http://arxiv.org/abs/2103.08737},
	doi = {10.48550/arXiv.2103.08737},
	abstract = {Neural Cellular Automata ({NCAs}) have been proven effective in simulating morphogenetic processes, the continuous construction of complex structures from very few starting cells. Recent developments in {NCAs} lie in the 2D domain, namely reconstructing target images from a single pixel or infinitely growing 2D textures. In this work, we propose an extension of {NCAs} to 3D, utilizing 3D convolutions in the proposed neural network architecture. Minecraft is selected as the environment for our automaton since it allows the generation of both static structures and moving machines. We show that despite their simplicity, {NCAs} are capable of growing complex entities such as castles, apartment blocks, and trees, some of which are composed of over 3,000 blocks. Additionally, when trained for regeneration, the system is able to regrow parts of simple functional machines, significantly expanding the capabilities of simulated morphogenetic systems. The code for the experiment in this paper can be found at: https://github.com/real-itu/3d-artefacts-nca.},
	number = {{arXiv}:2103.08737},
	publisher = {{arXiv}},
	author = {Sudhakaran, Shyam and Grbic, Djordje and Li, Siyan and Katona, Adam and Najarro, Elias and Glanois, Claire and Risi, Sebastian},
	urldate = {2023-07-03},
	date = {2021-06-04},
	eprinttype = {arxiv},
	eprint = {2103.08737 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-Encoding Variational Bayes},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	number = {{arXiv}:1312.6114},
	publisher = {{arXiv}},
	author = {Kingma, Diederik P. and Welling, Max},
	urldate = {2023-07-03},
	date = {2022-12-10},
	eprinttype = {arxiv},
	eprint = {1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{palm_variational_2022,
	title = {Variational Neural Cellular Automata},
	url = {http://arxiv.org/abs/2201.12360},
	doi = {10.48550/arXiv.2201.12360},
	abstract = {In nature, the process of cellular growth and differentiation has lead to an amazing diversity of organisms -- algae, starfish, giant sequoia, tardigrades, and orcas are all created by the same generative process. Inspired by the incredible diversity of this biological generative process, we propose a generative model, the Variational Neural Cellular Automata ({VNCA}), which is loosely inspired by the biological processes of cellular growth and differentiation. Unlike previous related works, the {VNCA} is a proper probabilistic generative model, and we evaluate it according to best practices. We find that the {VNCA} learns to reconstruct samples well and that despite its relatively few parameters and simple local-only communication, the {VNCA} can learn to generate a large variety of output from information encoded in a common vector format. While there is a significant gap to the current state-of-the-art in terms of generative modeling performance, we show that the {VNCA} can learn a purely self-organizing generative process of data. Additionally, we show that the {VNCA} can learn a distribution of stable attractors that can recover from significant damage.},
	number = {{arXiv}:2201.12360},
	publisher = {{arXiv}},
	author = {Palm, Rasmus Berg and González-Duque, Miguel and Sudhakaran, Shyam and Risi, Sebastian},
	urldate = {2023-07-03},
	date = {2022-02-02},
	eprinttype = {arxiv},
	eprint = {2201.12360 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@article{mordvintsev_growing_2020,
	title = {Growing Neural Cellular Automata},
	volume = {5},
	issn = {2476-0757},
	url = {https://distill.pub/2020/growing-ca},
	doi = {10.23915/distill.00023},
	pages = {10.23915/distill.00023},
	number = {2},
	journaltitle = {Distill},
	shortjournal = {Distill},
	author = {Mordvintsev, Alexander and Randazzo, Ettore and Niklasson, Eyvind and Levin, Michael},
	urldate = {2023-07-03},
	date = {2020-02-11},
}

@article{mordvintsev_growing_2020-1,
	title = {Growing Neural Cellular Automata},
	volume = {5},
	issn = {2476-0757},
	url = {https://distill.pub/2020/growing-ca},
	doi = {10.23915/distill.00023},
	pages = {10.23915/distill.00023},
	number = {2},
	journaltitle = {Distill},
	shortjournal = {Distill},
	author = {Mordvintsev, Alexander and Randazzo, Ettore and Niklasson, Eyvind and Levin, Michael},
	urldate = {2023-07-03},
	date = {2020-02-11},
}

@article{gilpin_cellular_2019,
	title = {Cellular automata as convolutional neural networks},
	volume = {100},
	issn = {2470-0045, 2470-0053},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.100.032402},
	doi = {10.1103/PhysRevE.100.032402},
	pages = {032402},
	number = {3},
	journaltitle = {Physical Review E},
	shortjournal = {Phys. Rev. E},
	author = {Gilpin, William},
	urldate = {2023-07-03},
	date = {2019-09-04},
	langid = {english},
}

@article{vichniac_simulating_1984,
	title = {Simulating physics with cellular automata},
	volume = {10},
	issn = {01672789},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0167278984902537},
	doi = {10.1016/0167-2789(84)90253-7},
	pages = {96--116},
	number = {1},
	journaltitle = {Physica D: Nonlinear Phenomena},
	shortjournal = {Physica D: Nonlinear Phenomena},
	author = {Vichniac, Gérard Y.},
	urldate = {2023-07-03},
	date = {1984-01},
	langid = {english},
}

@article{wolfram_cellular_1984,
	title = {Cellular automata as models of complexity},
	volume = {311},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/311419a0},
	doi = {10.1038/311419a0},
	pages = {419--424},
	number = {5985},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Wolfram, Stephen},
	urldate = {2023-07-03},
	date = {1984-10},
	langid = {english},
}

@book{morris_cognitive_2006,
	title = {Cognitive Systems - Information Processing Meets Brain Science},
	isbn = {978-0-12-088566-4},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780120885664X50004},
	publisher = {Elsevier},
	author = {Morris, Richard and Tarassenko, Lionel and Kenward, Michael},
	urldate = {2023-07-03},
	date = {2006},
	langid = {english},
	doi = {10.1016/B978-0-12-088566-4.X5000-4},
}

@book{wiskott_face_1996,
	title = {Face Recognition by Dynamic Link Matching},
	publisher = {Ruhr-Univ., Inst. für Neuroinformatik},
	author = {Wiskott, Laurenz and von der Malsburg, Christoph},
	date = {1996},
}

@article{bienenstock_neural_1987,
	title = {A Neural Network for Invariant Pattern Recognition},
	volume = {4},
	issn = {0295-5075, 1286-4854},
	url = {https://iopscience.iop.org/article/10.1209/0295-5075/4/1/020},
	doi = {10.1209/0295-5075/4/1/020},
	pages = {121--126},
	number = {1},
	journaltitle = {Europhysics Letters ({EPL})},
	shortjournal = {Europhys. Lett.},
	author = {Bienenstock, Ellie and von der Malsburg, Christoph},
	urldate = {2023-07-03},
	date = {1987-07-01},
}

@article{lades_distortion_1993,
	title = {Distortion invariant object recognition in the dynamic link architecture},
	volume = {42},
	issn = {00189340},
	url = {http://ieeexplore.ieee.org/document/210173/},
	doi = {10.1109/12.210173},
	pages = {300--311},
	number = {3},
	journaltitle = {{IEEE} Transactions on Computers},
	shortjournal = {{IEEE} Trans. Comput.},
	author = {Lades, Martin and Vorbruggen, Jan C. and Buhmann, Joachim and Lange, Jörg and von der Malsburg, Christoph and Wurtz, Rolf P. and Konen, Wolfgang},
	urldate = {2023-07-03},
	date = {1993-03},
}

@article{romanski_information_1993,
	title = {Information Cascade from Primary Auditory Cortex to the Amygdala: Corticocortical and Corticoamygdaloid Projections of Temporal Cortex in the Rat},
	volume = {3},
	issn = {1047-3211, 1460-2199},
	url = {https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/3.6.515},
	doi = {10.1093/cercor/3.6.515},
	shorttitle = {Information Cascade from Primary Auditory Cortex to the Amygdala},
	pages = {515--532},
	number = {6},
	journaltitle = {Cerebral Cortex},
	shortjournal = {Cereb Cortex},
	author = {Romanski, Lizabeth M. and {LeDoux}, Joseph E.},
	urldate = {2023-07-02},
	date = {1993},
	langid = {english},
}

@article{tanigawa_organization_2005,
	title = {Organization of Horizontal Axons in the Inferior Temporal Cortex and Primary Visual Cortex of the Macaque Monkey},
	volume = {15},
	issn = {1460-2199, 1047-3211},
	url = {http://academic.oup.com/cercor/article/15/12/1887/339721/Organization-of-Horizontal-Axons-in-the-Inferior},
	doi = {10.1093/cercor/bhi067},
	pages = {1887--1899},
	number = {12},
	journaltitle = {Cerebral Cortex},
	author = {Tanigawa, Hisashi and Wang, {QuanXin} and Fujita, Ichiro},
	urldate = {2023-07-02},
	date = {2005-12-01},
	langid = {english},
}

@article{yamada_somatotopic_2007,
	title = {Somatotopic Organization of Thalamocortical Projection Fibers as Assessed with {MR} Tractography},
	volume = {242},
	issn = {0033-8419, 1527-1315},
	url = {http://pubs.rsna.org/doi/10.1148/radiol.2423060297},
	doi = {10.1148/radiol.2423060297},
	pages = {840--845},
	number = {3},
	journaltitle = {Radiology},
	shortjournal = {Radiology},
	author = {Yamada, Kei and Nagakane, Yoshinari and Yoshikawa, Kenji and Kizu, Osamu and Ito, Hirotoshi and Kubota, Takao and Akazawa, Kentaro and Oouchi, Hiroyuki and Matsushima, Shigenori and Nakagawa, Masanori and Nishimura, Tsunehiko},
	urldate = {2023-07-02},
	date = {2007-03},
	langid = {english},
}

@article{yasui_subparafascicular_1990,
	title = {The subparafascicular thalamic nucleus of the rat receives projection fibers from the inferior colliculus and auditory cortex},
	volume = {537},
	issn = {00068993},
	url = {https://linkinghub.elsevier.com/retrieve/pii/000689939090378O},
	doi = {10.1016/0006-8993(90)90378-O},
	pages = {323--327},
	number = {1},
	journaltitle = {Brain Research},
	shortjournal = {Brain Research},
	author = {Yasui, Yukihiko and Kayahara, Tetsuro and Nakano, Katsuma and Mizuno, Noboru},
	urldate = {2023-07-02},
	date = {1990-12},
	langid = {english},
}

@article{mumuni_cnn_2021,
	title = {{CNN} Architectures for Geometric Transformation-Invariant Feature Representation in Computer Vision: A Review},
	volume = {2},
	issn = {2662-995X, 2661-8907},
	url = {https://link.springer.com/10.1007/s42979-021-00735-0},
	doi = {10.1007/s42979-021-00735-0},
	shorttitle = {{CNN} Architectures for Geometric Transformation-Invariant Feature Representation in Computer Vision},
	pages = {340},
	number = {5},
	journaltitle = {{SN} Computer Science},
	shortjournal = {{SN} {COMPUT}. {SCI}.},
	author = {Mumuni, Alhassan and Mumuni, Fuseini},
	urldate = {2023-07-02},
	date = {2021-09},
	langid = {english},
}

@misc{ahmad_properties_2015,
	title = {Properties of Sparse Distributed Representations and their Application to Hierarchical Temporal Memory},
	url = {http://arxiv.org/abs/1503.07469},
	doi = {10.48550/arXiv.1503.07469},
	abstract = {Empirical evidence demonstrates that every region of the neocortex represents information using sparse activity patterns. This paper examines Sparse Distributed Representations ({SDRs}), the primary information representation strategy in Hierarchical Temporal Memory ({HTM}) systems and the neocortex. We derive a number of properties that are core to scaling, robustness, and generalization. We use the theory to provide practical guidelines and illustrate the power of {SDRs} as the basis of {HTM}. Our goal is to help create a unified mathematical and practical framework for {SDRs} as it relates to cortical function.},
	number = {{arXiv}:1503.07469},
	publisher = {{arXiv}},
	author = {Ahmad, Subutai and Hawkins, Jeff},
	urldate = {2023-07-02},
	date = {2015-03-25},
	eprinttype = {arxiv},
	eprint = {1503.07469 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Quantitative Biology - Neurons and Cognition},
}

@misc{du_implicit_2020,
	title = {Implicit Generation and Generalization in Energy-Based Models},
	url = {http://arxiv.org/abs/1903.08689},
	doi = {10.48550/arXiv.1903.08689},
	abstract = {Energy based models ({EBMs}) are appealing due to their generality and simplicity in likelihood modeling, but have been traditionally difficult to train. We present techniques to scale {MCMC} based {EBM} training on continuous neural networks, and we show its success on the high-dimensional data domains of {ImageNet}32x32, {ImageNet}128x128, {CIFAR}-10, and robotic hand trajectories, achieving better samples than other likelihood models and nearing the performance of contemporary {GAN} approaches, while covering all modes of the data. We highlight some unique capabilities of implicit generation such as compositionality and corrupt image reconstruction and inpainting. Finally, we show that {EBMs} are useful models across a wide variety of tasks, achieving state-of-the-art out-of-distribution classification, adversarially robust classification, state-of-the-art continual online class learning, and coherent long term predicted trajectory rollouts.},
	number = {{arXiv}:1903.08689},
	publisher = {{arXiv}},
	author = {Du, Yilun and Mordatch, Igor},
	urldate = {2023-07-01},
	date = {2020-06-29},
	eprinttype = {arxiv},
	eprint = {1903.08689 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{bengio_gflownet_2022,
	title = {{GFlowNet} Foundations},
	url = {http://arxiv.org/abs/2111.09266},
	doi = {10.48550/arXiv.2111.09266},
	abstract = {Generative Flow Networks ({GFlowNets}) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of {GFlowNets}. They can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. {GFlowNets} amortize the work typically done by computationally expensive {MCMC} methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entropy and mutual information, sampling from a Pareto frontier, connections to reward-maximizing policies, and extensions to stochastic environments, continuous actions and modular energy functions.},
	number = {{arXiv}:2111.09266},
	publisher = {{arXiv}},
	author = {Bengio, Yoshua and Lahlou, Salem and Deleu, Tristan and Hu, Edward J. and Tiwari, Mo and Bengio, Emmanuel},
	urldate = {2023-07-01},
	date = {2022-08-15},
	eprinttype = {arxiv},
	eprint = {2111.09266 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{lecun_path_2022,
	title = {A Path Towards Autonomous Machine Intelligence},
	volume = {62},
	journaltitle = {Open Review},
	author = {{LeCun}, Yann},
	date = {2022-06},
}

@article{piaget_part_1964,
	title = {Part I: Cognitive development in children: Piaget development and learning},
	volume = {2},
	issn = {0022-4308, 1098-2736},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/tea.3660020306},
	doi = {10.1002/tea.3660020306},
	shorttitle = {Part I},
	pages = {176--186},
	number = {3},
	journaltitle = {Journal of Research in Science Teaching},
	shortjournal = {J. Res. Sci. Teach.},
	author = {Piaget, Jean},
	urldate = {2023-07-01},
	date = {1964-09},
	langid = {english},
}

@misc{keurti_homomorphism_2023,
	title = {Homomorphism Autoencoder -- Learning Group Structured Representations from Observed Transitions},
	url = {http://arxiv.org/abs/2207.12067},
	doi = {10.48550/arXiv.2207.12067},
	abstract = {How can agents learn internal models that veridically represent interactions with the real world is a largely open question. As machine learning is moving towards representations containing not just observational but also interventional knowledge, we study this problem using tools from representation learning and group theory. We propose methods enabling an agent acting upon the world to learn internal representations of sensory information that are consistent with actions that modify it. We use an autoencoder equipped with a group representation acting on its latent space, trained using an equivariance-derived loss in order to enforce a suitable homomorphism property on the group representation. In contrast to existing work, our approach does not require prior knowledge of the group and does not restrict the set of actions the agent can perform. We motivate our method theoretically, and show empirically that it can learn a group representation of the actions, thereby capturing the structure of the set of transformations applied to the environment. We further show that this allows agents to predict the effect of sequences of future actions with improved accuracy.},
	number = {{arXiv}:2207.12067},
	publisher = {{arXiv}},
	author = {Keurti, Hamza and Pan, Hsiao-Ru and Besserve, Michel and Grewe, Benjamin F. and Schölkopf, Bernhard},
	urldate = {2023-07-01},
	date = {2023-06-06},
	eprinttype = {arxiv},
	eprint = {2207.12067 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Group Theory, Statistics - Machine Learning},
}

@article{keller_sensorimotor_2012,
	title = {Sensorimotor Mismatch Signals in Primary Visual Cortex of the Behaving Mouse},
	volume = {74},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627312003844},
	doi = {10.1016/j.neuron.2012.03.040},
	pages = {809--815},
	number = {5},
	journaltitle = {Neuron},
	shortjournal = {Neuron},
	author = {Keller, Georg B. and Bonhoeffer, Tobias and Hübener, Mark},
	urldate = {2023-07-01},
	date = {2012-06},
	langid = {english},
}

@article{knoblich_social_2006,
	title = {The Social Nature of Perception and Action},
	volume = {15},
	issn = {0963-7214, 1467-8721},
	url = {http://journals.sagepub.com/doi/10.1111/j.0963-7214.2006.00415.x},
	doi = {10.1111/j.0963-7214.2006.00415.x},
	abstract = {Humans engage in a wide range of social activities. Previous research has focused on the role of higher cognitive functions, such as mentalizing (the ability to infer others' mental states) and language processing, in social exchange. This article reviews recent studies on action perception and joint action suggesting that basic perception–action links are crucial for many social interactions. Mapping perceived actions to one's own action repertoire enables direct understanding of others' actions and supports action identification. Joint action relies on shared action representations and involves modeling of others' performance in relation to one's own. Taking the social nature of perception and action seriously not only contributes to the understanding of dedicated social processes but has the potential to create a new perspective on the individual mind and brain.},
	pages = {99--104},
	number = {3},
	journaltitle = {Current Directions in Psychological Science},
	shortjournal = {Curr Dir Psychol Sci},
	author = {Knoblich, Günther and Sebanz, Natalie},
	urldate = {2023-07-01},
	date = {2006-06},
	langid = {english},
}

@article{zhou_does_2019,
	title = {Does computer vision matter for action?},
	volume = {4},
	issn = {2470-9476},
	url = {https://www.science.org/doi/10.1126/scirobotics.aaw6661},
	doi = {10.1126/scirobotics.aaw6661},
	abstract = {Controlled experiments indicate that explicit intermediate representations help action.
          , 
            Controlled experiments indicate that explicit intermediate representations help action.},
	pages = {eaaw6661},
	number = {30},
	journaltitle = {Science Robotics},
	shortjournal = {Sci. Robot.},
	author = {Zhou, Brady and Krähenbühl, Philipp and Koltun, Vladlen},
	urldate = {2023-07-01},
	date = {2019-05-22},
	langid = {english},
}

@article{ma_principles_2022,
	title = {On the principles of Parsimony and Self-consistency for the emergence of intelligence},
	volume = {23},
	issn = {2095-9184, 2095-9230},
	url = {https://link.springer.com/10.1631/FITEE.2200297},
	doi = {10.1631/FITEE.2200297},
	pages = {1298--1323},
	number = {9},
	journaltitle = {Frontiers of Information Technology \& Electronic Engineering},
	shortjournal = {Front Inform Technol Electron Eng},
	author = {Ma, Yi and Tsao, Doris and Shum, Heung-Yeung},
	urldate = {2023-07-01},
	date = {2022-09},
	langid = {english},
}

@report{krizhevsky_learning_2009,
	title = {Learning Multiple Layers of Features from Tiny Images},
	institution = {Canadian Institute for Advanced Research},
	author = {Krizhevsky, Alex},
	date = {2009},
}

@article{george_generative_2017,
	title = {A generative vision model that trains with high data efficiency and breaks text-based {CAPTCHAs}},
	volume = {358},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.aag2612},
	doi = {10.1126/science.aag2612},
	abstract = {Computer or human?
            
              Proving that we are human is now part of many tasks that we do on the internet, such as creating an email account, voting in an online poll, or even downloading a scientific paper. One of the most popular tests is text-based {CAPTCHA}, where would-be users are asked to decipher letters that may be distorted, partially obscured, or shown against a busy background. This test is used because computers find it tricky, but (most) humans do not. George
              et al.
              developed a hierarchical model for computer vision that was able to solve {CAPTCHAs} with a high accuracy rate using comparatively little training data. The results suggest that moving away from text-based {CAPTCHAs}, as some online services have done, may be a good idea.
            
            
              Science
              , this issue p.
              eaag2612
            
          , 
            A hierarchical computer vision model solves {CAPTCHAs} with a high accuracy rate using relatively little training data.
          , 
            
              {INTRODUCTION}
              Compositionality, generalization, and learning from a few examples are among the hallmarks of human intelligence. {CAPTCHAs} (Completely Automated Public Turing test to tell Computers and Humans Apart), images used by websites to block automated interactions, are examples of problems that are easy for people but difficult for computers. {CAPTCHAs} add clutter and crowd letters together to create a chicken-and-egg problem for algorithmic classifiers—the classifiers work well for characters that have been segmented out, but segmenting requires an understanding of the characters, which may be rendered in a combinatorial number of ways. {CAPTCHAs} also demonstrate human data efficiency: A recent deep-learning approach for parsing one specific {CAPTCHA} style required millions of labeled examples, whereas humans solve new styles without explicit training.
              By drawing inspiration from systems neuroscience, we introduce recursive cortical network ({RCN}), a probabilistic generative model for vision in which message-passing–based inference handles recognition, segmentation, and reasoning in a unified manner. {RCN} learns with very little training data and fundamentally breaks the defense of modern text-based {CAPTCHAs} by generatively segmenting characters. In addition, {RCN} outperforms deep neural networks on a variety of benchmarks while being orders of magnitude more data-efficient.
            
            
              {RATIONALE}
              Modern deep neural networks resemble the feed-forward hierarchy of simple and complex cells in the neocortex. Neuroscience has postulated computational roles for lateral and feedback connections, segregated contour and surface representations, and border-ownership coding observed in the visual cortex, yet these features are not commonly used by deep neural nets. We hypothesized that systematically incorporating these findings into a new model could lead to higher data efficiency and generalization. Structured probabilistic models provide a natural framework for incorporating prior knowledge, and belief propagation ({BP}) is an inference algorithm that can match the cortical computational speed. The representational choices in {RCN} were determined by investigating the computational underpinnings of neuroscience data under the constraint that accurate inference should be possible using {BP}.
            
            
              {RESULTS}
              {RCN} was effective in breaking a wide variety of {CAPTCHAs} with very little training data and without using {CAPTCHA}-specific heuristics. By comparison, a convolutional neural network required a 50,000-fold larger training set and was less robust to perturbations to the input. Similar results are shown on one- and few-shot {MNIST} (modified National Institute of Standards and Technology handwritten digit data set) classification, where {RCN} was significantly more robust to clutter introduced during testing. As a generative model, {RCN} outperformed neural network models when tested on noisy and cluttered examples and generated realistic samples from one-shot training of handwritten characters. {RCN} also proved to be effective at an occlusion reasoning task that required identifying the precise relationships between characters at multiple points of overlap. On a standard benchmark for parsing text in natural scenes, {RCN} outperformed state-of-the-art deep-learning methods while requiring 300-fold less training data.
            
            
              {CONCLUSION}
              
                Our work demonstrates that structured probabilistic models that incorporate inductive biases from neuroscience can lead to robust, generalizable machine learning models that learn with high data efficiency. In addition, our model’s effectiveness in breaking text-based {CAPTCHAs} with very little training data suggests that websites should seek more robust mechanisms for detecting automated interactions.
                
              
              
                
                  Breaking {CAPTCHAs} using a generative vision model.
                  Text-based {CAPTCHAs} exploit the data efficiency and generative aspects of human vision to create a challenging task for machines. By handling recognition and segmentation in a unified way, our model fundamentally breaks the defense of text-based {CAPTCHAs}. Shown are the parses by our model for a variety of {CAPTCHAs} .
                
                
              
            
          , 
            Learning from a few examples and generalizing to markedly different situations are capabilities of human visual intelligence that are yet to be matched by leading machine learning models. By drawing inspiration from systems neuroscience, we introduce a probabilistic generative model for vision in which message-passing–based inference handles recognition, segmentation, and reasoning in a unified way. The model demonstrates excellent generalization and occlusion-reasoning capabilities and outperforms deep neural networks on a challenging scene text recognition benchmark while being 300-fold more data efficient. In addition, the model fundamentally breaks the defense of modern text-based {CAPTCHAs} (Completely Automated Public Turing test to tell Computers and Humans Apart) by generatively segmenting characters without {CAPTCHA}-specific heuristics. Our model emphasizes aspects such as data efficiency and compositionality that may be important in the path toward general artificial intelligence.},
	pages = {eaag2612},
	number = {6368},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {George, Dileep and Lehrach, Wolfgang and Kansky, Ken and Lázaro-Gredilla, Miguel and Laan, Christopher and Marthi, Bhaskara and Lou, Xinghua and Meng, Zhaoshi and Liu, Yi and Wang, Huayan and Lavin, Alex and Phoenix, D. Scott},
	urldate = {2023-07-01},
	date = {2017-12-08},
	langid = {english},
}

@misc{ferrier_toward_2014,
	title = {Toward a Universal Cortical Algorithm: Examining Hierarchical Temporal Memory in Light of Frontal Cortical Function},
	url = {http://arxiv.org/abs/1411.4702},
	doi = {10.48550/arXiv.1411.4702},
	shorttitle = {Toward a Universal Cortical Algorithm},
	abstract = {A wide range of evidence points toward the existence of a common algorithm underlying the processing of information throughout the cerebral cortex. Several hypothesized features of this cortical algorithm are reviewed, including sparse distributed representation, Bayesian inference, hierarchical organization composed of alternating template matching and pooling layers, temporal slowness and predictive coding. Hierarchical Temporal Memory ({HTM}) is a family of learning algorithms and corresponding theories of cortical function that embodies these principles. {HTM} has previously been applied mainly to perceptual tasks typical of posterior cortex. In order to evaluate {HTM} as a candidate model of cortical function, it is necessary also to investigate its compatibility with the requirements of frontal cortical function. To this end, a variety of models of frontal cortical function are reviewed and integrated, to arrive at the hypothesis that frontal functions including attention, working memory and action selection depend largely upon the same basic algorithms as do posterior functions, with the notable additions of a mechanism for the active maintenance of representations and of multiple cortico-striato-thalamo-cortical loops that allow communication between regions of frontal cortex to be gated in an adaptive manner. Computational models of this system are reviewed. Finally, there is a discussion of how {HTM} can contribute to the understanding of frontal cortical function, and of what the requirements of frontal cortical function mean for the future development of {HTM}.},
	number = {{arXiv}:1411.4702},
	publisher = {{arXiv}},
	author = {Ferrier, Michael R.},
	urldate = {2023-07-01},
	date = {2014-11-17},
	eprinttype = {arxiv},
	eprint = {1411.4702 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, I.2.6, Quantitative Biology - Neurons and Cognition},
}

@article{yang_survey_2023,
	title = {A Survey on ensemble learning under the era of deep learning},
	volume = {56},
	issn = {0269-2821, 1573-7462},
	url = {https://link.springer.com/10.1007/s10462-022-10283-5},
	doi = {10.1007/s10462-022-10283-5},
	pages = {5545--5589},
	number = {6},
	journaltitle = {Artificial Intelligence Review},
	shortjournal = {Artif Intell Rev},
	author = {Yang, Yongquan and Lv, Haijun and Chen, Ning},
	urldate = {2023-07-01},
	date = {2023-06},
	langid = {english},
}

@article{lewis_locations_2019,
	title = {Locations in the Neocortex: A Theory of Sensorimotor Object Recognition Using Cortical Grid Cells},
	volume = {13},
	issn = {1662-5110},
	url = {https://www.frontiersin.org/article/10.3389/fncir.2019.00022/full},
	doi = {10.3389/fncir.2019.00022},
	shorttitle = {Locations in the Neocortex},
	pages = {22},
	journaltitle = {Frontiers in Neural Circuits},
	shortjournal = {Front. Neural Circuits},
	author = {Lewis, Marcus and Purdy, Scott and Ahmad, Subutai and Hawkins, Jeff},
	urldate = {2023-07-01},
	date = {2019-04-24},
}

@article{lewis_locations_2019-1,
	title = {Locations in the Neocortex: A Theory of Sensorimotor Object Recognition Using Cortical Grid Cells},
	volume = {13},
	issn = {1662-5110},
	doi = {10.3389/fncir.2019.00022},
	pages = {22},
	journaltitle = {Frontiers in Neural Circuits},
	author = {Lewis, Marcus and Purdy, Scott and Ahmad, Subutai and Hawkins, Jeff},
	date = {2019-04},
}

@article{mountcastle_columnar_1997,
	title = {The columnar organization of the neocortex},
	volume = {120},
	issn = {14602156},
	url = {https://academic.oup.com/brain/article-lookup/doi/10.1093/brain/120.4.701},
	doi = {10.1093/brain/120.4.701},
	pages = {701--722},
	number = {4},
	journaltitle = {Brain},
	author = {Mountcastle, V.},
	urldate = {2023-07-01},
	date = {1997-04-01},
}

@article{hawkins_framework_2019,
	title = {A Framework for Intelligence and Cortical Function Based on Grid Cells in the Neocortex},
	volume = {12},
	issn = {1662-5110},
	url = {https://www.frontiersin.org/article/10.3389/fncir.2018.00121/full},
	doi = {10.3389/fncir.2018.00121},
	pages = {121},
	journaltitle = {Frontiers in Neural Circuits},
	shortjournal = {Front. Neural Circuits},
	author = {Hawkins, Jeff and Lewis, Marcus and Klukas, Mirko and Purdy, Scott and Ahmad, Subutai},
	urldate = {2023-07-01},
	date = {2019-01-11},
}

@article{hospedales_meta-learning_2021,
	title = {Meta-Learning in Neural Networks: A Survey},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/9428530/},
	doi = {10.1109/TPAMI.2021.3079209},
	shorttitle = {Meta-Learning in Neural Networks},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Hospedales, Timothy M and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos J.},
	urldate = {2023-07-01},
	date = {2021},
}

@article{sager_unsupervised_2022,
	title = {Unsupervised Domain Adaptation for Vertebrae Detection and Identification in 3D {CT} Volumes Using a Domain Sanity Loss},
	volume = {8},
	issn = {2313-433X},
	url = {https://www.mdpi.com/2313-433X/8/8/222},
	doi = {10.3390/jimaging8080222},
	abstract = {A variety of medical computer vision applications analyze 2D slices of computed tomography ({CT}) scans, whereas axial slices from the body trunk region are usually identified based on their relative position to the spine. A limitation of such systems is that either the correct slices must be extracted manually or labels of the vertebrae are required for each {CT} scan to develop an automated extraction system. In this paper, we propose an unsupervised domain adaptation ({UDA}) approach for vertebrae detection and identification based on a novel Domain Sanity Loss ({DSL}) function. With {UDA} the model’s knowledge learned on a publicly available (source) data set can be transferred to the target domain without using target labels, where the target domain is defined by the specific setup ({CT} modality, study protocols, applied pre- and processing) at the point of use (e.g., a specific clinic with its specific {CT} study protocols). With our approach, a model is trained on the source and target data set in parallel. The model optimizes a supervised loss for labeled samples from the source domain and the {DSL} loss function based on domain-specific “sanity checks” for samples from the unlabeled target domain. Without using labels from the target domain, we are able to identify vertebra centroids with an accuracy of 72.8\%. By adding only ten target labels during training the accuracy increases to 89.2\%, which is on par with the current state-of-the-art for full supervised learning, while using about 20 times less labels. Thus, our model can be used to extract 2D slices from 3D {CT} scans on arbitrary data sets fully automatically without requiring an extensive labeling effort, contributing to the clinical adoption of medical imaging by hospitals.},
	pages = {222},
	number = {8},
	journaltitle = {Journal of Imaging},
	shortjournal = {J. Imaging},
	author = {Sager, Pascal and Salzmann, Sebastian and Burn, Felice and Stadelmann, Thilo},
	urldate = {2023-07-01},
	date = {2022-08-19},
	langid = {english},
}

@article{zhuang_comprehensive_2021,
	title = {A Comprehensive Survey on Transfer Learning},
	volume = {109},
	issn = {0018-9219, 1558-2256},
	url = {https://ieeexplore.ieee.org/document/9134370/},
	doi = {10.1109/JPROC.2020.3004555},
	pages = {43--76},
	number = {1},
	journaltitle = {Proceedings of the {IEEE}},
	shortjournal = {Proc. {IEEE}},
	author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
	urldate = {2023-07-01},
	date = {2021-01},
}

@article{hoi_online_2021,
	title = {Online learning: A comprehensive survey},
	volume = {459},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231221006706},
	doi = {10.1016/j.neucom.2021.04.112},
	shorttitle = {Online learning},
	pages = {249--289},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Hoi, Steven C.H. and Sahoo, Doyen and Lu, Jing and Zhao, Peilin},
	urldate = {2023-07-01},
	date = {2021-10},
	langid = {english},
}

@misc{manakul_selfcheckgpt_2023,
	title = {{SelfCheckGPT}: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models},
	url = {http://arxiv.org/abs/2303.08896},
	doi = {10.48550/arXiv.2303.08896},
	shorttitle = {{SelfCheckGPT}},
	abstract = {Generative Large Language Models ({LLMs}) such as {GPT}-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, {LLMs} are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as {ChatGPT}) or external databases that are interfaced via separate, often complex, modules. In this work, we propose "{SelfCheckGPT}", a simple sampling-based approach that can be used to fact-check black-box models in a zero-resource fashion, i.e. without an external database. {SelfCheckGPT} leverages the simple idea that if a {LLM} has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using {GPT}-3 to generate passages about individuals from the {WikiBio} dataset, and manually annotate the factuality of the generated passages. We demonstrate that {SelfCheckGPT} can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that in sentence hallucination detection, our approach has {AUC}-{PR} scores comparable to or better than grey-box methods, while {SelfCheckGPT} is best at passage factuality assessment.},
	number = {{arXiv}:2303.08896},
	publisher = {{arXiv}},
	author = {Manakul, Potsawee and Liusie, Adian and Gales, Mark J. F.},
	urldate = {2023-07-01},
	date = {2023-05-07},
	eprinttype = {arxiv},
	eprint = {2303.08896 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{feldman_trapping_2023,
	title = {Trapping {LLM} Hallucinations Using Tagged Context Prompts},
	url = {http://arxiv.org/abs/2306.06085},
	doi = {10.48550/arXiv.2306.06085},
	abstract = {Recent advances in large language models ({LLMs}), such as {ChatGPT}, have led to highly sophisticated conversation agents. However, these models suffer from "hallucinations," where the model generates false or fabricated information. Addressing this challenge is crucial, particularly with {AI}-driven platforms being adopted across various sectors. In this paper, we propose a novel method to recognize and flag instances when {LLMs} perform outside their domain knowledge, and ensuring users receive accurate information. We find that the use of context combined with embedded tags can successfully combat hallucinations within generative language models. To do this, we baseline hallucination frequency in no-context prompt-response pairs using generated {URLs} as easily-tested indicators of fabricated data. We observed a significant reduction in overall hallucination when context was supplied along with question prompts for tested generative engines. Lastly, we evaluated how placing tags within contexts impacted model responses and were able to eliminate hallucinations in responses with 98.88\% effectiveness.},
	number = {{arXiv}:2306.06085},
	publisher = {{arXiv}},
	author = {Feldman, Philip and Foulds, James R. and Pan, Shimei},
	urldate = {2023-07-01},
	date = {2023-06-09},
	eprinttype = {arxiv},
	eprint = {2306.06085 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, I.2.7, K.4.2},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	doi = {10.48550/arXiv.2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the {OpenAI} {API}, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune {GPT}-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models {InstructGPT}. In human evaluations on our prompt distribution, outputs from the 1.3B parameter {InstructGPT} model are preferred to outputs from the 175B {GPT}-3, despite having 100x fewer parameters. Moreover, {InstructGPT} models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public {NLP} datasets. Even though {InstructGPT} still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	number = {{arXiv}:2203.02155},
	publisher = {{arXiv}},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	urldate = {2023-07-01},
	date = {2022-03-04},
	eprinttype = {arxiv},
	eprint = {2203.02155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{touvron_llama_2023,
	title = {{LLaMA}: Open and Efficient Foundation Language Models},
	url = {http://arxiv.org/abs/2302.13971},
	doi = {10.48550/arXiv.2302.13971},
	shorttitle = {{LLaMA}},
	abstract = {We introduce {LLaMA}, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, {LLaMA}-13B outperforms {GPT}-3 (175B) on most benchmarks, and {LLaMA}-65B is competitive with the best models, Chinchilla-70B and {PaLM}-540B. We release all our models to the research community.},
	number = {{arXiv}:2302.13971},
	publisher = {{arXiv}},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	urldate = {2023-07-01},
	date = {2023-02-27},
	eprinttype = {arxiv},
	eprint = {2302.13971 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{kelso_selforganizing_1995,
	title = {Self‐organizing dynamics of the human brain: Critical instabilities and Šil’nikov chaos},
	volume = {5},
	issn = {1054-1500, 1089-7682},
	url = {https://pubs.aip.org/aip/cha/article/5/1/64-69/136508},
	doi = {10.1063/1.166087},
	shorttitle = {Self‐organizing dynamics of the human brain},
	pages = {64--69},
	number = {1},
	journaltitle = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	shortjournal = {Chaos},
	author = {Kelso, J. A. Scott and Fuchs, Armin},
	urldate = {2023-06-30},
	date = {1995-03},
	langid = {english},
}

@article{singer_brain_1986,
	title = {The brain as a self-organizing system},
	volume = {236},
	issn = {0175-758X, 1433-8491},
	url = {http://link.springer.com/10.1007/BF00641050},
	doi = {10.1007/BF00641050},
	pages = {4--9},
	number = {1},
	journaltitle = {European Archives of Psychiatry and Neurological Sciences},
	shortjournal = {Eur Arch Psychiatr Neurol Sci},
	author = {Singer, Wolf},
	urldate = {2023-06-30},
	date = {1986},
	langid = {english},
}

@article{mountcastle_columnar_1997-1,
	title = {The columnar organization of the neocortex},
	volume = {120},
	issn = {14602156},
	url = {https://academic.oup.com/brain/article-lookup/doi/10.1093/brain/120.4.701},
	doi = {10.1093/brain/120.4.701},
	pages = {701--722},
	number = {4},
	journaltitle = {Brain},
	author = {Mountcastle, Vernon},
	urldate = {2023-06-30},
	date = {1997-04-01},
}

@article{bassett_understanding_2011,
	title = {Understanding complexity in the human brain},
	volume = {15},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661311000416},
	doi = {10.1016/j.tics.2011.03.006},
	pages = {200--209},
	number = {5},
	journaltitle = {Trends in Cognitive Sciences},
	shortjournal = {Trends in Cognitive Sciences},
	author = {Bassett, Danielle S. and Gazzaniga, Michael S.},
	urldate = {2023-06-30},
	date = {2011-05},
	langid = {english},
}

@article{gazzaniga_organization_1989,
	title = {Organization of the Human Brain},
	volume = {245},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.2672334},
	doi = {10.1126/science.2672334},
	pages = {947--952},
	number = {4921},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Gazzaniga, Michael S.},
	urldate = {2023-06-30},
	date = {1989-09},
	langid = {english},
}

@thesis{lehmann_leveraging_2022,
	title = {Leveraging Neuroscience for Deep Learning Based Object Recognition},
	institution = {Zurich University of Applied Sciences},
	type = {Master's Thesis},
	author = {Lehmann, Claude},
	date = {2022},
}

@article{fernandes_self-organization_2015,
	title = {Self-Organization of Control Circuits for Invariant Fiber Projections},
	volume = {27},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/27/5/1005-1032/8081},
	doi = {10.1162/NECO_a_00725},
	abstract = {Assuming that patterns in memory are represented as two-dimensional arrays of local features, just as they are in primary visual cortices, pattern recognition can take the form of elastic graph matching (Lades et al., 1993 ). Neural implementation of this may be based on preorganized fiber projections that can be activated rapidly with the help of control units (Wolfrum, Wolff, Lücke, \& von der Malsburg, 2008 ). Each control unit governs a set of projection fibers that form part of a coherent mapping. We describe a mathematical model for the ontogenesis of the underlying connectivity based on a principle of network self-organization as described by the Häussler system (Häussler \& von der Malsburg, 1983 ), modified to be sensitive to pattern similarity and to support formation of multiple mappings, each under the command of a control unit. The process takes the form of a soft-winner-take-all, where units compete for the representation of maps. We show simulations for invariant point-to-point and feature-to-feature mappings.},
	pages = {1005--1032},
	number = {5},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Fernandes, Tomas and von der Malsburg, Christoph},
	urldate = {2023-06-30},
	date = {2015-05},
	langid = {english},
}

@article{olshausen_multiscale_1995,
	title = {A multiscale dynamic routing circuit for forming size- and position-invariant object representations},
	volume = {2},
	issn = {0929-5313, 1573-6873},
	url = {http://link.springer.com/10.1007/BF00962707},
	doi = {10.1007/BF00962707},
	pages = {45--62},
	number = {1},
	journaltitle = {Journal of Computational Neuroscience},
	shortjournal = {J Comput Neurosci},
	author = {Olshausen, Bruno A. and Anderson, Charles H. and Van Essen, David C.},
	urldate = {2023-06-30},
	date = {1995-03},
	langid = {english},
}

@article{von_der_malsburg_concerning_2018,
	title = {Concerning the Neuronal Code},
	volume = {19},
	url = {https://doi.org/10.17791/JCS.2018.19.4.511},
	doi = {10.17791/JCS.2018.19.4.511},
	pages = {511--550},
	number = {4},
	journaltitle = {Journal of Cognitive Science},
	author = {von der Malsburg, Christoph},
	urldate = {2023-06-30},
	date = {2018-12},
}

@article{mcpherson_physical_2001,
	title = {A Physical Map of the Human Genome},
	volume = {409},
	issn = {1476-4687},
	url = {https://doi.org/10.1038/35057157},
	doi = {10.1038/35057157},
	pages = {934--941},
	number = {6822},
	journaltitle = {Nature},
	author = {{McPherson}, John D. and Marra, Marco and Hillier, {LaDeana} and Waterston, Robert H. and Chinwalla, Asif and Wallis, John and Sekhon, Mandeep and Wylie, Kristine and Mardis, Elaine R. and Wilson, Richard K. and Fulton, Robert and Kucaba, Tamara A. and Wagner-{McPherson}, Caryn and Barbazuk, William B. and Gregory, Simon G. and Humphray, Sean J. and French, Lisa and Evans, Richard S. and Bethel, Graeme and Whittaker, Adam and Holden, Jane L. and {McCann}, Owen T. and Dunham, Andrew and Soderlund, Carol and Scott, Carol E. and Bentley, David R. and Schuler, Gregory and Chen, Hsiu-Chuan and Jang, Wonhee and Green, Eric D. and Idol, Jacquelyn R. and Maduro, Valerie V. Braden and Montgomery, Kate T. and Lee, Eunice and Miller, Ashley and Emerling, Suzanne and Kucherlapati, Raju and Gibbs, Richard and Scherer, Steve and Gorrell, J. Harley and Sodergren, Erica and Clerc-Blankenburg, Kerstin and Tabor, Paul and Naylor, Susan and Garcia, Dawn and de Jong, Pieter J. and Catanese, Joseph J. and Nowak, Norma and Osoegawa, Kazutoyo and Qin, Shizhen and Rowen, Lee and Madan, Anuradha and Dors, Monica and Hood, Leroy and Trask, Barbara and Friedman, Cynthia and Massa, Hillary and Cheung, Vivian G. and Kirsch, Ilan R. and Reid, Thomas and Yonescu, Raluca and Weissenbach, Jean and Bruls, Thomas and Heilig, Roland and Branscomb, Elbert and Olsen, Anne and Doggett, Norman and Cheng, Jan-Fang and Hawkins, Trevor and Myers, Richard M. and Shang, Jin and Ramirez, Lucia and Schmutz, Jeremy and Velasquez, Olivia and Dixon, Kami and Stone, Nancy E. and Cox, David R. and Haussler, David and Kent, W. James and Furey, Terrence and Rogic, Sanja and Kennedy, Scot and Jones, Steven and Rosenthal, Andre and Wen, Gaiping and Schilhabel, Markus and Gloeckner, Gernot and Nyakatura, Gerald and Siebert, Reiner and Schlegelberger, Brigitte and Korenberg, Julie and Chen, Xiao-Ning and Fujiyama, Asao and Hattori, Masahira and Toyoda, Atsushi and Yada, Tetsushi and Park, Hong-Seok and Sakaki, Yoshiyuki and Shimizu, Nobuyoshi and Asakawa, Shuichi and Kawasaki, Kazuhiko and Sasaki, Takashi and Shintani, Ai and Shimizu, Atsushi and Shibuya, Kazunori and Kudoh, Jun and Minoshima, Shinsei and Ramser, Juliane and Seranski, Peter and Hoff, Celine and Poustka, Annemarie and Reinhardt, Richard and Lehrach, Hans},
	date = {2001-02-01},
}

@article{the_international_human_genome_mapping_consortium_physical_2001,
	title = {A physical map of the human genome},
	volume = {409},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/35057157},
	doi = {10.1038/35057157},
	pages = {934--941},
	number = {6822},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {{The International Human Genome Mapping Consortium} and {Washington University School of Medicine, Genome Sequencing Center:} and McPherson, John D. and Marra, Marco and Hillier, LaDeana and Waterston, Robert H. and Chinwalla, Asif and Wallis, John and Sekhon, Mandeep and Wylie, Kristine and Mardis, Elaine R. and Wilson, Richard K. and Fulton, Robert and Kucaba, Tamara A. and Wagner-McPherson, Caryn and Barbazuk, William B. and {Wellcome Trust Genome Campus:} and Gregory, Simon G. and Humphray, Sean J. and French, Lisa and Evans, Richard S. and Bethel, Graeme and Whittaker, Adam and Holden, Jane L. and McCann, Owen T. and Dunham, Andrew and Soderlund, Carol and Scott, Carol E. and Bentley, David R. and {National Center for Biotechnology Information:} and Schuler, Gregory and Chen, Hsiu-Chuan and Jang, Wonhee and {National Human Genome Research Insititute:} and Green, Eric D. and Idol, Jacquelyn R. and Maduro, Valerie V. Braden and {Albert Einstein College of Medicine:} and Montgomery, Kate T. and Lee, Eunice and Miller, Ashley and Emerling, Suzanne and Kucherlapati, Raju and {Baylor College of Medicine, Human Genome Sequencing Center:} and Gibbs, Richard and Scherer, Steve and Gorrell, J. Harley and Sodergren, Erica and Clerc-Blankenburg, Kerstin and Tabor, Paul and Naylor, Susan and Garcia, Dawn and {Roswell Park Cancer Institute:} and De Jong, Pieter J. and Catanese, Joseph J. and Nowak, Norma and Osoegawa, Kazutoyo and {Multimegabase Sequencing Center:} and Qin, Shizhen and Rowen, Lee and Madan, Anuradha and Dors, Monica and Hood, Leroy and {Fred Hutchinson Cancer Research Institute:} and Trask, Barbara and Friedman, Cynthia and Massa, Hillary and {The Children's Hospital of Philadelphia:} and Cheung, Vivian G. and Kirsch, Ilan R. and Reid, Thomas and Yonescu, Raluca and {Genoscope:} and Weissenbach, Jean and Bruls, Thomas and Heilig, Roland and {US DOE Joint Genome Institute:} and Branscomb, Elbert and Olsen, Anne and Doggett, Norman and Cheng, Jan-Fang and Hawkins, Trevor and {Stanford Human Genome Center and Department of Genetics:} and Myers, Richard M. and Shang, Jin and Ramirez, Lucia and Schmutz, Jeremy and Velasquez, Olivia and Dixon, Kami and Stone, Nancy E. and Cox, David R. and {University of California, Santa Cruz:} and Haussler, David and Kent, W. James and Furey, Terrence and Rogic, Sanja and Kennedy, Scot and {British Columbia Cancer Research Centre:} and Jones, Steven and {Department of Genome Analysis, Institute of Molecular Biotechnology:} and Rosenthal, André and Wen, Gaiping and Schilhabel, Markus and Gloeckner, Gernot and Nyakatura, Gerald and Siebert, Reiner and Schlegelberger, Brigitte and {Departments of Human Genetics and Pediatrics, University of California:} and Korenberg, Julie and Chen, Xiao-Ning and {RIKEN Genomic Sciences Center:} and Fujiyama, Asao and Hattori, Masahira and Toyoda, Atsushi and Yada, Tetsushi and Park, Hong-Seok and Sakaki, Yoshiyuki and {Department of Molecular Biology, Keio University School of Medicine:} and Shimizu, Nobuyoshi and Asakawa, Shuichi and Kawasaki, Kazuhiko and Sasaki, Takashi and Shintani, Ai and Shimizu, Atsushi and Shibuya, Kazunori and Kudoh, Jun and Minoshima, Shinsei and {Max-Planck-Institute for Molecular Genetics:} and Ramser, Juliane and Seranski, Peter and Hoff, Celine and Poustka, Annemarie and Reinhardt, Richard and Lehrach, Hans},
	urldate = {2023-06-30},
	date = {2001-02-15},
	langid = {english},
}

@article{vlachas_backpropagation_2020,
	title = {Backpropagation algorithms and Reservoir Computing in Recurrent Neural Networks for the forecasting of complex spatiotemporal dynamics},
	volume = {126},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608020300708},
	doi = {10.1016/j.neunet.2020.02.016},
	pages = {191--217},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Vlachas, Pantelis R. and Pathak, Jaideep and Hunt, Brian R. and Sapsis, Themistoklis P. and Girvan, Michelle and Ott, Edward and Koumoutsakos, Petros},
	urldate = {2023-06-30},
	date = {2020-06},
	langid = {english},
}

@article{krishnagopal_separation_2020,
	title = {Separation of chaotic signals by reservoir computing},
	volume = {30},
	issn = {1054-1500, 1089-7682},
	url = {https://pubs.aip.org/aip/cha/article/1078551},
	doi = {10.1063/1.5132766},
	pages = {023123},
	number = {2},
	journaltitle = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	shortjournal = {Chaos},
	author = {Krishnagopal, Sanjukta and Girvan, Michelle and Ott, Edward and Hunt, Brian R.},
	urldate = {2023-06-30},
	date = {2020-02},
	langid = {english},
}

@article{ghosh_quantum_2019,
	title = {Quantum reservoir processing},
	volume = {5},
	issn = {2056-6387},
	url = {https://www.nature.com/articles/s41534-019-0149-8},
	doi = {10.1038/s41534-019-0149-8},
	abstract = {Abstract
            The concurrent rise of artificial intelligence and quantum information poses an opportunity for creating interdisciplinary technologies like quantum neural networks. Quantum reservoir processing, introduced here, is a platform for quantum information processing developed on the principle of reservoir computing that is a form of an artificial neural network. A quantum reservoir processor can perform qualitative tasks like recognizing quantum states that are entangled as well as quantitative tasks like estimating a nonlinear function of an input quantum state (e.g., entropy, purity, or logarithmic negativity). In this way, experimental schemes that require measurements of multiple observables can be simplified to measurement of one observable on a trained quantum reservoir processor.},
	pages = {35},
	number = {1},
	journaltitle = {npj Quantum Information},
	shortjournal = {npj Quantum Inf},
	author = {Ghosh, Sanjib and Opala, Andrzej and Matuszewski, Michał and Paterek, Tomasz and Liew, Timothy C. H.},
	urldate = {2023-06-30},
	date = {2019-04-29},
	langid = {english},
}

@article{chen_temporal_2020,
	title = {Temporal Information Processing on Noisy Quantum Computers},
	volume = {14},
	issn = {2331-7019},
	url = {https://link.aps.org/doi/10.1103/PhysRevApplied.14.024065},
	doi = {10.1103/PhysRevApplied.14.024065},
	pages = {024065},
	number = {2},
	journaltitle = {Physical Review Applied},
	shortjournal = {Phys. Rev. Applied},
	author = {Chen, Jiayin and Nurdin, Hendra I. and Yamamoto, Naoki},
	urldate = {2023-06-30},
	date = {2020-08-24},
	langid = {english},
}

@article{tanaka_recent_2019,
	title = {Recent advances in physical reservoir computing: A review},
	volume = {115},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608019300784},
	doi = {10.1016/j.neunet.2019.03.005},
	shorttitle = {Recent advances in physical reservoir computing},
	pages = {100--123},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Tanaka, Gouhei and Yamane, Toshiyuki and Héroux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
	urldate = {2023-06-30},
	date = {2019-07},
	langid = {english},
}

@article{bi_synaptic_2001,
	title = {Synaptic Modification by Correlated Activity: Hebb's Postulate Revisited},
	volume = {24},
	issn = {0147-006X, 1545-4126},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.neuro.24.1.139},
	doi = {10.1146/annurev.neuro.24.1.139},
	shorttitle = {Synaptic Modification by Correlated Activity},
	abstract = {▪ Abstract  Correlated spiking of pre- and postsynaptic neurons can result in strengthening or weakening of synapses, depending on the temporal order of spiking. Recent findings indicate that there are narrow and cell type–specific temporal windows for such synaptic modification and that the generally accepted input- (or synapse-) specific rule for modification appears not to be strictly adhered to. Spike timing–dependent modifications, together with selective spread of synaptic changes, provide a set of cellular mechanisms that are likely to be important for the development and functioning of neural networks.
            When an axon of cell A is near enough to excite cell B or repeatedly or consistently takes part in firing it, some growth or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased.      Donald Hebb (1949)},
	pages = {139--166},
	number = {1},
	journaltitle = {Annual Review of Neuroscience},
	shortjournal = {Annu. Rev. Neurosci.},
	author = {Bi, Guo-qiang and Poo, Mu-ming},
	urldate = {2023-06-29},
	date = {2001-03},
	langid = {english},
}

@article{erdos_random_1959,
	title = {On Random Graphs I},
	volume = {6},
	pages = {290},
	journaltitle = {Publicationes Mathematicae Debrecen},
	author = {Erdös, Paul and Rényi, Alfred},
	date = {1959},
	keywords = {graph sna},
}

@article{tanaka_recent_2019-1,
	title = {Recent advances in physical reservoir computing: A review},
	volume = {115},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608019300784},
	doi = {10.1016/j.neunet.2019.03.005},
	shorttitle = {Recent advances in physical reservoir computing},
	pages = {100--123},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Tanaka, Gouhei and Yamane, Toshiyuki and Héroux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
	urldate = {2023-06-29},
	date = {2019-07},
	langid = {english},
}

@article{maass_real-time_2002,
	title = {Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations},
	volume = {14},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/14/11/2531-2560/6650},
	doi = {10.1162/089976602760407955},
	shorttitle = {Real-Time Computing Without Stable States},
	abstract = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.},
	pages = {2531--2560},
	number = {11},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
	urldate = {2023-06-29},
	date = {2002-11-01},
	langid = {english},
}

@article{jager_echo_2001,
	title = {The "Echo State" Approach to Analysing and Training Recurrent Neural Networks-with an Erratum Note'},
	volume = {148},
	journaltitle = {German National Research Institute for Computer Science {GMD}: Technical Report},
	author = {Jäger, Herbert},
	date = {2001-01},
}

@article{kheradpisheh_stdp-based_2018,
	title = {{STDP}-based spiking deep convolutional neural networks for object recognition},
	volume = {99},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608017302903},
	doi = {10.1016/j.neunet.2017.12.005},
	pages = {56--67},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Kheradpisheh, Saeed Reza and Ganjtabesh, Mohammad and Thorpe, Simon J. and Masquelier, Timothée},
	urldate = {2023-06-29},
	date = {2018-03},
	langid = {english},
}

@article{nunes_spiking_2022,
	title = {Spiking Neural Networks: A Survey},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9787485/},
	doi = {10.1109/ACCESS.2022.3179968},
	shorttitle = {Spiking Neural Networks},
	pages = {60738--60764},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Nunes, Joao D. and Carvalho, Marcelo and Carneiro, Diogo and Cardoso, Jaime S.},
	urldate = {2023-06-29},
	date = {2022},
}

@article{brette_adaptive_2005,
	title = {Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity},
	volume = {94},
	issn = {0022-3077, 1522-1598},
	url = {https://www.physiology.org/doi/10.1152/jn.00686.2005},
	doi = {10.1152/jn.00686.2005},
	abstract = {We introduce a two-dimensional integrate-and-fire model that combines an exponential spike mechanism with an adaptation equation, based on recent theoretical findings. We describe a systematic method to estimate its parameters with simple electrophysiological protocols (current-clamp injection of pulses and ramps) and apply it to a detailed conductance-based model of a regular spiking neuron. Our simple model predicts correctly the timing of 96\% of the spikes (±2 ms) of the detailed model in response to injection of noisy synaptic conductances. The model is especially reliable in high-conductance states, typical of cortical activity in vivo, in which intrinsic conductances were found to have a reduced role in shaping spike trains. These results are promising because this simple model has enough expressive power to reproduce qualitatively several electrophysiological classes described in vitro.},
	pages = {3637--3642},
	number = {5},
	journaltitle = {Journal of Neurophysiology},
	shortjournal = {Journal of Neurophysiology},
	author = {Brette, Romain and Gerstner, Wulfram},
	urldate = {2023-06-29},
	date = {2005-11},
	langid = {english},
}

@article{izhikevich_simple_2003,
	title = {Simple model of spiking neurons},
	volume = {14},
	issn = {1045-9227},
	url = {http://ieeexplore.ieee.org/document/1257420/},
	doi = {10.1109/TNN.2003.820440},
	pages = {1569--1572},
	number = {6},
	journaltitle = {{IEEE} Transactions on Neural Networks},
	shortjournal = {{IEEE} Trans. Neural Netw.},
	author = {Izhikevich, Eeugene M.},
	urldate = {2023-06-29},
	date = {2003-11},
	langid = {english},
}

@article{maass_networks_1997,
	title = {Networks of spiking neurons: The third generation of neural network models},
	volume = {10},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608097000117},
	doi = {10.1016/S0893-6080(97)00011-7},
	shorttitle = {Networks of spiking neurons},
	pages = {1659--1671},
	number = {9},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Maass, Wolfgang},
	urldate = {2023-06-29},
	date = {1997-12},
	langid = {english},
}

@misc{ramsauer_hopfield_2021,
	title = {Hopfield Networks is All You Need},
	url = {http://arxiv.org/abs/2008.02217},
	doi = {10.48550/arXiv.2008.02217},
	abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the {UCI} benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers},
	number = {{arXiv}:2008.02217},
	publisher = {{arXiv}},
	author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
	urldate = {2023-06-29},
	date = {2021-04-28},
	eprinttype = {arxiv},
	eprint = {2008.02217 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{demircigil_model_2017,
	title = {On a Model of Associative Memory with Huge Storage Capacity},
	volume = {168},
	issn = {0022-4715, 1572-9613},
	url = {http://link.springer.com/10.1007/s10955-017-1806-y},
	doi = {10.1007/s10955-017-1806-y},
	pages = {288--299},
	number = {2},
	journaltitle = {Journal of Statistical Physics},
	shortjournal = {J Stat Phys},
	author = {Demircigil, Mete and Heusel, Judith and Löwe, Matthias and Upgang, Sven and Vermet, Franck},
	urldate = {2023-06-29},
	date = {2017-07},
	langid = {english},
}

@article{mceliece_capacity_1987,
	title = {The capacity of the Hopfield associative memory},
	volume = {33},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1057328/},
	doi = {10.1109/TIT.1987.1057328},
	pages = {461--482},
	number = {4},
	journaltitle = {{IEEE} Transactions on Information Theory},
	shortjournal = {{IEEE} Trans. Inform. Theory},
	author = {{McEliece}, Robert J. and Posner, Edward C. and Rodemich, Eugener and Venkatesh, Santoshs},
	urldate = {2023-06-29},
	date = {1987-07},
	langid = {english},
}

@article{hopfield_unlearning_1983,
	title = {‘Unlearning’ has a stabilizing effect in collective memories},
	volume = {304},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/304158a0},
	doi = {10.1038/304158a0},
	pages = {158--159},
	number = {5922},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Hopfield, John J. and Feinstein, David I. and Palmer, Richard G.},
	urldate = {2023-06-29},
	date = {1983-07},
	langid = {english},
}

@misc{weston_memory_2015,
	title = {Memory Networks},
	url = {http://arxiv.org/abs/1410.3916},
	doi = {10.48550/arXiv.1410.3916},
	abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering ({QA}) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale {QA} task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
	number = {{arXiv}:1410.3916},
	publisher = {{arXiv}},
	author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
	urldate = {2023-06-29},
	date = {2015-11-29},
	eprinttype = {arxiv},
	eprint = {1410.3916 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Statistics - Machine Learning},
}

@article{fix_discriminatory_1989,
	title = {Discriminatory Analysis. Nonparametric Discrimination: Consistency Properties},
	volume = {57},
	issn = {03067734},
	url = {https://www.jstor.org/stable/1403797?origin=crossref},
	doi = {10.2307/1403797},
	shorttitle = {Discriminatory Analysis. Nonparametric Discrimination},
	pages = {238},
	number = {3},
	journaltitle = {International Statistical Review / Revue Internationale de Statistique},
	shortjournal = {International Statistical Review / Revue Internationale de Statistique},
	author = {Fix, Evelyn and Hodges, Joseph L.},
	urldate = {2023-06-29},
	date = {1989-12},
}

@article{hopfield_neural_1982,
	title = {Neural networks and physical systems with emergent collective computational abilities.},
	volume = {79},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.79.8.2554},
	doi = {10.1073/pnas.79.8.2554},
	abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
	pages = {2554--2558},
	number = {8},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Hopfield, John J.},
	urldate = {2023-06-29},
	date = {1982-04},
	langid = {english},
}

@article{vogels_inhibitory_2011,
	title = {Inhibitory Plasticity Balances Excitation and Inhibition in Sensory Pathways and Memory Networks},
	volume = {334},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.1211095},
	doi = {10.1126/science.1211095},
	abstract = {Plasticity at inhibitory synapses maintains balanced excitatory and inhibitory synaptic inputs at cortical neurons.
          , 
            Cortical neurons receive balanced excitatory and inhibitory synaptic currents. Such a balance could be established and maintained in an experience-dependent manner by synaptic plasticity at inhibitory synapses. We show that this mechanism provides an explanation for the sparse firing patterns observed in response to natural stimuli and fits well with a recently observed interaction of excitatory and inhibitory receptive field plasticity. The introduction of inhibitory plasticity in suitable recurrent networks provides a homeostatic mechanism that leads to asynchronous irregular network states. Further, it can accommodate synaptic memories with activity patterns that become indiscernible from the background state but can be reactivated by external stimuli. Our results suggest an essential role of inhibitory plasticity in the formation and maintenance of functional cortical circuitry.},
	pages = {1569--1573},
	number = {6062},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Vogels, T. P. and Sprekeler, H. and Zenke, F. and Clopath, C. and Gerstner, W.},
	urldate = {2023-06-29},
	date = {2011-12-16},
	langid = {english},
}

@article{simoncelli_natural_2001,
	title = {Natural Image Statistics and Neural Representation},
	volume = {24},
	issn = {0147-006X, 1545-4126},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.neuro.24.1.1193},
	doi = {10.1146/annurev.neuro.24.1.1193},
	abstract = {▪ Abstract  It has long been assumed that sensory neurons are adapted, through both evolutionary and developmental processes, to the statistical properties of the signals to which they are exposed. Attneave (1954) , Barlow (1961) proposed that information theory could provide a link between environmental statistics and neural responses through the concept of coding efficiency. Recent developments in statistical modeling, along with powerful computational tools, have enabled researchers to study more sophisticated statistical models for visual images, to validate these models empirically against large sets of data, and to begin experimentally testing the efficient coding hypothesis for both individual neurons and populations of neurons.},
	pages = {1193--1216},
	number = {1},
	journaltitle = {Annual Review of Neuroscience},
	shortjournal = {Annu. Rev. Neurosci.},
	author = {Simoncelli, Eero P. and Olshausen, Bruno A.},
	urldate = {2023-06-29},
	date = {2001-03},
	langid = {english},
}

@article{intrator_objective_1992,
	title = {Objective function formulation of the {BCM} theory of visual cortical plasticity: Statistical connections, stability conditions},
	volume = {5},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608005800036},
	doi = {10.1016/S0893-6080(05)80003-6},
	shorttitle = {Objective function formulation of the {BCM} theory of visual cortical plasticity},
	pages = {3--17},
	number = {1},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Intrator, Nathan and Cooper, Leon N.},
	urldate = {2023-06-29},
	date = {1992-01},
	langid = {english},
}

@article{bienenstock_theory_1982,
	title = {Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex},
	volume = {2},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.02-01-00032.1982},
	doi = {10.1523/JNEUROSCI.02-01-00032.1982},
	shorttitle = {Theory for the development of neuron selectivity},
	abstract = {The development of stimulus selectivity in the primary sensory cortex of higher vertebrates is considered in a general mathematical framework. A synaptic evolution scheme of a new kind is proposed in which incoming patterns rather than converging afferents compete. The change in the efficacy of a given synapse depends not only on instantaneous pre- and postsynaptic activities but also on a slowly varying time-averaged value of the postsynaptic activity. Assuming an appropriate nonlinear form for this dependence, development of selectivity is obtained under quite general conditions on the sensory environment. One does not require nonlinearity of the neuron's integrative power nor does one need to assume any particular form for intracortical circuitry. This is first illustrated in simple cases, e.g., when the environment consists of only two different stimuli presented alternately in a random manner. The following formal statement then holds: the state of the system converges with probability 1 to points of maximum selectivity in the state space. We next consider the problem of early development of orientation selectivity and binocular interaction in primary visual cortex. Giving the environment an appropriate form, we obtain orientation tuning curves and ocular dominance comparable to what is observed in normally reared adult cats or monkeys. Simulations with binocular input and various types of normal or altered environments show good agreement with the relevant experimental data. Experiments are suggested that could test our theory further.},
	pages = {32--48},
	number = {1},
	journaltitle = {The Journal of Neuroscience},
	shortjournal = {J. Neurosci.},
	author = {Bienenstock, Elie L. and Cooper, Leon N. and Munro, Paul W.},
	urldate = {2023-06-29},
	date = {1982-01-01},
	langid = {english},
}

@article{oja_simplified_1982,
	title = {Simplified neuron model as a principal component analyzer},
	volume = {15},
	issn = {0303-6812, 1432-1416},
	url = {http://link.springer.com/10.1007/BF00275687},
	doi = {10.1007/BF00275687},
	pages = {267--273},
	number = {3},
	journaltitle = {Journal of Mathematical Biology},
	shortjournal = {J. Math. Biology},
	author = {Oja, Erkki},
	urldate = {2023-06-29},
	date = {1982-11},
	langid = {english},
}

@article{diamond_identifying_2019,
	title = {Identifying what makes a neuron fire},
	volume = {597},
	issn = {0022-3751, 1469-7793},
	url = {https://onlinelibrary.wiley.com/doi/10.1113/JP278049},
	doi = {10.1113/JP278049},
	pages = {2607--2608},
	number = {10},
	journaltitle = {The Journal of Physiology},
	shortjournal = {J Physiol},
	author = {Diamond, Mathew E.},
	urldate = {2023-06-28},
	date = {2019-05},
	langid = {english},
}

@article{wilson_spontaneous_1981,
	title = {Spontaneous firing patterns of identified spiny neurons in the rat neostriatum},
	volume = {220},
	issn = {00068993},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0006899381902110},
	doi = {10.1016/0006-8993(81)90211-0},
	pages = {67--80},
	number = {1},
	journaltitle = {Brain Research},
	shortjournal = {Brain Research},
	author = {Wilson, Charles J. and Groves, Philip M.},
	urldate = {2023-06-28},
	date = {1981-09},
	langid = {english},
}

@article{garcia-martin_estimation_2019,
	title = {Estimation of energy consumption in machine learning},
	volume = {134},
	issn = {07437315},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0743731518308773},
	doi = {10.1016/j.jpdc.2019.07.007},
	pages = {75--88},
	journaltitle = {Journal of Parallel and Distributed Computing},
	shortjournal = {Journal of Parallel and Distributed Computing},
	author = {García-Martín, Eva and Rodrigues, Crefeda Faviola and Riley, Graham and Grahn, Håkan},
	urldate = {2023-06-28},
	date = {2019-12},
	langid = {english},
}

@article{allenby_hierarchical_2005,
	title = {Hierarchical Bayes Models: A Practitioners Guide},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=655541},
	doi = {10.2139/ssrn.655541},
	shorttitle = {Hierarchical Bayes Models},
	journaltitle = {{SSRN} Electronic Journal},
	shortjournal = {{SSRN} Journal},
	author = {Allenby, Greg M. and Rossi, Peter E. and {McCulloch}, Robert E.},
	urldate = {2023-06-28},
	date = {2005},
	langid = {english},
}

@misc{marcus_deep_2018,
	title = {Deep Learning: A Critical Appraisal},
	url = {http://arxiv.org/abs/1801.00631},
	doi = {10.48550/arXiv.1801.00631},
	shorttitle = {Deep Learning},
	abstract = {Although deep learning has historical roots going back decades, neither the term "deep learning" nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.},
	number = {{arXiv}:1801.00631},
	publisher = {{arXiv}},
	author = {Marcus, Gary},
	urldate = {2023-06-28},
	date = {2018-01-02},
	eprinttype = {arxiv},
	eprint = {1801.00631 [cs, stat]},
	keywords = {97R40, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.0, I.2.6, Statistics - Machine Learning},
}

@article{madan_when_2022,
	title = {When and how convolutional neural networks generalize to out-of-distribution category–viewpoint combinations},
	volume = {4},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-021-00437-5},
	doi = {10.1038/s42256-021-00437-5},
	pages = {146--153},
	number = {2},
	journaltitle = {Nature Machine Intelligence},
	shortjournal = {Nat Mach Intell},
	author = {Madan, Spandan and Henry, Timothy and Dozier, Jamell and Ho, Helen and Bhandari, Nishchal and Sasaki, Tomotake and Durand, Frédo and Pfister, Hanspeter and Boix, Xavier},
	urldate = {2023-06-28},
	date = {2022-02-21},
	langid = {english},
}

@article{parisi_continual_2019,
	title = {Continual lifelong learning with neural networks: A review},
	volume = {113},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608019300231},
	doi = {10.1016/j.neunet.2019.01.012},
	shorttitle = {Continual lifelong learning with neural networks},
	pages = {54--71},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
	urldate = {2023-06-28},
	date = {2019-05},
	langid = {english},
}

@article{zhang_survey_2022,
	title = {A Survey on Multi-Task Learning},
	volume = {34},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {https://ieeexplore.ieee.org/document/9392366/},
	doi = {10.1109/TKDE.2021.3070203},
	pages = {5586--5609},
	number = {12},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{IEEE} Trans. Knowl. Data Eng.},
	author = {Zhang, Yu and Yang, Qiang},
	urldate = {2023-06-28},
	date = {2022-12-01},
}

@article{kirkpatrick_overcoming_2017,
	title = {Overcoming catastrophic forgetting in neural networks},
	volume = {114},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1611835114},
	doi = {10.1073/pnas.1611835114},
	abstract = {Significance
            Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially. In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by synaptic consolidation in neuroscience, enables state of the art results on multiple reinforcement learning problems experienced sequentially.
          , 
            The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.},
	pages = {3521--3526},
	number = {13},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
	urldate = {2023-06-28},
	date = {2017-03-28},
	langid = {english},
}

@article{liu_overcoming_2021,
	title = {Overcoming Catastrophic Forgetting in Graph Neural Networks},
	volume = {35},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17049},
	doi = {10.1609/aaai.v35i10.17049},
	abstract = {Catastrophic forgetting refers to the tendency that a neural network ``forgets'' the previous learned knowledge upon learning new tasks. Prior methods have been focused on overcoming this problem on convolutional neural networks ({CNNs}), where the input samples like images lie in a grid domain, but have largely overlooked graph neural networks ({GNNs}) that handle non-grid data. In this paper, we propose a novel scheme dedicated to overcoming catastrophic forgetting problem and hence strengthen continual learning in {GNNs}. At the heart of our approach is a generic module, termed as topology-aware weight preserving ({TWP}), applicable to arbitrary form of {GNNs} in a plug-and-play fashion. Unlike the main stream of {CNN}-based continual learning methods that rely on solely slowing down the updates of parameters important to the downstream task, {TWP} explicitly explores the local structures of the input graph, and attempts to stabilize the parameters playing pivotal roles in the topological aggregation. We evaluate {TWP} on different {GNN} backbones over several datasets, and demonstrate that it yields performances superior to the state of the art. Code is publicly available at https://github.com/hhliu79/{TWP}.},
	pages = {8653--8661},
	number = {10},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	shortjournal = {{AAAI}},
	author = {Liu, Huihui and Yang, Yiding and Wang, Xinchao},
	urldate = {2023-06-28},
	date = {2021-05-18},
}

@article{choudhary_comprehensive_2020,
	title = {A comprehensive survey on model compression and acceleration},
	volume = {53},
	issn = {0269-2821, 1573-7462},
	url = {http://link.springer.com/10.1007/s10462-020-09816-7},
	doi = {10.1007/s10462-020-09816-7},
	pages = {5113--5155},
	number = {7},
	journaltitle = {Artificial Intelligence Review},
	shortjournal = {Artif Intell Rev},
	author = {Choudhary, Tejalal and Mishra, Vipul and Goswami, Anurag and Sarangapani, Jagannathan},
	urldate = {2023-06-28},
	date = {2020-10},
	langid = {english},
}

@misc{wu_integer_2020,
	title = {Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation},
	url = {http://arxiv.org/abs/2004.09602},
	doi = {10.48550/arXiv.2004.09602},
	shorttitle = {Integer Quantization for Deep Learning Inference},
	abstract = {Quantization techniques can reduce the size of Deep Neural Networks and improve inference latency and throughput by taking advantage of high throughput integer instructions. In this paper we review the mathematical aspects of quantization parameters and evaluate their choices on a wide range of neural network models for different application domains, including vision, speech, and language. We focus on quantization techniques that are amenable to acceleration by processors with high-throughput integer math pipelines. We also present a workflow for 8-bit quantization that is able to maintain accuracy within 1\% of the floating-point baseline on all networks studied, including models that are more difficult to quantize, such as {MobileNets} and {BERT}-large.},
	number = {{arXiv}:2004.09602},
	publisher = {{arXiv}},
	author = {Wu, Hao and Judd, Patrick and Zhang, Xiaojie and Isaev, Mikhail and Micikevicius, Paulius},
	urldate = {2023-06-28},
	date = {2020-04-20},
	eprinttype = {arxiv},
	eprint = {2004.09602 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{berthelier_deep_2021,
	title = {Deep Model Compression and Architecture Optimization for Embedded Systems: A Survey},
	volume = {93},
	issn = {1939-8018, 1939-8115},
	url = {https://link.springer.com/10.1007/s11265-020-01596-1},
	doi = {10.1007/s11265-020-01596-1},
	shorttitle = {Deep Model Compression and Architecture Optimization for Embedded Systems},
	pages = {863--878},
	number = {8},
	journaltitle = {Journal of Signal Processing Systems},
	shortjournal = {J Sign Process Syst},
	author = {Berthelier, Anthony and Chateau, Thierry and Duffner, Stefan and Garcia, Christophe and Blanc, Christophe},
	urldate = {2023-06-28},
	date = {2021-08},
	langid = {english},
}

@misc{smith_using_2022,
	title = {Using {DeepSpeed} and Megatron to Train Megatron-Turing {NLG} 530B, A Large-Scale Generative Language Model},
	url = {http://arxiv.org/abs/2201.11990},
	doi = {10.48550/arXiv.2201.11990},
	abstract = {Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and {NVIDIA}, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing {NLG} 530B ({MT}-{NLG}), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using {DeepSpeed} and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by {MT}-{NLG}. We demonstrate that {MT}-{NLG} achieves superior zero-, one-, and few-shot learning accuracies on several {NLP} benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.},
	number = {{arXiv}:2201.11990},
	publisher = {{arXiv}},
	author = {Smith, Shaden and Patwary, Mostofa and Norick, Brandon and {LeGresley}, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and Zhang, Elton and Child, Rewon and Aminabadi, Reza Yazdani and Bernauer, Julie and Song, Xia and Shoeybi, Mohammad and He, Yuxiong and Houston, Michael and Tiwary, Saurabh and Catanzaro, Bryan},
	urldate = {2023-06-28},
	date = {2022-02-04},
	eprinttype = {arxiv},
	eprint = {2201.11990 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{kumar_fundamental_2015,
	title = {Fundamental Limits to Moore's Law},
	url = {http://arxiv.org/abs/1511.05956},
	doi = {10.48550/arXiv.1511.05956},
	abstract = {The theoretical and practical aspects of the fundamental, ultimate, physical limits to scaling, or Moore-s law, is presented.},
	number = {{arXiv}:1511.05956},
	publisher = {{arXiv}},
	author = {Kumar, Suhas},
	urldate = {2023-06-27},
	date = {2015-11-17},
	eprinttype = {arxiv},
	eprint = {1511.05956 [cond-mat]},
	keywords = {Condensed Matter - Mesoscale and Nanoscale Physics},
}

@article{moore_cramming_1965,
	title = {Cramming More Components onto Integrated Circuits},
	volume = {38},
	pages = {114--1116},
	number = {8},
	journaltitle = {Electronics},
	author = {Moore, Gordon E.},
	date = {1965-04},
	note = {Publisher: {McGraw}-Hill New York},
}

@article{asgari_taghanaki_deep_2021,
	title = {Deep semantic segmentation of natural and medical images: a review},
	volume = {54},
	issn = {0269-2821, 1573-7462},
	url = {https://link.springer.com/10.1007/s10462-020-09854-1},
	doi = {10.1007/s10462-020-09854-1},
	shorttitle = {Deep semantic segmentation of natural and medical images},
	pages = {137--178},
	number = {1},
	journaltitle = {Artificial Intelligence Review},
	shortjournal = {Artif Intell Rev},
	author = {Asgari Taghanaki, Saeid and Abhishek, Kumar and Cohen, Joseph Paul and Cohen-Adad, Julien and Hamarneh, Ghassan},
	urldate = {2023-06-27},
	date = {2021-01},
	langid = {english},
}

@article{zou_object_2023,
	title = {Object Detection in 20 Years: A Survey},
	volume = {111},
	issn = {0018-9219, 1558-2256},
	url = {https://ieeexplore.ieee.org/document/10028728/},
	doi = {10.1109/JPROC.2023.3238524},
	shorttitle = {Object Detection in 20 Years},
	pages = {257--276},
	number = {3},
	journaltitle = {Proceedings of the {IEEE}},
	shortjournal = {Proc. {IEEE}},
	author = {Zou, Zhengxia and Chen, Keyan and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
	urldate = {2023-06-27},
	date = {2023-03},
}

@article{schmarje_survey_2021,
	title = {A Survey on Semi-, Self- and Unsupervised Learning for Image Classification},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9442775/},
	doi = {10.1109/ACCESS.2021.3084358},
	pages = {82146--82168},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Schmarje, Lars and Santarossa, Monty and Schroder, Simon-Martin and Koch, Reinhard},
	urldate = {2023-06-27},
	date = {2021},
}

@online{venture_beat_new_2023,
	title = {New Deep Learning Model Brings Image Segmentation to Edge Devices},
	url = {https://venturebeat.com/ai/new-deep-learning-model-brings-image-segmentation-to-edge-devices/},
	author = {{Venture Beat}},
	urldate = {2023-02-19},
	date = {2023},
}

@article{sharma_implications_2019,
	title = {Implications of Pooling Strategies in Convolutional Neural Networks: A Deep Insight},
	volume = {44},
	issn = {2300-3405},
	url = {https://www.sciendo.com/article/10.2478/fcds-2019-0016},
	doi = {10.2478/fcds-2019-0016},
	shorttitle = {Implications of Pooling Strategies in Convolutional Neural Networks},
	abstract = {Abstract
            Convolutional neural networks ({CNN}) is a contemporary technique for computer vision applications, where pooling implies as an integral part of the deep {CNN}. Besides, pooling provides the ability to learn invariant features and also acts as a regularizer to further reduce the problem of overfitting. Additionally, the pooling techniques significantly reduce the computational cost and training time of networks which are equally important to consider. Here, the performances of pooling strategies on different datasets are analyzed and discussed qualitatively. This study presents a detailed review of the conventional and the latest strategies which would help in appraising the readers with the upsides and downsides of each strategy. Also, we have identified four fundamental factors namely network architecture, activation function, overlapping and regularization approaches which immensely affect the performance of pooling operations. It is believed that this work would help in extending the scope of understanding the significance of {CNN} along with pooling regimes for solving computer vision problems.},
	pages = {303--330},
	number = {3},
	journaltitle = {Foundations of Computing and Decision Sciences},
	author = {Sharma, Shallu and Mehra, Rajesh},
	urldate = {2023-06-27},
	date = {2019-09-01},
	langid = {english},
}

@article{sharma_implications_2019-1,
	title = {Implications of Pooling Strategies in Convolutional Neural Networks: A Deep Insight},
	volume = {44},
	issn = {2300-3405},
	url = {https://www.sciendo.com/article/10.2478/fcds-2019-0016},
	doi = {10.2478/fcds-2019-0016},
	shorttitle = {Implications of Pooling Strategies in Convolutional Neural Networks},
	abstract = {Abstract
            Convolutional neural networks ({CNN}) is a contemporary technique for computer vision applications, where pooling implies as an integral part of the deep {CNN}. Besides, pooling provides the ability to learn invariant features and also acts as a regularizer to further reduce the problem of overfitting. Additionally, the pooling techniques significantly reduce the computational cost and training time of networks which are equally important to consider. Here, the performances of pooling strategies on different datasets are analyzed and discussed qualitatively. This study presents a detailed review of the conventional and the latest strategies which would help in appraising the readers with the upsides and downsides of each strategy. Also, we have identified four fundamental factors namely network architecture, activation function, overlapping and regularization approaches which immensely affect the performance of pooling operations. It is believed that this work would help in extending the scope of understanding the significance of {CNN} along with pooling regimes for solving computer vision problems.},
	pages = {303--330},
	number = {3},
	journaltitle = {Foundations of Computing and Decision Sciences},
	author = {Sharma, Shallu and Mehra, Rajesh},
	urldate = {2023-06-27},
	date = {2019-09-01},
	langid = {english},
}

@article{zhang_parallel_1990,
	title = {Parallel distributed processing model with local space-invariant interconnections and its optical architecture},
	volume = {29},
	issn = {0003-6935, 1539-4522},
	url = {https://opg.optica.org/abstract.cfm?URI=ao-29-32-4790},
	doi = {10.1364/AO.29.004790},
	pages = {4790},
	number = {32},
	journaltitle = {Applied Optics},
	shortjournal = {Appl. Opt.},
	author = {Zhang, Wei and Itoh, Kazuyoshi and Tanida, Jun and Ichioka, Yoshiki},
	urldate = {2023-06-27},
	date = {1990-11-10},
	langid = {english},
}

@article{lecun_backpropagation_1989,
	title = {Backpropagation Applied to Handwritten Zip Code Recognition},
	volume = {1},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/1/4/541-551/5515},
	doi = {10.1162/neco.1989.1.4.541},
	abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
	pages = {541--551},
	number = {4},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {{LeCun}, Yann and Boser, Bernhard and Denker, John S. and Henderson, Donnie and Howard, Richard E. and Hubbard, Wayne and Jackel, Lawrence D.},
	urldate = {2023-06-27},
	date = {1989-12},
	langid = {english},
}

@article{waibel_phoneme_1989,
	title = {Phoneme recognition using time-delay neural networks},
	volume = {37},
	issn = {00963518},
	url = {http://ieeexplore.ieee.org/document/21701/},
	doi = {10.1109/29.21701},
	pages = {328--339},
	number = {3},
	journaltitle = {{IEEE} Transactions on Acoustics, Speech, and Signal Processing},
	shortjournal = {{IEEE} Trans. Acoust., Speech, Signal Processing},
	author = {Waibel, Alexander and Hanazawa, Toshiyuki and Hinton, Geoffrey E. and Shikano, Kiyohiro and Lang, Kevin J.},
	urldate = {2023-06-27},
	date = {1989-03},
}

@article{fukushima_neocognitron_1980,
	title = {Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
	volume = {36},
	issn = {0340-1200, 1432-0770},
	url = {http://link.springer.com/10.1007/BF00344251},
	doi = {10.1007/BF00344251},
	shorttitle = {Neocognitron},
	pages = {193--202},
	number = {4},
	journaltitle = {Biological Cybernetics},
	shortjournal = {Biol. Cybernetics},
	author = {Fukushima, Kunihiko},
	urldate = {2023-06-27},
	date = {1980-04},
	langid = {english},
}

@article{fukushima_neocognitron_2007,
	title = {Neocognitron},
	volume = {2},
	issn = {1941-6016},
	url = {http://www.scholarpedia.org/article/Neocognitron},
	doi = {10.4249/scholarpedia.1717},
	pages = {1717},
	number = {1},
	journaltitle = {Scholarpedia},
	shortjournal = {Scholarpedia},
	author = {Fukushima, Kunihiko},
	urldate = {2023-06-27},
	date = {2007},
	langid = {english},
}

@article{hubel_receptive_1968,
	title = {Receptive fields and functional architecture of monkey striate cortex},
	volume = {195},
	issn = {00223751},
	url = {https://onlinelibrary.wiley.com/doi/10.1113/jphysiol.1968.sp008455},
	doi = {10.1113/jphysiol.1968.sp008455},
	pages = {215--243},
	number = {1},
	journaltitle = {The Journal of Physiology},
	author = {Hubel, D. H. and Wiesel, T. N.},
	urldate = {2023-06-27},
	date = {1968-03-01},
	langid = {english},
}

@misc{kingma_adam_2017,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	number = {{arXiv}:1412.6980},
	publisher = {{arXiv}},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2023-06-27},
	date = {2017-01-29},
	eprinttype = {arxiv},
	eprint = {1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{wang_comprehensive_2022,
	title = {A Comprehensive Survey of Loss Functions in Machine Learning},
	volume = {9},
	issn = {2198-5804, 2198-5812},
	url = {https://link.springer.com/10.1007/s40745-020-00253-5},
	doi = {10.1007/s40745-020-00253-5},
	pages = {187--212},
	number = {2},
	journaltitle = {Annals of Data Science},
	shortjournal = {Ann. Data. Sci.},
	author = {Wang, Qi and Ma, Yue and Zhao, Kun and Tian, Yingjie},
	urldate = {2023-06-27},
	date = {2022-04},
	langid = {english},
}

@article{liu_self-supervised_2021,
	title = {Self-supervised Learning: Generative or Contrastive},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {https://ieeexplore.ieee.org/document/9462394/},
	doi = {10.1109/TKDE.2021.3090866},
	shorttitle = {Self-supervised Learning},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{IEEE} Trans. Knowl. Data Eng.},
	author = {Liu, Xiao and Zhang, Fanjin and Hou, Zhenyu and Mian, Li and Wang, Zhaoyu and Zhang, Jing and Tang, Jie},
	urldate = {2023-06-26},
	date = {2021},
}

@article{van_engelen_survey_2020,
	title = {A survey on semi-supervised learning},
	volume = {109},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-019-05855-6},
	doi = {10.1007/s10994-019-05855-6},
	abstract = {Abstract
            Semi-supervised learning is the branch of machine learning concerned with using labelled as well as unlabelled data to perform certain learning tasks. Conceptually situated between supervised and unsupervised learning, it permits harnessing the large amounts of unlabelled data available in many use cases in combination with typically smaller sets of labelled data. In recent years, research in this area has followed the general trends observed in machine learning, with much attention directed at neural network-based models and generative learning. The literature on the topic has also expanded in volume and scope, now encompassing a broad spectrum of theory, algorithms and applications. However, no recent surveys exist to collect and organize this knowledge, impeding the ability of researchers and engineers alike to utilize it. Filling this void, we present an up-to-date overview of semi-supervised learning methods, covering earlier work as well as more recent advances. We focus primarily on semi-supervised classification, where the large majority of semi-supervised learning research takes place. Our survey aims to provide researchers and practitioners new to the field as well as more advanced readers with a solid understanding of the main approaches and algorithms developed over the past two decades, with an emphasis on the most prominent and currently relevant work. Furthermore, we propose a new taxonomy of semi-supervised classification algorithms, which sheds light on the different conceptual and methodological approaches for incorporating unlabelled data into the training process. Lastly, we show how the fundamental assumptions underlying most semi-supervised learning algorithms are closely connected to each other, and how they relate to the well-known semi-supervised clustering assumption.},
	pages = {373--440},
	number = {2},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Van Engelen, Jesper E. and Hoos, Holger H.},
	urldate = {2023-06-26},
	date = {2020-02},
	langid = {english},
}

@article{arulkumaran_deep_2017,
	title = {Deep Reinforcement Learning: A Brief Survey},
	volume = {34},
	issn = {1053-5888},
	url = {http://ieeexplore.ieee.org/document/8103164/},
	doi = {10.1109/MSP.2017.2743240},
	shorttitle = {Deep Reinforcement Learning},
	pages = {26--38},
	number = {6},
	journaltitle = {{IEEE} Signal Processing Magazine},
	shortjournal = {{IEEE} Signal Process. Mag.},
	author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
	urldate = {2023-06-26},
	date = {2017-11},
}

@book{rosenblatt_principles_1962,
	title = {Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms},
	url = {https://books.google.ch/books?id=7FhRAAAAMAAJ},
	series = {Cornell Aeronautical Laboratory. Report no. {VG}-1196-G-8},
	publisher = {Spartan Books},
	author = {Rosenblatt, Frank},
	date = {1962},
	lccn = {62012882},
}

@article{linnainmaa_taylor_1976,
	title = {Taylor expansion of the accumulated rounding error},
	volume = {16},
	issn = {0006-3835, 1572-9125},
	url = {http://link.springer.com/10.1007/BF01931367},
	doi = {10.1007/BF01931367},
	pages = {146--160},
	number = {2},
	journaltitle = {{BIT}},
	shortjournal = {{BIT}},
	author = {Linnainmaa, Seppo},
	urldate = {2023-06-26},
	date = {1976-06},
	langid = {english},
}

@article{linnainmaa_taylor_1976-1,
	title = {Taylor expansion of the accumulated rounding error},
	volume = {16},
	issn = {0006-3835, 1572-9125},
	url = {http://link.springer.com/10.1007/BF01931367},
	doi = {10.1007/BF01931367},
	pages = {146--160},
	number = {2},
	journaltitle = {{BIT}},
	shortjournal = {{BIT}},
	author = {Linnainmaa, Seppo},
	urldate = {2023-06-26},
	date = {1976-06},
	langid = {english},
}

@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {0932-4194, 1435-568X},
	url = {http://link.springer.com/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	pages = {303--314},
	number = {4},
	journaltitle = {Mathematics of Control, Signals, and Systems},
	shortjournal = {Math. Control Signal Systems},
	author = {Cybenko, G.},
	urldate = {2023-06-26},
	date = {1989-12},
	langid = {english},
}

@article{fukushima_visual_1969,
	title = {Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements},
	volume = {5},
	issn = {0536-1567},
	url = {http://ieeexplore.ieee.org/document/4082265/},
	doi = {10.1109/TSSC.1969.300225},
	pages = {322--333},
	number = {4},
	journaltitle = {{IEEE} Transactions on Systems Science and Cybernetics},
	shortjournal = {{IEEE} Trans. Syst. Sci. Cyber.},
	author = {Fukushima, Kunihiko},
	urldate = {2023-06-26},
	date = {1969},
}

@book{hebb_organization_1949,
	location = {Oxford,  England},
	title = {The Organization of Behavior; A Neuropsychological Theory},
	publisher = {Psychology Press},
	author = {Hebb, Donald O.},
	date = {1949},
}

@article{coombs_specific_1955,
	title = {The specific ionic conductances and the ionic movements across the motoneuronal membrane that produce the inhibitory post-synaptic potential},
	volume = {130},
	issn = {00223751},
	url = {https://onlinelibrary.wiley.com/doi/10.1113/jphysiol.1955.sp005412},
	doi = {10.1113/jphysiol.1955.sp005412},
	pages = {326--373},
	number = {2},
	journaltitle = {The Journal of Physiology},
	author = {Coombs, Jo S. and Eccles, John C. and Fatt, Paul},
	urldate = {2023-06-26},
	date = {1955-11-28},
	langid = {english},
}

@article{takagi_roles_2000,
	title = {Roles of ion channels in {EPSP} integration at neuronal dendrites},
	volume = {37},
	issn = {01680102},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168010200001206},
	doi = {10.1016/S0168-0102(00)00120-6},
	pages = {167--171},
	number = {3},
	journaltitle = {Neuroscience Research},
	shortjournal = {Neuroscience Research},
	author = {Takagi, Hiroshi},
	urldate = {2023-06-26},
	date = {2000-07},
	langid = {english},
}

@article{felleman_distributed_1991,
	title = {Distributed Hierarchical Processing in the Primate Cerebral Cortex},
	volume = {1},
	issn = {1047-3211, 1460-2199},
	url = {https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/1.1.1},
	doi = {10.1093/cercor/1.1.1},
	pages = {1--47},
	number = {1},
	journaltitle = {Cerebral Cortex},
	shortjournal = {Cerebral Cortex},
	author = {Felleman, Daniel J. and Van Essen, David C.},
	urldate = {2023-06-26},
	date = {1991-01-01},
	langid = {english},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	pages = {533--536},
	number = {6088},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	urldate = {2023-06-26},
	date = {1986-10},
	langid = {english},
}

@article{herculano-houzel_human_2009,
	title = {The human brain in numbers: a linearly scaled-up primate brain},
	volume = {3},
	issn = {16625161},
	url = {http://journal.frontiersin.org/article/10.3389/neuro.09.031.2009/abstract},
	doi = {10.3389/neuro.09.031.2009},
	shorttitle = {The human brain in numbers},
	journaltitle = {Frontiers in Human Neuroscience},
	shortjournal = {Front. Hum. Neurosci.},
	author = {Herculano-Houzel, Suzana},
	urldate = {2023-06-26},
	date = {2009},
}

@article{zeng_neuronal_2017,
	title = {Neuronal cell-type classification: challenges, opportunities and the path forward},
	volume = {18},
	issn = {1471-003X, 1471-0048},
	url = {https://www.nature.com/articles/nrn.2017.85},
	doi = {10.1038/nrn.2017.85},
	shorttitle = {Neuronal cell-type classification},
	pages = {530--546},
	number = {9},
	journaltitle = {Nature Reviews Neuroscience},
	shortjournal = {Nat Rev Neurosci},
	author = {Zeng, Hongkui and Sanes, Joshua R.},
	urldate = {2023-06-26},
	date = {2017-09},
	langid = {english},
}

@book{james_principles_1890,
	title = {The principles of psychology},
	pagetotal = {1393},
	publisher = {Henry Holt and Company},
	author = {James, William},
	date = {1890},
}

@misc{radford_improving_2018,
	title = {Improving language understanding by generative pre-training},
	url = {https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	urldate = {2023-06-24},
	date = {2018},
}

@article{wolfrum_recurrent_2008,
	title = {A recurrent dynamic model for correspondence-based face recognition},
	volume = {8},
	issn = {1534-7362},
	url = {http://jov.arvojournals.org/article.aspx?doi=10.1167/8.7.34},
	doi = {10.1167/8.7.34},
	pages = {34},
	number = {7},
	journaltitle = {Journal of Vision},
	shortjournal = {Journal of Vision},
	author = {Wolfrum, Philipp and Wolff, Christian and Lücke, Jörg and von der Malsburg, Christoph},
	urldate = {2023-03-06},
	date = {2008-12-29},
	langid = {english},
}

@misc{von_der_malsburg_theory_2022,
	title = {A Theory of Natural Intelligence},
	url = {http://arxiv.org/abs/2205.00002},
	doi = {10.48550/arXiv.2205.00002},
	abstract = {Introduction: In contrast to current {AI} technology, natural intelligence -- the kind of autonomous intelligence that is realized in the brains of animals and humans to attain in their natural environment goals defined by a repertoire of innate behavioral schemata -- is far superior in terms of learning speed, generalization capabilities, autonomy and creativity. How are these strengths, by what means are ideas and imagination produced in natural neural networks? Methods: Reviewing the literature, we put forward the argument that both our natural environment and the brain are of low complexity, that is, require for their generation very little information and are consequently both highly structured. We further argue that the structures of brain and natural environment are closely related. Results: We propose that the structural regularity of the brain takes the form of net fragments (self-organized network patterns) and that these serve as the powerful inductive bias that enables the brain to learn quickly, generalize from few examples and bridge the gap between abstractly defined general goals and concrete situations. Conclusions: Our results have important bearings on open problems in artificial neural network research.},
	number = {{arXiv}:2205.00002},
	publisher = {{arXiv}},
	author = {von der Malsburg, Christoph and Stadelmann, Thilo and Grewe, Benjamin F.},
	urldate = {2022-07-11},
	date = {2022-04-22},
	eprinttype = {arxiv},
	eprint = {2205.00002 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, I.2, Quantitative Biology - Neurons and Cognition},
}

@article{yarats_improving_2021,
	title = {Improving Sample Efficiency in Model-Free Reinforcement Learning from Images},
	volume = {35},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17276},
	doi = {10.1609/aaai.v35i12.17276},
	abstract = {Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning ({RL}) has proven difficult. A promising approach is to learn a latent representation together with the control policy. However, fitting a high-capacity encoder using a scarce reward signal is sample inefficient and leads to poor performance.
Prior work has shown that auxiliary losses, such as image reconstruction, can aid efficient representation learning.  
However, incorporating reconstruction loss into an off-policy learning algorithm often leads to training instability. We explore the underlying reasons and 
identify variational autoencoders, used by previous investigations, as the cause of the divergence.   
Following these findings, we propose effective techniques to improve training stability. 
This results in a simple approach capable of
matching state-of-the-art model-free and model-based algorithms on {MuJoCo} control tasks. Furthermore, our approach demonstrates robustness to observational noise, surpassing existing approaches in this setting. Code, results, and videos are anonymously available at https://sites.google.com/view/sac-ae/home.},
	pages = {10674--10681},
	number = {12},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	shortjournal = {{AAAI}},
	author = {Yarats, Denis and Zhang, Amy and Kostrikov, Ilya and Amos, Brandon and Pineau, Joelle and Fergus, Rob},
	urldate = {2023-06-24},
	date = {2021-05-18},
}

@article{sager_unsupervised_2022,
	title = {Unsupervised Domain Adaptation for Vertebrae Detection and Identification in 3D {CT} Volumes Using a Domain Sanity Loss},
	volume = {8},
	rights = {All rights reserved},
	issn = {2313-433X},
	url = {https://www.mdpi.com/2313-433X/8/8/222},
	doi = {10.3390/jimaging8080222},
	abstract = {A variety of medical computer vision applications analyze 2D slices of computed tomography ({CT}) scans, whereas axial slices from the body trunk region are usually identified based on their relative position to the spine. A limitation of such systems is that either the correct slices must be extracted manually or labels of the vertebrae are required for each {CT} scan to develop an automated extraction system. In this paper, we propose an unsupervised domain adaptation ({UDA}) approach for vertebrae detection and identification based on a novel Domain Sanity Loss ({DSL}) function. With {UDA} the model’s knowledge learned on a publicly available (source) data set can be transferred to the target domain without using target labels, where the target domain is defined by the specific setup ({CT} modality, study protocols, applied pre- and processing) at the point of use (e.g., a specific clinic with its specific {CT} study protocols). With our approach, a model is trained on the source and target data set in parallel. The model optimizes a supervised loss for labeled samples from the source domain and the {DSL} loss function based on domain-specific “sanity checks” for samples from the unlabeled target domain. Without using labels from the target domain, we are able to identify vertebra centroids with an accuracy of 72.8\%. By adding only ten target labels during training the accuracy increases to 89.2\%, which is on par with the current state-of-the-art for full supervised learning, while using about 20 times less labels. Thus, our model can be used to extract 2D slices from 3D {CT} scans on arbitrary data sets fully automatically without requiring an extensive labeling effort, contributing to the clinical adoption of medical imaging by hospitals.},
	pages = {222},
	number = {8},
	journaltitle = {Journal of Imaging},
	shortjournal = {J. Imaging},
	author = {Sager, Pascal and Salzmann, Sebastian and Burn, Felice and Stadelmann, Thilo},
	urldate = {2022-09-01},
	date = {2022-08-19},
	langid = {english},
}

@article{long_survey_2022,
	title = {A survey on adversarial attacks in computer vision: Taxonomy, visualization and future directions},
	volume = {121},
	issn = {01674048},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167404822002413},
	doi = {10.1016/j.cose.2022.102847},
	shorttitle = {A survey on adversarial attacks in computer vision},
	pages = {102847},
	journaltitle = {Computers \& Security},
	shortjournal = {Computers \& Security},
	author = {Long, Teng and Gao, Qi and Xu, Lili and Zhou, Zhangbing},
	urldate = {2023-06-24},
	date = {2022-10},
	langid = {english},
}

@misc{rosenbloom_defining_2023,
	title = {Defining and Explorting the Intelligence Space},
	url = {http://arxiv.org/abs/2306.06499},
	doi = {10.48550/arXiv.2306.06499},
	abstract = {Intelligence is a difficult concept to define, despite many attempts at doing so. Rather than trying to settle on a single definition, this article introduces a broad perspective on what intelligence is, by laying out a cascade of definitions that induces both a nested hierarchy of three levels of intelligence and a wider-ranging space that is built around them and approximations to them. Within this intelligence space, regions are identified that correspond to both natural -- most particularly, human -- intelligence and artificial intelligence ({AI}), along with the crossover notion of humanlike intelligence. These definitions are then exploited in early explorations of four more advanced, and likely more controversial, topics: the singularity, generative {AI}, ethics, and intellectual property.},
	number = {{arXiv}:2306.06499},
	publisher = {{arXiv}},
	author = {Rosenbloom, Paul S.},
	urldate = {2023-06-24},
	date = {2023-06-16},
	eprinttype = {arxiv},
	eprint = {2306.06499 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{mitchell_debate_2023,
	title = {The debate over understanding in {AI}’s large language models},
	volume = {120},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/10.1073/pnas.2215907120},
	doi = {10.1073/pnas.2215907120},
	abstract = {We survey a current, heated debate in the artificial intelligence ({AI}) research community on whether large pretrained language models can be said to understand language—and the physical and social situations language encodes—in any humanlike sense. We describe arguments that have been made for and against such understanding and key questions for the broader sciences of intelligence that have arisen in light of these arguments. We contend that an extended science of intelligence can be developed that will provide insight into distinct modes of understanding, their strengths and limitations, and the challenge of integrating diverse forms of cognition.},
	pages = {e2215907120},
	number = {13},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Mitchell, Melanie and Krakauer, David C.},
	urldate = {2023-06-24},
	date = {2023-03-28},
	langid = {english},
}

@article{bertolini_machine_2021,
	title = {Machine Learning for industrial applications: A comprehensive literature review},
	volume = {175},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095741742100261X},
	doi = {10.1016/j.eswa.2021.114820},
	shorttitle = {Machine Learning for industrial applications},
	pages = {114820},
	journaltitle = {Expert Systems with Applications},
	shortjournal = {Expert Systems with Applications},
	author = {Bertolini, Massimo and Mezzogori, Davide and Neroni, Mattia and Zammori, Francesco},
	urldate = {2023-06-24},
	date = {2021-08},
	langid = {english},
}

@article{ning_review_2019,
	title = {A Review of Deep Learning Based Speech Synthesis},
	volume = {9},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/9/19/4050},
	doi = {10.3390/app9194050},
	abstract = {Speech synthesis, also known as text-to-speech ({TTS}), has attracted increasingly more attention. Recent advances on speech synthesis are overwhelmingly contributed by deep learning or even end-to-end techniques which have been utilized to enhance a wide range of application scenarios such as intelligent speech interaction, chatbot or conversational artificial intelligence ({AI}). For speech synthesis, deep learning based techniques can leverage a large scale of {\textless}text, speech{\textgreater} pairs to learn effective feature representations to bridge the gap between text and speech, thus better characterizing the properties of events. To better understand the research dynamics in the speech synthesis field, this paper firstly introduces the traditional speech synthesis methods and highlights the importance of the acoustic modeling from the composition of the statistical parametric speech synthesis ({SPSS}) system. It then gives an overview of the advances on deep learning based speech synthesis, including the end-to-end approaches which have achieved start-of-the-art performance in recent years. Finally, it discusses the problems of the deep learning methods for speech synthesis, and also points out some appealing research directions that can bring the speech synthesis research into a new frontier.},
	pages = {4050},
	number = {19},
	journaltitle = {Applied Sciences},
	shortjournal = {Applied Sciences},
	author = {Ning, Yishuang and He, Sheng and Wu, Zhiyong and Xing, Chunxiao and Zhang, Liang-Jie},
	urldate = {2023-06-24},
	date = {2019-09-27},
	langid = {english},
}

@article{otter_survey_2021,
	title = {A Survey of the Usages of Deep Learning for Natural Language Processing},
	volume = {32},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/9075398/},
	doi = {10.1109/TNNLS.2020.2979670},
	pages = {604--624},
	number = {2},
	journaltitle = {{IEEE} Transactions on Neural Networks and Learning Systems},
	shortjournal = {{IEEE} Trans. Neural Netw. Learning Syst.},
	author = {Otter, Daniel W. and Medina, Julian R. and Kalita, Jugal K.},
	urldate = {2023-06-24},
	date = {2021-02},
}

@article{bhatt_cnn_2021,
	title = {{CNN} Variants for Computer Vision: History, Architecture, Application, Challenges and Future Scope},
	volume = {10},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/10/20/2470},
	doi = {10.3390/electronics10202470},
	shorttitle = {{CNN} Variants for Computer Vision},
	abstract = {Computer vision is becoming an increasingly trendy word in the area of image processing. With the emergence of computer vision applications, there is a significant demand to recognize objects automatically. Deep {CNN} (convolution neural network) has benefited the computer vision community by producing excellent results in video processing, object recognition, picture classification and segmentation, natural language processing, speech recognition, and many other fields. Furthermore, the introduction of large amounts of data and readily available hardware has opened new avenues for {CNN} study. Several inspirational concepts for the progress of {CNN} have been investigated, including alternative activation functions, regularization, parameter optimization, and architectural advances. Furthermore, achieving innovations in architecture results in a tremendous enhancement in the capacity of the deep {CNN}. Significant emphasis has been given to leveraging channel and spatial information, with a depth of architecture and information processing via multi-path. This survey paper focuses mainly on the primary taxonomy and newly released deep {CNN} architectures, and it divides numerous recent developments in {CNN} architectures into eight groups. Spatial exploitation, multi-path, depth, breadth, dimension, channel boosting, feature-map exploitation, and attention-based {CNN} are the eight categories. The main contribution of this manuscript is in comparing various architectural evolutions in {CNN} by its architectural change, strengths, and weaknesses. Besides, it also includes an explanation of the {CNN}’s components, the strengths and weaknesses of various {CNN} variants, research gap or open challenges, {CNN} applications, and the future research direction.},
	pages = {2470},
	number = {20},
	journaltitle = {Electronics},
	shortjournal = {Electronics},
	author = {Bhatt, Dulari and Patel, Chirag and Talsania, Hardik and Patel, Jigar and Vaghela, Rasmika and Pandya, Sharnil and Modi, Kirit and Ghayvat, Hemant},
	urldate = {2023-06-24},
	date = {2021-10-11},
	langid = {english},
}

@article{dabre_survey_2021,
	title = {A Survey of Multilingual Neural Machine Translation},
	volume = {53},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3406095},
	doi = {10.1145/3406095},
	abstract = {We present a survey on multilingual neural machine translation ({MNMT}), which has gained a lot of traction in recent years. {MNMT} has been useful in improving translation quality as a result of translation knowledge transfer (transfer learning). {MNMT} is more promising and interesting than its statistical machine translation counterpart, because end-to-end modeling and distributed representations open new avenues for research on machine translation. Many approaches have been proposed to exploit multilingual parallel corpora for improving translation quality. However, the lack of a comprehensive survey makes it difficult to determine which approaches are promising and, hence, deserve further exploration. In this article, we present an in-depth survey of existing literature on {MNMT}. We first categorize various approaches based on their central use-case and then further categorize them based on resource scenarios, underlying modeling principles, core-issues, and challenges. Wherever possible, we address the strengths and weaknesses of several techniques by comparing them with each other. We also discuss the future directions for {MNMT}. This article is aimed towards both beginners and experts in {NMT}. We hope this article will serve as a starting point as well as a source of new ideas for researchers and engineers interested in {MNMT}.},
	pages = {1--38},
	number = {5},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Dabre, Raj and Chu, Chenhui and Kunchukuttan, Anoop},
	urldate = {2023-06-24},
	date = {2021-09-30},
	langid = {english},
}

@misc{liu_summary_2023,
	title = {Summary of {ChatGPT}/{GPT}-4 Research and Perspective Towards the Future of Large Language Models},
	url = {http://arxiv.org/abs/2304.01852},
	doi = {10.48550/arXiv.2304.01852},
	abstract = {This paper presents a comprehensive survey of {ChatGPT} and {GPT}-4, state-of-the-art large language models ({LLM}) from the {GPT} series, and their prospective applications across diverse domains. Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback ({RLHF}) have played significant roles in enhancing {LLMs}' adaptability and performance. We performed an in-depth analysis of 194 relevant papers on {arXiv}, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains. The findings reveal a significant and increasing interest in {ChatGPT}/{GPT}-4 research, predominantly centered on direct natural language processing applications, while also demonstrating considerable potential in areas ranging from education and history to mathematics, medicine, and physics. This study endeavors to furnish insights into {ChatGPT}'s capabilities, potential implications, ethical concerns, and offer direction for future advancements in this field.},
	number = {{arXiv}:2304.01852},
	publisher = {{arXiv}},
	author = {Liu, Yiheng and Han, Tianle and Ma, Siyuan and Zhang, Jiayue and Yang, Yuanyuan and Tian, Jiaming and He, Hao and Li, Antong and He, Mengshen and Liu, Zhengliang and Wu, Zihao and Zhu, Dajiang and Li, Xiang and Qiang, Ning and Shen, Dingang and Liu, Tianming and Ge, Bao},
	urldate = {2023-06-24},
	date = {2023-05-10},
	eprinttype = {arxiv},
	eprint = {2304.01852 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{bubeck_sparks_2023,
	title = {Sparks of Artificial General Intelligence: Early experiments with {GPT}-4},
	url = {http://arxiv.org/abs/2303.12712},
	doi = {10.48550/arXiv.2303.12712},
	shorttitle = {Sparks of Artificial General Intelligence},
	abstract = {Artificial intelligence ({AI}) researchers have been developing and refining large language models ({LLMs}) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by {OpenAI}, {GPT}-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of {GPT}-4, when it was still in active development by {OpenAI}. We contend that (this early version of) {GPT}-4 is part of a new cohort of {LLMs} (along with {ChatGPT} and Google's {PaLM} for example) that exhibit more general intelligence than previous {AI} models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, {GPT}-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, {GPT}-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as {ChatGPT}. Given the breadth and depth of {GPT}-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence ({AGI}) system. In our exploration of {GPT}-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of {AGI}, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
	number = {{arXiv}:2303.12712},
	publisher = {{arXiv}},
	author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
	urldate = {2023-06-24},
	date = {2023-04-13},
	eprinttype = {arxiv},
	eprint = {2303.12712 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

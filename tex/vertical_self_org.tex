%% vertical_self_org.tex
This chapter presents a method based on vertical self-organisation.
The idea of vertical self-organisation is that the input is analysed by several smaller models instead of one big model.
This means that each network sees only a patch of the input data and cannot decide on its own what is represented in the input image but must agree on a representation with neighbouring models.
This agreement takes place through lateral communication.
It is essential that each model is independent of the other models and that the parameters are not shared between them. Otherwise, the architecture would be comparable to vision transformer \sidecite{Dosovitskiy_Beyer_Kolesnikov_Weissenborn_Zhai_Unterthiner_Dehghani_Minderer_Heigold_Gelly_2021} and the input patches would no longer be analysed independently. Thus, the architecture would not be self-organising anymore. In the following, the methodology (i.e. how vertical self-organisation is implemented) is presented, and the obtained results are discussed in detail.

\section{Methods}\seclbl{vertical_self_org_methods}
A crucial design decision for vertical self-organisation is the architecture of the models.
In this thesis are variational autoencoders \sidecite{Kingma_Welling_2014} (c.f. \secref{visual_rep_learning}) used to process patches of the input image as they have a continuous latent space and are thus more robust and better interpretable (c.f. \secref{neuro_concepts_net_fragments}).

Typical autoencoders fed an input image $\boldsymbol{x}$ through an encoder to map the input data to a latent space and through a decoder to recreate the image $\boldsymbol{\hat{x}}$. Thereby, the latent space is limited in size, forcing the model to compress the image with the encoder and to de-compress the image using the decoder (c.f. \secref{visual_rep_learning_ae}).

A variational autoencoder models the latent space as a multivariate Gaussian distribution. The input image $\boldsymbol{x}$ is also fed through an encoder, and a latent representation $\boldsymbol{h}$ is obtained. Afterwards, $\boldsymbol{h}$ is fed through two parallel fully connected layers to obtain the $\boldsymbol{\mu}$ and $\boldsymbol{\sigma}$ vectors of the multivariate Gaussian distribution. Afterwards, the re-parametrisation trick is used to sample a variable $\boldsymbol{z}'$ from this distribution:

\begin{align}\eqlbl{hso_1}
	\boldsymbol{z}' \sim \mathcal{N}(\boldsymbol{\mu},\,\boldsymbol{\sigma}^{2} \cdot \epsilon)
\end{align}

$\epsilon$ is a random variable sampled from  $\mathcal{N}(0, 1)$ and determines the magnitude of the variance. The sampled latent representation $\boldsymbol{z}'$ is fed through the decoder, and the image $\hat{\boldsymbol{x}}$ is recreated. The model is trained to minimise a distance measure between the input image $\boldsymbol{x}$ and the reconstructed image $\hat{\boldsymbol{x}}$. In this thesis, the mean square error is used. For $m$ images $\boldsymbol{X} = \boldsymbol{x}^{(1)}, ..., \boldsymbol{x}^{(m)}$, the loss is defined as

\begin{align}\eqlbl{hso_2}
	\mathcal{L}_{\text{rec}} = \frac{1}{m} \sum_{i=1}^{m} (\boldsymbol{x}^{(i)} - \boldsymbol{\hat{x}}^{(i)})^2
\end{align}

However, the latent space does not form a proper multivariate Gaussian distribution based on this reconstruction loss and the network architecture. A second loss constraint is necessary to obtain a probability distribution in the latent space: Let $P(\boldsymbol{X})$ be the probability distribution of the data $\boldsymbol{X}$, $P(\boldsymbol{h})$ the probability distribution of the latent variable $\boldsymbol{h}$ and $P(\boldsymbol{X}|\boldsymbol{h})$ the conditional probability of generating $\boldsymbol{X}$ for a given $\boldsymbol{h}$. The objective of the encoder is to infer $P(\boldsymbol{h})$ from $P(\boldsymbol{h}|\boldsymbol{X})$ which is the probability distribution that maps $\boldsymbol{X}$ into latent space. Simply put: the goal is to know the latent variable $\boldsymbol{h}$ of the input data $\boldsymbol{X}$.
However, $P(\boldsymbol{h}|\boldsymbol{X})$ is unknown, and we have to estimate it from a simpler distribution $Q(\boldsymbol{h}|\boldsymbol{X})$. This simpler distribution $Q(\boldsymbol{h}|\boldsymbol{X})$ is learned by the encoder and should be as close as possible to the real distribution $P(\boldsymbol{h}|\boldsymbol{X})$. This is accomplished by minimising the KL divergence\sidenote{the Kullback-Leibler (KL) divergence can ``measure'' the difference between two probability distributions} between these two probability distributions.

\begin{align}\eqlbl{hso_3}
	KL \left[ Q(\boldsymbol{h}|\boldsymbol{X}) || P(\boldsymbol{h}|\boldsymbol{X}) \right] = E \left[ \log Q(\boldsymbol{h}|\boldsymbol{X}) - \log P(\boldsymbol{h}|\boldsymbol{X}) \right]
\end{align}

In the case of two Gaussian distributions, the KL divergence is defined as:
\begin{align}\eqlbl{hso_4}
	KL\left[ \mathcal{N}_Q(\boldsymbol{\mu}_Q, \boldsymbol{\sigma}_Q)||\mathcal{N}_P(\boldsymbol{\mu}_P, \boldsymbol{\sigma}_P) \right] = \log \frac{\boldsymbol{\sigma}_P}{\boldsymbol{\sigma}_Q} + \frac{\boldsymbol{\sigma}_Q^2 + (\boldsymbol{\mu}_Q-\boldsymbol{\mu}_P)^2}{2\boldsymbol{\sigma}_P^2} - \frac{1}{2}
\end{align}


If the target distribution $P(\boldsymbol{h}|\boldsymbol{X})$ is a multivariate Gaussian distribution with $\boldsymbol{\mu}_P = 0$ and $\boldsymbol{\sigma}_P=1$ (i.e. $\mathcal{N}(0, 1)$) and the encoder predicts $\boldsymbol{\mu}_Q$ and $\boldsymbol{\sigma}_Q$ to model $Q(\boldsymbol{h}|\boldsymbol{X})$ as $\mathcal{N}(\boldsymbol{\mu}_Q, \boldsymbol{\sigma}_Q)$, then the KL divergence simplifies to


\begin{align}\eqlbl{hso_5}
\begin{split}
		\mathcal{L}_{KLD} = KL \left[ \mathcal{N}_Q(\boldsymbol{\mu}_Q, \boldsymbol{\sigma}_Q)||\mathcal{N}(0, 1) \right] & = -\log \boldsymbol{\sigma}_Q + \frac{\boldsymbol{\sigma}_Q^2 \boldsymbol{\mu}_Q^2}{2} - \frac{1}{2} \\
		 & = \frac{1}{2} \left( \boldsymbol{\sigma}_Q^2 \boldsymbol{\mu}_Q^2 - 1 - 2\log \boldsymbol{\sigma}_Q \right)
\end{split}
\end{align}

Interested readers may find a formal proof of the formulas above and a more detailed derivation in the original paper \cite{Kingma_Welling_2014}.
Thus, the loss for the variational autoencoder is defined as:


\begin{align}\eqlbl{hso_6}
\begin{split}
		\mathcal{L}_{\text{VAE}} & = \mathcal{L}_{\text{rec}} + \beta \cdot KL \left[ \mathcal{N}_Q(\boldsymbol{\mu}_Q, \boldsymbol{\sigma}_Q)||\mathcal{N}(0, 1) \right] \\
		  & = \underbrace{\frac{1}{n} \sum_{i=1}^{n} (\boldsymbol{x}_i - \boldsymbol{\hat{x}}_i)^2}_{\text{reconstruction loss}} + \lambda \cdot \overbrace{\frac{1}{2} (\boldsymbol{\sigma}_Q^2 \boldsymbol{\mu}_Q^2 - 1 - 2\log \boldsymbol{\sigma}_Q)}^{\text{KL divergence}}
\end{split}
\end{align}

where $\beta$ is a weight factor of the KL divergence. Training a variational autoencoder with this loss function can lead to excellent image reconstruction.
In the case of vertical self-organisation, however, a well-formed latent space is of more interest than an outstanding image reconstruction: The goal is to obtain good image representations, and thus the goal is to have a well-formed latent space and not perfect reconstruction.
Variational autoencoders have the notorious problem that the KL divergence term becomes vanishingly small during training \sidecite{bowman2016generating}. This issue is known as the KL vanishing problem.
This problem can be alleviated by applying annealing schedules to the KL term (i.e., changing $\beta$ over time).
In this thesis, monotonic annealing is used as proposed by \sideciteay{bowman2016generating}.
However, cyclic annealing strategies might lead to even better results \sidecite{Fu_Li_Liu_Gao_Celikyilmaz_Carin_2019}.
The disadvantage is that training with cyclic annealing takes longer, which outweighs the advantage of only minimally better latent representation.
These annealing strategies are shown in \figref{beta_annealing}. When $\beta$ is small, the model is forced to focus on reconstructing the input rather than minimising the KL loss.
When $\beta$ is increased, the model gradually improves the probabilistic distribution in the latent space.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{beta_annealing}
    \caption[Annealing strategy of the KL weight term of variational autoencoders]{Two annealing strategies for the $\beta$ term that weights the KL divergence in the loss function of variational autoencoders. The upper graph shows a monotonic increase of $\beta$, and the lower graph a cyclical annealing strategy. The picture is from \cite{Fu_Li_Liu_Gao_Celikyilmaz_Carin_2019}.}
    \figlbl{beta_annealing}
\end{figure}

The number of variational autoencoders is a design decision. As a baseline, one single VAE is used. This VAE receives the entire image as input and creates a latent representation. This model is compared to two versions of vertical self-organisation; one model is based on $4$ variational autoencoders ($2 \times 2$), and one model is based on $16$ variational autoencoders ($4 \times 4$). Thus, the input is not divided into patches for the baseline model and is split into either $4$ or $16$ patches for the models based on vertical self-organisation.
Each VAE consists of an encoder, two fully-connected layers to calculate $\boldsymbol{\mu}$ and $\boldsymbol{\sigma}$, and a decoder. The encoder consists of convolutional layers with a stride of $2$, halving the input size in each layer, and the decoder of transposed convolutional layers with a stride of $2$, doubling the output size in each layer.
Different numbers of layers and channels are used to retain a fair comparison between the models.

\begin{description}
   \item[The baseline model] has an encoder consisting of $4$ convolutional layers with $32$, $64$, $128$, and $256$ channels. The decoder has the inverse structure, consisting of $4$ transposed convolutional layers with $256$, $128$, $64$, and $32$ channels. The fully connected layers for predicting $\boldsymbol{\mu}$ and $\boldsymbol{\sigma}$ have $64$ neurons.
 \item[The autoencoders of the model with $4$ VAEs] have an encoder with $3$ convolutional layers with $32$, $64$, and $128$ channels. As for the baseline model, the decoder has the inverse structure and consists of $3$ transposed convolutional layers. The fully connected layers have $32$ neurons.
  \item[The autoencoders of the model with $16$ VAEs] have an encoder with $2$ convolutional layers with $32$, and $64$ channels, and a decoder with $2$ transposed convolutional layers with $64$ and $32$ channels.The fully connected layers have $16$ neurons.
\end{description}

As a representative illustration, the architecture of a VAE of the model with $4$ VAEs is shown in \figref{vertical_org_arch1}.

\begin{figure}[h]
    \centering
    \resizebox{0.99\textwidth}{!}
{
\begin{tikzpicture}
\tikzstyle{connection}=[ultra thick,every node/.style={sloped,allow upside down},draw=\edgecolor,opacity=0.7]
\tikzstyle{copyconnection}=[ultra thick,every node/.style={sloped,allow upside down},draw={rgb:blue,4;red,1;green,1;black,3},opacity=0.7]


\node[canvas is zy plane at x=0] (input) at (0,0,0) {\includegraphics[width=8cm,height=8cm]{imgs/mnist.jpeg}};


\pic[shift={(3,0,0)}] at (input) 
    {Box={
        name=conv1,
        caption=Conv + ReLU,
        xlabel={{1, }},
        zlabel=32,
        fill=\ConvColor,
        height=20,
        width=2,
        depth=20
        }
    };


\draw [connection]  (input) ++(0,0,0)    -- node {\midarrow} (conv1-west);


\pic[shift={(2,0,0)}] at (conv1-east) 
    {Box={
        name=conv2,
        caption=Conv + ReLU,
        xlabel={{32, }},
        zlabel=64,
        fill=\ConvColor,
        height=12,
        width=4,
        depth=12
        }
    };


\draw [connection]  (conv1-east)    -- node {\midarrow} (conv2-west);


\pic[shift={(2,0,0)}] at (conv2-east) 
    {Box={
        name=conv3,
        caption=Conv + ReLU,
        xlabel={{64, }},
        zlabel=128,
        fill=\ConvColor,
        height=6,
        width=6,
        depth=6
        }
    };


\draw [connection]  (conv2-east)    -- node {\midarrow} (conv3-west);


\pic[shift={(2,2,0)}] at (conv3-east) 
    {Box={
        name=fcn1,
        caption=FC + ReLU,
        xlabel={{" ","dummy"}},
        zlabel=32,
        fill=\SoftmaxColor,
        opacity=0.8,
        height=3,
        width=3,
        depth=6
        }
    };


\pic[shift={(2,-2,0)}] at (conv3-east) 
    {Box={
        name=fcn2,
        caption=FC + ReLU,
        xlabel={{" ","dummy"}},
        zlabel=32,
        fill=\SoftmaxColor,
        opacity=0.8,
        height=3,
        width=3,
        depth=6
        }
    };


\draw [connection]  (conv3-east)    -- node {\midarrow} (fcn1-west);


\draw [connection]  (conv3-east)    -- node {\midarrow} (fcn2-west);


\pic[shift={(2,-2,0)}] at (fcn1-east) 
    {Box={
        name=conv4,
        caption=Conv + ReLU,
        xlabel={{128, }},
        zlabel=64,
        fill=\ConvColor,
        height=6,
        width=6,
        depth=6
        }
    };


\draw [connection]  (fcn1-east)    -- node {\midarrow} (conv4-west);


\draw [connection]  (fcn2-east)    -- node {\midarrow} (conv4-west);


\pic[shift={(2,0,0)}] at (conv4-east) 
    {Box={
        name=conv5,
        caption=Conv + ReLU,
        xlabel={{64, }},
        zlabel=32,
        fill=\ConvColor,
        height=12,
        width=4,
        depth=12
        }
    };


\draw [connection]  (conv4-east)    -- node {\midarrow} (conv5-west);


\pic[shift={(2,0,0)}] at (conv5-east) 
    {Box={
        name=conv6,
        caption=Conv + ReLU,
        xlabel={{32, }},
        zlabel=1,
        fill=\ConvColor,
        height=20,
        width=2,
        depth=20
        }
    };


\draw [connection]  (conv5-east)    -- node {\midarrow} (conv6-west);


\node[canvas is zy plane at x=3] (output) at (conv6-east) {\includegraphics[width=8cm,height=8cm]{imgs/mnist.jpeg}};


\draw [connection]  (conv6-east)     -- node {\midarrow} ($(output) + (0.5,0,0)$);


\end{tikzpicture}
}
    \caption[Architecture of the VAEs]{Architecture of a variational autoencoder of the model using $4$ VAEs for vertical self-organisation.}
    \figlbl{vertical_org_arch1}
\end{figure}

Each VAE of each model is trained independently to minimise the loss function as described in \eqref{hso_6}.
For all models, Adam \sidecite{Kingma2015AdamAM} is used as optimisation algorithm with a learning rate of $1\cdot 10^{-3}$, and the mini-batch size is $32$.


\subsection{Predicting bigger Patches}\seclbl{vertical_self_org_methods_bigger_patches}

\begin{figure}[h]
    \centering
    \resizebox{0.99\textwidth}{!}
{
\begin{tikzpicture}
\tikzstyle{connection}=[ultra thick,every node/.style={sloped,allow upside down},draw=\edgecolor,opacity=0.7]
\tikzstyle{copyconnection}=[ultra thick,every node/.style={sloped,allow upside down},draw={rgb:blue,4;red,1;green,1;black,3},opacity=0.7]


\node[canvas is zy plane at x=0] (input) at (0,0,0) {\includegraphics[width=8cm,height=8cm]{imgs/mnist.jpeg}};


\pic[shift={(3,0,0)}] at (input) 
    {Box={
        name=conv1,
        caption=Conv + ReLU,
        xlabel={{1, }},
        zlabel=32,
        fill=\ConvColor,
        height=20,
        width=2,
        depth=20
        }
    };


\draw [connection]  (input) ++(0,0,0)    -- node {\midarrow} (conv1-west);


\pic[shift={(2,0,0)}] at (conv1-east) 
    {Box={
        name=conv2,
        caption=Conv + ReLU,
        xlabel={{32, }},
        zlabel=64,
        fill=\ConvColor,
        height=12,
        width=4,
        depth=12
        }
    };


\draw [connection]  (conv1-east)    -- node {\midarrow} (conv2-west);


\pic[shift={(2,0,0)}] at (conv2-east) 
    {Box={
        name=conv3,
        caption=Conv + ReLU,
        xlabel={{64, }},
        zlabel=128,
        fill=\ConvColor,
        height=6,
        width=6,
        depth=6
        }
    };


\draw [connection]  (conv2-east)    -- node {\midarrow} (conv3-west);


\pic[shift={(2,2,0)}] at (conv3-east) 
    {Box={
        name=fcn1,
        caption=FC + ReLU,
        xlabel={{" ","dummy"}},
        zlabel=32,
        fill=\SoftmaxColor,
        opacity=0.8,
        height=3,
        width=3,
        depth=6
        }
    };


\pic[shift={(2,-2,0)}] at (conv3-east) 
    {Box={
        name=fcn2,
        caption=FC + ReLU,
        xlabel={{" ","dummy"}},
        zlabel=32,
        fill=\SoftmaxColor,
        opacity=0.8,
        height=3,
        width=3,
        depth=6
        }
    };


\draw [connection]  (conv3-east)    -- node {\midarrow} (fcn1-west);


\draw [connection]  (conv3-east)    -- node {\midarrow} (fcn2-west);


\pic[shift={(2,-2,0)}] at (fcn1-east) 
    {Box={
        name=conv4,
        caption=Conv + ReLU,
        xlabel={{128, }},
        zlabel=64,
        fill=\ConvColor,
        height=6,
        width=6,
        depth=6
        }
    };


\draw [connection]  (fcn1-east)    -- node {\midarrow} (conv4-west);


\draw [connection]  (fcn2-east)    -- node {\midarrow} (conv4-west);


\pic[shift={(2,0,0)}] at (conv4-east) 
    {Box={
        name=conv5,
        caption=Conv + ReLU,
        xlabel={{64, }},
        zlabel=32,
        fill=\ConvColor,
        height=12,
        width=4,
        depth=12
        }
    };


\draw [connection]  (conv4-east)    -- node {\midarrow} (conv5-west);


\pic[shift={(2,0,0)}] at (conv5-east) 
    {Box={
        name=conv6,
        caption=Conv + ReLU,
        xlabel={{32, }},
        zlabel=32,
        fill=\ConvColor,
        height=20,
        width=2,
        depth=20
        }
    };


\draw [connection]  (conv5-east)    -- node {\midarrow} (conv6-west);


\pic[shift={(2,0,0)}] at (conv6-east) 
    {Box={
        name=conv7,
        caption=Conv + ReLU,
        xlabel={{32, }},
        zlabel=1,
        fill=\ConvColor,
        height=20,
        width=2,
        depth=20
        }
    };


\draw [connection]  (conv6-east)    -- node {\midarrow} (conv7-west);


\node[canvas is zy plane at x=3] (output) at (conv7-east) {\includegraphics[width=8cm,height=8cm]{imgs/mnist.jpeg}};


\draw [connection]  (conv6-east)     -- node {\midarrow} ($(output) + (0.5,0,0)$);


\end{tikzpicture}
}
    \caption[Architecture of the VAEs predicting bigger output patches]{Architecture of a variational autoencoder of the model using $4$ VAEs for vertical self-organisation and predicting bigger output patches.}
    \figlbl{vertical_org_arch2}
\end{figure}
Applying monotonic annealing to the KL divergence weight term $\beta$ is the first measure to improve the latent space distribution.
Another measure is to predict bigger patches:
Each VAE receives an image-patch as input and thus has a very limited field of view. I.e., a VAE only sees a quarter (in the case of $4$ VAEs) or a sixteenth (in the case of $16$ VAEs) as input and, therefore, cannot distinguish some of the digits on their own (c.f. \secref{vertical_self_org_methods_communication}).

The models are trained on the MNIST dataset \cite{Lecun_Bottou_Bengio_Haffner_1998} that consists of images depicting the digits $0-9$. 
In the case of $16$ VAEs, almost none of the numbers can be recognised based on a single input patch.
However, distinguishing digits based on a quarter of the image (model using $4$ VAEs) is already challenging; for example, the patches for the digits $4$, $5$, and $6$ look very similar for the first VAE that receives patches extracted from the top left corner of the samples (c.f. \figref{average_sample}).
However, by predicting bigger patches, the VAEs can be encouraged to distinguish similar-looking patches better.

Since patches of different digits look similar, they are located closely together in the latent space.
When a VAE predicts bigger patches or the entire image, the target predictions become different. Thus, while similar latent representations are sufficient to predict only a patch of these digits, predicting larger patches or the whole picture requires more separated latent representations.
Thus, predicting bigger patches helps to push apart latent representations of objects with a high patch-wise similarity but a low image-wise similarity.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{bigger_patches}
    \caption[Reconstruction of bigger output than input patches]{Reconstruction of bigger output than input patches for the model based on vertical self-organisation with $4$ VAEs. Each VAE receives a quarter of the image as input and predicts the entire image.}
    \figlbl{bigger_patches}
\end{figure}

In this thesis, an additional layer is used in the decoder to predict larger patches: The last layer of the decoder with $32$ channels and a stride of $2$ is used twice so that the output size of the decoder is doubled. The architecture of a variational autoencoder of the model using $4$ VAEs is shown in \figref{vertical_org_arch2}.
When using $4$ VAEs, the input is divided into $4$ patches. If the decoder makes the output twice as large as the input by up-sampling, the entire image is predicted by each of the VAEs. This process is visualised in \figref{bigger_patches}.
In the case of $16$ VAEs, each VAE predicts a quarter of the image instead of a sixteenth.


\subsection{Communication}\seclbl{vertical_self_org_methods_communication}
The VAEs receive patches of the input image and thus have a limited field of view on the image.
\figref{average_sample} visualises the patches of the model with $4$ VAEs that are fed into each autoencoder when the MNIST dataset \cite{Lecun_Bottou_Bengio_Haffner_1998} is used.
The first row shows the average of all images of the same class. Thus, these images are very representative of the dataset.
However, none of the models receives the entire image as input. Instead, the patches shown in the 2nd to the 5th row are fed into the VAEs.
The patches, which only depict a quarter of the image, look rather similar for some classes, for example, the top-left patches of the classes $0$ and $9$, as well as the classes $4$, $5$, and $6$.
Thus, these classes are not distinguishable by \emph{one} single VAE on its own.
This limitation of the field-of-view is obviously even more significant with $16$ VAEs.
Therefore, communication with neighbouring models is necessary to agree on an image-level representation.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{average_sample}
    \caption[Average sample of the MNIST dataset per class]{The average of all samples in the MNIST dataset for each class. The first row shows the average of all samples, and the 2nd to 5th row is the average per patch fed into the model with $4$ VAEs.}
    \figlbl{average_sample}
\end{figure}


This concept links well to the biological model: A single VAE can be seen as a neuron or a group of neurons. Initially, the VAEs have many suggestions as to which classes the received patch could belong. However, through communication with neighbouring VAEs, suggestions are constantly ruled out because neighbouring VAEs do not support them. Thus, out of many suggestions, only the most valid ones are retained, resulting in an image representation. The biological model behaves similarly: At first, many neurons are active because they are excited by the captured image. However, this strong activation quickly becomes sparse as only the neurons supporting each other remain active. This process leads to the emergence of higher-level features (i.e. net fragments) that are representative of the input \sidecite{Malsburg_1987}.

One of the biggest strengths of autoencoders is that they do not rely on labels but can be trained in an unsupervised manner. To keep this strength, also communication between the models should not rely on labels. Therefore, different types of communication are proposed that work without labels: communication-based on latent representations, communication-based on reconstructed patches, and communication via a dedicated communication channel.

All these types of communication take place over two or more time steps. First, image patches are reconstructed by all VAEs. As a result, each VAE generates information in the latent space. This information can be communicated to the neighbouring VAEs. Thus, in a second step, the VAEs have more information available; their prediction and information from neighbouring VAEs. This additional information allows them to improve their prediction. The process can be repeated over a fixed number of time steps or until the network reaches an attractor state (i.e. the latent representations no longer change). In the following, different types of communication are proposed using the example of the model with $4$ VAEs. The same concept can of course also be applied to the model with $16$ VAEs.


\subsubsection{Model Heads}
Each VAE learns the mapping from a patch into the latent space, i.e. obtains a $\boldsymbol{\mu}$ and $\boldsymbol{\sigma}$ for each sample that is used to predict a reconstructed version of the input.
The arrangement of the representations within the latent space is not predetermined by an external system but is self-organised. A consequence of independent systems is that $\boldsymbol{\mu}$ and $\boldsymbol{\sigma}$ have different meanings in the latent spaces of different VAEs. Thus, one VAE cannot interpret the Gaussian parameters of another VAE.

In this thesis, it is proposed to learn a linear transformation to map the Gaussian parameters of one VAE to the Gaussian parameters of another VAE. The mapping from $\boldsymbol{\mu}_1$ to $\boldsymbol{\mu}_2$, respectively from $\boldsymbol{\sigma}_1$ to $\boldsymbol{\sigma}_2$ is done by the simple transformation

\begin{align}\eqlbl{hso_7}
	\boldsymbol{\hat{\mu}}_2 = \boldsymbol{w}_{m12} \cdot \boldsymbol{\mu}_1
\end{align}

resp.

\begin{align}\eqlbl{hso_8}
	\boldsymbol{\hat{\sigma}}_2 = \boldsymbol{w}_{s12} \cdot \boldsymbol{\sigma}_1
\end{align}

The weights $\boldsymbol{w}_{mij}$ and $\boldsymbol{w}_{sij}$ between VAE $i$ and VAE $j$ are learned by minimising the mean square error with gradient descent between the predicted and true Gaussian parameters, i.e. between $\boldsymbol{\mu}_j$ and $\boldsymbol{\hat{\mu}}_j$, resp. $\boldsymbol{\sigma}_j$ and $\boldsymbol{\hat{\sigma}}_j$. This process is illustrated for the first VAE of the model using $4$ VAEs in \figref{hso_head}: First, the Gaussian parameters of each model must be predicted for the same sample. Afterwards, the Gaussian parameters of one model can be used to predict the Gaussian parameters of the other models with a linear transformation.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{hso_head}
    \caption[Prediction of Gaussian parameters of other VAEs]{Visualisation of how the first VAE can predict the Gaussian parameters of the other VAEs.}
    \figlbl{hso_head}
\end{figure}

If each VAE predicts the Gaussian parameters of the other VAEs, then these predictions can be communicated, and each VAE receives further suggestions from neighbouring VAEs. Thus, each VAE has its own calculated Gaussian parameters and suggestions from neighbours about what its Gaussian parameters might be. This allows a VAE to correct its prediction. Each VAE receives additional information because the suggestions made by neighbouring VAEs are based on a different patch. For example, the VAE that sees only one patch from the top left of the image cannot distinguish the digits $5$ and $6$. On the other hand, the VAE that sees the patch at the bottom left can distinguish these digits. Thus, the VAE at the bottom left can help the VAE at the top left to choose a better latent representation.


\subsubsection{Reconstructed Patches}
As described in \secref{vertical_self_org_methods_bigger_patches}, the prediction of larger patches helps to arrange the latent space better. However, the prediction of larger patches can also be used for communication with neighbouring VAEs: Instead of extracting representations from the latent space and transmitting them to neighbouring VAEs as described in the previous section, the reconstruction can also be communicated. This is done by predicting a part of the image in a first time-step and by adding this prediction to an additional input channel of another VAE in the next time-step. Various data can be forwarded to other VAEs:


\begin{figure}
    \centering
    \includegraphics[width=0.80\textwidth]{recon_patch_forw}
    \caption[Communication between VAEs by forwarding reconstructed patch]{Communication between VAEs by forwarding reconstructed patch, example based on the first VAE: The first VAE predicts the image and forwards the reconstructed patch (the same patch that was fed into the model) to the other VAEs (illustrated based on the model with $4$ VAEs).}
    \figlbl{recon_patch_forw}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.80\textwidth]{neighbouring_patch_forw}
    \caption[Communication between VAEs by forwarding prediction of neighbouring patches]{Communication between VAEs by forwarding prediction of neighbouring patches, example based on the first VAE: The first VAE predicts the image and forwards a prediction of how the neighbourhood could look like to the other VAEs. Thus, each other VAE receives a patch and a prediction of the first VAE how this patch could look as input (illustrated based on the model with $4$ VAEs).}
    \figlbl{neighbouring_patch_forw}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.80\textwidth]{recon_img_forw}
    \caption[Communication between VAEs by forwarding prediction of the entire image]{Communication between VAEs by forwarding prediction of the entire image, example based on the first VAE: The first VAE predicts the image and forwards the prediction of the image to the other VAEs (illustrated based on the model with $4$ VAEs).}
    \figlbl{recon_img_forw}
\end{figure}



\begin{description}
	\item [Reconstructed Patch] Each VAE reconstructs its input patch and optionally a part of the neighbourhood. It tells the other VAEs how the reconstructed version of its input patch looks. Information about the optionally reconstructed neighbourhood is not communicated to other VAEs. Thus, a reconstructed version of the input is sent to other VAEs and added as a new channel to their input. This process is visualised for the first VAE in \figref{recon_patch_forw}. 
	\item [Neighbouring Patch] Instead of communicating the reconstructed input patch to the neighbours, a prediction can be made of what the input patch of the neighbour looks like, and this prediction can be communicated. This has the advantage that each VAE receives different versions of the same patch as input and thus has only very local image information available. This process for the first VAE is visualised in \figref{neighbouring_patch_forw}. Intuitively, this can solve the following problem: If a VAE cannot decide which digit to predict, neighbouring VAEs can help it by predicting its input patch in a form that is more similar to one of the digits and thus support the VAE by making its decision. 
	\item[Bigger patch] The simplest version is when each VAE predicts a bigger patch (or the entire image in the case of $4$ VAEs) and communicates this to all other VAEs. In the first time-step, only local information is available. In the second time-step, various predictions of how the image could look are additionally provided. Each VAE can then correct its prediction based on these predictions of the other VAEs. This process is represented in \figref{recon_img_forw}. 
\end{description}

\subsubsection{Global vs. Local Communication}
\begin{figure}
    \centering
    \includegraphics[width=0.80\textwidth]{vso_local_communication}
    \caption[Local communication between VAEs]{Local communication between VAEs illustrated for the VAE $1$ and $10$ of the model with $16$ VAEs.}
    \figlbl{vso_local_communication}
\end{figure}


Communication between VAEs using the aforementioned methods can be either global or local.
With global communication, each VAE sends data to all other VAEs. Thus, global information is exchanged between VAEs.
With local communication, the communication of each VAE is limited to its  $4$ neighbouring VAEs, i.e. the VAEs on the left, right, top and bottom.
For the model with $4$ VAEs, global communication is used. Thus, each autoencoder sends information to the $3$ other autoencoders and receives data from them.
For the model with $16$ VAEs, global communication is not feasible since each autoencoder would communicate with $15$ other models. Therefore, local communication is used.
This further encourages self-organisation since information is kept locally.
This process is illustrated for the VAEs $1$ and $10$ in \figref{vso_local_communication}. 
With local communication, global information can only flow into each VAE via proxy VAEs over several communication steps. For example, VAE $1$ cannot directly receive information from VAE $3$. However, VAE $3$ can indirectly exchange data with VAE $1$ by communication via VAE $2$ (as proxy) over at least two communication cycles.


\subsection{Class Prediction}
The VAEs are examined to determine whether they are suitable for predicting the class label. First, all VAEs are trained until they can reliably reconstruct images and the latent space is well formed.
After training, the average value of $\boldsymbol{\mu}$ is calculated for each class $c \in C$ from the training set and each VAE. This average value is denoted as $\boldsymbol{\overline{\mu}}_{vc}$ for VAE $v$ and class $c$. It is defined for $n_c$ samples of class $c$ as

\begin{align}\eqlbl{hso_12}
		\boldsymbol{\overline{\mu}}_{vc} = \frac{1}{n_c} \sum_{i=1}^{n_c} \boldsymbol{\mu}_i
\end{align}

This average value can be considered the ``cluster centre'' of a class in latent space or as a world model of a class. A given sample is matched against these world models per class to make a prediction. As with horizontal self-organisation, the cosine similarity between a sample and the class prototypes is used to determine their similarity. To predict a sample $s$, it is fed through the encoder to obtain $\boldsymbol{\mu}_{vs}$. Then, this sample is compared with all class prototypes by computing the cosine similarity: 

\begin{align}\eqlbl{hso_13}
		\text{cos}_{vsc} = \text{cos}(\boldsymbol{\mu}_{vs}, \boldsymbol{\overline{\mu}}_{vc}) = \frac{\boldsymbol{\mu}_vs \cdot \boldsymbol{\overline{\mu}}_{vc}}{\max(||\boldsymbol{\mu}_{vs}||_2, ||\boldsymbol{\overline{\mu}}_{vc}||_2)}
\end{align}

Thus, the cosine similarity $\text{cos}_{vsc}$ is calculated for each VAE $v$ between the class prototypes $c$ and the sample $s$.
Afterwards, the class $c$ with the highest average cosine similarity between the sample activations $\boldsymbol{\mu}_{vs}$ and the class prototype $\boldsymbol{\overline{\mu}}_{vc}$ of all VAEs $v$ is used as the prediction.

\begin{align}\eqlbl{hso_14}
		\argmax_{c \in C} \frac{1}{n_v} \sum_{v=1}^{n_v} \text{cos}_{vsc}
\end{align}

where $n_v$ is the number of VAEs. Similar to horizontal self-organisation, VAEs with higher confidence have a higher cosine similarity between a class prototype and a specific class and thus have more influence on the class prediction than if the confidence is lower and the cosine similarity between the sample and all class prototypes is similarly large.



\section{Results}\seclbl{vertical_self_org_results}
As for horizontal self-organisation, the MNIST \cite{Lecun_Bottou_Bengio_Haffner_1998}, and MNIST-C \cite{Mu_Gilmer_2019} datasets are used to evaluate the models (c.f. \secref{horizontal_self_org_methods_dataset}). 

Different models are trained to evaluate the proposed architecture. \tabref{hso_models} gives an overview of the models. A single VAE is used as the baseline (model no. (i)), which receives the entire image as input and reconstructs it. This VAE is obviously trained without time steps and communication. Vertical self-organisation is done using $4$ VAEs ($2\times 2$) or $16$ VAEs ($2\times 2$) in different settings:

\begin{description}
	\item [Without time steps and communication:] Some models are trained independently from each other without time steps and communication. Each VAE receives a non-overlapping image patch as input and either reconstructs the received patch or predicts the entire image. These are the models number (ii), (iii), (viii), and (ix). Models (ii) and (iii) are based on $4$ VAEs, models (viii) and (ix) are based on $16$ VAEs.
	\item [Forward the reconstructed input patch:] The VAEs forward the reconstructed \emph{input patch} to the other VAEs (according to \figref{recon_patch_forw}). This communication takes place over $4$ time steps, and the VAEs are trained by either reconstructing the input patch (model no. (iv) and (x)) or predicting the entire image\sidenote{but despite the prediction of the entire image, only the prediction of the reconstructed patch is communicated to the other VAEs.} (model no. (v) and (xi)). Models (iv) and (v) are based on $4$ VAEs, models (x) and (xi) are based on $16$ VAEs.
	\item [Forward predicted input patch of the neighbour:] Each VAE predicts the whole image and forwards the predicted \emph{input patch of the neighbour} to the other VAEs over $4$ time steps (c.f. \figref{neighbouring_patch_forw}). Model no. (vi) implements this for $4$ VAEs, model no. (xii) for $16$ VAEs.
	\item [Forward prediction of bigger patches:] The VAEs are used to predict a patch twice the size of the input over one time step, and the \emph{bigger patch} is forwarded to the other VAEs (e.g. \figref{recon_img_forw}). Model no. (vii) implements this for $4$ VAEs, model no. (xiii) for $16$ VAEs. In the case of $4$ VAEs, each VAE predicts the entire image. In the case of $16$ VAEs, each autoencoder predicts a quarter of the image.
\end{description}

\begin{table}[h] 
    \centering
	 \begin{tabular}{l l l l l}
    	\textbf{No.} & \textbf{$n$ VAEs} & \textbf{Reconstruction} & \textbf{Communication} & \textbf{Time-Steps}\\
        \hline
		i & 1 & Image & -  & -\\
		ii & 4 & Patch & -  & -\\
		iii & 4 & Image & - & -\\
		iv & 4 & Patch & Input Patch & $4$\\ %p_diff
		v & 4 & Image & Input Patch & $4$\\  %p_diff
		vi & 4 & Image & Neighbouring Patch & $4$\\ %p_same
		vii & 4 & Image & Image & $1$\\
		viii & 16 & Patch & -  & -\\
		ix & 16 & Bigger Patch & - & -\\
		x & 16 & Patch & Input Patch & $4$\\ %p_diff
		xi & 16 & Bigger Patch & Input Patch & $4$\\  %p_diff
		xii & 16 & Bigger Patch & Neighbouring Patch & $4$\\ %p_same
		xiii & 16 & Bigger Patch & Bigger Patch & $1$\\		
		
    \end{tabular}
    \caption[Different models to evaluate vertical self-organisation]{Different models for evaluating vertical self-organisation: The first column shows a number used as a reference id in this thesis. The second column shows the number of VAEs used, the third column whether the input field or the whole image is reconstructed, the fourth column what is communicated to the neighbouring VAEs, and the fifth column over how many time steps the communication takes place.}
    \tablbl{hso_models}
\end{table}


The number of time steps is a hyperparameter: Obviously, the models without communication do not need time steps. In the case where each VAE predicts the entire image and forwards it to the other VAEs, only one time step is used. This means that the entire image is predicted once and then forwarded to the other VAEs. It has been observed that one time step is sufficient and that the predictions do not change in further time steps. This could be due to the fact that all relevant information is already transmitted within one time step when the entire image is sent, and no further communication cycles are necessary. However, if only a patch of the image is forwarded to the other VAEs, the predictions may be updated over several time steps. Therefore, $4$ time steps are used. The predictions usually do not change after $4$ time steps, and thus, using $4$ time steps seems enough. When only patches are forwarded, the models contain much more dynamics: A VAE makes a prediction, communicates it to its neighbours and receives data from its neighbours. In a subsequent time step, the VAE has more information and can correct its prediction, which in turn can lead to a correction of a neighbour's prediction. Thus, the VAEs correct each other over several time steps. An example of four predicted samples is shown in \figref{hso_over_time}: The left side shows the initial prediction of model no. (vi), and the right side the prediction of the same image after $4$ time steps. This visualisation indicates that the predictions are improved thanks to communication. The investigation of the reconstruction error confirms the efficiency of the communication: The initial prediction without communication has an average reconstruction error of $0.23$, and the prediction after $4$ communication cycles has an error of $0.16$.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.79\textwidth]{hso_over_time}
    \caption[Change of the model's prediction over time]{Four random predictions of model no. (vi) over $4$ time-steps: The initial prediction is shown on the left, and the prediction after $4$ communication cycles on the right.}
    \figlbl{hso_over_time}
\end{figure}

Of particular interest is not how well the samples are reconstructed but how well the latent space is shaped. Visualising the $\boldsymbol{\mu}$ values is a simple way to investigate the latent space. Therefore, the $\boldsymbol{\mu}$ vectors with a length of $n=32$ ($4$ VAEs) or $n=16$ ($16$ VAEs) are reduced to $2$ dimensions using t-SNE \sidecite{van2008visualizing} and visualised. Since many different versions are trained for this evaluation, visualising all t-SNE plots is infeasible. Instead, \figref{hso_tSNE} only shows the plot of the baseline (model no. (i)), the plot of the model no. (ii) as an example of four VAEs reconstructing patches, and the plot of model no. (vi) as a representative example of an architecture with communication and image reconstruction.

As expected, the latent space of model no. (i), which receives the entire image as input, is well organised, and the classes are distinguishable within the latent space. Model no. (ii) consists of $4$ VAEs without communication. Its latent space is less well organised: the classes are no longer clearly identifiable as clusters. However, it is interesting to note that each VAE can separate certain classes better than others because of the different patches it receives as input. Considering all VAEs, each class can be relatively well identified by at least one VAE, while no VAE can identify all classes. This indicates the benefit of communication between the VAEs.
By applying the proposed measures (i.e. communication and predicting the whole image instead of reconstructing the input patch), the structure of the latent space for each VAE improves significantly: almost every VAE is able to separate the classes. This supports the usefulness of the proposed measures.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{hso_tSNE}
    \caption[t-SNE plot of the $\boldsymbol{\mu}$ values of different models]{t-SNE plot of the $\boldsymbol{\mu}$ values of four different models: The first row shows the baseline model ($1$ VAE), the second row the model no. (ii) that consists of $4$ VAEs without communication, and the third row the model (vi) that uses $4$ VAEs with communication channels. Each point in the plot corresponds to a sample, the samples are coloured according to their class.}
    \figlbl{hso_tSNE}
\end{figure}

\begin{table}[h] 
    \centering
	 \begin{tabular}{l l l l}
    	\textbf{No.} & \textbf{Avg. NMI} & \textbf{Min. NMI} & \textbf{Max. NMI}\\
        \hline
		i & $0.60$ & $0.60$ & $0.60$ \\
		
		ii & $0.32$  & $0.29$ & $0.37$ \\
		iii & $0.53$ & $0.49$ & $0.56$ \\
		iv & $0.39$ & $0.33$ & $0.41$\\
		v & $0.59$ & $0.58$ & $0.61$ \\
		vi & $0.54$ & $0.49$ & $0.60$ \\
		vii & $0.54$ & $0.49$ & $0.59$\\
		
		viii & $0.11$ & $0.0$ & $0.2$\\
		ix & $0.21$ & $0.0$ & $0.45$\\
		x & $0.31$ & $0.0$ & $0.47$\\
		xi & $0.42$ & $0.0$ & $0.63$ \\
		xii & $0.56$ & $0.42$ & $0.65$ \\
		xiii & $0.55$ & $0.41$ & $0.62$\\
    \end{tabular}
    \caption[NMI score of different architectures]{The NMI score between the $\boldsymbol{\mu}$ cluster centres and the ground-truth label. The clusters are calculated by using the k-means algorithm. Most models consist of several VAEs, therefore the average, minimum and maximum NMI values are reported for all VAEs per model.}
    \tablbl{hso_nmi}
\end{table}

Another way to evaluate the latent space is to measure its suitability for classification. The suitability for classification can be measured by applying a k-means clustering and then calculating the normalised mutual information (NMI) score between the cluster centres and the labels. This metric compares cluster assignments and labels by calculating their consistency: It measures how well a cluster can be assigned to a label. \tabref{hso_nmi} shows the NMI scores of the models.
The NMI score is challenging to interpret as an absolute number. However, a relative comparison of the scores is of much more interest. Model no. (i) is a single VAE that receives the entire image as input, not just a patch of it. Therefore, this model is assumed to have a well-formed latent space, and the value of $0.6$ is close to an upper bound.
The models no. (ii) and (viii) are considered a lower bound as these models use either $4$ or $16$ VAEs but none of the proposed measures is used (i.e. do not predict the entire image nor communicate with each other). 
As shown in \tabref{hso_nmi}, some of the VAEs of the models with $16$ autoencoders have an NMI scrore of $0$ and thus their features have no correlation of the actual label. This is due to the fact that these autoencoders receive only very small patches at the border of the image and these patches are, in the case of MNIST, always black. Thus, they cannot detect any features within the image that can be correlated with a class.

It is evident that the latent space is significantly better shaped when bigger patches are predicted and not only the input patch is reconstructed. For example, the only different between the models no. (ii) and (iii), (iv) and (v), (viii) and (ix), as well as (x) and (xi) is that bigger patches are predicted. In all these cases, the NMI score improves significantly: 
\begin{itemize}
	\item Model no. (ii) $\rightarrow$ (iii): NMI increases from $0.32$ to $0.53$ ($+0.21$)
	\item Model no. (iv) $\rightarrow$ (v): NMI increases from $0.39$ to $0.59$ ($+0.20$)
	\item Model no. (viii) $\rightarrow$ (ix): NMI increases from $0.11$ to $0.21$ ($+0.10$)
	\item Model no. (x) $\rightarrow$ (xi): NMI increases from $0.31$ to $0.42$ ($+0.11$)
\end{itemize}

The improvement of the NMI score for the model with $4$ VAEs is more significant ($\sim 0.2$) than for the model with $16$ VAEs ($\sim 0.1$). This is probably because the model with $4$ VAEs predicts the entire image when predicting an output patch twice as large as the input patch. Thus, the latent space of each VAE is very well organised, and even by upsampling without communication, an NMI score of $0.53$ is achieved, which is close to the upper bound. The model with $16$ VAEs also shows a clear improvement. However, the improvement is smaller than for the model with $4$ VAEs. This could be because only a quarter of the entire image is predicted by these models. This leads to a less well-organised latent space than when the entire image is predicted. In addition, some VAEs only have a black patch as input (i.e. only see the background), which does not allow predicting the neighbourhood.
Overall, the efficiency of predicting bigger patches is clearly visible for both models. Thus, this proposed measure is considered helpful to improve the model's performance significantly.

Besides the improvement due to the prediction of bigger patches, there is also an apparent gain in performance due to lateral communication. The improvement for the model with $4$ VAEs is less noticeable than in the model with $16$ VAEs. The model without upsampling and communication has an NMI score of $0.32$. By using lateral communication, the score improves to $0.39$. Communication also improves the models with upsampling, from $0.53$ to $0.54$ and $0.59$, respectively. The improvement of the models with $4$ VAEs is not as significant because each model already has relatively large patches as input and communication does not provide much information. Moreover, predicting larger patches (i.e. the whole image) leads to a very well-organised latent space. Thus, the models are already working well before communication takes place, and consequently, lateral communication improves performance only minimally.
For the model with $16$ VAEs, however, lateral communication improves the NMI score significantly. The score improves for the model without upsampling from $0.11$ to $0.31$. With upsampling, the model without communication has an NMI score of $0.21$, and the models with communication scores of $0.42$, $0.56$, and $0.55$, respectively. Thus, for these models, the improvement due to lateral communication is remarkable. The improvement is significant because each VAE has a minimal field of view due to the small patches and receives much additional information about the image from neighbouring autoencoders through lateral communication.


\begin{table}[h] 
    \centering
	 \begin{tabular}{l l l l l}
	 	& \textbf{Avg. VAEs} & \textbf{Overall} & \textbf{Avg. VAEs} & \textbf{Overall}\\
    	\textbf{No.} & \textbf{MNIST} & \textbf{MNIST} & \textbf{MNIST-C} & \textbf{MNIST-C}\\
        \hline
		i & - & $85.2\%$ & - & $63.4\%$ \\
		
		ii & $62.4\%$ & $81.7\%$ & $42.3\%$ & $60.9\%$ \\
		iii & $78.2\%$ & $89.9\%$ & $45.4\%$ & $64.9\%$ \\
		iv & $63.8\%$ & $82.6\%$ & $41.8\%$ & $60.5\%$  \\
		v & $86.1\%$ & $87.6\%$ & $62.4\%$ & $65.2\%$ \\
		vi & $88.4\%$ & $91.1\%$ & $50.4\%$ & $59.2\%$ \\
		vii & $78.5\%$ & $90.0\%$ & $46.8\%$ & $64.2\%$ \\
		
		viii & $38.7\%$ & $61.3\%$ & $29.3\%$ & $42.8\%$ \\
		ix & $51.2\%$ & $71.5\%$ & $34.2\%$ & $50.7\%$ \\
		x & $48.4\%$ & $80.8\%$ & $38.7\%$ & $59.1\%$  \\
		xi & $64.2\%$ & $88.1\%$ & $41.2\%$ & $66.7\%$ \\
		xii & $66.8\%$ & $90.7\%$ & $40.0\%$ & $63.7\%$ \\
		xiii & $65.7\%$ & $89.9\%$ & $41.6\%$ & $65.4\%$ \\
    \end{tabular}
    \caption[Accuracy of different architectures]{The accuracy of different models on the MNIST and MNIST-C dataset. The average accuracy per VAE and overall accuracy are reported for both datasets. The average per VAE is the average accuracy when each VAE makes a prediction on its own (i.e. choosing the highest cosine similarity per VAE according to \eqref{hso_13}). The overall accuracy is calculated by averaging the cosine similarity according to \eqref{hso_14}.}
    \tablbl{hso_accuracy}
\end{table}


Finally, the proposed classification method is evaluated. The results are shown in \tabref{hso_accuracy}.
For each VAE of a model, the cosine similarity according to \eqref{hso_13} between a sample and a class prototype is calculated. Thus, the accuracy can be calculated not only for the entire model but also for each VAE independently. \tabref{hso_accuracy} shows the average accuracy per VAE in the column ``Avg. VAEs''. Furthermore, by calculating the average cosine similarity, voting between VAEs is done. The accuracy of this voting is shown in the column ``Overall''. The voting works exceptionally well: for example, model no. (ii) has $4$ VAEs with an average accuracy of $62.4\%$ on the MNIST dataset. The best VAE has an accuracy of $66.1\%$, and the worst $59.7\%$. By voting, the overall accuracy increases to $81.7\%$, significantly better than the best VAE.

On the MNIST dataset, the base model (model no. (i)) is significantly outperformed by some models in terms of accuracy. Also robustness (measured on MNIST-C) is slightly improved by using multiple VAEs. For the models with $4$ VAEs, predicting larger patches improves performance on MNIST from $81.7\%$ to $89.9\%$, outperforming the baseline model by $4.7$ p.p. Thus, using multiple VAEs and leveraging voting seems very promising. Adding communication further improves the accuracy to up to $91.1\%$.
For the models using $16$ VAEs, communication improves accuracy even more significantly. For example, the models without communication achieve an accuracy of $61.3\%$ (without upsampling)  and $71.5\%$ (with upsampling) on MNIST.  When adding lateral communication, accuracy improves up to $80.8\%$ (without upsampling) and $90.7\%$ (with upsampling). Thus, lateral communication improves the result.
Overall, using multiple VAEs that can communicate with each other improves performance compared to using a single VAE. Thus, self-organisation seems to be an efficient measure the improve model performance.







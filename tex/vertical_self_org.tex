%% vertical_self_org.tex
In this thesis, two different concepts of self-organisation are implemented.
This chapter presents a method based on vertical self-organisation (c.f. \secref{neuro_concepts_self_org}).
Vertical self-organisation is based on training small units within a network independent from each other.
The first section presents the methodology (i.e. how vertical self-organization was implemented) and the second section describes the obtained results.


\section{Methods}\seclbl{vertical_self_org_methods}
The first choice for networks based on vertical self-organisation is what the independent units are, i.e. which network parameters are trained independently.
In this thesis, a linear layer was selected as a self-contained unit. This is a small unit (e.g. compared to a model part that contains many layers) but can still be trained efficiently.
Smaller units would be neurons or parts of a layer, but training them separately would be very inefficient, since the efficiency of matrix operations on GPUs could no longer be fully exploited\sidenote{the layer output can be efficiently calculated by a single matrix multiplications and addition, i.e. $\boldsymbol{z} = \boldsymbol{w} \cdot \boldsymbol{x} + \boldsymbol{b}$ (c.f. \secref{ann})}.

Each layer optimises its own proxy objective function.
Proxy objective functions are used because this type of local optimisation allows very good performance even on larger data sets, in contrast to other target functions such as target propagation, synthetic gradients or feedback alignment (c.f. \secref{alt_train_algo}). 
Figure \figref{vertical_gradient_flow} visualizes the updates based on a proxy objective function.
A objective function is calculated for each layer and the optimization algorithm only updates the parameters within this layer.
Thus, the gradients do not flow backwards to preceding layers.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{vertical_gradient_flow}
    \caption[The flow of gradients within the network based on vertical self-organisation]{The flow of gradients within the network based on vertical self-organisation: The data is fed from layer to layer during the forward pass. The layers, are trained independently and the gradients do not flow from one layer to the previous one.}
    \figlbl{vertical_gradient_flow}
\end{figure}

A combination of diversity and sparsity constraints is used as the loss function, which leads to representations that are easy to interpret and suitable for net fragments (c.f. \secref{neuro_concepts_net_fragments}) and also increases robustness (c.f. \secref{neuro_concepts_sparsity}).
Identical to the preliminary experiments in Appendix \chref{net_fragments}, the sparsity is achieved by using the kullback-leibler (KL) divergence \sidecite{10-5555-3042573-3042641}.
The model consists of several linear layers $l$ with a relu activation function $(z)^+ = max(0, z)$.
A layer consists of $m^{(l)}$ neurons and the activations $\boldsymbol{z}^{(l)}$ of a linear layer $l$ are calculated for a given input $\boldsymbol{z}^{(l-1)}$ as

\begin{equation}\eqlbl{vso_1}
		\boldsymbol{z}^{(l)} = z^{(l)}_1, ..., z^{(l)}_m = \boldsymbol{w}^{(l)} \cdot \boldsymbol{z}^{(l-1)} + \boldsymbol{b}^{(l)}
\end{equation}

whereby $\boldsymbol{w}^{(l)}$ is the weight and $\boldsymbol{b}^{(l)}$ the bias of layer $l$.

The activation probability can be calculated for each neuron. If a mini-batch contains $n$ samples, the activation probability $\hat{\rho}^{(i)}$ of a neuron $z^{(i)}$ can be calculated as:

\begin{equation}\eqlbl{vso_2}
		\hat{\rho}^{(l)}_i = \frac{1}{n} \sum^n_i \frac{1}{1+e^{-z^{(l)}_i}}
\end{equation}

where $\frac{1}{1+e^{-z^{(i)}}}$ is the sigmoid function that squeezes the activation in the range between $0$ and $1$.
With the KL divergence, the divergence of the current activation probability $\hat{\rho}^{(i)}$ and a desired activation probability $\rho=0.05$ can be calculated:

\begin{equation}\eqlbl{vso_3}
		KL(\rho || \hat{\rho}^{(l)}_i) = \rho \cdot \log \frac{\rho}{\hat{\rho}^{(l)}_i} + (1-\rho) \cdot \log \frac{1-\rho}{1-\hat{\rho}^{(l)}_i}
\end{equation}

The sparsity loss $L_{s}$ is the sum of the divergence between all $\hat{\rho}^{(i)}$ and $\rho$:

\begin{equation}\eqlbl{vso_4}
		L_{s}(\rho, \hat{\rho}) = \sum_{i=1}^{m} KL(\rho || \hat{\rho}^{(l)}_i)
\end{equation}

The second constraint is a diversity constraint. The goal is that the activations of \emph{different} objects are diverse. For this purpose, the activations $\boldsymbol{z}^{(l)}$ are made as identical as possible (i.e. pushed together in feature space) if they stem from the same class and as different as possible if they stem from different classes.
The cosine similarity is used to calculate the similarity between two activations $z^{(l)}_i$ and $z^{(l)}_j$:

\begin{equation}\eqlbl{vso_5}
		\text{cos}(z^{(l)}_i, z^{(l)}_j) = \frac{z^{(l)}_i \cdot z^{(l)}_j}{\max(||z^{(l)}_i||_2, ||z^{(l)}_j||_2)}
\end{equation}

In order to make representations of different objects different, resp. to make representations of identical objects identical, the information of image labels $y_i$ is needed. Thus, $y_i \in C$ where $C$ is the set of classes.
The diversity loss $L_d$ minimises the similarity $\text{cos}(z^{(l)}_i, z^{(l)}_j)$ if the two activations stem from different classes $y_i \neq y_j$, or maximises the similarity if they stem from the same class $y_i = y_j$.

\begin{equation}\eqlbl{vso_6}
		L_{d}(\boldsymbol{z}^{(l)}) = \frac{1}{n^2} \sum_{i=1}^{n} \sum_{j=1}^{n} k \cdot \text{cos}(z^{(l)}_i, z^{(l)}_j)
\end{equation}

whereby $k$ changes sign depending on the class:

\begin{equation}\eqlbl{vso_7}
		k = \begin{cases}
      		+1, & \text{if}\ y_i \neq y_j \\
      		-1, & \text{otherwise}
    	\end{cases}
\end{equation}

However, it was found that the loss is more stable if the similarity is not calculated between two activations but between one activation and the average activation of a class. The average activation of a class $c$ can be calculated over $n_c$ samples from this class as:

\begin{equation}\eqlbl{vso_8}
		z^{(l)}[c] = \frac{1}{n} \sum_{i=1}^{n_c} z^{(l)}_i \text{, for } y_i = c
\end{equation}

Another problem of is loss is if all activations are identical, then $\text{cos}(z^{(l)}_i, z^{(l)}_j) = 1$ and the loss gets $L_{d}=0$, since the similarities of the same and different classes neutralise each other.
Therefore, a margin between the similarities is enforced, as done for the triplet-margin-loss  \sidecite{Balntas_Riba_Ponsa_Mikolajczyk_2016}.

\begin{equation}\eqlbl{vso_9}
		L_{d}(\boldsymbol{z}^{(l)}) = \frac{1}{n} \sum_{i=1}^{n} \max(\text{cos}(z^{(l)}_i, z^{(l)}[v]) - \text{cos}(z^{(l)}_i, z^{(l)}[y_i]) + \text{margin}, 1)
\end{equation}

where $v$ is a random class drawn from the set $v ~ \{C \\ y_i\}$ and $\text{margin}$ is a hpyer-parameter that was set to $1$.
Thus, the triplet-margin-loss is calculated but the $L2$-norm is replaced by the cosine-similarity as distance measure and the positive, resp. negative anchor is replaced with the average class activation from the same resp. from a randomly selected different class.

The loss used is the sum of sparsity loss and diversity loss, with the diversity loss weighted by $\lambda=0.1$:
\begin{equation}\eqlbl{vso_10}
		L = L_{s}(\rho, \hat{\rho}) + \lambda \cdot L_{d}(\boldsymbol{z}^{(l)}) 
\end{equation}


One problem with proxy objective functions is that this type of training often requires layer-wise training. First, layer $1$ is trained completely, then the weights are frozen, then layer $2$ is trained and so on.
This is inefficient because it requires more forward-passes than if the model is trained with end-to-end backpropagation.
It was found that this loss functions allows to train all layers simultaneously. With a forward-pass, all activations are calculated, followed by a layer-wise backward-pass where the gradients from one layer do not propagate back into the previous layer.
Since the gradients are only needed locally, no large graph of gradients has to be calculated, which reduces the memory utilization on GPUs and allows larger mini-batch sizes. In addition, this type of architecture is very easy to parallelise, since theoretically each layer (i.e. each independently trained unit) or a group of layers can be assigned to a different GPU.
In contrast to end-to-end backpropagation of error architectures, the gradients only flow backwards locally on a GPU and do not have to be passed on to other GPUs.

\begin{figure}[h]
    \centering
    \resizebox{0.99\textwidth}{!}
{
\begin{tikzpicture}
\tikzstyle{connection}=[ultra thick,every node/.style={sloped,allow upside down},draw=\edgecolor,opacity=0.7]
\tikzstyle{copyconnection}=[ultra thick,every node/.style={sloped,allow upside down},draw={rgb:blue,4;red,1;green,1;black,3},opacity=0.7]


\node[canvas is zy plane at x=0] (input) at (0,0,0) {\includegraphics[width=8cm,height=8cm]{imgs/mnist.jpeg}};


\pic[shift={(3,0,0)}] at (input) 
    {Box={
        name=fcn1,
        caption=FC + ReLU,
        xlabel={{" ","dummy"}},
        zlabel=512,
        fill=\SoftmaxColor,
        opacity=0.8,
        height=3,
        width=3,
        depth=52
        }
    };


\draw [connection]  (input) ++(0,0,0)    -- node {\midarrow} (fcn1-west);


\pic[shift={(2,0,0)}] at (fcn1-east) 
    {Box={
        name=fcn2,
        caption=FC + ReLU,
        xlabel={{" ","dummy"}},
        zlabel=256,
        fill=\SoftmaxColor,
        opacity=0.8,
        height=3,
        width=3,
        depth=25
        }
    };


\draw [connection]  (fcn1-east)    -- node {\midarrow} (fcn2-west);


\pic[shift={(2,0,0)}] at (fcn2-east) 
    {Box={
        name=fcn3,
        caption=FC + ReLU,
        xlabel={{" ","dummy"}},
        zlabel=128,
        fill=\SoftmaxColor,
        opacity=0.8,
        height=3,
        width=3,
        depth=13
        }
    };


\draw [connection]  (fcn2-east)    -- node {\midarrow} (fcn3-west);


\pic[shift={(2,0,0)}] at (fcn3-east) 
    {Box={
        name=fcn4,
        caption=FC + ReLU,
        xlabel={{" ","dummy"}},
        zlabel=64,
        fill=\SoftmaxColor,
        opacity=0.8,
        height=3,
        width=3,
        depth=6
        }
    };


\draw [connection]  (fcn3-east)    -- node {\midarrow} (fcn4-west);


\end{tikzpicture}
}
    \caption[Architecture of the fully connected model with vertical self-organisation]{The network architecture of the fully connected model for vertical self-organisation with fully connected layers.}
    \figlbl{vertical_org_arch1}
\end{figure}


The model used consists of $4$ fully connected layers with relu activation. The first layer has $512$ neurons, the second $256$ neurons, the third $128$ neurons and the fourth $64$ neurons. The model is illustrated in Figure \figref{vertical_org_arch1}. Each layer is trained separately by minimising the loss of equation \eqref{vso_10} with the Adam optimizer\sidecite{Kingma_Ba_2017} and a learning rate of $\eta = 1 \cdot 10^{-3}$. The mini-batch size is $60,000$. 

\subsection{Extraction of Representations}\seclbl{vertical_self_org_representations}
In accordance with net-fragments as in \secref{neuro_concepts_net_fragments}, the representations are not extracted at a specific point (i.e. a pre-defined layer) but all representations from all layers are taken into account to fulfil a task.
In the following, this is demonstrated based on a classification task, but other tasks are also conceivable in the future.
After training, the average activation $z^{(l)}[c]$ for each class $c \in C$ in each layer $l$ is determined, as done in Equation \eqref{vso_8}.
These averages from the training set represent prototypes of each class object in each layer and can be considered as reference representation per class. Thus, the representations needed for this task are computed \emph{after} training and are not part of the training as for example in the case of a classification loss based on cross-entropy\sidenote{but in the case of classification this information is implicit in the loss function}.
When a new sample $\boldsymbol{x}_s$ is classified, the cosine similarity between the activations $\boldsymbol{z}^{(l)}_s$ of this sample and the class prototypes $\boldsymbol{z}^{(l)}[c]$ is calculated in each layer.

\begin{equation}\eqlbl{vso_11}
		\text{cos}^{(l)}_s[c] = \text{cos}(\boldsymbol{z}^{(l)}_s, \boldsymbol{z}^{(l)}[c]) = \frac{\boldsymbol{z}^{(l)}_s \cdot \boldsymbol{z}^{(l)}[c]}{\max(||\boldsymbol{z}^{(l)}_s||_2, ||\boldsymbol{z}^{(l)}[c]||_2)} \text{, for } c \in C
\end{equation}

Thus, the cosine similarity $\text{cos}^{(l)}[c]$ for each class $c \in C$ and each of the for layers $l \in {1, ..., 4}$ is calculated. Afterwards, the average class $c$ with the highest average cosine similarity between the sample activations $\boldsymbol{z}^{(l)}_s$ and the class prototypes $\boldsymbol{z}^{(l)}[c]$ is used as prediction.

\begin{equation}\eqlbl{vso_12}
		\argmax_{c \in C} \frac{1}{4} \sum_{l=1}^{4} \text{cos}^{(l)}_s[c]
\end{equation}

This results in a weighted voting; if a layer is very sure that the sample belongs to a specific class, then the sample has a high cosine similarity with one class prototype and a low similarity with all other class prototypes. Accordingly, this layer influences the prediction more than a layer that cannot clearly assign the sample to one class and calculates a similarly high cosine similarity between the sample and all prototypes.


\subsection{Lateral Connections}
As described in section \secref{neuro_concepts_lateral_connections}, lateral connections serve neurons to support their activations among each other.
In this chapter it is also described in detail that this can be implemented through recurrent connections.
There are several ways to implement recurrent connections. A very simple possibility is to concatenate the layer input $\boldsymbol{z}^{(l-1)}_t$ at time $t$ with the layer output $\boldsymbol{z}^{(l)}_{t-1}$  at time $t-1$. This is visualized in Figure \figref{lateral_concat}. Of course, $\boldsymbol{z}^{(l)}_{t-1}$ is undefined at $t=0$. In this case, $\boldsymbol{z}^{(l)}_{t-1}$ is initialized with zeros.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{lateral_concat}
    \caption[Lateral connections by concatenating the layer's output with the layer's input]{The lateral connections can be implemented by concatenating the layer's output at the previous time-step with the layer's input at the current time-step.}
    \figlbl{lateral_concat}
\end{figure}


A second option is to use a second weight matrix as it is usually done in recurrent layers and to expand the equation \eqref{vso_1} for the activation function as follows:

\begin{equation}\eqlbl{vso_13}
		\boldsymbol{z}^{(l)}_t =  \boldsymbol{w}_x^{(l)} \cdot \boldsymbol{z}^{(l-1)}_t + \boldsymbol{b}^{(l)} + \overbrace{\boldsymbol{w}_h^{(l)} \cdot \boldsymbol{z}^{(l)}_{t-1} }^{\text{lateral connection}}
\end{equation}

whereby $\boldsymbol{w}_x^{(l)}$ is the weight multiplied with the layer input and $\boldsymbol{w}_h^{(l)}$ the weight multiplied with the previous layer output. In both cases, the layer receives information about the activations at the previous time-step.
However, this only seems helpful if the model input is not static. Therefore, the model input is available over several time-steps and is augmented after each time-step. Thus, the model receives different views of the same image and can adjust its activations from the previous time-step if necessary. The following image augmentation techniques are applied, each with a probability of $p=0.8$:

\begin{itemize}
	\item \textbf{Color Jitter}: Randomly change brightness, contrast, and saturation of the image.
	\item \textbf{Gaussian Blur}: Blur the image with randomly chosen Gaussian blur.
	\item \textbf{Random Rotation}: Randomly rotate the image with an angle in the range $[-15°, ..., 15°]$
	\item \textbf{Adjust Sharpness}: Randomly adjust the sharpness of the image.
\end{itemize}

Figure \figref{mnist_augmented} visualises how this augmentation affects samples of the MNIST data set \cite{Lecun_Bottou_Bengio_Haffner_1998}. The original image is shown on the left and $9$ augmented versions of it are shown on the right. This allows the model to perceive the same image from different views.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{mnist_augmented}
    \caption[Data augmentation applied on $10$ samples of the MNIST data set]{Data augmentation applied on $10$ samples of the MNIST data set. The original samples as it is in the data set is shown on the left, $9$ augmented versions of the same samples are shown on the right.}
    \figlbl{mnist_augmented}
\end{figure}


Recurrent connections, which in this context represent lateral connections, are typically used to process sequential data or text. In this case, the data is collected cumulatively before an output is generated. For example, in text processing, all word tokens of a sentence are typically read before the model classifies the sentence as heat-speech or not. This is necessary because all sentence information is required and classification cannot be done on the basis of a single token. Such models are typically trained with backpropagation through time (BTT). Thereby, the gradients flow backwards over several time steps. This leads to well-known problems such as vanishing and exploding gradients.

In this thesis, BTT is not used, which means that a prediction is made after each time-step, but the prediction potentially improves with more time-steps. This leads to desirable properties: (i) Problems with vanishing or exploiding gradients do not exist, regardless of how many time-steps the model requires. (ii) After each time-step, representations can be extracted according to Section \secref{vertical_self_org_representations}. If a task can be solved correctly with a high probability on the basis of these representations (e.g. the object representation can clearly be assigned to one class label), the sequential analysis of the image can be aborted. If this is not the case, further time-steps can be carried out until the model has a sufficiently high confidence in its prediction. Thus, the number of time-steps can be sample-dependent.


% Idee: Mehr Robustheit indem z.b. in einem timestep noise als Input genutzt wird oder bei 1 von 10 timesteps (ausser in den ersten 2) das Bild zufälligerweise vertauscht wird. -> Lateral connections should help


\subsection{Hierarchical Features}
A criticism of the proposed model is that it does not learn hierarchical features, even tough hierarchical features are one of the main reasons for the good performance of deep learning systems. The diversity loss (c.f. Equation \eqref{vso_9}) forces enforces in each layer that the latent representations of objects of the same class are similar and that the representations of different classes are different. This violates the concept of hierarchical feautres; the first layers should learn general features that are helpful for all classes, but cannot necessarily be assigned to a specific class. Only later layers build class representations that are specific to a class. In the current setting, however, already the first layers generate class specific representations.

In the following, three possible measures are described to counteract this problem: (i) The fully connected layers are replaced by convolutional layers, so that the first layers have a smaller field of view and can only recognise local features. (ii) An adapted version of diversity loss forces a separation of features by class only in the last layers. (iii) While the very specific image information flows into the network from one side, general information in the form of class labels is fed into the network from the other side and these two types of information are fused. These three measures can be applied either individually or in combination with each other.

\subsubsection{Convolutional Architecture}
corresponding layer has a sufficiently large field-of-view\sidenote{the field-of-view are all pixels of the input image that can influence a neuron's activity}. In a CNN, the field-of-view in the first layers is restricted by design, but not in fully connected layers. Consequently, it might help to use a CNN architecture instead of a model based on fully connected layers only. This means that the network is no longer able to separate the representations based on the class in the first layers as the field-of-view is too small, but can only do this in the later layers, which have a larger field-of-view.


\begin{figure}[h]
    \centering
    \resizebox{0.99\textwidth}{!}
{
\begin{tikzpicture}
\tikzstyle{connection}=[ultra thick,every node/.style={sloped,allow upside down},draw=\edgecolor,opacity=0.7]
\tikzstyle{copyconnection}=[ultra thick,every node/.style={sloped,allow upside down},draw={rgb:blue,4;red,1;green,1;black,3},opacity=0.7]


\node[canvas is zy plane at x=0] (input) at (0,0,0) {\includegraphics[width=8cm,height=8cm]{imgs/mnist.jpeg}};


\pic[shift={(3,0,0)}] at (input) 
    {Box={
        name=conv1,
        caption=Conv + ReLU,
        xlabel={{1, }},
        zlabel=16,
        fill=\ConvColor,
        height=16,
        width=2,
        depth=16
        }
    };


\draw [connection]  (input) ++(0,0,0)    -- node {\midarrow} (conv1-west);


\pic[shift={ (0,0,0) }] at (conv1-east) 
    {Box={
        name=pool1,
        caption= ,
        fill=\PoolColor,
        opacity=0.5,
        height=8,
        width=1,
        depth=8
        }
    };


\pic[shift={(2,0,0)}] at (pool1-east) 
    {Box={
        name=conv2,
        caption=Conv + ReLU,
        xlabel={{16, }},
        zlabel=32,
        fill=\ConvColor,
        height=8,
        width=4,
        depth=8
        }
    };


\draw [connection]  (pool1-east)    -- node {\midarrow} (conv2-west);


\pic[shift={ (0,0,0) }] at (conv2-east) 
    {Box={
        name=pool2,
        caption= ,
        fill=\PoolColor,
        opacity=0.5,
        height=4,
        width=1,
        depth=4
        }
    };


\pic[shift={(2,0,0)}] at (pool2-east) 
    {Box={
        name=conv3,
        caption=Conv + ReLU,
        xlabel={{32, }},
        zlabel=64,
        fill=\ConvColor,
        height=4,
        width=6,
        depth=4
        }
    };


\draw [connection]  (pool2-east)    -- node {\midarrow} (conv3-west);


\end{tikzpicture}
}
    \caption[Architecture of the CNN for vertical self-organisation]{The network architecture of the CNN for vertical self-organisation with fully connected layers.}
    \figlbl{vertical_org_arch2}
\end{figure}

The CNN architecture used in this thesis is shown in Figure \figref{vertical_org_arch2}.
Three convolutional layers with ReLU activation function and with $16$, $32$, and $64$ channels are used, and max-pooling layers are applied between each convolutional layer. For training, the same hyperparameters and loss functions are used as for the model with fully connected layers.

\subsubsection{Hierarchical Diversity Loss}
A second measure can be to adapt the diversity constraint of the loss function. In the current version, it forces the latent representations of objects of the same class to be similar and those of different classes to be different. This is useful in the last layers, where high-level features (i.e. high-level net-fragments) or object representations should be detected and be separated from each other. In the first layers, on the other hand, the separation should not depend on the class label. Nevertheless, the activations should also be diverse if low-level features are detected (c.f. Section \secref{neuro_concepts_net_fragments}).

Therefore, the sparsity constraint is split into two parts: One part ensures that the activations within a (large) mini-batch are diverse and thus enforces that different features are captured and represented by different neurons. The second part ensures, as before, that the activations are diverse for different classes. Thus, one part of the constraint ensures \emph{diversity within the mini-batch} and the other part ensures \emph{diversity between different classes}.

These two parts are weighted linearly from the first to the last layer. Since the first layer should have a high diversity within the mini-batch, it has a high weight on the first part of the diversity constraint and a low weight on the second part. The last layer has an inverse weighting and pays more attention to the second part of the diversity constraint than to the first part.

The first part of the diversity constraints ensures diversity within a mini-batch. This is achieved by ensuring that each neuron within a mini-batch should be active. 

TODO: At the moment, various experiments are still ongoing to find out how this can be implemented (therefore, not yet explained in more detail).


\subsubsection{Two-Way Information Flow}
Hinton \sidecite{ff_algo} introduced with the forward-forward (FF) algorithm (c.f. Section \secref{alt_train_algo}) a promising idea for models based on proxy objective functions; 
The image is fed into the input layer of the network and the corresponding label is fed into the output layer of the network. The image remains static for several time-steps and the network is trained to push the layer's activations above a certain threshold. In a second stage, the image is fed into the network together with the wrong label and the network is trained to push the activations below a certain threshold. He found that when low-level features (i.e. images) are fed into the network from one end and high-level features (i.e. class labels) are fed into the network at the other end, a feature hierarchy is created. However, this approach has one major disadvantage: During inference, each possible sample-label combination has to be fed into the model and the label that caused the highest neural activity is used as the model's prediction.

In this thesis, this is implemented in a different way, which does not have this disadvantage. Identical to the FF algorithm, the image is fed into the input layer of the network for multiple time-steps. The label, on the other hand, is not fed directly into the output layer but is made available to the last layer within the loss function. Each layer maximizes the mutual information (MI) between its own activations and the activations of the previous and subsequent layers. Thus, the first layer maximizes the MI between the input image and its activations, the last layer maximises the MI between its activations and a label vector. This creates a feature pyramid in which a smooth transition from concrete sample to abstract class label is learned. During inference, the last label then directly predicts latent representations that can be assigned to a class.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{mi_loss}
    \caption[Vertical self-organization with mutual information loss]{Vertical self-organization with mutual information loss: Each layer maximizes the mutual information (MI) between its activations and the activations of the previous and subsequent layer. The first layer maximizes the MI between its activations and the input image (instead of the previous layer) and the last layer maximizes the MI between its activations and the image label (instead of the subsequent layer).}
    \figlbl{mi_loss}
\end{figure}

This process is visualized in Figure \figref{mi_loss}. 


TODO: At the moment, various experiments are still ongoing to find out how this can be implemented (therefore, not yet explained in more detail).


%\subsection{Efficient Parallel Training}
%Idee: Nachfolgelayer einbeziehen in Loss -> Nachfolgelayer ist gefroren, Output muss aber möglichst gut sein (sollte Performance nicht verschlechtern von Nachfolgelayer) + eigene Performance verbessern



\section{Results}\seclbl{vertical_self_org_methods_results}





% Future Work: keine Labels aber z.B. clustering oder self-supervised durch augmentations
% Future Work: Repräsentationen untersuchen -> was kann alles gefunden werden, e.g. information about object detection?
% Future Work: Hierarchical clustering um mehrere Objektprototypen zu verwenden anstatt nur Durchschnittswert pro Klasse (i.e. mehrere repräsentative Object prototypen anstelle nur eines prototypen)
% Selbst-Kritik: Weder Fisch noch Vogel -> zu wenig gut als geeignet für neues Klassifizierungsnetzwerk (obwohl das nie das Ziel war), zu wenig radikal neu als neurowissenschaftlich plausibel





%TODO: vereinheitliche Mathe: Was ist underline, was ist hochgestellt in Klammern ($z^{(i)}$) und was ist hochgestellt in eckigen Klammern? Bei NN: Was ist n, was ist m, ... (+ Lernraten Symbol, Loss Symbol, etc.)
%Sind alle Vektoren bold?

% TODO: target function und objective function vereinheitlichen
% TODO: net-fragment or net fragment


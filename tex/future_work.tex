%% future_work.tex

Deep learning is far from being a system with human-like intelligence. Consequently, there is a massive amount of work to be done in the future to get closer to this ultimate goal. However, the implementations presented in this thesis are far from such a system. Consequently, a conclusion is drawn and next steps are proposed for the models presented in this thesis, but a long-term vision is presented as well. Specifically, some insights are give to the neuroscientific concept that seem very promising in section \secref{future_neuro_concepts} and future work is described for vertical self-organisation in \secref{future_vso} as well as for horizontal self-organisation in \secref{future_hso}. Afterwards, in \secref{future_3_stage}, a new model is presented on a very abstract level, which could work better intuitively, but a concrete implementation is unclear. In \secref{useful_representations} the usefulness of representations is discussed. In the last \secref{cognition_reasoning}, the author gives a personal opinion about the future of AI, which is not supported by scientific arguments but might be interesting for (motivated) readers.



\section{Neuroscientific Concepts}\seclbl{future_neuro_concepts}
In \chref{neuro_concepts}, several concepts from neuroscience are identified that are believed to be fundamental to biological intelligence. However, since the discrepancy between the biological model and the artificial model is large, it is difficult to incorporate these biological concepts into a computer system. In this thesis, concrete suggestions are made how such concepts can be realised as a concrete implementation. By implementing these concepts in the form of horizontal and vertical self-organisation, it is demonstrated that these ideas can facilitate the learning process. The evaluation of the suitability of the proposed implementation was very iterative: many concepts were tested but discarded in the course of the work because they did not add a lot of value to existing systems. For example, a lot of time was spent on generating sparsity over several time-steps as it is done in the human brain. However, it was found that sparsification of representations over several time-steps has no advantage without additional system dynamics. This could be because the data is static and already contains all the information. Therefore, the result is the same if the sparsification is done in one time-step than if it is done over several time-steps.

The most promising concepts identified in this work for improving deep learning systems are self-organisation, net-fragments, sparsity, lateral connections, continuous input, and embodiment. For each of these mechanisms, a concept for implementation is proposed. However, these proposals are the interpretation of the author and are only one of many possible solutions. So it could be that these concepts can also be interpreted differently and an alternative implementation could lead to better results.


\section{Implemented Models}\seclbl{future_so}
The concepts from neuroscience are implemented in two specific ways, these are called vertical and horizontal self-organisation. It is important to note that the implementation of \emph{neuroscientific concepts} is the main focus of this thesis and not to push scores like accuracy on benchmarks. In general, a comparison with end-to-end backpropagation of error is not appropriate in this field: Backpropagation has been optimised for over 30 years by many institutes and even more researchers, and therefore alternative learning concepts cannot be expected to outperform backpropagation of error instantly. Thus, backpropagation of error is note considered an appropriate baseline.

Rather, the proposed implementations show that this type of implementation of neuroscientific concepts can facilitate learning. However, the implementation has not yet been optimised and incorporated in big networks that might perform on the level of existing systems. Rather, these implementations are to be understood as a basis for further work, which, for example, deals with the concepts presented in \secref{future_3_stage}, \secref{useful_representations} and \secref{cognition_reasoning}. Thus, this thesis is to be understood as a preliminary work of a larger research project to develop new concepts that improve AI in a long-term way.


\subsection{Vertical Self-Organisation}\seclbl{future_vso}
TODO: this section is not yet complete as many experiments are still ongoing...


Currently, a sample is compared to the average activation of each class for classification. This average activation can be seen as a kind of object prototype within a world-model. Thus, it is compared to which prototype a sample fits best. The problem is that a single object prototype is most likely not sufficient. As an example, \figref{digit_4_versions} shows different versions of the digit $4$ that can be found in the MNIST data set. This demonstrates that a digit can be written in different ways\sidenote{this applies to all kind of data sets: Most objects can have various visual characteristics (size, color, shape) or look different from different viewpoints}. Consequently, the digits and thus the activations look different for samples from the same class and the use of more than one prototype per class seems helpful. Thus, instead of calculating the average activation per class, it could help to divide the activations per class into representative groups using a hierarchical clustering algorithm and to calculate the average activation (i.e. the cluster centre) per group. By doing so, multiple representative prototypes per class are obtained and the prediction could become lot more accurate.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\textwidth]{digit_4_versions}
    \caption[Different versions of the digit $4$ in the MNIST data set]{Different versions of the digit $4$ in the MNIST data set.}
    \figlbl{digit_4_versions}
\end{figure}


The generated representations have so far only been examined for classification. However, since the objective function does not explicitly optimise the model for classification, it would be interesting to investigate what other information is contained in the representations. Furthermore, the representations are currently generated by a supervised approach, since the labels are used in the loss function. The use of labels is generally not desirable because the manual creation of labels is time-consuming. An unsupervised (or self-supervised) approach, on the other hand, extracts features without using labels. Moreover, it is known that the brain also functions unsupervised to a very large extent and is thus biologically more plausible.
Thus, it would be beneficial be the implementation of a loss function that does not rely on labels. Furthermore, the model should be tested on other data sets than MNIST to evaluate how well it can scale.

The proposed approach for vertical self-organisation uses a novel kind of proxy objective function. Models based on proxy objective functions are biologically more plausible and a hot research topic (c.f. \secref{alt_train_algo}). Recently, many algorithms have been proposed, such as the forward-forward algorithm \sidecite{ff_algo}, which have caused a stir in the community by challenging the classical backpropagation of error as optimisation algorithm. However, despite the fact that they are biologically more plausible, these algorithms don't provide many benefits yet. Models trained with backpropagation of error are still superior, especially on well-known tasks such as classification. One of the author's hope was that such algorithms have other advantages such as better robustness. However, the evaluation of the models presented in this work and other models such as the forward-forward algorithm have crushed this initial believes.


\subsection{Horizontal Self-Organisation}\seclbl{future_hso}
TODO: this section is not yet complete as many experiments are still ongoing...


TODO: Conclusion


Similar to vertical self-organisation (c.f. \secref{future_vso}), the prediction accuracy could be improved by using more prototypes per class, as digits of the same class can look different. In the case of horizontal self-organisation, the $\boldsymbol{\mu}$ values per VAE should be clustered hierarchically to obtain groups of similar looking digits per class. Afterwards, the average value per group can be calculated to obtain more prototypes per class.

A very important concept in the human brain is the prevention of early commitment\sidecite{Marr_2010} (c.f. \secref{future_3_stage}), i.e. that a single entity such as a network layer not commits to something and then persists. Typical CNN architectures for classification have this problem by design, as they combine low-level features to higher-level features. With low-level features, it is already determined which feature is within the image before they are combined to higher-level features. The proposed architecture for horizontal self-organisation reduces this problem by creating representations of image patches. The patches are that small that they contain only information to build low-level features. For example, the representations from a single VAE cannot be clearly assigned to a class and thus do not contain enough information to commit to an object. Therefore, the VAEs can be considered as low-level feature extractors such as the first layer of a neural network. To create higher-level features, communication with neighbouring VAEs (i.e. when looking at the big picture) is needed. Each VAE extracts low-level features (local view) and has to communicate with other VAEs (global view) to determine what they represent. Therefore, this architecture is less susceptible to the early commitment problem than typical end-to-end trained neural networks. Furthermore, the representations are learned unsupervised. For classification, the labels are fed into the network \emph{after} training in order to compute average class representations. Thus, no class representation but an image representation is learned during training. This further reduces the problem of individual VAEs committing to a class too early.

In future work, this effect should even be intensified by further reducing the size of the patches and using more VAEs. It can be assumed that with $16$ VAEs ($4\times4$) the field of view is so restricted that each VAE can only produce very poor representations and these can only be assigned to a class in a random fashion. With more VAEs, interaction can also be restricted to local communication. With $2\times2$ VAEs, each VAE automatically receives information about the entire image when it communicates with its neighbours. With $4\times4$ VAEs, on the other hand, it is possible to have each VAE communicate only with its immediate neighbours. In this case, global image information must be propagated over several time-steps from one VAE to the next.

Future work should also address a key problem of this architecture: Currently, this concept only works with images in which the objects always appear at approximately the same place in the image. This can be remedied if the VAEs are applied at different places in the image, similar to a kernel in a convolutional layer. In this case, however, the same VAE sees all image data, which means that it implicitly receives not only local but global image information. This could reduce the independence of each VAE. Consequently, it should be investigated how this architecture can become translation invariant in a similar way as CNNs.


\section{3-Staged Model}\seclbl{future_3_stage}
Christoph von der Malsburg, co-supervisor of this thesis and one of the main authors of the ``Natural Intelligence'' paper \sidecite{von_der_Malsburg_Stadelmann_Grewe_2022} which is the main inspiration of this thesis, is one of the pioneers of the theory of self-organisation of ordered fibre connections in the visual system \sidecite{Wolfrum_Wolff_Lücke_vonderMalsburg_2008, Willshaw_VonDerMalsburg_1979, wiskott1996face, Fernandes_vonderMalsburg_2015}. His theory is based on a $3$-staged model. Currently, all people involved in this thesis are working on writing down this model and relating it to current deep learning architectures. This work has resulted from the collaboration between neuroscientists and computer scientists and the fact that there are often misunderstandings between these field due to a non-uniform technical vocabulary. Writing it down should enable non-neuroscientists to understand these exciting concepts better and to relate them to the context of deep learning. The resulting (hopefully publishable) paper will not be included as part of this Master's thesis, as it was not written by the author alone. Nevertheless, it explains interesting concepts that are promising for future research and highly relevant to this thesis.

Von der Malsburg's theory refers to a image perception model with three stages S$1$-S$3$, and projection fibres between these stages.
The stage S$1$ refers to the first stage which is responsible for the extraction of features from images. In this stage, one principle is particularly important, namely to avoid the ``fallacy of early commitment''. \sidecite{Marr_2010}.
The idea is that local decisions should be only be taken if plausible in the light of high-level features while the high-level features can only be defined on the basis of low-level patterns.
The brain can handle this very well, as the Gestalt psychology shows\sidenote{Gestalt psychology is about intelligent beings perceiving entire patterns, not individual components}. \sidecite{kohler1929gestalt}.
Modern deep learning networks can't deal with this and often recognise patterns in the first layers, committing to something specific that is actually something else in the light of the big picture. The core mechanism in the human brain to deal with this is, according to von der Malsburg, a mechanism based on lateral connections between neurons that are spatially close to each other. Initially, a large number of neurons are activated by the retina's signals. However, these neurons are immediately turned off if they do not have sufficient lateral support from neighbouring neurons. The lateral connections are thus there to support each other in order to remain active. This not only has the effect of ``mutually confirming each other'' but also helps to form higher-level features from low-level patterns. Intuitively speaking, and without neuroscientific correctness, one can imagine that one red, two brown, and one green pixel cluster are captured by the retina and active some neurons. The green pixel group receives too little lateral support and the corresponding neurons are quickly switched off again. The two brown and the red pixel groups support each other and remain active. But this is only because the two brown pixel groups are arranged in a circle at the same height and below them the red pixel group is in an elongated shape - a familiar pattern. If one looks at the big picture, one can see that this is a mouth and two eyes (locally, however, is remain unknown that this might be a face). It is important to notice that

\begin{itemize}
	\item A global view on the whole pattern is necessary to tell what the local patterns are. For example, we don't see eyes and decide directly on eyes, but wait until we see the whole image to conclude that these brown pixel clusters could be eyes and maybe belong to a face
	\item Pixel constellations only support each other if they are arranged accordingly. For example, if all three pixel clusters were at the same height, we would not perceive this pattern as a typical face (the lateral support might be insufficient and the assumption that it is a face is rejected).
	\item The emergence of higher-level features is done in one layer through lateral connections and is not hierarchically formed across multiple layers as in deep learning.
\end{itemize}


Since these pixel groups keep each other active, neurons that represent insignificant features are continually switched off and thus implicitly higher-level features are formed. The lateral support, however, is limited to local patterns, i.e. it is not sufficient to recognise whole objects or scenes, but only parts of them. In S$1$, many higher-level features are precisely identified. In stage S$2$, however, these features are mapped to whole objects, for example faces (in terms of our example). These object representations in S$2$ are independent of transformation and position. The mapping of several low-level features to a concrete object is done by projection fibers. These implicitly remove transformations and position information from the features and assemble them into objects. Projection fibers can be interpreted as a kind of graph that combines features hierarchically, ignoring local distortions to create transformation independent object detection. Stage S$3$, on the other hand, stores very abstract prototypes of such objects, and by matching S$2$ and S$3$, the captured object can finally be identified as the face of a specific person.

The model described contains very interesting concepts. Especially the prevention of early commitment and the use of dynamic fibres seem very promising. A concrete implementation that works for complex images like natural photos has not been implemented yet, despite a lot of effort in research. Maybe,  there is potential in using the recently popularised graph networks as projection fibres and thus learning the projections without relying on restricted mathematical models. For example, either VAE or VQ-VAE could be used as feature extractors, as in horizontal self-organisation. The advantage of these auto-encoder types is that they can extract local patterns and describe them as vectors in a ``meaningful'' latent space, i.e. similar vectors lie closer together in this latent space or, in the case of VQ-VAEs, are even discrete values. This facilitates the statistical learning and mapping of similar embedding vectors together. Horizontal self-organisation (c.f. \secref{future_hso}) is considered an important preliminary work to implement such a system. In contrast to horizontal self-organisation, auto-encoders could be applied at any image position to obtain continuous pixel representations from the same latent space. The auto-encoders can thus be used as feature extractors to extract good representations of small image patches. Afterwards, graph neural networks (GNNs) could be used to combine the embedding vectors into higher-level features. GNNs have already been used for image classification in recent years, mainly by identifying super-pixels and connecting them by graphs to recognise object structures \sidecite{Long_yan_chen_2021}.  One problem with GNNs is that they are often flat, i.e. many nodes lie on the same plane. This does not comply to the model of projection fibres, which must be hierarchical due to the large number of combinations. A remedy to this issue could be using differential pooling \sidecite{10.5555/3327345.3327389, Vasudevan_Bassenne_Islam_Xing_2023}.

However, the goal is not to identify objects with graph structures. Rather, the graph should be able to match objects from images with abstract prototypes. In this way, the network should learn to ignore slight transformations and rotations. Therefore, I suggest using feeding embeddings instead of super-pixels in to GNNs to obtain graph representations of images, as embeddings can be tuned to the use case and contain more information than super-pixels. Next, and much more importantly, GNNs for object recognition can be combined with graph matching networks \sidecite{li2019graph, Xu_Nikolentzos_Vazirgiannis_Boström_2022} and thus might be used to match objects locally and globally despite slight transformations. This matching would then correspond to projection fibers from L$1$ to L$2$ and compare objects within an image with mental prototypes. However, many questions remain, such as how objects are identified in images (i.e. how to remove the background) or how the mental prototypes get into L$2$ in order that they can be matched. Furthermore, one must think about an iterative matching to avoid early commitment. 



\section{Useful Representations}\seclbl{useful_representations}
In this thesis, models are proposed that extract representations of images.
However, the question how useful this representations are still remains\sidenote{despite for the usual ML tasks such as image classifications}.
How useful a representation is often depends on the use case. For example, representations from modern deep learning systems are very useful for typical vision tasks such as classification or segmentation. In fact, it can even be argued that deep learning is often superior to humans. For example, modern deep learning systems \sidecite{DBLP:conf/bmvc/JungLO0SP22} are able to identify millions of faces with more than $99\%$ accuracy, which is very difficult for humans \sidenote{at least to distinguish such a large number of people}. For such tasks, the representations of deep learning systems seem very well suited, 

However, what works poorly for deep learning systems is to recognise an object as the same instance, regardless of which transformations have been applied to the object. In general, it seems to be a problem that deep learning systems cannot learn a good world model and understand transformations applied to the objects of this model. One way to address this problem is to allow the model to interact with the world (i.e. perform actions). This allows it to learn how an action changes the view of an object. If the same actions are applied to different models, an object-independent transformation behaviour can be learned. This could allow a model to understand which views represent the same object in the world model and what kind of transformations have been applied to it.

It is known from the study of animals that both eye movements and the behavioural state influence the responses of neurons in the visual cortex \sidecite{Keller_Bonhoeffer_Hübener_2012}.
Thus, animals integrate their action (i.e. the movement they are doing) with currently incoming sensory signals to predict future sensory inputs.
The internal copy of an outflowing movement-producing signal generated by an organism's motor system is also known as efference copy.
Keurti et al. \sidecite{Keurti_Pan_Besserve_Grewe_Schölkopf_2022} argue that such efference copies are useful to learn \emph{useful} latent representations perceived by the visual system.
They translated this idea into an AI-based system by allowing an agent to interact with the environment and to observe its state to build internal representations.
In fact, the enforce that transformations of the real-world can also be applied on latent representations, i.e. that the representations of object and the real-world objects remain consistent when similar transformations are applied on them.

Giving an agent an embodiment\sidenote{in this context, an embodiment can also be virtual, i.e. allow the agent to interact with objects} to interact with the world to better understand it and to create better representations seems not only important from a neuroscientific perspective, but is also in line with theories from psychology.
Piaget \sidecite{Piaget_1964} argues that perceiving an object rather about understanding of how an object transforms and behaves under different interaction and not about creating a mental copy of it.

Such an agent can be implemented, for example, with reinforcement learning.
The training process could be explicitly modelled by predicting future states based on a given state and possible actions before the action is executed and the actual outcome is observed in the world model.
This procedure corresponds to the perception-action episode that is also postulated by LeCun \sidecite{LeCun_AMI}.
He divides the process in seven steps;
(i) First, the perception system extracts a representation of the current state of the world $s[0]=P(x)$. (ii) The actor then proposes an initial sequence of actions $(a[0], ..., a[t], ... a[T])$ that is evaluated by the world model. (iii) The world model in turn predicts likely sequence of world state representations resulting from the proposed action sequence $(s[1], ..., s[t], ... s[T])$. (iv) A cost model estimates the total costs for each state sequence as a sum over time steps $F(x)=\sum_{t=1}^{T}C(s[t])$. (v) Based on the cost predictions, the actor proposes the action sequence with the lowest costs. (vi) The actor then executes one or a few actions (and not the entire action sequence) and the entire process is repeated. (vii) Additionally, every action, the states and associated costs are stored in a short-term memory that can be used to optimize the system.




\section{Cognition and Reasoning}\seclbl{cognition_reasoning}
At the end of this thesis, I want to share my personal opinion on the future of deep learning systems. I am of the opinion that deep learning systems can extract pixel-representations from images very well. I intentionally call it ``pixel''-representations and not object representations because these vectors contain information about pixel constellations which are not sufficient to describe objects but very well suited for tasks like classification or segmentation\sidenote{end-to-end backpropagation of error seems to be well suited for such tasks}. According to my definition, these pixel-representations are only a part of object representations. Object representations are, in my understanding, a mental construct that contains much more information about objects than how they look. For example, object representations should contain information about how an object behaves under different transformations. In the previous two sections (c.f. \secref{future_3_stage}, \secref{useful_representations}). it is discussed how such transformations could be learned. However, there are many other parameters that make up an object, such as what abilities (e.g. can it move?) it has and how it feels. The appearance of an object (which corresponds to the pixel-representations) is thus only one dimension of a high-dimensional formula that describes an object. For example, fish and ships look completely different on a pixel-level description and have visually nothing in common. For us humans, these two objects have a clear relationship defined by their ability to swim and the typical place where they are found, namely the sea.
However, many modern pattern extraction system only model the one-dimensional pixel-representations and not multi-dimensional object-representations and thus cannot build such relationships.
For me personally, the extension of these one-dimensional object representations (``what does the object look like'') to multi-dimensional representations is one of the key elements to be able to create a system with cognition and reasoning. This allows object hierarchies to be built on multiple dimensions: Instead of just a hierarchy of objects that look similar, we also need hierarchies of objects that behave similarly, have similar capabilities, feel similar, etc. In addition to this construction of multi-dimensional object representations, an understanding of the physics of the world is also necessary. This second type of representation summarises knowledge that relates to all objects. For example, the gravitational force of the earth, the fact that living beings cannot pass walk solid materials such as walls, etc.

The question that arises now is how such a system can be implemented. It is obvious that simply showing pictures and the corresponding labels is far from sufficient to learn such complex relationships. At least three key elements are required:


\begin{description}
	\item[The world] in which the agent lives must allow interactions. This allows the agent to learn representations better, for example through interactions, getting eference copies, and observations how the world behaves. This allows differences and similarities between objects to be identified, for example how two different objects behave when the same actions are applied to them. It also allows the agent to define what it does not yet know and to learn these things consciously (for example with an entropy-based loss function \sidecite{storck1995reinforcement}). This also means that a continuous input is necessary and the model cannot be trained with a sequence of random and independent images like many current vision models.
	\item[The network architecture] must, to some extent, not only enable but encourage the capture of the world structure. Personally, I think that a multi-dimensional world model is helpful to store and relate the appearance, behaviour, transformation capabilities, abilities, etc. of each object as well as a second model to store general knowledge about the world. % Multi dimensional world model, per object appearance, transformation behaviour, abilities, feeling, smell, ...
	\item[The learning algorithm] should take several things into account to learn a good world model. A key element seems to be that the learning algorithm decides itself what to learn (entropy-based on the knowledge in the world model). Furthermore, curriculum learning seems promising: for a high level of intelligence, an equally complex world must be available in order to be able to learn it. However, such worlds might be so complex that the agent is overwhelmed without a step-by-step complexation of the world\sidenote{this also applies to us humans: We have a prior knowledge through evolution, then our parents explain the world to us. In fact, we are not able to survive without proper support in the first years}.% continous learning, curriculum learning % LLM -> predicten next token -> predicten objects next state
\end{description}


Furthermore, I think we often underestimate the amount of information that such a system has to learn. It is often argued that today's models are huge and see far more data than a human. However, this does not take into account, for example, that people receive a boost through curriculum learning from many other people (e.g. parents, siblings, relatives, teachers, trainers, colleagues, etc.). Moreover, these people in turn draw on tens of thousands of generations of previous knowledge. A certain amount of this knowledge is also implicitly pre-programmed through the structure of the nervous system. According to Darwin's theory, this goes back to the first living creatures that lived billions of years ago. Why is this relevant? I see it as a very critical link to the design of the world in which an agent lives: we humans often define a task for agents such as landing a spaceship (LunarLander from Open AI \sidecite{1606.01540}) and believe that the agent learns to control such a ship. We humans immediately realise that we have to activate the boost in the correct direction to balance the rocket and make it sink less quickly to the ground. But we already have this prior knowledge before we even play the game. For example, we know the physics of gravity and it seems logical to us that a spaceship sinks to the ground within the atmosphere, while mountains or stars do not. An AI agent, on the other hand, does not have this knowledge - this does not have to be bad by definition but can be seen as a knowledge gap that has to be learned. The question here is whether an understanding of this can ever be learned if the input is only a visual scene and the agent cannot experience this force itself. Or why should an agent ever understand that a spaceship looks like a spaceship and that it is being pulled towards the earth by gravity, and not just see a funny looking collection of pixels that happens to be moving downwards at a constant speed?
In other words, we humans consider ourselves intelligent in the real 3D world we live in. But this intelligence is based on the fact that we are part of a huge population that has lived in this highly complex world for generations. Personally, I think it is questionable whether an agent can learn knowledge comparable to that of humans by interacting in a minimally simple world-simulation.

Assuming such a world exists. Then there is still the question how the design of a proper learning system looks like. An intelligent system consists of several components: One very crucial component is the perception system. From today's point of view, this is probably the component that is best mastered\sidenote{although there are undoubtedly things that need to be improved, c.f. \secref{future_3_stage}}. Around this system, a world-model has to be created, which stores the understanding about the world. One question is how this world-model can be created. It seems to be important that the agent learns to learn, i.e. finds out for itself what knowledge it lacks in the world model and acquires it from the real world. As mentioned, such behaviour can be implemented by an entropy-based loss function \cite{storck1995reinforcement}. This shifts the problem to the question of what knowledge is extracted from the world and stored in the world model (and thus implicitly defines what the agent learns). The knowledge stored in the world-model should be the multi-dimensional object information described in the first paragraph of this section, which provides a much richer understanding of the world than the current one-dimensional pixel representations. To get an actual understanding of the world, interactions with the world and prediction how the next state of the world look like seems to be very central\sidenote{for example, the simple task of predicting the next token of a text has led to the enormous capabilities of large language models}. A very simple implementation is a roll-out (i.e. predict how the world looks like after applying some actions and comparing this to the real world after these actions have been applied) \sidecite{LeCun_AMI}, more advanced seem to be GFlowNetworks \sidecite{NEURIPS2021_e614f646}. After such predictions are learned, I think that reversing this process as in diffusion models could help casual reasoning. In diffusion models, one learns to map from a specific state (a concrete image) to a very general state (noise). This process is reversed and over several steps, the model can build trajectories from noise to a sample through a combination of direct motion and random motion. If a similar concept is applied to the learned prediction of actions, then it can be predicted which action combinations have led to the current world state.

However, all these ideas are very abstract and have emerged in the course of this work. The concretisation of these ideas could be a thesis in itself and, unfortunately, the elaboration of such ideas is out-of-scope of this thesis. However, I hope to be able to address such questions as concrete research topics in the near future, ideally in a similar constellation as in this Master's thesis.

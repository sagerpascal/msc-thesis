%% future_work.tex

\section{Useful Representations}\seclbl{useful_representations}
In this thesis I describe an approach to generate representations of a visual scene.
However, the question how useful this representations are still remains\sidenote{despite for the usual ML tasks such as image classifications}.
The ultimate goal of perception systems is to build a simple and informative model of the external world.
This internal model should not be too detailed but include all behavior-relevant information.
Thus, the internal model should allow an agent to take the optimal action to fulfil a given task.

It is known from the study of animals that both eye movements and the behavioral state influence the responses of neurons in the visual cortex \sidecite{Keller_Bonhoeffer_Hübener_2012}.
Thus, animals integrate their action (i.e. the movement they are doing) with currently incoming sensory signals to predict future sensory inputs.
The internal copy of an outflowing movement-producing signal generated by an organism's motor system is also known as efference copy.
Keurti et al. \sidecite{Keurti_Pan_Besserve_Grewe_Schölkopf_2022} argue that such efference copies are useful to learn \emph{useful} latent representations perceived by the visual system.
They translated this idea into an AI-based system by allowing an agent to interact with the environment and to observe its state to build internal representations.

If useful latent representations of a visual scene are considered to be a world-model with all behavior-relevant information, it should be evaluated whether the representations obtain by the proposed model correspond to this definition.
If not, which is likely according to the author's intuition, the latent representations should be optimized accordingly.
This implies that the existing perception system should be extended by cognition.
Thus, the AI system needs an embodiment to interact with the environment.
For example, this can be simulated with a reinforcement learning based agent.
The training process could be explicitly modeled by predicting future states based on a given state and possible actions before the action is executed and the actual outcome in the world model is observed.
This procedure corresponds to the perception-action episode that was proposed by LeCun \sidecite{LeCun_AMI}.
He divides the process in seven steps;
(i) First, the perception system extracts a representation of the current state of the world \(s[0]=P(x)\). (ii) The actor then proposes an initial sequence of actions \((a[0], ..., a[t], ... a[T])\) that is evaluated by the world model. (iii) The world model in turn predicts likely sequence of world state representations resulting from the proposed action sequence \((s[1], ..., s[t], ... s[T])\). (iv) A cost model estimates the total costs for each state sequence as a sum over time steps \(F(x)=\sum_{t=1}^{T}C(s[t])\). (v) Based on the cost predictions, the actor proposes the action sequence with the lowest costs. (vi) The actor then executes one or a few actions (and not the entire action sequence) and the entire process is repeated. (vii) Additionally, every action, the states and associated costs are stored in a short-term memory that can be used to optimize the system.

%TODO: Write more about Paper from Grewe \sidecite{Keurti_Pan_Besserve_Grewe_Schölkopf_2022}


%TODO: Stand jetzt sind es eher neue Module wie lateral connections -> gute Repräsentationen benötigen aber neue Architekturen, z.B. eine Hierarchie (Tiere -> Säugetiere, Reptilien, -> ...) und eine Menge an Actions, die jedes Element in der Hierarchie ausführen kann (z.B. drehen kann jedes Tier, Gehen nur die Landtiere und Schreiben nur der Mensch) -> Schreibe konkrete Vorschläge für Folgearbeiten!

% Bengio in TheBatch vom Dez.: Recent advances in deep learning largely have come by brute force: taking the latest architectures and scaling up compute power, data, and engineering. Do we have the architectures we need, and all that remains is to develop better hardware and datasets so we can keep scaling up? Or are we still missing something?
%I believe we’re missing something, and I hope for progress toward finding it in the coming year. 
%I’ve been studying, in collaboration with neuroscientists and cognitive neuroscientists, the performance gap between state-of-the-art systems and humans. The differences lead me to believe that simply scaling up is not going to fill the gap. Instead, building into our models a human-like ability to discover and reason with high-level concepts and relationships between them can make the difference.
%Consider the number of examples necessary to learn a new task, known as sample complexity. It takes a huge amount of gameplay to train a deep learning model to play a new video game, while a human can learn this very quickly. Related issues fall under the rubric of reasoning. A computer needs to consider numerous possibilities to plan an efficient route from here to there, while a human doesn’t.
%Humans can select the right pieces of knowledge and paste them together to form a relevant explanation, answer, or plan. Moreover, given a set of variables, humans are pretty good at deciding which is a cause of which. Current AI techniques don’t come close to this human ability to generate reasoning paths. Often, they’re highly confident that their decision is right, even when it’s wrong. Such issues can be amusing in a text generator, but they can be life-threatening in a self-driving car or medical diagnosis system.
%Current systems behave in these ways partly because they’ve been designed that way. For instance, text generators are trained simply to predict the next word rather than to build an internal data structure that accounts for the concepts they manipulate and how they are related to each other. But I think we can design systems that track the meanings at play and reason over them while keeping the numerous advantages of current deep learning methodologies. In doing so, we can address a variety of challenges from excessive sample complexity to overconfident incorrectness. 
%I’m excited by generative flow networks, or GFlowNets, an approach to training deep nets that my group started about a year ago. This idea is inspired by the way humans reason through a sequence of steps, adding a new piece of relevant information at each step. It’s like reinforcement learning, because the model sequentially learns a policy to solve a problem. It’s also like generative modeling, because it can sample solutions in a way that corresponds to making a probabilistic inference.
%If you think of an interpretation of an image, your thought can be converted to a sentence, but it’s not the sentence itself. Rather, it contains semantic and relational information about the concepts in that sentence. Generally, we represent such semantic content as a graph, in which each node is a concept or variable. GFlowNets generate such graphs one node or edge at a time, choosing which concept should be added and connected to which others in what kind of relation. 
%I don’t think this is the only possibility, and I look forward to seeing a multiplicity of approaches. Through a diversity of exploration, we’ll increase our chance to find the ingredients we’re missing to bridge the gap between current AI and human-level AI.

%Input von Thilo: https://xcorr.net/2023/01/01/2022-in-review-neuroai-comes-of-age/?utm_source=substack&utm_medium=email


%TODO: Meinung von Autor ergänzen (von mir): Backprop gut für Pattern recognition -> sinnvolle Vektoren aus Zahlen -> Cognition fehlt, Cognition sollte Feedback an Pattern Extraction sein, zurzeit keine Cognition sondern nur Korrelation zwischen Pattern und Label
%TODO: Umschreiben: Schreibe DL funktioniert gut, sehr gut für Pattern Extraction. Dies ist aber nur ein Baustein eines intelligenten Systems. Beispielsweise fehlt cognition komplett (z.B. in NLP sind Vektoren wie König - Königin = Mann - Frau -> das System korrepiert die Informationen aber nur und hat keine Ahnung, was dies bedeutet. Um dies zu erreichen könnte nach dem mit Backprop trainierten Pattern-Extraction System ein weiteres System nötig sein. Weitere Bausteine könnten sein: Continous Input, Netzfragmente, Sparsity, anderer Lernalgorithmus, World-Modell, ...

%TODO: VAE funktioniert aktuell nur bei zentrierten Objekten -> bei grossen Bilder muss Möglichkeit bestehen zu sagen ich bin Background oder ich bin Objekt xy und der andere VAE sagt ich bin Objekt ab
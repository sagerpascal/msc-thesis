%% future_work.tex

Deep learning is far from being a system with human-like intelligence. Consequently, there is a massive amount of work to be done in the future to get closer to this ultimate goal. However, the implementations presented in this thesis are far from such a system. Consequently, a conclusion is drawn and next steps are proposed for the models presented in this thesis, but a long-term vision is presented as well. Specifically, future work is described for vertical self-organisation in \secref{future_vso} and for horizontal self-organisation in \secref{future_hso}. Afterwards, in \secref{future_3_stage}, a new model is presented on a very abstract level, which could work better intuitively, but a concrete implementation is unclear. In \secref{useful_representations} the usefulness of representations is discussed. In the last \secref{cognition_reasoning}, the author gives a personal opinion about the future of AI, which is not supported by scientific arguments but might be interesting for (motivated) readers.


\section{Vertical Self-Organisation}\seclbl{future_vso}


\section{Horizontal Self-Organisation}\seclbl{future_hso}

%TODO: VAE funktioniert aktuell nur bei zentrierten Objekten -> bei grossen Bilder muss Möglichkeit bestehen zu sagen ich bin Background oder ich bin Objekt xy und der andere VAE sagt ich bin Objekt ab



\section{3-Staged Model}\seclbl{future_3_stage}
Christoph von der Malsburg, co-supervisor of this thesis and one of the main authors of the ``Natural Intelligence'' paper \sidecite{von_der_Malsburg_Stadelmann_Grewe_2022} which is the main inspiration of this thesis, is one of the pioneers of the theory of self-organisation of ordered fibre connections in the visual system \sidecite{Wolfrum_Wolff_Lücke_vonderMalsburg_2008, Willshaw_VonDerMalsburg_1979, wiskott1996face, Fernandes_vonderMalsburg_2015}. His theory is based on a $3$-staged model. Currently, all people involved in this thesis are working on writing down this model and relating it to current deep learning architectures. This work has resulted from the collaboration between neuroscientists and computer scientists and the fact that there are often misunderstandings between these field due to a non-uniform technical vocabulary. Writing it down should enable non-neuroscientists to understand these exciting concepts better and to relate them to the context of deep learning. The resulting (hopefully publishable) paper will not be included as part of this Master's thesis, as it was not written by the author alone. Nevertheless, it explains interesting concepts that are promising for future research and highly relevant to this thesis.

Von der Malsburg's theory refers to a image perception model with three stages S$1$-S$3$, and projection fibres between these stages.
The stage S$1$ refers to the first stage which is responsible for the extraction of features from images. In this stage, one principle is particularly important, namely to avoid the ``fallacy of early commitment''. \sidecite{Marr_2010}.
The idea is that local decisions should be only be taken if plausible in the light of high-level features while the high-level features can only be defined on the basis of low-level patterns.
The brain can handle this very well, as the Gestalt psychology shows\sidenote{Gestalt psychology is about intelligent beings perceiving entire patterns, not individual components}. \sidecite{kohler1929gestalt}.
Modern deep learning networks can't deal with this and often recognise patterns in the first layers, committing to something specific that is actually something else in the light of the big picture. The core mechanism in the human brain to deal with this is, according to von der Malsburg, a mechanism based on lateral connections between neurons that are spatially close to each other. Initially, a large number of neurons are activated by the retina's signals. However, these neurons are immediately turned off if they do not have sufficient lateral support from neighbouring neurons. The lateral connections are thus there to support each other in order to remain active. This not only has the effect of ``mutually confirming each other'' but also helps to form higher-level features from low-level patterns. Intuitively speaking, and without neuroscientific correctness, one can imagine that one red, two brown, and one green pixel cluster are captured by the retina and active some neurons. The green pixel group receives too little lateral support and the corresponding neurons are quickly switched off again. The two brown and the red pixel groups support each other and remain active. But this is only because the two brown pixel groups are arranged in a circle at the same height and below them the red pixel group is in an elongated shape - a familiar pattern. If one looks at the big picture, one can see that this is a mouth and two eyes (locally, however, is remain unknown that this might be a face). It is important to notice that

\begin{itemize}
	\item A global view on the whole pattern is necessary to tell what the local patterns are. For example, we don't see eyes and decide directly on eyes, but wait until we see the whole image to conclude that these brown pixel clusters could be eyes and maybe belong to a face
	\item Pixel constellations only support each other if they are arranged accordingly. For example, if all three pixel clusters were at the same height, we would not perceive this pattern as a typical face (the lateral support might be insufficient and the assumption that it is a face is rejected).
	\item The emergence of higher-level features is done in one layer through lateral connections and is not hierarchically formed across multiple layers as in deep learning.
\end{itemize}


Since these pixel groups keep each other active, neurons that represent insignificant features are continually switched off and thus implicitly higher-level features are formed. The lateral support, however, is limited to local patterns, i.e. it is not sufficient to recognise whole objects or scenes, but only parts of them. In S$1$, many higher-level features are precisely identified. In stage S$2$, however, these features are mapped to whole objects, for example faces (in terms of our example). These object representations in S$2$ are independent of transformation and position. The mapping of several low-level features to a concrete object is done by projection fibers. These implicitly remove transformations and position information from the features and assemble them into objects. Projection fibers can be interpreted as a kind of graph that combines features hierarchically, ignoring local distortions to create transformation independent object detection. Stage S$3$, on the other hand, stores very abstract prototypes of such objects, and by matching S$2$ and S$3$, the captured object can finally be identified as the face of a specific person.

The model described contains very interesting concepts. Especially the prevention of early commitment and the use of dynamic fibres seem very promising. A concrete implementation that works for complex images like natural photos has not been implemented yet, despite a lot of effort in research. Maybe,  there is potential in using the recently popularised graph networks as projection fibres and thus learning the projections without relying on restricted mathematical models. For example, either VAE or VQ-VAE could be used as feature extractors, as in horizontal self-organisation. The advantage of these auto-encoder types is that they can extract local patterns and describe them as vectors in a ``meaningful'' latent space, i.e. similar vectors lie closer together in this latent space or, in the case of VQ-VAEs, are even discrete values. This facilitates the statistical learning and mapping of similar embedding vectors together. In contrast to horizontal self-organisation, auto-encoders could be applied at any image position to obtain continuous pixel representations from the same latent space. The auto-encoders can thus be used as feature extractors to extract good representations of small image patches. Afterwards, graph neural networks (GNNs) could be used to combine the embedding vectors into higher-level features. GNNs have already been used for image classification in recent years, mainly by identifying super-pixels and connecting them by graphs to recognise object structures \sidecite{Long_yan_chen_2021}.  One problem with GNNs is that they are often flat, i.e. many nodes lie on the same plane. This does not comply to the model of projection fibres, which must be hierarchical due to the large number of combinations. A remedy to this issue could be using differential pooling \sidecite{10.5555/3327345.3327389, Vasudevan_Bassenne_Islam_Xing_2023}.

However, the goal is not to identify objects with graph structures. Rather, the graph should be able to match objects from images with abstract prototypes. In this way, the network should learn to ignore slight transformations and rotations. Therefore, I suggest using feeding embeddings instead of super-pixels in to GNNs to obtain graph representations of images, as embeddings can be tuned to the use case and contain more information than super-pixels. Next, and much more importantly, GNNs for object recognition can be combined with graph matching networks \sidecite{li2019graph, Xu_Nikolentzos_Vazirgiannis_Boström_2022} and thus might be used to match objects. This matching would then correspond to projection fibers from L$1$ to L$2$ and compare objects within an image with mental prototypes. However, many questions remain, such as how objects are identified in images (i.e. how to remove the background) or how the mental prototypes get into L$2$ in order that they can be matched.



\section{Useful Representations}\seclbl{useful_representations}
In this thesis, models are proposed that extract representations of images.
However, the question how useful this representations are still remains\sidenote{despite for the usual ML tasks such as image classifications}.
How useful a representation is often depends on the use case. For example, representations from modern deep learning systems are very useful for typical vision tasks such as classification or segmentation. In fact, it can even be argued that deep learning is often superior to humans. For example, modern deep learning systems \sidecite{DBLP:conf/bmvc/JungLO0SP22} are able to identify millions of faces with more than $99\%$ accuracy, which is very difficult for humans \sidenote{at least to distinguish such a large number of people}. For such tasks, the representations of deep learning systems seem very well suited, 

However, what works poorly for deep learning systems is to recognise an object as the same instance, regardless of which transformations have been applied to the object. In general, it seems to be a problem that deep learning systems cannot learn a good world model and understand transformations applied to the objects of this model. One way to address this problem is to allow the model to interact with the world (i.e. perform actions). This allows it to learn how an action changes the view of an object. If the same actions are applied to different models, an object-independent transformation behaviour can be learned. This could allow a model to understand which views represent the same object in the world model and what kind of transformations have been applied to it.

It is known from the study of animals that both eye movements and the behavioural state influence the responses of neurons in the visual cortex \sidecite{Keller_Bonhoeffer_Hübener_2012}.
Thus, animals integrate their action (i.e. the movement they are doing) with currently incoming sensory signals to predict future sensory inputs.
The internal copy of an outflowing movement-producing signal generated by an organism's motor system is also known as efference copy.
Keurti et al. \sidecite{Keurti_Pan_Besserve_Grewe_Schölkopf_2022} argue that such efference copies are useful to learn \emph{useful} latent representations perceived by the visual system.
They translated this idea into an AI-based system by allowing an agent to interact with the environment and to observe its state to build internal representations.
In fact, the enforce that transformations of the real-world can also be applied on latent representations, i.e. that the representations of object and the real-world objects remain consistent when similar transformations are applied on them.

Giving an agent an embodiment\sidenote{in this context, an embodiment can also be virtual, i.e. allow the agent to interact with objects} to interact with the world to better understand it and to create better representations seems not only important from a neuroscientific perspective, but is also in line with theories from psychology.
Piaget \sidecite{Piaget_1964} argues that perceiving an object rather about understanding of how an object transforms and behaves under different interaction and not about creating a mental copy of it.

Such an agent can be implemented, for example, with reinforcement learning.
The training process could be explicitly modelled by predicting future states based on a given state and possible actions before the action is executed and the actual outcome is observed in the world model.
This procedure corresponds to the perception-action episode that is also postulated by LeCun \sidecite{LeCun_AMI}.
He divides the process in seven steps;
(i) First, the perception system extracts a representation of the current state of the world $s[0]=P(x)$. (ii) The actor then proposes an initial sequence of actions $(a[0], ..., a[t], ... a[T])$ that is evaluated by the world model. (iii) The world model in turn predicts likely sequence of world state representations resulting from the proposed action sequence $(s[1], ..., s[t], ... s[T])$. (iv) A cost model estimates the total costs for each state sequence as a sum over time steps $F(x)=\sum_{t=1}^{T}C(s[t])$. (v) Based on the cost predictions, the actor proposes the action sequence with the lowest costs. (vi) The actor then executes one or a few actions (and not the entire action sequence) and the entire process is repeated. (vii) Additionally, every action, the states and associated costs are stored in a short-term memory that can be used to optimize the system.




\section{Cognition and Reasoning}\seclbl{cognition_reasoning}
At the end of this thesis, I want to share my personal opinion on the future of deep learning systems. I am of the opinion that deep learning systems can extract pixel-representations from images very well. I intentionally call it ``pixel''-representations and not object representations because these vectors contain information about pixel constellations which are not sufficient to describe objects but very well suited for tasks like classification or segmentation\sidenote{end-to-end backpropagation of error seems to be well suited for such tasks}. According to my definition, these pixel-representations are only a part of object representations. Object representations are, in my understanding, a mental construct that contains much more information about objects than how they look. For example, object representations should contain information about how an object behaves under different transformations. In the previous two sections (c.f. \secref{future_3_stage}, \secref{useful_representations}). it is discussed how such transformations could be learned. However, there are many other parameters that make up an object, such as what abilities (e.g. can it move?) it has and how it feels. The appearance of an object (which corresponds to the pixel-representations) is thus only one dimension of a high-dimensional formula that describes an object. For example, fish and ships look completely different on a pixel-level description and have visually nothing in common. For us humans, these two objects have a clear relationship defined by their ability to swim and the typical place where they are found, namely the sea.
However, many modern pattern extraction system only model the one-dimensional pixel-representations and not multi-dimensional object-representations and thus cannot build such relationships.
For me personally, the extension of these one-dimensional object representations (``what does the object look like'') to multi-dimensional representations is one of the key elements to be able to create a system with cognition and reasoning. This allows object hierarchies to be built on multiple dimensions: Instead of just a hierarchy of objects that look similar, we also need hierarchies of objects that behave similarly, have similar capabilities, feel similar, etc. In addition to this construction of multi-dimensional object representations, an understanding of the physics of the world is also necessary. This second type of representation summarises knowledge that relates to all objects. For example, the gravitational force of the earth, the fact that living beings cannot pass walk solid materials such as walls, etc.

The question that arises now is how such a system can be implemented. It is obvious that simply showing pictures and the corresponding labels is far from sufficient to learn such complex relationships. At least three key elements are required:


\begin{description}
	\item[The world] in which the agent lives must allow interactions. This allows the agent to learn representations better, for example through interactions, getting eference copies, and observations how the world behaves. This allows differences and similarities between objects to be identified, for example how two different objects behave when the same actions are applied to them. It also allows the agent to define what it does not yet know and to learn these things consciously (for example with an entropy-based loss function \sidecite{storck1995reinforcement}). This also means that a continuous input is necessary and the model cannot be trained with a sequence of random and independent images like many current vision models.
	\item[The network architecture] must, to some extent, not only enable but encourage the capture of the world structure. Personally, I think that a multi-dimensional world model is helpful to store and relate the appearance, behaviour, transformation capabilities, abilities, etc. of each object as well as a second model to store general knowledge about the world. % Multi dimensional world model, per object appearance, transformation behaviour, abilities, feeling, smell, ...
	\item[The learning algorithm] should take several things into account to learn a good world model. A key element seems to be that the learning algorithm decides itself what to learn (entropy-based on the knowledge in the world model). Furthermore, curriculum learning seems promising: for a high level of intelligence, an equally complex world must be available in order to be able to learn it. However, such worlds might be so complex that the agent is overwhelmed without a step-by-step complexation of the world\sidenote{this also applies to us humans: We have a prior knowledge through evolution, then our parents explain the world to us. In fact, we are not able to survive without proper support in the first years}.% continous learning, curriculum learning % LLM -> predicten next token -> predicten objects next state
\end{description}


Furthermore, I think we often underestimate the amount of information that such a system has to learn. It is often argued that today's models are huge and see far more data than a human. However, this does not take into account, for example, that people receive a boost through curriculum learning from many other people (e.g. parents, siblings, relatives, teachers, trainers, colleagues, etc.). Moreover, these people in turn draw on tens of thousands of generations of previous knowledge. A certain amount of this knowledge is also implicitly pre-programmed through the structure of the nervous system. According to Darwin's theory, this goes back to the first living creatures that lived billions of years ago. Why is this relevant? I see it as a very critical link to the design of the world in which an agent lives: we humans often define a task for agents such as landing a spaceship (LunarLander from Open AI \sidecite{1606.01540}) and believe that the agent learns to control such a ship. We humans immediately realise that we have to activate the boost in the correct direction to balance the rocket and make it sink less quickly to the ground. But we already have this prior knowledge before we even play the game. For example, we know the physics of gravity and it seems logical to us that a spaceship sinks to the ground within the atmosphere, while mountains or stars do not. An AI agent, on the other hand, does not have this knowledge - this does not have to be bad by definition but can be seen as a knowledge gap that has to be learned. The question here is whether an understanding of this can ever be learned if the input is only a visual scene and the agent cannot experience this force itself. Or why should an agent ever understand that a spaceship looks like a spaceship and that it is being pulled towards the earth by gravity, and not just see a funny looking collection of pixels that happens to be moving downwards at a constant speed?
In other words, we humans consider ourselves intelligent in the real 3D world we live in. But this intelligence is based on the fact that we are part of a huge population that has lived in this highly complex world for generations. Personally, I think it is questionable whether an agent can learn knowledge comparable to that of humans by interacting in a minimally simple world-simulation.

Assuming such a world exists. Then there is still the question how the design of a proper learning system looks like. An intelligent system consists of several components: One very crucial component is the perception system. From today's point of view, this is probably the component that is best mastered\sidenote{although there are undoubtedly things that need to be improved, c.f. \secref{future_3_stage}}. Around this system, a world-model has to be created, which stores the understanding about the world. One question is how this world-model can be created. It seems to be important that the agent learns to learn, i.e. finds out for itself what knowledge it lacks in the world model and acquires it from the real world. As mentioned, such behaviour can be implemented by an entropy-based loss function \cite{storck1995reinforcement}. This shifts the problem to the question of what knowledge is extracted from the world and stored in the world model (and thus implicitly defines what the agent learns). The knowledge stored in the world-model should be the multi-dimensional object information described in the first paragraph of this section, which provides a much richer understanding of the world than the current one-dimensional pixel representations. To get an actual understanding of the world, interactions with the world and prediction how the next state of the world look like seems to be very central\sidenote{for example, the simple task of predicting the next token of a text has led to the enormous capabilities of large language models}. A very simple implementation is a roll-out (i.e. predict how the world looks like after applying some actions and comparing this to the real world after these actions have been applied) \sidecite{LeCun_AMI}, more advanced seem to be GFlowNetworks \sidecite{NEURIPS2021_e614f646}. After such predictions are learned, I think that reversing this process as in diffusion models could help casual reasoning. In diffusion models, one learns to map from a specific state (a concrete image) to a very general state (noise). This process is reversed and over several steps, the model can build trajectories from noise to a sample through a combination of direct motion and random motion. If a similar concept is applied to the learned prediction of actions, then it can be predicted which action combinations have led to the current world state.

However, all these ideas are very abstract and have emerged in the course of this work. The concretisation of these ideas could be a thesis in itself and, unfortunately, the elaboration of such ideas is out-of-scope of this thesis. However, I hope to be able to address such questions as concrete research topics in the near future, ideally in a similar constellation as in this Master's thesis.

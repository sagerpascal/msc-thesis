%% methods.tex
This chapter describes the methodology used.
A large part of the contribution of this thesis consists of identifying appropriate findings from neuroscience and adapting them to a deep learning setting.
This identified concept and the link between the two disciplines is described in chapter \secref*{neuro_concepts}.
It should be noted that this section presents only one possible interpretation for the implementation of neuroscientific concepts in the context of deep learning and that alternative interpretations might also be promising.
Afterwards, two possible implementation approaches are described, which differ mainly in the type of local self-organisation.
These types of self-organisation are referred to as horizontal and vertical self-organisation and are described in Section \secref*{vertical_self_org} and Section \secref*{horizontal_self_org} respectively.




\section{Neuroscientific Concepts}\seclbl{neuro_concepts}

\subsection{Self-Organisation}\seclbl{neuro_concepts_self_org}
It is known that large parts of the human brain are self-organizing \sidecite{kelso1995dynamic}.
Self-organization is the process by which systems consisting of many units acquire their function through local interaction and without interference from a external supervisory system.
Recently, renowned scientists \sidecite{von_der_Malsburg_Stadelmann_Grewe_2022} put forward the hypothesis that this process of self-organization is \emph{the} key mechanism of natural intelligent systems such as the human brain.
Dresp \sidecite{Dresp2020SevenPO} describes seven clearly identified properties of self-organization in the human brain: (i) modular connectivity, (ii) unsupervised learning, (iii) adaptive ability, (iv) functional resiliency, (v) functional plasticity, (vi) from-local-to-global functional organization, and (vii) dynamic system growth.
However, it is not obvious how these insights from neuroscience can be integrated into a deep learning framework.

Deep learning networks are usually optimized with end-to-end backpropagation of error.
Thus, the entire network is optimized for a specific target.
This is considered a violation of the self-organisation principle, as a global update algorithm (i.e. the optimizer) adjusts all network weights to minimise a global target function.


\begin{claim}
	End-to-end backpropagation of error violates the principle of self-organization. Self-organisation in neural networks requires dividing the network into smaller units that are optimised independently of each other.
\end{claim}

In fact, the plausibility of backpropagation of error for explaining how the brain works was questioned soon after it was published \sidecite{Crick_1989, Grossberg_1987}.
Since then, many alternative and biologically more plausible algorithms have been proposed such as the feedback alignment (FA) algorithm \sidecite{Lillicrap_Cownden_Tweed_Akerman_2014}, generalized recirculation \sidecite{O_Reilly_1996}, as well as target propagation (TP) \sidecite{Le__Cun_1986} (c.f. Section \secref*{alt_train_algo}).
However, Bartunov et al. \sidecite{Bartunov_Santoro_Richards_Marris_Hinton_Lillicrap_2018} have demonstrated that these algorithms do not scale to large vision datasets such as ImageNet \cite{deng2009imagenet} and only work for smaller datasets such as MNIST \cite{MNIST} and CIFAR-10 \cite{cifar_10}.
The only algorithm that seems to scale well is using a proxy objective functions (c.f. Section \secref*{alt_train_algo}).

The biologically most plausible learning algorithm is Hebbian learning (c.f. Section \secref{hebbian}) and its variants such as contrastive Hebbian learning \sidecite{Movellan_1991}.
However, even tough I obtain some promising results in preliminary experiments with Hebbian learning (c.f. Appendix \chref{exp_hebb_learning}), this algorithm doesn't seem to be well suited to learn good image representation if a network is trained from scratch.

Thus, the use of proxy objective functions\sidenote{proxy objective functions are loss functions that are only applied to local units of a system} seems promising; the updates of weights are done in separate local units, yet the power of current deep learning training algorithms can be exploited and systems can be created that can solve complex problems and scale to large datasets by interacting with each other.

\begin{implementation}
	Instead of using end-to-end backpropagation of error to optimise the whole system according to a single global objective function, proxy objective functions are applied to local units. Thus, self-organisation takes place through the optimisation of local units instead of an overall system.
\end{implementation}

The next ambiguity is what local units are in a deep learning setting.
In deep learning models, typically not neurons are modelled but the trainable parameters\sidenote{the weights $\boldsymbol{w}$ and the bias $\boldsymbol{b}$ are modelled, so that the output $y$ can be calculated as $\boldsymbol{y}=\boldsymbol{w} \cdot \boldsymbol{x} + \boldsymbol{b}$ for given data $\boldsymbol{x}$}.
One of the strengths of deep learning systems is that matrix multiplications allow to calculate the layer outputs in one step.
Calculating each neuron activity separately, on the other hand, would be very inefficient.
Therefore, the smallest meaningful unit for local updates seems to be a layer and not a single neuron.
If a neural network is visualised layer-wise from left to right, then the self-organising units line up horizontally. This is why layer-wise self-organisation is also referred to as ``horizontal self-organisation'' in this thesis (c.f. Figure \figref*{horizontal_vertical_self_org}).

\begin{implementation}
	Self-organisation takes place within local units. A local unit can be a part of a model such as layer.
	In this thesis, this type of self-organisation is called horizontal self-organisation.
\end{implementation}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{horizontal_vertical_self_org}
    \caption[Overview of horizontal and vertical self-organization]{Two different ways of building self-organizing units. Self-organization can either take place horizontally (i.e. layer-wise) within a model (a) or vertically by splitting the data into patches and processing them with independent units (b). The independent units are marked with a red frame.}
    \figlbl{horizontal_vertical_self_org}
\end{figure}
 
A second type of self-organisation is not to split the model into separate units but to split the data.
The input data can be divided into smaller patches and then be processed by independent models.
It is important that one model does not process the entire set of existing patches, as is the case with the vision transformer. If only one model is used, there would again be an end-to-end backpropagation of error on a single unit.
But if the patches are processed by a graph of independent models, then each model can be considered as a self-organising unit.
In this thesis we call this kind of self-organisation the ``vertical self-organisation'' (c.f. Figure \figref*{horizontal_vertical_self_org}).

\begin{implementation}
	A second type of self-organising unit can be a model that processes a subset of input data that is not shared with other models. In this thesis, this type of self-organisation is called vertical self-organisation.
\end{implementation}


\subsection{Net-Fragments}\seclbl{neuro_concepts_net_fragments}
Another very important principle according to von der Malsburg et al. \cite{von_der_Malsburg_Stadelmann_Grewe_2022} is that neurons form net-fragments (a.k.a. sub-networks) that represent features of objects (c.f. section \secref{natural_intelligence}).
For example, some net-fragments may represent shapes and structures while a multitude of such net-fragments together represent objects such as persons or entire scenes.
Net fragments are a compositional data structure, meaning that some low-level features can be composed to a higher-level feature and multiple higher-level features are composed to an object.
To some extend, neural networks do this as well; data is fed into the network, the first layer extract some low-level patterns, and subsequent layer combine these patterns in higher-level features in a hierarchical manner.
However, there is one big difference: The features that are used to fulfil a specific task such as classification are extracted from the latent space of \emph{one} single layer.
Net-fragments in the human brain, on the other hand, are not considered to be present at a specific point in time but to be built up over a short period of time.

\begin{claim}
	Net fragments are represented by groups of neurons and their activity over multiple time-dependent spikes. In addition, several net fragments can be composed into a higher-level fragments. Thus, objects are not represented by a single neuron but by all neurons that were activated due to this object.
\end{claim}

Deep learning models are not based on time-dependent spikes of neurons that compose features to more complex features.
However, a time-step could be interpreted as a forward-step from one layer to the next within a network of layers.
Thus, net-fragments would be the activation of multiple layers.
This interpretation is also in line with the compositional property of net-fragments.
Low-level features that are detect within the first layers activate higher-level features in subsequent layers.
An object, however, is not represented in the last layer of a neural network but through all activations within the network.
Thus, representations of objects cannot be extracted from a single layer but from multiple layers.
Intuitively, this seems promising.
For example, auto-regressive models applied on speech capture different information at different layers of the network \sidecite{Chung_Hsu_Tang_Glass_2019}.
While the first layers contain more information to distinguish speakers, representations in later layers provide more phonetic content.
Thus, extracting information from several layers could lead to representations containing more information.

\begin{implementation}
	Net-fragments cannot be extracted from one single layer. Therefore, the representations of an image are extracted from multiple layers.
\end{implementation}

In the human brain, distinct groups of neurons represent specific net fragments.
This means that neurons are only active when the corresponding feature is present in the input and are inactive otherwise.
Moreover, the features represented by the neurons should be meaningful\sidenote{interpretable in the sense that a neuron is active if a specific feature is present and inactive otherwise} and consequently not active for every existing object.
This inevitably leads to the sparse and diverse activations.
The activations are sparse and diverse because, the object in the image consists of only a small sub-set of all learned features. Thus, only a small sub-set of the neurons should be active for a given object.
When the input changes and a different object is shown to the model, also the set of active neurons should change.

\begin{claim}
	The activation patterns of neurons that represent net-fragments must be sparse and diverse to obtain meaningful activation patterns.
\end{claim}

The extent to which neuronal networks contain or can produce net fragments with meaningful activation patterns is investigated in detail in a preliminary study.
The methodology used as well as an in-depth evaluation is described in the appendix in the Chapter \chref{net_fragments}.
In summary, it was found that neural networks do not contain net fragments with meaningful activation patterns by default. Typically, there are neurons that are always active regardless of the input data and encode most of the input information in their activation strength.
Other neurons, however, are never active and are therefore not needed by the network.
If, on the other hand, a sparsity and diversity constraint is applied, then layers are obtained with neurons that appear to be suitable for net fragments.

\begin{implementation}
	Sparse and diverse activation patterns can be obtained by imposing sparsity and diversity constraints to the objective function of the model.
\end{implementation}

However, sparsity and diversity constraints are mainly necessary to obtain meaningful and more robust activation patterns.
They don't seem to be necessary, for example, if the goal is to obtain good classification performance.
An alternative that lead also to robust and meaningful activations is to model the net-fragments as probability distribution.
For example, variational autoencoders (c.f. Section \secref*{visual_rep_learning}) model their latent space as a multivariate Gaussian distribution.

%Especially for models based on horizontal self-organisation, it seems important to impose these constraints on all layers.
%For models with vertical self-organisation, on the other hand, these constraints do not need to be applied to all layers, as long as there are enough models that can only see very small input patches. In this case, a sub-model can only recognise a very spatially-limited feature, which corresponds to a low-level feature. In order to recognise higher-level features or whole objects, interaction with neighbouring sub-models is necessary.
%Thus, net-fragments are composed over several sub-models.
%In this case, it is sufficient to impose the constraints to the function that composes the the representation of the sub-models to higher-level net fragments or to objects.

\subsubsection{Sparsity}\seclbl{neuro_concepts_sparsity}
Sparistiy seems to be an important principle in biological networks.
Presumably, this is because the neurons in the brain can be inhibitory or excitatory by firing spikes at different time intervals. An artificial neuron, on the other hand, uses a floating point number as its state and can thus represent an infinite number of different states, in contrast to the biological neuron.
Thus, an artificial neuron is able to represent many different things while the biological neuron is responsible for specific features.
Therefore, the activations of biological neurons \emph{must be} sparse, while the activations of artificial neurons \emph{do not have to be} sparse (but being sparse has also advantages for artificial neurons, see below).

Sparsity is deeply embedded in the biological learning process.
For example, in the visual cortex of mice are more than 75\% of the neurons active before the first opening of the eyes, 36\% after the opening of the eyes and only 12\% in adulthood \sidecite{Rochefort_Garaschuk_Milos_Narushima_Marandi_Pichler_Kovalchuk_Konnerth_2009}.
Thus, a sparsification of neuronal activations takes place through visual experience.
In the field of deep learning, sparsity is often interpreted in two different forms; sparse weight matrices and sparse activation matrices.
Sparse weight matrices are often chosen to make models smaller or to increase inference speed \sidecite{Louizos_Welling_Kingma_2018, Hoefler_Alistarh_Ben_Nun_Dryden_Peste_2021}.
From a biological point of view, this process of first creating a large network and then shrinking it is obviously not plausible\sidenote{otherwise we would have a large brain at the beginning, which becomes smaller by factors in the course of time}.
Sparse activations, on the other hand, can increase robustness \cite{Panousis_Chatzis_Theodoridis_2021}.
Intuitively, sparse activations enforce that only the most relevant information is passed to the subsequent layer.
Furthermore, combining sparsity with diversity can help to obtain more interpretable activations (c.f. Appendix Chapter \chref*{net_fragments}).

\subsection{Lateral Connections}\seclbl{neuro_concepts_lateral_connections}
In addition to forward connections, lateral connections are also located in visual cortex \sidecite{gilbert1990lateral}.
Thus, the biological neurons are not only connected to the neurons in the subsequent layer but also within the same layer.
Von der Malsburg et al. \cite{von_der_Malsburg_Stadelmann_Grewe_2022} describe that lateral connections help active neurons to support each other in order to remain active:
Initially, many neurons are active, but relatively quickly a part of the neuron becomes inactive and only those neurons that support themselves remain active. In this way, lateral connections can lead to the emergence of high-level features from initial activations.

\begin{claim}
	Lateral connections allow active neurons to support each other and to remain active.
\end{claim}


Neural networks lack this time dynamic and neurons do not suddenly become inactive during a forward pass. 
In addition, initial experiments have shown that performance does not improve when the activation maps of neural networks are sparsified over several steps.
One assumption is that all information is already contained in the data at the beginning and the same sparsified activations can also be generated in one initial step.

However, the lateral support between neurons seems promising if the input changes slightly:
An activation map can be calculated for a given input image. In a next step, the input can slightly be changed through data augmentation to btain another activation map from the same image. Thus, the network receives different views of the same image and generates different activation maps of the same image. In every step, the model can decide whether the activation map from the previous step was correct or if the activation map should be updated.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{lateral_connections}
    \caption[Illustrative network architecture of a model with lateral connections]{An illustrative network architecture of a model with lateral connections: An in sample $x$ is augmented $n$ times to obtain $n$ different views $x'[1], ..., x'[n]$ of the same sample. At every timestep $t$, the sample $x'[t]$ is fed through the model. Each layer receives the activation map of the previous layer as input as well as its own activation maps from the previous time-step $t-1$.}
    \figlbl{lateral_connections}
\end{figure}

This could be implemented by interpreting the lateral connection as a recurrent connection.
An illustrative architecture of such a model is shown in Figure \figref{lateral_connections}.
A data sample $x$ can be augmented with data augmentation methods $n$ times to obtain $n$ different views $x'[1], ..., x'[n]$ of the input sample.
Afterwards, the $n$ views are iteratively fed into the model.
At every time-step $t$, the model calculates an activation map $a_l[t]$ for each layer $l$.
If $t>1$, the input of the layer $l$ is not only the activation map of the layer $l-1$ but also the activation map of the previous time-step $a_l[t-1]$.
The activation map of the previous time-step is stored $a_l[t-1]$ by the recurrent (lateral) connection.
Thus, the activation map $a_l[t]$ is not only calculated based on the activation map of the previous layer $a_{l-1}[t]$ but also based on the activation map of the previous time-step $a_l[t-1]$.
This allows each layer to preserve the activation map $a_l[t-1]$ from the previous time-step or to correct it based on a slightly different input $a_{l-1}[t]$ from the previous layer. The lateral connections thus support the activations within a layer; a layer can look at several inputs and decide which features are present over several time-steps (support these features over several time-steps).

\begin{implementation}
	A lateral connection can be implemented as a recurrent connection that ``stores'' the layer's activation of a previous time-step.
	If the same input is present for several time-steps in a slightly augmented version, the layers can keep their previous activation maps (lateral support) or correct them.
\end{implementation}

With vertical self-organisation, the lateral connections can be implemented not only as recurrent connections within layers, but also as connections between the sub-models.
In this case, each sub-model extracts a distinct low-level feature.
Based on this feature, a guess can be made to which higher-level feature or object the extracted feature belongs to.
Through communication with neighbouring sub-models, this guess can either be supported or rejected.
Thus, the lateral connections are used as a ``support channel'' between sub-models. 

\begin{implementation}
	In the case of vertical self-organisation, a lateral connection can be implemented as connection between sub-models to vote for which high-level feature or object is represented in the image. Either the prediction of a model can be supported by neighbouring models or rejected.
\end{implementation}


\subsection{Other Principles}\seclbl{neuro_concepts_others}
Other important principles that are considered promising are a \emph{continuous input} signal and an \emph{embodiment} of the agent.
However, implementing such an interaction between an agent and an outside world is out of scope for this thesis but might be interesting for future work.

The visual cortex receives a continuous input signal.
This allows the tracking of moving objects and enables an object to be perceived from different angles. Since the change of the object between the captured frames is small, it can be determined that it is always the same object instance and consequently mapped to the same mental object prototype of a world model.

An ANN, on the other hand, is typically trained on samples that have little relation to each other.
When the system is trained on images, each frame is different; with videos, each sequence of frames is different.
A continuous input might help to get better representation of objects through self-supervised learning.
If an input is continuous and shows the same object from different angles or in different transformations (e.g. stretching) and it can be inferred that it is the same object then the object representations derived from this continuous stream can be homogenized.
These principles are already applied to some extent by self-supervised learning systems for computer vision.
In contrastive learning, a popular form of self-supervised learning, two different views are derived from one image by data augmentation, and their representations are then pushed closed together in the feature space \sidecite{chen2020simple, chen2020big, caron2020unsupervised}.
However, this paradigm is still quite limited since only two views of the same scene and not the continuous transformation of an object are presented to the learning system.

\begin{claim}
	A continuous input stream can help to build a better representation of objects, especially if the objects are slightly transformed between captured frames or if the point of view changes continuously and smoothly.
\end{claim}


Furthermore, efference copies of motor signals in the form of neuronal activities are directly sent to the brain’s sensory system if animals are moving \sidecite{Keller_Bonhoeffer_Hübener_2012}.
Such efference copies can be useful to better understand objects and how they behave when undergoing object transformations.
To do so, the agent must be allowed to perform actions and interact with its world.
This gives the agent more information about the objects but also about the physical properties of a (virtual or real) world. For example, he can perform different actions on different objects: He can rotate, squeeze, stretch or move objects. By doing so, he receives different visual and sensory feedback for identical actions on different objects.
The agent can map the captured feedback signals to the object representations of an internal world model.

\begin{claim}
	Allowing an agent to interact with the world can help to learn better object representations and to build a better world model.
\end{claim}

Furthermore, the agent can learn to understand objects better by improving his internal object representations on its own. For example, if the agent examines an object and his current internal object representation does not describe how the object looks from the side, then the agent can rotate the object accordingly and complete or correct the representation. 
Such a behaviour could be implemented within the agent itself, for example, by optimising an entropy-based loss in order that the agent explores the world and tries to learn unknown things \sidecite{storck1995reinforcement, Klapper_Rybicka_Schraudolph_Schmidhuber_2001}.



\section{Vertical Self-Organization}\seclbl{vertical_self_org}
In this thesis, two different concepts of self-organisation are implemented.
The first uses vertical self-organisation.
This means that within a model, smaller units are identified that are trained independently of the other units. (c.f. \secref{neuro_concepts_self_org}).
A linear layer was selected as one unit. This is a small unit (e.g. compared to a model part that contains many layers) but can still be trained efficiently.
Smaller units would be neurons or parts of a layer, but training them separately would be very inefficient, since the efficiency of matrix operations on GPUs could no longer be fully exploited\sidenote{the layer output can be efficiently calculated by a single matrix multiplications and addition, i.e. $\boldsymbol{z} = \boldsymbol{w} \cdot \boldsymbol{x} + \boldsymbol{b}$ (c.f. \secref{ann})}.

Each layer optimises its own proxy objective function.
Proxy objective functions are used because this type of local optimisation allows very good performance even on larger data sets, in contrast to other target functions such as target propagation, synthetic gradients or feedback alignment (c.f. \secref{alt_train_algo}). 
Figure \figref{vertical_gradient_flow} visualizes the updates based on a proxy objective function.
A objective function is calculated for each layer and the optimization algorithm only updates the parameters within this layer.
Thus, the gradients do not flow backwards to preceding layers.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{vertical_gradient_flow}
    \caption[The flow of gradients within the network based on vertical self-organisation]{The flow of gradients within the network based on vertical self-organisation: The data is fed from layer to layer during the forward pass. The layers, are trained independently and the gradients do not flow from one layer to the previous one.}
    \figlbl{vertical_gradient_flow}
\end{figure}

A combination of diversity and sparsity constraints is used as the loss function, which leads to representations that are easy to interpret and suitable for net fragments (c.f. \secref{neuro_concepts_net_fragments}) and also increases robustness (c.f. \secref{neuro_concepts_sparsity}).
Identical to the preliminary experiments in Appendix \chref{net_fragments}, the sparsity is achieved by using the kullback-leibler (KL) divergence \sidecite{10-5555-3042573-3042641}.
The model consists of several linear layers $l$ with a relu activation function $(z)^+ = max(0, z)$.
A layer consists of $m^{(l)}$ neurons and the activations $\boldsymbol{z}^{(l)}$ of a linear layer $l$ are calculated for a given input $\boldsymbol{z}^{(l-1)}$ as

\begin{equation}\eqlbl{vso_1}
		\boldsymbol{z}^{(l)} = z^{(l)}_1, ..., z^{(l)}_m = \boldsymbol{w}^{(l)} \cdot \boldsymbol{z}^{(l-1)} + \boldsymbol{b}^{(l)}
\end{equation}

whereby $\boldsymbol{w}^{(l)}$ is the weight and $\boldsymbol{b}^{(l)}$ the bias of layer $l$.

The activation probability can be calculated for each neuron. If a mini-batch contains $n$ samples, the activation probability $\hat{\rho}^{(i)}$ of a neuron $z^{(i)}$ can be calculated as:

\begin{equation}\eqlbl{vso_2}
		\hat{\rho}^{(l)}_i = \frac{1}{n} \sum^n_i \frac{1}{1+e^{-z^{(l)}_i}}
\end{equation}

where $\frac{1}{1+e^{-z^{(i)}}}$ is the sigmoid function that squeezes the activation in the range between $0$ and $1$.
With the KL divergence, the divergence of the current activation probability $\hat{\rho}^{(i)}$ and a desired activation probability $\rho=0.05$ can be calculated:

\begin{equation}\eqlbl{vso_3}
		KL(\rho || \hat{\rho}^{(l)}_i) = \rho \cdot \log \frac{\rho}{\hat{\rho}^{(l)}_i} + (1-\rho) \cdot \log \frac{1-\rho}{1-\hat{\rho}^{(l)}_i}
\end{equation}

The sparsity loss $L_{s}$ is the sum of the divergence between all $\hat{\rho}^{(i)}$ and $\rho$:

\begin{equation}\eqlbl{vso_4}
		L_{s}(\rho, \hat{\rho}) = \sum_{i=1}^{m} KL(\rho || \hat{\rho}^{(l)}_i)
\end{equation}

The second constraint is a diversity constraint. The goal is that the activations of \emph{different} objects are diverse. For this purpose, the activations $\boldsymbol{z}^{(l)}$ are made as identical as possible (i.e. pushed together in feature space) if they stem from the same class and as different as possible if they stem from different classes.
The cosine similarity is used to calculate the similarity between two activations $z^{(l)}_i$ and $z^{(l)}_j$:

\begin{equation}\eqlbl{vso_5}
		\text{cos}(z^{(l)}_i, z^{(l)}_j) = \frac{z^{(l)}_i \cdot z^{(l)}_j}{\max(||z^{(l)}_i||_2, ||z^{(l)}_j||_2)}
\end{equation}

In order to make representations of different objects different, resp. to make representations of identical objects identical, the information of image labels $y_i$ is needed. Thus, $y_i \in C$ where $C$ is the set of classes.
The diversity loss $L_d$ minimises the similarity $\text{cos}(z^{(l)}_i, z^{(l)}_j)$ if the two activations stem from different classes $y_i \neq y_j$, or maximises the similarity if they stem from the same class $y_i = y_j$.

\begin{equation}\eqlbl{vso_6}
		L_{d}(\boldsymbol{z}^{(l)}) = \frac{1}{n^2} \sum_{i=1}^{n} \sum_{j=1}^{n} k \cdot \text{cos}(z^{(l)}_i, z^{(l)}_j)
\end{equation}

whereby $k$ changes sign depending on the class:

\begin{equation}\eqlbl{vso_7}
		k = \begin{cases}
      		+1, & \text{if}\ y_i \neq y_j \\
      		-1, & \text{otherwise}
    	\end{cases}
\end{equation}

However, it was found that the loss is more stable if the similarity is not calculated between two activations but between one activation and the average activation of a class. The average activation of a class $c$ can be calculated over $n_c$ samples from this class as:

\begin{equation}\eqlbl{vso_8}
		z^{(l)}[c] = \frac{1}{n} \sum_{i=1}^{n_c} z^{(l)}_i \text{, for } y_i = c
\end{equation}

Another problem of is loss is if all activations are identical, then $\text{cos}(z^{(l)}_i, z^{(l)}_j) = 1$ and the loss gets $L_{d}=0$, since the similarities of the same and different classes neutralise each other.
Therefore, a margin between the similarities is enforced, as done for the triplet-margin-loss  \sidecite{Balntas_Riba_Ponsa_Mikolajczyk_2016}.

\begin{equation}\eqlbl{vso_9}
		L_{d}(\boldsymbol{z}^{(l)}) = \frac{1}{n} \sum_{i=1}^{n} \max(\text{cos}(z^{(l)}_i, z^{(l)}[v]) - \text{cos}(z^{(l)}_i, z^{(l)}[y_i]) + \text{margin}, 1)
\end{equation}

where $v$ is a random class drawn from the set $v ~ \{C \\ y_i\}$ and $\text{margin}$ is a hpyer-parameter that was set to $1$.
Thus, the triplet-margin-loss is calculated but the $L2$-norm is replaced by the cosine-similarity as distance measure and the positive, resp. negative anchor is replaced with the average class activation from the same resp. from a randomly selected different class.

The loss used is the sum of sparsity loss and diversity loss, with the diversity loss weighted by $\lambda=0.1$:
\begin{equation}\eqlbl{vso_10}
		L = L_{s}(\rho, \hat{\rho}) + \lambda \cdot L_{d}(\boldsymbol{z}^{(l)}) 
\end{equation}


One problem with proxy objective functions is that this type of training often requires layer-wise training. First, layer $1$ is trained completely, then the weights are frozen, then layer $2$ is trained and so on.
This is inefficient because it requires more forward-passes than if the model is trained with end-to-end backpropagation.
It was found that this loss functions allows to train all layers simultaneously. With a forward-pass, all activations are calculated, followed by a layer-wise backward-pass where the gradients from one layer do not propagate back into the previous layer.
Since the gradients are only needed locally, no large graph of gradients has to be calculated, which reduces the memory utilization on GPUs and allows larger mini-batch sizes. In addition, this type of architecture is very easy to parallelise, since theoretically each layer (i.e. each independently trained unit) or a group of layers can be assigned to a different GPU.
In contrast to end-to-end backpropagation of error architectures, the gradients only flow backwards locally on a GPU and do not have to be passed on to other GPUs.

\begin{figure}[h]
    \centering
    \resizebox{0.99\textwidth}{!}
{
\begin{tikzpicture}
\tikzstyle{connection}=[ultra thick,every node/.style={sloped,allow upside down},draw=\edgecolor,opacity=0.7]
\tikzstyle{copyconnection}=[ultra thick,every node/.style={sloped,allow upside down},draw={rgb:blue,4;red,1;green,1;black,3},opacity=0.7]


\node[canvas is zy plane at x=0] (input) at (0,0,0) {\includegraphics[width=8cm,height=8cm]{imgs/mnist.jpeg}};


\pic[shift={(3,0,0)}] at (input) 
    {Box={
        name=fcn1,
        caption=FC + ReLU,
        xlabel={{" ","dummy"}},
        zlabel=512,
        fill=\SoftmaxColor,
        opacity=0.8,
        height=3,
        width=3,
        depth=52
        }
    };


\draw [connection]  (input) ++(0,0,0)    -- node {\midarrow} (fcn1-west);


\pic[shift={(2,0,0)}] at (fcn1-east) 
    {Box={
        name=fcn2,
        caption=FC + ReLU,
        xlabel={{" ","dummy"}},
        zlabel=256,
        fill=\SoftmaxColor,
        opacity=0.8,
        height=3,
        width=3,
        depth=25
        }
    };


\draw [connection]  (fcn1-east)    -- node {\midarrow} (fcn2-west);


\pic[shift={(2,0,0)}] at (fcn2-east) 
    {Box={
        name=fcn3,
        caption=FC + ReLU,
        xlabel={{" ","dummy"}},
        zlabel=128,
        fill=\SoftmaxColor,
        opacity=0.8,
        height=3,
        width=3,
        depth=13
        }
    };


\draw [connection]  (fcn2-east)    -- node {\midarrow} (fcn3-west);


\pic[shift={(2,0,0)}] at (fcn3-east) 
    {Box={
        name=fcn4,
        caption=FC + ReLU,
        xlabel={{" ","dummy"}},
        zlabel=64,
        fill=\SoftmaxColor,
        opacity=0.8,
        height=3,
        width=3,
        depth=6
        }
    };


\draw [connection]  (fcn3-east)    -- node {\midarrow} (fcn4-west);


\end{tikzpicture}
}
    \caption[Architecture of the fully connected model with vertical self-organisation]{The network architecture of the fully connected model for vertical self-organisation with fully connected layers.}
    \figlbl{vertical_org_arch1}
\end{figure}


The model used consists of $4$ fully connected layers with relu activation. The first layer has $512$ neurons, the second $256$ neurons, the third $128$ neurons and the fourth $64$ neurons. The model is illustrated in Figure \figref{vertical_org_arch1}. Each layer is trained separately by minimising the loss of equation \eqref{vso_10} with the Adam optimizer\sidecite{Kingma_Ba_2017} and a learning rate of $\eta = 1 \cdot 10^{-3}$. The mini-batch size is $60,000$. 

\subsection{Extraction of Representations}\seclbl{vertical_self_org_representations}
In accordance with net-fragments as in \secref{neuro_concepts_net_fragments}, the representations are not extracted at a specific point (i.e. a pre-defined layer) but all representations from all layers are taken into account to fulfil a task.
In the following, this is demonstrated based on a classification task, but other tasks are also conceivable in the future.
After training, the average activation $z^{(l)}[c]$ for each class $c \in C$ in each layer $l$ is determined, as done in Equation \eqref{vso_8}.
These averages from the training set represent prototypes of each class object in each layer and can be considered as reference representation per class. Thus, the representations needed for this task are computed \emph{after} training and are not part of the training as for example in the case of a classification loss based on cross-entropy\sidenote{but in the case of classification this information is implicit in the loss function}.
When a new sample $\boldsymbol{x}_s$ is classified, the cosine similarity between the activations $\boldsymbol{z}^{(l)}_s$ of this sample and the class prototypes $\boldsymbol{z}^{(l)}[c]$ is calculated in each layer.

\begin{equation}\eqlbl{vso_11}
		\text{cos}^{(l)}_s[c] = \text{cos}(z\boldsymbol{z}^{(l)}_s, \boldsymbol{z}^{(l)}[c]) = \frac{\boldsymbol{z}^{(l)}_s \cdot \boldsymbol{z}^{(l)}[c]}{\max(||\boldsymbol{z}^{(l)}_s||_2, ||\boldsymbol{z}^{(l)}[c]||_2)} \text{, for } c \in C
\end{equation}

Thus, the cosine similarity $\text{cos}^{(l)}[c]$ for each class $c \in C$ and each of the for layers $l \in {1, ..., 4}$ is calculated. Afterwards, the average class $c$ with the highest average cosine similarity between the sample activations $\boldsymbol{z}^{(l)}_s$ and the class prototypes $\boldsymbol{z}^{(l)}[c]$ is used as prediction.

\begin{equation}\eqlbl{vso_12}
		\argmax_{c \in C} \frac{1}{4} \sum_{l=1}^{4} \text{cos}^{(l)}_s[c]
\end{equation}

This results in a weighted voting; if a layer is very sure that the sample belongs to a specific class, then the sample has a high cosine similarity with one class prototype and a low similarity with all other class prototypes. Accordingly, this layer influences the prediction more than a layer that cannot clearly assign the sample to one class and calculates a similarly high cosine similarity between the sample and all prototypes.


\subsection{Lateral Connections}
As described in section \secref{neuro_concepts_lateral_connections}, lateral connections serve neurons to support their activations among each other.
In this chapter it is also described in detail that this can be implemented through recurrent connections.
There are several ways to implement recurrent connections. A very simple possibility is to concatenate the layer input $\boldsymbol{z}^{(l-1)}_t$ at time $t$ with the layer output $\boldsymbol{z}^{(l)}_{t-1}$  at time $t-1$. This is visualized in Figure \figref{lateral_concat}. Of course, $\boldsymbol{z}^{(l)}_{t-1}$ is undefined at $t=0$. In this case, $\boldsymbol{z}^{(l)}_{t-1}$ is initialized with zeros.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{lateral_concat}
    \caption[Lateral connections by concatenating the layer's output with the layer's input]{The lateral connections can be implemented by concatenating the layer's output at the previous time-step with the layer's input at the current time-step.}
    \figlbl{lateral_concat}
\end{figure}


A second option is to use a second weight matrix as it is usually done in recurrent layers and to expand the equation \eqref{vso_1} for the activation function as follows:

\begin{equation}\eqlbl{vso_13}
		\boldsymbol{z}^{(l)}_t =  \boldsymbol{w}_x^{(l)} \cdot \boldsymbol{z}^{(l-1)}_t + \boldsymbol{b}^{(l)} + overbrace{\boldsymbol{w}_h^{(l)} \cdot \boldsymbol{z}^{(l)}_{t-1} }^{\text{lateral connection}}
\end{equation}

whereby $\boldsymbol{w}_x^{(l)}$ is the weight multiplied with the layer input and $\boldsymbol{w}_h^{(l)}$ the weight multiplied with the previous layer output. In both cases, the layer receives information about the activations at the previous time-step.
However, this only seems helpful if the model input is not static. Therefore, the model input is available over several time-steps and is augmented after each time-step. Thus, the model receives different views of the same image and can adjust its activations from the previous time-step if necessary. The following image augmentation techniques are applied, each with a probability of $p=0.8$:

\begin{itemize}
	\item \textbf{Color Jitter}: Randomly change brightness, contrast, and saturation of the image.
	\item \textbf{Gaussian Blur}: Blur the image with randomly chosen Gaussian blur.
	\item \textbf{Random Rotation}: Randomly rotate the image with an angle in the range $[-15°, ..., 15°]$
	\item \textbf{Adjust Sharpness}: Randomly adjust the sharpness of the image.
\end{itemize}

Figure \figref{mnist_augmented} visualises how this augmentation affects samples of the MNIST data set \cite{Lecun_Bottou_Bengio_Haffner_1998}. The original image is shown on the left and $9$ augmented versions of it are shown on the right. This allows the model to perceive the same image from different views.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{mnist_augmented}
    \caption[Data augmentation applied on $10$ samples of the MNIST data set]{Data augmentation applied on $10$ samples of the MNIST data set. The original samples as it is in the data set is shown on the left, $9$ augmented versions of the same samples are shown on the right.}
    \figlbl{mnist_augmented}
\end{figure}


Recurrent connections, which in this context represent lateral connections, are typically used to process sequential data or text. In this case, the data is collected cumulatively before an output is generated. For example, in text processing, all word tokens of a sentence are typically read before the model classifies the sentence as heat-speech or not. This is necessary because all sentence information is required and classification cannot be done on the basis of a single token. Such models are typically trained with backpropagation through time (BTT). Thereby, the gradients flow backwards over several time steps. This leads to well-known problems such as vanishing and exploding gradients.

In this thesis, BTT is not used, which means that a prediction is made after each time-step, but the prediction potentially improves with more time-steps. This leads to desirable properties: (i) Problems with vanishing or exploiding gradients do not exist, regardless of how many time-steps the model requires. (ii) After each time-step, representations can be extracted according to Section \secref{vertical_self_org_representations}. If a task can be solved correctly with a high probability on the basis of these representations (e.g. the object representation can clearly be assigned to one class label), the sequential analysis of the image can be aborted. If this is not the case, further time-steps can be carried out until the model has a sufficiently high confidence in its prediction. Thus, the number of time-steps can be sample-dependent.


% Idee: Mehr Robustheit indem z.b. in einem timestep noise als Input genutzt wird oder bei 1 von 10 timesteps (ausser in den ersten 2) das Bild zufälligerweise vertauscht wird. -> Lateral connections should help


\subsection{Hierarchical Features}
A criticism of the proposed model is that it does not learn hierarchical features, even tough hierarchical features are one of the main reasons for the good performance of deep learning systems. The diversity loss (c.f. Equation \eqref{vso_9}) forces enforces in each layer that the latent representations of objects of the same class are similar and that the representations of different classes are different. This violates the concept of hierarchical feautres; the first layers should learn general features that are helpful for all classes, but cannot necessarily be assigned to a specific class. Only later layers build class representations that are specific to a class. In the current setting, however, already the first layers generate class specific representations.

In the following, three possible measures are described to counteract this problem: (i) The fully connected layers are replaced by convolutional layers, so that the first layers have a smaller field of view and can only recognise local features. (ii) An adapted version of diversity loss forces a separation of features by class only in the last layers. (iii) While the very specific image information flows into the network from one side, general information in the form of class labels is fed into the network from the other side and these two types of information are fused. These three measures can be applied either individually or in combination with each other.

\subsubsection{Convolutional Architecture}
corresponding layer has a sufficiently large field-of-view\sidenote{the field-of-view are all pixels of the input image that can influence a neuron's activity}. In a CNN, the field-of-view in the first layers is restricted by design, but not in fully connected layers. Consequently, it might help to use a CNN architecture instead of a model based on fully connected layers only. This means that the network is no longer able to separate the representations based on the class in the first layers as the field-of-view is too small, but can only do this in the later layers, which have a larger field-of-view.


\begin{figure}[h]
    \centering
    \resizebox{0.99\textwidth}{!}
{
\begin{tikzpicture}
\tikzstyle{connection}=[ultra thick,every node/.style={sloped,allow upside down},draw=\edgecolor,opacity=0.7]
\tikzstyle{copyconnection}=[ultra thick,every node/.style={sloped,allow upside down},draw={rgb:blue,4;red,1;green,1;black,3},opacity=0.7]


\node[canvas is zy plane at x=0] (input) at (0,0,0) {\includegraphics[width=8cm,height=8cm]{imgs/mnist.jpeg}};


\pic[shift={(3,0,0)}] at (input) 
    {Box={
        name=conv1,
        caption=Conv + ReLU,
        xlabel={{1, }},
        zlabel=16,
        fill=\ConvColor,
        height=16,
        width=2,
        depth=16
        }
    };


\draw [connection]  (input) ++(0,0,0)    -- node {\midarrow} (conv1-west);


\pic[shift={ (0,0,0) }] at (conv1-east) 
    {Box={
        name=pool1,
        caption= ,
        fill=\PoolColor,
        opacity=0.5,
        height=8,
        width=1,
        depth=8
        }
    };


\pic[shift={(2,0,0)}] at (pool1-east) 
    {Box={
        name=conv2,
        caption=Conv + ReLU,
        xlabel={{16, }},
        zlabel=32,
        fill=\ConvColor,
        height=8,
        width=4,
        depth=8
        }
    };


\draw [connection]  (pool1-east)    -- node {\midarrow} (conv2-west);


\pic[shift={ (0,0,0) }] at (conv2-east) 
    {Box={
        name=pool2,
        caption= ,
        fill=\PoolColor,
        opacity=0.5,
        height=4,
        width=1,
        depth=4
        }
    };


\pic[shift={(2,0,0)}] at (pool2-east) 
    {Box={
        name=conv3,
        caption=Conv + ReLU,
        xlabel={{32, }},
        zlabel=64,
        fill=\ConvColor,
        height=4,
        width=6,
        depth=4
        }
    };


\draw [connection]  (pool2-east)    -- node {\midarrow} (conv3-west);


\end{tikzpicture}
}
    \caption[Architecture of the CNN for vertical self-organisation]{The network architecture of the CNN for vertical self-organisation with fully connected layers.}
    \figlbl{vertical_org_arch2}
\end{figure}

The CNN architecture used in this thesis is shown in Figure \figref{vertical_org_arch2}.
Three convolutional layers with ReLU activation function and with $16$, $32$, and $64$ channels are used, and max-pooling layers are applied between each convolutional layer. For training, the same hyperparameters and loss functions are used as for the model with fully connected layers.

\subsubsection{Hierarchical Diversity Loss}
A second measure can be to adapt the diversity constraint of the loss function. In the current version, it forces the latent representations of objects of the same class to be similar and those of different classes to be different. This is useful in the last layers, where high-level features (i.e. high-level net-fragments) or object representations should be detected and be separated from each other. In the first layers, on the other hand, the separation should not depend on the class label. Nevertheless, the activations should also be diverse if low-level features are detected (c.f. Section \secref{neuro_concepts_net_fragments}).

Therefore, the sparsity constraint is split into two parts: One part ensures that the activations within a (large) mini-batch are diverse and thus enforces that different features are captured and represented by different neurons. The second part ensures, as before, that the activations are diverse for different classes. Thus, one part of the constraint ensures \emph{diversity within the mini-batch} and the other part ensures \emph{diversity between different classes}.

These two parts are weighted linearly from the first to the last layer. Since the first layer should have a high diversity within the mini-batch, it has a high weight on the first part of the diversity constraint and a low weight on the second part. The last layer has an inverse weighting and pays more attention to the second part of the diversity constraint than to the first part.

The first part of the diversity constraints ensures diversity within a mini-batch. This is achieved by ensuring that each neuron within a mini-batch should be active. 

TODO: At the moment, various experiments are still ongoing to find out how this can be implemented (therefore, not yet explained in more detail).


\subsubsection{Two-Way Information Flow}
Hinton \sidecite{ff_algo} introduced with the forward-forward (FF) algorithm (c.f. Section \secref{alt_train_algo}) a promising idea for models based on proxy objective functions; 
The image is fed into the input layer of the network and the corresponding label is fed into the output layer of the network. The image remains static for several time-steps and the network is trained to push the layer's activations above a certain threshold. In a second stage, the image is fed into the network together with the wrong label and the network is trained to push the activations below a certain threshold. He found that when low-level features (i.e. images) are fed into the network from one end and high-level features (i.e. class labels) are fed into the network at the other end, a feature hierarchy is created. However, this approach has one major disadvantage: During inference, each possible sample-label combination has to be fed into the model and the label that caused the highest neural activity is used as the model's prediction.

In this thesis, this is implemented in a different way, which does not have this disadvantage. Identical to the FF algorithm, the image is fed into the input layer of the network for multiple time-steps. The label, on the other hand, is not fed directly into the output layer but is made available to the last layer within the loss function. Each layer maximizes the mutual information (MI) between its own activations and the activations of the previous and subsequent layers. Thus, the first layer maximizes the MI between the input image and its activations, the last layer maximises the MI between its activations and a label vector. This creates a feature pyramid in which a smooth transition from concrete sample to abstract class label is learned. During inference, the last label then directly predicts latent representations that can be assigned to a class.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{mi_loss}
    \caption[Vertical self-organization with mutual information loss]{Vertical self-organization with mutual information loss: Each layer maximizes the mutual information (MI) between its activations and the activations of the previous and subsequent layer. The first layer maximizes the MI between its activations and the input image (instead of the previous layer) and the last layer maximizes the MI between its activations and the image label (instead of the subsequent layer).}
    \figlbl{mi_loss}
\end{figure}

This process is visualized in Figure \figref{mi_loss}. 


TODO: At the moment, various experiments are still ongoing to find out how this can be implemented (therefore, not yet explained in more detail).


%\subsection{Efficient Parallel Training}
%Idee: Nachfolgelayer einbeziehen in Loss -> Nachfolgelayer ist gefroren, Output muss aber möglichst gut sein (sollte Performance nicht verschlechtern von Nachfolgelayer) + eigene Performance verbessern




% Future Work: keine Labels aber z.B. clustering oder self-supervised durch augmentations
% Future Work: Repräsentationen untersuchen -> was kann alles gefunden werden, e.g. information about object detection?
% Future Work: Hierarchical clustering um mehrere Objektprototypen zu verwenden anstatt nur Durchschnittswert pro Klasse (i.e. mehrere repräsentative Object prototypen anstelle nur eines prototypen)
% Selbst-Kritik: Weder Fisch noch Vogel -> zu wenig gut als geeignet für neues Klassifizierungsnetzwerk (obwohl das nie das Ziel war), zu wenig radikal neu als neurowissenschaftlich plausibel

\section{Horizontal Self-Organization}\seclbl{horizontal_self_org}















%TODO: vereinheitliche Mathe: Was ist underline, was ist hochgestellt in Klammern ($z^{(i)}$) und was ist hochgestellt in eckigen Klammern? Bei NN: Was ist n, was ist m, ... (+ Lernraten Symbol, Loss Symbol, etc.)
%Sind alle Vektoren bold?

% TODO: target function und objective function vereinheitlichen
% TODO: net-fragment or net fragment


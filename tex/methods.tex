%% methods.tex

\section{Neuroscientific Concepts}\seclbl{neuro_concepts}
% TODO: Dieser Teil wurde hier einfach reinkopiert -> passend zusammenschreiben




% TODO: VON HIER SCHIEBE IN NEUES KAPITEL -> ABSTRAKTE KONZEPTE von Neuroscience IN DL Übertragen
It is known that large parts of the human brain are self-organizing \sidecite{kelso1995dynamic}.
Recently, renowned scientists \sidecite{von_der_Malsburg_Stadelmann_Grewe_2022} put forward the hypothesis that the key mechanism of natural intelligent systems such as the human brain to interpret visual scenes is self-organization.
Self-organization is the process by which systems consisting of many units spontaneously acquire their structure or function without interference from a external agent or system.
They organize their global behavior by local interactions amongst themselves.
The absence of a central control unit allow self-organizing systems to quickly adjust to new environmental conditions.
Additionally, such systems have in-built redundancy with a high degree of robustness as they  are made of many simpler individual units.
These individual units can even fail without the overall system breaking down.
Dresp \sidecite{Dresp2020SevenPO} describes seven clearly identified properties of self-organization in the human brain: (i) modular connectivity, (ii) unsupervised learning, (iii) adaptive ability, (iv) functional resiliency, (v) functional plasticity, (vi) from-local-to-global functional organization, and (vii) dynamic system growth.

However, it is not obvious how these insights from neuroscience can be integrated into a deep learning framework.
One interpretation of self-organization may be that instead of optimizing the entire network by statistical learning for a single task (i.e. using backpropagation), local optimization based on local input data takes place.
The suitability of backpropagation for explaining how the brain learns was questioned soon after it was published \sidecite{Crick_1989, Grossberg_1987}.
Many alternative and biologically more plausible algorithms have been proposed in recent years such as the feedback alignment (FA) algorithm \sidecite{Lillicrap_Cownden_Tweed_Akerman_2014}, contrastive Hebbian learning \sidecite{Movellan_1991}, generalized recirculation \sidecite{O_Reilly_1996}, as well as target propagation (TP) and its variants \sidecite{Le__Cun_1986, hinton2007backpropagation, Lee_Zhang_Fischer_Bengio_2015}.
However, Bartunov et al. \sidecite{Bartunov_Santoro_Richards_Marris_Hinton_Lillicrap_2018} have shown that these algorithms do not scale to large vision datasets such as ImageNet \cite{deng2009imagenet} and only work for smaller datasets such as MNIST \cite{MNIST} and CIFAR-10 \cite{cifar_10}.
The biologically most plausible learning algorithm is Hebbian learning (c.f. Section \secref{hebbian}).
However, even tough I obtain some promising results in preliminary results with Hebbian learning (c.f. Appendix \secref{exp_hebb_learning}), this algorithm doesn't seem to be well suited to learn good image representation if a network is trained from scratch.

TODO: Füge FF-Algorithmus von Hinton hinzu

Another interpretation of self-organization within neural network is that the networks adapts its structure during training.
According to von der Malsburg et al. \cite{von_der_Malsburg_Stadelmann_Grewe_2022}, neurons form net-fragments (a.k.a. sub-networks) that represent objects with different levels of abstraction within an image (c.f. section \secref{natural_intelligence}).
For example, some net-fragments may represent shapes and structures while a multitude of such net-fragments together represent objects such as persons or entire scenes.
Thus, self-organization could be the algorithm to form net-fragments that represent objects independent of their transformation.

Such a mechanism to form networks that can capture scenes in the form of net-fragment has not been successfully implemented in deep learning nor neurocomputing systems.
This thesis aims to develop a learning paradigm that explicitly foster creation of net-fragments that represent objects independent of their transformation.
... TODO ...




% TODO: VON HIER SCHIEBE IN NEUES KAPITEL -> ABSTRAKTE KONZEPTE von Neuroscience IN DL Übertragen
In addition to these well-defined terms, there are various biological principles observed in the field of Neuroscience that are either not completely understood or it is unclear how they can be incorporated into artificial networks.
The concepts relevant for this thesis are discussed in the following.
However, due to the aforementioned challenges, no definition of these principles is given but rather my own interpretation in the context of this work.

A central concept of this work are sub-networks.
Typically, in current DL architectures for computer vision, an encoder is used to compute object representations in a latent space.
Thus, the output of a single layer can be understood as an object representation\sidenote{this is also true for image classification: Usually, a classification head consisting of fully connected layers and subsequent softmax activation is used after an image encoder}.
According to von der Malsburg et al. \cite{von_der_Malsburg_Stadelmann_Grewe_2022}, biological sub-networks span multiple layers.
This concept is interpreted in such a way that the latent representations should not be extracted from the last layer of an encoder but from several layers.
While the first layers of an image encoder contain more local information, the last layers contain very global information.
It is known from speech representation learning that different layers capture different speech information \sidecite{Chung_Hsu_Tang_Glass_2019}.
For example, the first layers contain more information to distinguish speakers, while representations in later layers provide more phonetic content.
However, such information seems to be poorly exploited in computer vision architectures.
One could argue that skip connections in segmentation networks \sidecite{Ronneberger_Fischer_Brox_2015} exploit such information.
While this may be true to some extent, information from previous layers is still squeezed into a feature vector and can be poorly exploited by downstream tasks.
TODO: Umsetzung Sub-Networks beschreiben

In addition to forward connections, lateral connections are also located in visual cortex \sidecite{gilbert1990lateral}.
Thus, the biological neurons are not only connected to the neurons in the subsequent layer but also within the same layer.
TODO: Je nach Umsetzung erklären, wie lateral connections in ANN interpretiert werden (+ Zeichnung einfügen)

Another principle that seems important in biological neural networks is sparsity.
Over time, the number of active neurons in the visual cortex decreases.
For example, in the visual cortex of mice are more than 75\% of the neurons active before the first opening of the eyes, 36\% after the opening of the eyes and only 12\% in adulthood \sidecite{Rochefort_Garaschuk_Milos_Narushima_Marandi_Pichler_Kovalchuk_Konnerth_2009}.
Thus, a sparsification of neuronal activations takes place through visual experience.
In the field of deep learning, sparisty is often interpreted in two different forms; sparse weight matrices and sparse activation matrices.
Sparse weight matrices are often chosen to make models smaller or to increase inference speed \cite{Louizos_Welling_Kingma_2018, Hoefler_Alistarh_Ben_Nun_Dryden_Peste_2021}.
From a biological point of view, this process of first creating a large network and then shrinking it is obviously not plausible\sidenote{otherwise we would have a large brain at the beginning, which becomes smaller by factors in the course of time}.
Sparse activations, on the other hand, can increase robustness \cite{Panousis_Chatzis_Theodoridis_2021}.
Intuitively, sparse activations enforce that only the most relevant information is passed to the subsequent layer.
Furthermore, sparse activations can help to obtain sub-networks.
Objects are represented in sub-networks (i.e. in multiple layers).
If all neurons in all layers are always active, then object representations cannot be extracted from sub-networks, since it is unclear which activation pattern represents which object.
If, on the other hand, only a fraction of the neurons are active, then certain neuron combinations can infer objects.
TODO: Umsetzung Sprase Activations beschreiben

A biological intelligent organism has an embodiment and can interact with the world.
At the same time, the visual system perceives continuous input all the time (except when sleeping or blinking).
As a result, the visual signal changes only minimally from one perceived frame to the next over a long course of time.
An ANN, on the other hand, is typically trained on samples that have little relation to each other.
When the system is trained on images, each frame is different; with videos, each sequence of frames is different.
A continuous input might help to get better representation of objects through self-supervised learning.
If an input is continuous and shows the same object from different angles or in different transformations (e.g. stretching) and it can be inferred that it is the same object then the object representations derived from this continuous stream can be homogenized.
These principles are already applied to some extent by self-supervised learning systems for computer vision.
In contrastive learning, a popular form of self-supervised learning, two different views are derived from one image by data augmentation, and their representations are then pushed closed together in the feature space \sidecite{chen2020simple, chen2020big, caron2020unsupervised}.
However, this paradigm is still quite limited since only two views of the same scene and not the continuous transformation of an object are presented to the learning system.
TODO: Umsetzung beschreiben

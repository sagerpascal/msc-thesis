%% related_work.tex
In this chapter, the related work is summarised. In \secref{alternative_approaches}, diverse learning frameworks are presented that deviate from the mainstream but might be promising for building more effective learning principles. Afterwards, in \secref{natural_intelligence}, an introduction to a publication titled ``A Theory of Natural Intelligence'' by \citeay{von_der_malsburg_theory_2022} is provided. This work is the main source of inspiration for this thesis and is summarised in detail.
This theory can be implemented by combining projection fibres with lateral connections in a self-organising manner. Therefore, projection fibres are summarised in \secref{projection_fibres} and work associated with self-organisation is reviewed in \secref{self_org_related}.


\section{Alternative AI Approaches}\seclbl{alternative_approaches}
Arguably, the only existing systems that, in non-experts' eyes, behave intelligently\sidenote{Which does not mean that such systems exhibit actual intelligence.} are foundation models such as large language models (LLMs) \sidecite{brown_language_2020, touvron_llama_2023}, and their fine-tuned versions \sidecite{ouyang_training_2022}. However, like other deep learning models, also foundation models suffer from the typical drawbacks of statistical learning (c.f. \secref{limitationsDL}). Therefore, a lot of research has been conducted to reduce these well-known problems: For example, current research aims to reduce hallucinations \sidecite{feldman_trapping_2023, manakul_selfcheckgpt_2023}, to implement an ongoing learning framework \sidecite{sahoo_online_2018, hoi_online_2021}, to transfer knowledge between tasks and data domains \sidecite{sager_unsupervised_2022, zhuang_comprehensive_2021}, or to learn what should be learned \cite{thrun_introduction_1998, hospedales_meta-learning_2022}. However, the underlying framework remains identical: Data is statistically mapped to manually or automatically generated labels, making it difficult to solve problems such as lack of robustness \cite{akhtar_threat_2018, long_survey_2022},  out-of-distribution generalisation \cite{madan_when_2022}, data efficiency \cite{marcus_deep_2018, smith_using_2022}, energy consumption \cite{garcia-martin_estimation_2019}, catastrophic forgetting \cite{kirkpatrick_overcoming_2017}, causal understanding or common sense reasoning \cite{rosenbloom_defining_2023, mitchell_debate_2023}. In the following, alternative approaches to training deep networks are summarised that attempt to alleviate these problems with new principles that do not rely solely on label prediction or data reconstruction. 

\paragraph{1000 Brains.} Jeff Hawking and his research group use the brain as the single source of inspiration and build, following their interpretation, a biologically highly plausible system that implements the brain's learning algorithm \cite{hawkins_framework_2019}. The learning algorithm is an adapted version of unsupervised Hebbian learning \cite{hebb_organization_1949}. Their theory is based on Vernon Mountcastle's proposal that the neocortex comprises many cortical columns, all having a similar architecture and performing similar functions \cite{mountcastle_organizing_1978, mountcastle_columnar_1997}. Thus, they argue that the brain does not comprise a single learning model like current deep learning systems but many similar but independent models for each object \cite{lewis_locations_2019}. Each model uses different inputs from different sensory system parts. These models make predictions based on the input they receive and vote to reach a consensus on the sensory observations.
This theory is also known as the 1000 brains theory, as many models work in parallel. The principle of many parallel models is closely related to ensemble approaches in deep learning \sidecite{yang_survey_2023}, with the constraint, similar to random forest \sidecite{ho_random_1995}, that only a portion of the data is available to each model. Current results are difficult to interpret as the proposed model has been trained and tested on the same dataset. In its current form, the model can recognise several hundred objects and is robust to noise. However, the number of available input signals severely limits the network's capacity, and the model cannot yet scale to recognise more objects.
Hidden units cannot just be added to increase the model's capacity, as increasing capacity requires additional input signals.

Strictly enforcing biological plausibility limits performance significantly, as the biologically inspired algorithm cannot fully exploit the mathematical capabilities of computational hardware. Intuitively, however, several proposed principles appear promising: \emph{Sparse coding} encourages compact and efficient representation of complex data \sidecite{ferrier_toward_2014}, \emph{self-organisation} of a large number of parallel models increases robustness \cite{yang_survey_2023}, and \emph{predicting future cell activation} allows learning in an unsupervised manner and does not restrict the model to specific predefined tasks.

\paragraph{Recursive Cortical Network.} A second type of network that has evolved from the 1000 brains theory is called the recursive cortical network (RCN) \sidecite{george_generative_2017}\sidenote{The driving force behind it, Dileep George, worked in Jeff Hawkings' group and was instrumental in developing the fundamentals of the 1000 brains theory.}. It is based on the insight that a human's vision system processes shape and appearance differently \sidecite{garg_color_2019} and that a familiar object with an unexpected colour can still be easily recognised.
The RCN uses separate mechanisms to process contours and appearances and uses lateral connections \sidecite{gilbert_lateral_1990} for internal consistency.
When the features cannot explain an image at a certain level $l$, the active features from the level below $l-1$ are combined to create a new feature at level $l$. Afterwards, features are pruned using a cost function considering reconstruction and compression errors.
A significant advantage of RCN is that it can learn from a few examples and has one-shot and few-shot learning capabilities.
However, in its current form, RCNs cannot be applied to natural images, as contour hierarchies only can be implemented if the object is clearly separated from its background. Therefore, the network, in its current form, is limited to MNIST \sidecite{lecun_gradient-based_1998} and text-based CAPTCHAs.

Similar to the 1000 brains theory, this model has drawbacks that prevent its practical application but introduces many promising concepts: The \emph{separation of shape and appearance} seems promising, as this limits the feature space, and these concepts can be learned independently. Furthermore, in the case of multiple classes, the network does not make a single prediction but creates hypotheses that are evaluated by an outer loop. Thus, the model loops between \emph{generating} and \emph{evaluating} hypotheses.

\paragraph{Capsule Networks.} Neuronal capsule networks (CapsNet) \sidecite{sabour_dynamic_2017} mimic biological neural organisation and explicitly model hierarchical relationships. These networks group neurons into ``capsules''; each capsule represents a property such as position, size, orientation, deformation, texture, colour or movement. Several of these capsules form more stable representations for higher-level capsules. \emph{Dynamic routing} \sidecite{ash_design_1981} matches bottom-up activations and top-down concepts generated based on available evidence over multiple iterations. It allows the network to learn viewpoint invariant knowledge and can deal with highly overlapping objects by detecting that one object is in front of another. However, CapsNets are still an active area of research and are not yet adopted in real-world scenarios. In particular, dynamic routing makes the algorithm slow, and so far, this approach has only worked on small datasets such as MNIST \sidecite{lecun_gradient-based_1998} or CIFAR-10 \sidecite{krizhevsky_learning_2009}.

Some concepts of CapsNets are closely related to those of the 1000 brains theory and RCNs. However, CapsNets have their origins in computer science and are oriented towards deep learning principles. In contrast, the other frameworks have originated in neuroscience and focus on biological plausibility. CapsNets \emph{divide the features} into capsules which is helpful for generalisation. However, a severe problem is the accumulation of spatial information, which prevents the model from scaling to larger datasets.
The dynamic process of \emph{iterating} between low-level and high-level features to analyse hypotheses is promising to prevent early commitment \sidecite{marr_vision_2010} and is highly relevant for this thesis.


\paragraph{Parsimony and Self-Consistency.} \sideciteay{ma_principles_2022} suggest that appropriate data structuring can lead to the emergence of intelligence, especially if the data is structured according to the principles of parsimony and self-consistency. The two principles describe \emph{what} and \emph{how} should be learned. Parsimony implies that an intelligent learning system should recognise low-dimensional structures in observed high-dimensional data and organise them in the most compact and structured way\sidenote{The goal is not to achieve the best possible compression but to obtain compact and structured representations in a computationally efficient way.}. The second principle of self-consistency states that an intelligent learning system minimises the internal discrepancy between observed and regenerated observations. Such internal representations can be learned through self-criticism \sidecite{rennie_self-critical_2017}. The described framework is promising as it incorporates biological concepts into a framework that unifies and clarifies many practical and empirical findings of deep learning.
Unfortunately, the authors do not present any metrics, so estimating the framework's performance is difficult.
The framework's challenge is implementing the parsimony principle, i.e., defining the constraints responsible for forming the latent space and scaling it to larger datasets.

\paragraph{Actionable Representations.} Another frequently used principle inspired by biological learning is the combination of representations with actions \sidecite{knoblich_social_2006, zhou_does_2019}. It is known that animals integrate their actions (i.e., movements) with incoming sensory signals \sidecite{keller_sensorimotor_2012}.
\citeay{keurti_homomorphism_2023} argue that such efference copies\sidenote{The internal copy of an outgoing motion-generating signal.} help to learn useful latent representations of input the visual system perceives. They allow an agent to perform actions by transforming objects and ensure that real-world transformations can also be applied to latent representations, i.e., mental objects and real-world objects remain consistent when similar transformations are applied.
Allowing an agent to interact with the world to understand it better and improve representations seems essential not only from a neuroscientific point of view but is also in line with theories from psychology:
\citeay{piaget_cognitive_1964} argues that perceiving an object is more about understanding how it changes and behaves than creating a mental copy of the object.
The combination of perception and action for autonomous machine intelligence is also postulated by \sideciteay{lecun_path_2022}. He proposes creating a world model whose future state can be predicted based on planned actions. He emphasises the importance of self-supervised learning in combination with energy functions \sidecite{hinton_training_2002}.

Actionable representations not only represent the system's input data but also capture their behaviour when undergoing actions applied to them. However, increasing the representation's information content comes with the costs of providing richer input data:
\begin{itemize}
    \item Learning visual representations of an object requires images only.
    \item Learning an object's appearance from different viewpoints requires sequences of images of the same object.
    \item Learning actionable representations requires a simulation in which actions can be applied to objects. 
\end{itemize}

\paragraph{Active Inference.} \citeay{lecun_path_2022} describes that energy functions are well suited to make predictions of the world state based on actions because they can shape the latent space well due to their regulating properties.
In the context of active inference \sidecite{friston_active_2016, parr_active_2022}, perception presents itself as a process of minimising the free energy \cite{hinton_training_2002} of variation with respect to beliefs about hidden variables. This process enables planning and inference by modelling generative processes $p(s,o)$. For example, if it rained at night ($p(s)$), one can infer that the grass is wet ($p(o)$). The model tries to model the chances of different hidden situations $p(s|o)$ based on prior beliefs ($p(s)$) and the likelihood of what it already observed ($p(o|s)$). 
Thus, active inference minimises the variational free energy by using past experiences to learn how the world works \cite{friston_active_2016}. This helps to predict what might happen in the future by minimising the probability of surprising or unexpected situations.
This principle is also actively researched by other well-known research groups. For example, Bengio's research group works on GFlowNets (Generative Flow Networks) \sidecite{bengio_flow_2021, bengio_gflownet_2022}, making it possible to disentangle the explanatory causal factors and the mechanisms that connect them.
Furthermore, energy-based models can generate new samples that resemble the training data by sampling from the energy function \sidecite{du_implicit_2020}. This allows for predicting future world states.

\paragraph{Conculsion.} There exist various alternative learning approaches with interesting principles. Many of these principles are adopted in this thesis; Similar to the 1000 brains theory, local self-organisation is used to build consistency between small models, i.e. between cells representing features.
Also, sparse coding is used in the thesis as it enhances interpretability and robustness \sidecite{ahmad_properties_2015}. The iterative process of generating and evaluating hypotheses, as applied in RCNs \sidecite{george_generative_2017} and CapsNets \cite{sabour_dynamic_2017}, is considered essential to prevent early commitment \cite{marr_vision_2010}. In the proposed framework, such a loop is implemented between cells of the same layer: A cell can only remain active when consistent with the global pattern (evaluation loop) while simultaneously defining the global pattern (generation loop). Lastly, the proposed framework leverages an energy function to implement a memory component. 

Besides these adopted principles, other methodologies seem essential for future work. Especially the organisation of internal representations should be investigated further. Promising approaches for organising the latent space are actionable representations that allow an agent to better understand objects' behaviour under certain transformations and the principles of parsimony and self-consistency.

\section{Natural Intelligence}\seclbl{natural_intelligence}
In this section, the theory of natural intelligence proposed by \sideciteay{von_der_malsburg_theory_2022} is discussed, as it serves as the foundation of this thesis.
The authors point out the massive gap between the amount of information needed to describe the brain's structure (1 PB) and the amount needed to generate it (few GB) \sidecite{mcpherson_physical_2001}. Therefore, the brain must be highly structured \sidecite{gazzaniga_organization_1989, ackerman_discovering_1992, bassett_understanding_2011}
The theory's authors argue that the ``Kolmogorov algorithm \sidecite{kolmogorov_tables_1998} of the brain''\sidenote{The Kolmogorov complexity describes the number of bits required by the shortest algorithm that can generate a given structure.} builds the neuronal structure by selecting from a set of pre-structured patterns. This aligns with other research that argues that the brain is dominated by similar cell patterns \sidecite{mountcastle_organizing_1978, mountcastle_columnar_1997}.
Self-organisation is the only mechanism that experiments have not yet disproved as the brains Kolmogorov algorithm \cite{willshaw_how_1976, willshaw_marker_1979, singer_brain_1986, kelso_selforganizing_1995, kelso_dynamic_1999}.

This mechanism loops between activity and connectivity, with activity acting back on connectivity through synaptic plasticity until a steady state, called an attractor network, is reached.
The consistency property of an attractor network implies that a network has many alternative signal pathways between pairs of neurons \sidecite{von_der_malsburg_neural_1987}.
Thus, the brain develops as an overlay of attractor networks called net fragments \sidecite{von_der_malsburg_concerning_2018}.
Net fragments consist of sets of neurons, whereby each neuron can be part of several net fragments.
In the case of visual processing, net fragments can be considered filters that detect previously seen patterns in the visual input signal.
An object is represented by multiple net fragments, where each fragment responds to the surface of that object and has shared neurons and connections with other net fragments representing that object.
Thus, net fragments render the topological structure of the surfaces that dominate the environment.
Nested net fragments of different sizes may represent a hierarchy of features.
Complex objects, such as mental constructs, can thus be seen as larger net fragments composed of mergers of pre-existing smaller net fragments.

\citeay{von_der_malsburg_theory_2022} have not addressed how their theoretical concepts can be translated into a computational model.
A concrete implementation is proposed by Lehmann \sidecite{lehmann_leveraging_2022}.
He proposes a new layer called the laterally connected layer (LCL) that forms lateral intra-layer connections based on the Hebbian learning rule \sidecite{hebb_organization_1949}.
Such lateral connections are synapses between cells in the same layer \sidecite{gilbert_lateral_1990}.
However, the proposed layer has only improved performance for small networks and datasets, and its effectiveness for larger models processing more complex has yet to be demonstrated.
Furthermore, lateral connections are only used at the end of the network, thereby not preventing early commitment since features are still processed hierarchically.
Moreover, Lehmann does not build net fragments and thus ignores some of the aspects of the theory.

In this thesis, lateral connections are used to build net fragments to overcome the limitations of the LCL layer.
Furthermore, it analyses the theory of natural intelligence by conducting experiments and contributes to making it more concrete.

\subsection{Projection Fibres}\seclbl{projection_fibres}
An important principle of the theory of natural intelligence is the rendering of topological structures.
This means that image features extracted by a filter are mapped to a corresponding reference image, whereby neighbouring features in the source image are mapped to neighbouring regions in the reference image.
This mapping is inspired by the human brain that maps cell activity in the primary visual cortex \sidecite{tong_primary_2003, grill-spector_human_2004} to transformation- and position-invariant reference objects stored in the 
temporal cortex \sidecite{miyashita_inferior_1993, conway_organization_2018}.
The mapping between corresponding cells is done with a particular type of axon called projection fibre \sidecite{greig_molecular_2013}.
In this section, different implementations of projection fibres are presented, while biological aspects are discussed in \secref{neuroscience_findings_projection_fibres}. 

Projection fibres map images to reference objects and can be defined as matching subgraphs \cite{bienenstock_neural_1987, lades_distortion_1993, wiskott_face_1996}:
An image can be described as a graph of image features, whereby the connections of the graph represent the spatial relationships between the features in the image. However, not all pixels usually belong to the same object, and thus, a subgraph represents an object.
In addition, an idealised version of this object's subgraph is stored as a prototype.
By matching the subgraphs of an image and the stored subgraph prototypes, projection fibres implement subgraph matching.

\sideciteay{lades_distortion_1993} divide the training into two phases: During a storage phase, sparse graphs labelled with Gabor-type wavelets are formed and stored as model prototypes. In the recognition phase, the Gabor wavelets of the perceived image are matched with the previously stored graph prototypes.
Therefore, a sparse graph of the image features is adaptively formed to best match a given prototype graph. The matching process is based on adjusting one-to-one links between vertices in the model and image graphs; the model's prototype graphs are moved over the input graph and locally slightly deformed. A cost function controls the graph adaptation by favouring similarities and penalising metric deformations. This matching process between image features and stored models is repeated for each stored model prototype. Finally, the prototype with the lowest cost is selected as the recognised model. This process is straightforward, but the sequential matching of models prevents the model from scaling to large datasets requiring many prototypes.

An alternative approach is described by \sideciteay{bienenstock_neural_1987}. They connect all prototype graphs to the input graph randomly, and a self-organising learning process iteratively improves these connections in a coarse-to-fine manner.
The matching process from prototypes to feature sub-graphs is implemented by minimising an energy function \sidecite{hinton_training_2002}.
However, this approach could not scale to larger datasets or photorealistic images.

\sideciteay{wiskott_face_1996} proposed self-organising projection fibres for face recognition. The mapping between the image array and the stored prototype array is based on synaptic plasticity and the constraint of preservation of topography to find matches. The preservation of topography is achieved with lateral connections, which are excitatory over a short range and inhibitory over a long range.
The connectivity matrix is initialised using the similarities between the features of the connected neurons. Afterwards, the connectivity matrix is adjusted by moving the prototype sub-graph across the image until a good match to the feature graph is found.
However, one problem with this model is that matching takes a long time and is, therefore, impractical.

\sideciteay{wolfrum_recurrent_2008} utilise three layers, an input layer to extract image features, an assembly layer for recurrent information integration, and a gallery layer for storing face prototypes.
The input layer is organised in a rectangular grid, while the assembly and gallery layers have face graph topology.
Projection fibres map the features from the input layer to the assembly layer, from where the face graph is compared with prototypes in the gallery.
This architecture is biologically plausible, works fast, and obtains good results. 
However, the main drawback is that the cells are organised according to a face graph topology. Thus, it does not work for different objects. 

Projection fibres map the features found in the image to stored object prototypes based on their local similarity. This creates an explicit one-to-one mapping between features and prototypes. Therefore, unlike CNNs, no position and transformation information is lost, and the mapping is better interpretable.
Current implementations cannot map images to prototypes if the features do not match well.
This problem is alleviated in this thesis by mapping net fragments \sidecite{von_der_malsburg_concerning_2018} instead of features.
Net fragments are based on image features but remove noise, reconstruct missing parts, and locally generalise features. Thus, net fragments transform features into suitable representations occurring similarly in reference frames, enabling a more robust mapping.

\section{Self-Organisation}\seclbl{self_org_related}
Self-organisation is the process by which systems consisting of many units spontaneously acquire their structure or function without interference from an external agent or system \sidecite{morris_cognitive_2006}.
They organise their global behaviour through local interactions amongst themselves.
The absence of a central control unit allows self-organising systems to adjust to new environmental conditions quickly.
Additionally, such systems have built-in redundancy with a high degree of robustness as they consist of many simpler individual units \sidecite{wagner_robustness_2013}.
These individual units can even fail without the overall system breaking down.

Self-organisation is highly relevant in this thesis: First, the theory of natural intelligence (c.f. \secref{natural_intelligence}) argues the brain's Kolmogorov algorithm is self-organising and a key to natural intelligence. Second, implementing a system that prevents early commitment cannot rely on a global error correction signal that controls the entire process. Instead, taking local decisions based on the feasibility of overarching patterns requires a self-organising approach.

\paragraph{Growing Patterns.} Cellular automata contain a grid of similar cells\sidenote{An image or features of an image can be interpreted as a 2D grid of cells.} with an internal state updated periodically.
Update rules define the transition from a given state to a subsequent state.
During an update, cells can only communicate with the neighbouring cells.
Thus, self-organisation is enforced by the definition of the update rules \sidecite{wolfram_cellular_1984, vichniac_simulating_1984}.
Neural cellular automata \sidecite{wulff_learning_1992, gilpin_cellular_2019} use neural networks to learn the update rule between cells.
The input in such a neural network is the state of a given cell and its neighbours, and the output is the subsequent cell state.

Cells in NCAs can be trained with gradient descent to grow learned 2D patterns such as images \sidecite{mordvintsev_growing_2020, mordvintsev_growing_2022}.
These images are grown through self-organisation (i.e. the pixels pick a colour based on the colour of neighbouring pixels) and are surprisingly resistant to damage.
For example, large parts of the images can be removed, and the system can still rebuild the entire image.
However, the aforementioned approaches can only grow the pattern they are trained on.
A recent method called Variational Neural Cellular Automata \sidecite{palm_variational_2022} uses an NCA as the decoder of a variational autoencoder \sidecite{kingma_auto-encoding_2022}.
This probabilistic generative model can grow images based on a vector sampled from a Gaussian distribution.
However, there is still a significant performance gap compared to state-of-the-art generative models.
Besides growing 2D patterns, NCAs can also create 3D patterns \cite{sudhakaran_growing_2021}, simulate robots \cite{hu_regenerating_2021}, or generalise to graph structures \cite{grattarola_learning_2021}.

\paragraph{Classify Images.} The process of growing images from cells of an NCA can also be inverted:
\sideciteay{randazzo_self-classifying_2020} propose to use NCAs to classify given structures such as images.
They apply the same network to each pixel of an image.
In an iterative process based on local communication with neighbouring pixels, the image fragments agree on which object they represent.
Intuitively, each pixel has a hypothesis about which object it might represent. By communicating with neighbours, pixels vote for and agree on one hypothesis over time.
However, this approach is limited to local interaction and only works for simple images such as MNIST.

\paragraph{Learning.} Self-organisation can also be used to optimise the weights of neural networks.
Hebbian learning is considered a learning rule that follows self-organising principles \sidecite{najarro_meta-learning_2020, pedersen_evolving_2021} as it updates weights between two cells based on the cells' state and does not require a global teaching signal (c.f. \secref{hebbian}).

\citeay{kirsch_meta_2021} use multiple tiny recurrent neural networks (RNNs) that have the same weight parameters but different internal states\sidenote{Intuitively, these tiny RNNs can be interpreted as more complex neurons.}.
By using self-organisation and Hebbian learning, they show that it is possible to learn powerful learning algorithms such as backpropagation while running the network in forward mode only.
However, it works only for small-scale problems as it can quickly get stuck in local optima.

%Self-organisation can even be used to control reinforcement learning (RL) agents.
%\sideciteay{variengien_towards_2021} use the observations of the environment as cell states of an NCA. The following states predicted by the NCA are used as Q-value estimates of a deep Q-learning algorithm \sidecite{mnih_playing_2013}. This allows training RL agents with local self-organisation instead of backpropagation of error.


\paragraph{Network Architectures.}
Unsupervised learning techniques usually map high-dimensional input data to a lower-dimensional representation \sidecite{russell_artificial_2021}. Such mappings can also be implemented in a self-organising manner, for example, based on self-organising maps (SOMs) \sidecite{kohonen_self-organized_1982, kohonen_self-organization_1989}.
SOMs map the input data to a discrete representation space of the training samples called a map.
Unlike ANNs, they use competitive learning \sidecite{grossberg_neural_1989} instead of error correction learning algorithms such as backpropagation of error \sidecite{rosenblatt_principles_1962, linnainmaa_taylor_1976}.
Local competitive learning ensures that samples are close in the input space are also closed in the resulting maps.
Thus, the data space is self-organised by local rearrangements.

However, SOMs have two significant limitations; First, the network structure must be predefined, limiting the mapping accuracy. Second, the map's capacity is predefined through the number of nodes.
Growing networks overcome these limitations and add nodes or whole layers of nodes into the network structure at the positions of the map where the error is highest \sidecite{reilly_neural_1982, fritzke_growing_1994, marsland_self-organising_2002}.

%Besides ANNs, the structure of spiking neural networks (c.f. \secref{spiking_networks}) can also be optimised using self-organisation. 
%SNNs can either be assembled from multiple stackable sub-systems \sidecite{raghavan_self-organization_2020}
%or be grown from a single cell \cite{raghavan_neural_2019} by adding additional cells and connections.

\paragraph{Relevance.} The proposed framework is highly related to the aforementioned self-organising principles: First, net fragments can reconstruct local patterns when occluded, resembling methodologies that can grow patterns. Second, projection fibres implement a mapping from scenes to reference frames, related to approaches that implement classification based on self-organising principles. Finally, the proposed framework relies on a self-organising Hebbian learning algorithm.
In contrast, the architecture of the network is predefined and has not yet adopted a self-organising approach. Nevertheless, it is essential for future research efforts to explore the dynamic incorporation of patterns in reference frames, thereby increasing the relevance of introduced concepts such as self-organising maps (SOMs).


\begin{comment}
\section{Learning Algorithms}\seclbl{alt_train_algo}
Neural networks are usually optimised with backpropagation of errors (c.f. \secref{ann}).
Soon after backpropagation was published, it was questioned whether this algorithm is suitable for explaining learning in the brain \sidecite{grossberg_competitive_1987, crick_recent_1989}.
Besides the fact that backpropagation seems biologically not plausible and might be responsible for many of the limitations described in \secref{limitationsDL}, it also has technical shortcomings:
First, gradients might vanish or explode when propagating through too many layers \sidecite{zhang_why_2020}.
Second, it turns the networks into ``black boxes'' and prevents users from getting valuable insights from trained models.
Third, training end-to-end does not allow controlling the loss function's effect on a hidden layer because of the non-linearity in the network \sidecite{lee_deeply-supervised_2015}.
Finally, the loss landscapes are non-optimal, which might lead to slow convergence, or the optimisation process can get stuck in a local minimum  \sidecite{ioffe_batch_2015}.
However, despite these deficits, it is still considered the best error correction algorithm to optimise neural networks on a specific task. 

Hebbian learning (c.f. \secref{hebbian}), on the other hand, is widely accepted in the fields of neurocomputing, psychology, neurology, and neurobiology \sidecite{widrow_natures_2019}.
Hebbian learning works well, for example, for topographic mappings \sidecite{kohonen_self-organization_1989, grajski_hebb-type_1990}, implementing neuroplasticity\sidenote{The network's ability to reorganise itself by forming new connections.} \sidecite{montague_spatial_1991}, or recognising non-linear patterns \sidecite{mel_nmda-based_1992}, but also has limitations \cite{anderson_biased_1998}.
For example, Hebb's learning rule is insufficient as a general rule for time-shifted signals as it requires (almost) synchronous stimuli \sidecite{anderson_biased_1998, rumelhart_learning_1986}.
Besides Hebbian learning, various other optimisation algorithms are inspired by the human brain. These algorithms are summarised in this section to provide a comprehensive overview of alternative learning principles. Readers only interested in the proposed framework may skip this section as it is only relevant for future work.


\paragraph{Proxy Objective Functions.} Typical deep learning models are trained by minimising a global objective function \sidecite{prince_understanding_2023}. \emph{Proxy objective functions} \cite{mostafa_deep_2018, marquez_deep_2018, belilovsky_greedy_2019} are objective functions applied to each layer in the model. For example, the loss function of the first layer (i.e. the proxy objective function of the first layer) influences only the trainable parameters of the first layer but not the parameters of the second layer.
Thus, proxy objective functions allow decoupling layers.
Instead of a metric, also an auxiliary model that evaluates the quality of representations can be used as proxy objective functions \sidecite{mostafa_deep_2018, marquez_deep_2018, belilovsky_greedy_2019, duan_kernel_2020, duan_modularizing_2022, wang_revisiting_2021} or a combination of auxiliary network and metric \sidecite{nokland_training_2019}.

A significant issue of models based on proxy objective functions is that they are usually trained layer-wise\sidenote{When layer $k$ is trained, the layers $1, ..., k-1$ are frozen, and the layers $k+1, ..., n$ are not updated.}.
The resulting computational inefficiency can be reduced with synchronous or asynchronous training \cite{belilovsky_greedy_2019, belilovsky_decoupled_2020}.
In the synchronous setting, a forward pass through all layers is done, and afterwards, all layers are trained simultaneously by minimising their own proxy objective function \cite{belilovsky_greedy_2019}.
In the asynchronous setting, all layers are trained simultaneously using a replay buffer for each layer that stores the layer's output \cite{belilovsky_decoupled_2020}.
Each layer receives input from such a buffer instead of the previous layer, eliminating the need for an end-to-end forward pass.

Proxy objective functions are the only known alternative to backpropagation of error that scales well to large neural networks and large datasets \cite{bartunov_assessing_2018, duan_modularizing_2022}.
However, training is typically slower than end-to-end backpropagation, even with synchronous or asynchronous training strategies  \cite{belilovsky_decoupled_2020}.
In addition, layer-by-layer training of a network for the sole purpose of biological plausibility seems unnecessary as long as the underlying principles remain identical.

\paragraph{Forward Forward Algorithm.}
Recently, Hinton \cite{hinton_forward-forward_2022} introduced the forward-forward \emph{(FF)} algorithm, which is considered a special instance of proxy objective functions.
This algorithm uses two forward passes, one with positive (i.e. real) data and one with negative data.
Positive data consists of an input vector and the correct label, and negative data consists of an input vector with the wrong label.
Each layer has its objective function that aims to have a high ``goodness'' for positive data and a low ``goodness'' for negative data. 
The sum of the squared neural activities measures the goodness. Thus, having many (strongly) active neurons means having high goodness and should only be the case for positive data (image and label match). The neuronal activity should be low for negative data, i.e. when the image and label do not match.
During inference, the data is fed with each label through the network, and the one with the highest goodness is kept as the prediction.

The same input is fed into the model over multiple timesteps to obtain hierarchical features \sidecite{hinton_how_2021}.
The trick is to feed the input into the network from one side and the label into the network from the other. A layer receives as input the activity vector of the previous layer and the activity vector of the subsequent layer at the previous timestep. By enforcing high goodness when the input and label match, information must flow in both directions. Thus, hierarchical features can be learned without a backward pass.

The FF algorithm did not yet scale to large datasets. Furthermore, inference is very slow for datasets with many labels, as each sample is compared with all existing labels. However, feeding specific information (e.g. images) into one end of the network and general information (e.g. labels) into the other end and effectively merging these inputs to extract hierarchical features seems promising.

\paragraph{Target Propagation.} Target propagation methods compute targets\sidenote{A target can be interpreted as the value a layer should predict.} rather than gradients at each layer \sidecite{bengio_how_2014, appice_difference_2015, meulemans_theoretical_2020}.
Each layer is trained to minimise the difference between its activations and its received target values.
In the context of target propagation, each layer is considered an encoder that maps its input to an output. Besides an encoder, a decoder is trained per layer, implementing the inverse mapping from the layers' output to its input.
During a forward pass, an input vector $\boldsymbol{x}$ is fed through all encoders to obtain a prediction $\boldsymbol{\hat{y}}$. For the backward pass, the target value $\boldsymbol{y}$ is fed through all decodes to obtain a target per layer\sidenote{If each layer would predict this target, the output would be $\boldsymbol{y}$.}.


An advantage of calculating targets is that only layer-wise gradients are required, which allows models to have non-differentiable operations \cite{appice_difference_2015}.
A significant drawback is that auxiliary decoders must be trained to approximate the hidden targets, which might outweigh the savings achieved by eliminating a complete backward pass.
Furthermore, this approach does not scale well in praxis \sidecite{bartunov_assessing_2018} as the decoder cannot perfectly calculate the layers' mapping function inverse. 
Thus, the learning process is somewhat unstable as the target is approximated per layer and might steer the learning process in the wrong direction.


\paragraph{Synthetic Gradients.} Synthetic gradients methods \sidecite{jaderberg_decoupled_2017, czarnecki_understanding_2017} replace the gradients of backward passes with approximated local gradients, estimated with auxiliary models, and utilise gradient-based optimisation algorithms such as stochastic gradient descent to update the weights locally.
The auxiliary models are usually fully-connected networks trained to regress a layer's gradients when given the layer's activations.
End-to-end backward passes are only performed occasionally to acquire actual gradients that can be used to train the gradient approximation models.
Thus, the frequency of end-to-end backward passes is reduced.
When additional auxiliary input models are used to predict the layer's input, the frequency of the forward pass can also be reduced in the same manner as for the backward pass \sidecite{jaderberg_decoupled_2017}.
It is also possible to eliminate the backward pass by training the auxiliary gradient models with local information only \sidecite{lansdell_learning_2020}.

However, the synthetic gradients method works significantly worse than when actual gradients are used to train auxiliary models.
An advantage of synthetic gradient methods is that they can approximate backpropagation through time (BTT) for unlimited timesteps \cite{jaderberg_decoupled_2017}.

\paragraph{Feedback Alignment.} Feedback alignment algorithms use fixed, random weights during the backward pass \sidecite{lillicrap_random_2016}.
The gradients are calculated similarly to stochastic gradient descent but with random weights.
Thus the symmetry between the weights used during the forward and the backward pass is broken.
The network still learns to make the feedback useful even though the weights during forward and backward passes are different. 
Later, the algorithm was improved by using fixed, random weights that share the signs with the actual weights of the network \sidecite{liao_how_2016}.

Feedback alignment algorithms are a more biologically plausible alternative to backpropagation as they avoid the weight transport problem.
When combined with application-specific integrated circuits, they can reduce training time and energy costs \cite{lillicrap_random_2016}.
However, they cannot yet scale to large datasets, and further research is required towards this direction.

\paragraph{Auxiliary Variables.} Lastly, another group of learning algorithms use \emph{auxiliary variables} \sidecite{carreira-perpinan_distributed_2014, taylor_training_2016, zhang_convergent_2017, lau_proximal_2018}.
These methods use the idea of variable splitting, i.e. transforming a complicated problem into a simpler one by introducing additional trainable variables.
Introducing auxiliary variables may pose scalability issues, and these methods require special, usually tailor-made solvers.
Therefore, such methods usually scale to specific use cases only.

\paragraph{Relevance.} In this chapter, several alternative learning algorithms to the broadly used backpropagation of error \sidecite{rosenblatt_principles_1962, linnainmaa_taylor_1976} are discussed. However, only proxy objective functions can scale to larger datasets. Backpropagation of error remains the state-of-the-art approach for various machine learning tasks, despite its biological implausibility \sidecite{grossberg_competitive_1987, crick_recent_1989} and associated drawbacks \cite{zhang_why_2020, ioffe_batch_2015, akhtar_threat_2018, long_survey_2022, madan_when_2022} (c.f. \secref{limitationsDL}).



\section{Representation Learning}\seclbl{visual_rep_learning}

\subsection{Autoencoders}\seclbl{visual_rep_learning_ae}
\emph{Autoencoders} are one of the most-used methods to learn visual representations in an unsupervised fashion.
Autoencoders were introduced in 1985 \sidecite{rumelhart1985learning} and learn an encoder and decoder function \sidecite{pmlrv27baldi12a}.
The encoder $E$ maps the input from a high-dimensional space to a lower-dimensional embedding space $E: \mathbb{R}^{n} \rightarrow \mathbb{R}^{e}$ and the decoder $D$ reverses this mapping $D: \mathbb{R}^{e} \rightarrow \mathbb{R}^{n}$.
Typically, neural networks are used to learn the encoder and decoder function by minimising a reconstruction loss between the encoder input and the decoder output \sidecite{Ranzato_Huang_Boureau_LeCun_2007}.
However, regularisation is needed so that the functions $E$ and $D$ do not converge to the identity operators.
One of the simplest regularisation methods is to introduce a bottleneck, i.e., compress the representation with the encoder while still re-creating the original input as well as possible with the decoder.
Thus, a bottleneck layer limits the number of neurons.
An alternative (or a supplement) is to limit the number of active neurons by enforcing sparsity.
Autoencoders with a sparsity constraint are also known as \emph{sparse autoencoders}.
A sparsity constraint can be imposed with $L_1$ or $L_2$ regularisation or with a kullback-leibler (KL) divergence\sidenote{the kullback-leibler divergence is a measure of how one probability distribution differs from another reference probability distribution} between average neuron activations and an ideal activation distribution \sidecite{10-5555-3042573-3042641}.
Other popular versions of autoencoders are denoising autoencoders \sidecite{10-1145-1390156-1390294}, contractive autoencoders \sidecite{10-5555-3104482-3104587}, and variational autoencoders (VAE) \sidecite{Kingma_Welling_2014}.
For \emph{denoising autoencoders}, the input is disrupted with noise (e.g. by adding Gaussian noise or removing some pixels by using dropout) before it is fed into the encoder.
The goal of the decoder is to reconstruct the clean version of the input without the added noise.
By doing so, denoising autoencoders learn to create a more robust representation of the input or can be used for error correction.
\emph{Contractive autoencoders}, on the other hand, try to make the feature extraction less sensitive to small perturbations.
By adding the squared Jacobian norm to the reconstruction loss, the latent representations of the input tend to be more similar to each other.
This diminishes latent representations that are unimportant for the reconstruction, and only essential variations between the inputs are kept.
The encoder of \emph{variational autoencoders} map the input to a probabilistic distribution; instead of mapping the input to a latent vector, the input is mapped to a vector representing the mean $\boldsymbol{\mu}$ and another vector representing the standard deviation $\boldsymbol{\sigma}$ of a multivariate Gaussian distribution.
This is done by adding two fully connected layers after the encoder, one layer to predict $\boldsymbol{\mu}$ and the other layer to predict $\boldsymbol{\sigma}$.
Afterwards, a variable is sampled from this distribution $z \sim \mathcal{N}(\boldsymbol{\mu},\, \boldsymbol{\sigma}^{2} \cdot \epsilon)$, where $\epsilon \sim \mathcal{N}(0,\,1)$ \sidenote{this process is also called the re-parameterisation trick} and fed into the decoder $D$.
By doing so, the latent space becomes, by design, continuous and allows random sampling and data interpolation.



\subsection{Self-Supervised Learning}\seclbl{visual_rep_learning_ssl}
Other approaches do not compress data but generate a supervision signal themselves.
These methods are thus based on \emph{self-supervised learning}.
Typically, the input is augmented, and either the augmentation parameters are predicted, the original version of the image is reconstructed, or consistent representations among augmented views of the image are learned.
For example, some models use masking to learn visual representation;
a part of the input image is removed, and the model predicts the missing part (image inpainting) \cite{Elharrouss_Almaadeed_Al-Maadeed_Akbari_2020}.
This not only allows to generate part of images but also to learn visual representations of the image \sidecite{Pathak_Krahenbuhl_Donahue_Darrell_Efros_2016, he2022masked, shi2022adversarial}.
Other approaches rotate images and predict the rotation angle to generate representations \sidecite{komodakis2018unsupervised, Zhai_Oliver_Kolesnikov_Beyer_2019}.
Many further methods exist, and a comprehensive overview is given by \sideciteay{chen2022semi}.

Another popular self-supervised learning paradigm is \emph{contrastive learning}.
The idea of contrastive learning is that similar images should yield similar representations.
Typically, contrastive learning models are trained without labels.
In this case, two augmented views of the same image are created, fed through an encoder and the representations of these two views are pushed together in the embedding space by maximising their similarity \sidecite{chen2020simple, he2020momentum, caron2020unsupervised}.
Subsequently, the encoder can be used to generate image representations that are used for downstream tasks, such as image classification.
However, contrastive learning can also leverage information from external annotations.
For example, \sideciteay{khosla2020supervised} use labels in a contrastive setting to pull the representations of images of the same class together in the embedding space while simultaneously pushing apart clusters of samples from different classes.

\subsection{Relevance}
In this thesis, image representations are learned with two different principles;
horizontal self-organisation uses contrastive learning by forcing samples from the same class to have similar representations and samples from different classes to have high diversity.
Vertical self-organisation uses independently trained variational autoencoders to obtain good representations of input patches.
Thus, image representations are learned with the aforementioned methods.


\section{Meta-Learning}\seclbl{meta_learning}
TODO: NOT SURE IF THIS SECTION IS NEEDED / SHOULD BE KEPT... -> NOT THAT RELEVANT SO FAR

\emph{Meta-learning} is the process of distilling experience from multiple learning cycles and using the accumulated experience to improve future learning processes \sidecite{Hospedales_Antoniou_Micaelli_Storkey_2021}.
Therefore, meta-learning is also referred to as ``learning-to-learn'' \sidecite{Thrun_Pratt_1998} and improves learning on a lifetime (i.e. single agent) and evolutionary timescale (i.e. population of agents) \sidecite{Schrier_1984}.
Typically, meta-learning includes \emph{base learning} (also referred to as inner or lower learning) and \emph{meta-learning} (also referred to as outer or upper learning).
During base learning, the model learns a typical task, such as image classification, based on a dataset and an objective function.
During meta-learning, an algorithm updates the base learning algorithm based on a meta objective function\sidenote{typical meta objective functions aim to improve generalisation or learning speed}.
Thus, meta-learning usually iterates between learning a task and improving the learning algorithm that is used to learn the task.

In the following, the categorisation according to Hospedales et al. \cite{Hospedales_Antoniou_Micaelli_Storkey_2021} is used to describe the different characteristics of meta-learning algorithms.
\begin{description}
   \item[Meta-Representation] What meta-knowledge should be learned by the outer meta-learning algorithm.
   \item[Meta-Optimiser] How the outer meta-learning algorithm learns, i.e. the form of the learning algorithm.
   \item[Meta-Objective] What the goal of the meta-learning algorithm is.
\end{description}


\subsection{Meta-Representation}
The first dimension of the meta-learning landscape is the \emph{meta-representation}, i.e. defining the meta-knowledge to be learned.
One possibility is to learn good initial parameters that the model can use during base learning.
Good initial parameters are only a few gradient steps away from a set of parameters that can solve a task drawn from a set of tasks.
A popular algorithm to learn initial parameters is called MAML \sidecite{10-5555-3305381-3305498, 10-5555-3327546-3327622} and works well on smaller networks.
However, a major challenge is that the outer meta-learning algorithm has to find a solution for as many parameters as the inner base learning needs.
Therefore, many approaches focus on isolating a subset of parameters to meta-learn \cite{lee2018gradient, qiao2018few, rusu2018meta}.

Black-box models use meta-learning to directly provide the parameters to the base learning algorithms that are required to fulfil a task such as classification, i.e. the base learning algorithm assigns a sample directly to a class with the given parameters, without parameter optimisation \cite{heskes2000empirical, 10-5555-3454287-3455002, Ha_Dai_Le_2016}.
Special cases of black-box models are approaches based on metric learning:
The outer meta-learning process optimises a model to transform inputs into representations that the inner process can use for classification by comparing the cosine similarity or euclidean distance between them \cite{10-5555-3294996-3295163, qiao2018few, Chen_Liu_Kira_Wang_Huang_2020}.

Other approaches meta-learn the optimiser of the inner base learning algorithm \cite{ravi2017optimisation, Li_Malik_2016, Li_Zhou_Chen_Li_2017}.
Typically, such approaches generate each optimisation step of the base learning algorithm based on the model's parameters and a given base objective function.
The trainable component can, for example, be simple hyper-parameters such as the learning rate \sidecite{Li_Zhou_Chen_Li_2017} or more sophisticated such as pre-conditioned matrices \sidecite{Park_Oliva_2020}.
Even the learning algorithm itself can be learned \sidecite{kirsch2021meta}.

The outer meta-learning algorithm can also be used to learn the inner objective function (while the outer objective function is fixed).
Such loss-learning approaches output a scalar value that is used as loss value by the inner optimiser.
Common goals of these approaches are to obtain a loss that has less local minima \cite{NEURIPS2018_7876acb6, Sung_Zhang_Xiang_Hospedales_Yang_2017}, provides better generalisation \cite{NEURIPS2018_b9a25e42, NEURIPS2019_e0e2b58d, gonzalez2020improved}, leads to more robust models \cite{li2019feature}, or to learn from unlabelled data \cite{NEURIPS2019_6018df18, Boney2018SemiSupervisedFL}.

Other approaches use the outer meta-learning loop to learn network architectures for the inner base learning cycle.
Some approaches use reinforcement learning in the outer loop to learn CNN architectures \sidecite{zoph2017neural}, while other approaches use evolutionary algorithms to learn the topology of LSTM cells \sidecite{Bayer_Wierstra_Togelius_Schmidhuber_2009} or to model the network by a graph of network-blocks \sidecite{10-1609-aaai-v33i01-33014780}.

The outer meta-learning loop can also meta-learn hyper-parameters such as regularisation-strength \cite{pmlr-v80-franceschi18a, Micaelli_Storkey_2021}, task-relatedness for multi-task learning \sidecite{10-5555-3305381-3305502}, or sparsity-strength \cite{10-5555-3305381-3305502}. Furthermore, it can meta-learn suitable data augmentation methods \cite{Cubuk_2019_CVPR, Li_Hu_Wang_Hospedales_Robertson_Yang_2020} or improve the selection process for the samples of a mini-batch \cite{10-1609-aaai-v33i01-33015741, fan2018learning}.
Even the dataset itself can be learned with dataset distillation; a rather large dataset can be summarised as a smaller dataset with only a few support images \cite{Wang_Zhu_Torralba_Efros_2020, pmlr-v108-lorraine20a} that still allow good generalisation on test images. In sim2real learning \sidecite{Andrychowicz_Baker_Chociej_Józefowicz_McGrew_Pachocki_Petron_Plappert_Powell_Ray_2020}, also the graphics engine \cite{ruiz2018learning, Vuong_Vikram_Su_Gao_Christensen_2019} can be trained in an outer loop so that the performance on real-world data is maximised.


\subsection{Meta-Optimiser}
The second dimension of the meta-learning landscape is the \emph{meta-optimiser}.
The meta-optimiser describes how the algorithm in the outer meta-learning loop is learned.
Many methods use backpropagation of error and gradient descent (c.f. \secref{ann}) to optimise the meta parameters \cite{ravi2017optimisation, 10-5555-3305381-3305498, li2019feature, 10-5555-3305381-3305502, pmlr-v80-franceschi18a, Micaelli_Storkey_2021, pmlr-v108-lorraine20a}.
However, this algorithm has some well-known weaknesses (c.f. \secref{limitationsDL}) and requires differentiability.
When the inner base learning algorithm includes non-differentiable steps \sidecite{Cubuk_2019_CVPR} or the outer meta objective function is non-differentiable \sidecite{huang2019addressing}, many methods utilise the RL paradigm to optimise the outer objective \sidecite{Duan_Schulman_Chen_Bartlett_Sutskever_Abbeel_2016}.
However, using RL to alleviate the differentiability requirement is usually computationally very costly.
Besides gradient descent and RL, evolutionary algorithms can be used for learning the outer meta-learning function \cite{schmidhuber-1987, Stanley_Clune_Lehman_Miikkulainen_2019, Salimans_Ho_Chen_Sidor_Sutskever_2017}.
These algorithms have no differentiability constraints, do not suffer from gradient degradation issues, are highly parallelisable, and can often avoid local minima better than gradient-based methods \sidecite{Salimans_Ho_Chen_Sidor_Sutskever_2017}.
The downside of evolutionary algorithms is that a large population size is required (especially if many parameters have to be learned), and the performance is generally inferior to gradient-based methods for large models.

\subsection{Meta-Objective}
The third dimension of the meta-learning landscape is the \emph{meta-objective}.
The meta-objective describes the goal of the meta-learning algorithm.
Typically, the performance of a meta-learning algorithm is evaluated with a metric on the inner loop or a meta-metric on the outer loop.
However, there are several design options within the meta-learning framework.
First, the inner base-learning episodes can either take few \sidecite{ravi2017optimisation, 10-5555-3305381-3305498} or many \sidecite{10-5555-3305381-3305502, 10-5555-3305890-3306069} samples, and thus the goal can be to either improve few- or many-shot performance.
Second, the validation loss of the inner base-learning algorithm can either be calculated at the end of a learning episode to encourage a better \emph{final} performance of the base task or as the sum of the loss calculated after each update step to encourage \emph{faster} learning \cite{antoniou2018how}.
Third, the goal of the meta-learning can either be to solve better any task drawn from a set of (often related) tasks (i.e. multi-task setting) \sidecite{10-5555-3305381-3305498, 10-5555-3294996-3295163, li2019feature}, or to solve one specific task better than when only the inner base learning algorithm is used (i.e. single task setting) \sidecite{10-5555-3305381-3305502}.
Finally, the meta-optimisation can either be done offline (i.e. the inner and outer loop alternate) \sidecite{10-5555-3305381-3305498} or online (i.e. the meta-learning takes place within a base learning episode) \cite{li2019feature}.






%\section{Correlation within CNNs}

%TODO: Not sure if this chapter is still relevant.... -> move or delete?

% Self-organisation in neural networks can be done based on the input data.
%If Hebbian learning is used\sidenote{``Cells that fire together wire together''}, cells are connected based on their correlation (i.e. cells with a high correlation are wired together).
%One way to capture the correlation within CNNs are Gram matrices.
%Gram matrices are essentially the dot-product between the channels of a feature map and can capture the style of given image.
%They are for example used for image style transfer\sidecite{Gatys_Ecker_Bethge_2015} or related fields such as texture synthesis \sidecite{Gatys_Ecker_Bethge_20152}.
%Appendix \chref{image_style_transfer} provides an intuitive explanation what image style transfer is and how it is related to Gram matrices.

%A Gram matrix can be calculated based on the output of a convolutional layer.
%Each filter of a convolutional layer (i.e. each channel) produces a so called convolutional map.
%A convolutional map contains information about the content of the image such as object structure and positioning as well as information about the style.
%Calculating a Gram matrix eliminates content-related information from the convolutional layer output but does not affect style information (c.f. Appendix \chref{image_style_transfer}).
%A Gram matrix calculates the correlations between the convolutional maps (i.e. between the filter responses) of a convolutional layer output.
%For a convolutional filter output $F$ of layer $l$ and two flattened convolutional maps $i$ and $j$ it is defined as \sidecite{7780634}:

%\begin{align}\eqlbl{Gram_mat}
%		G_{ij}^{l} = \sum_k F^{l}_{ik} \cdot F^{l}_{jk}
%\end{align}%

%Thereby, $k$ is a hyperparameter defining how many elements of the convolutional output $F$ are compared.
%Gatys et al. \sidecite{7780634} applied this formula the first time to convolutional filters but did not fully explain why it works.
%However, they found that the style is captured well in the correlation between convolutional maps.
%Later, it was shown \sidecite{10555531720773172198} that matching the Gram matrices between two convolutional filters can be reformulated as minimising the Maximum Mean Discrepancy (MMD) \sidecite{JMLRv13gretton12a} and thus that the style information is intrinsically represented by the distribution of activations in a CNN.

%Intuitively, several filters together can describe the style of the image.
%For example, if one filter reacts to vertical white and black lines and a second filter reacts to horizontal white and black lines and the input image has a checkerboard style, then these two filters have a high correlation, which is reflected in the Gram Matrix (c.f. Appendix \chref{image_style_transfer}).

%When Hebbian learning is used for self-organisation, neurons of filters that often trigger together are connected.
%A dataset usually contains specific patterns, which are represented in the Gram Matrix\sidenote{In the following the term pattern is used instead of style, because the Gram matrix can represent not only styles like photorealistic images, drawings, etc., but any non-content related information.
%For example, for an animal dataset, one filter could have high activation on white color, a second filter on black vertical lines, and a third filter on black dots.
%For a photo of a zebra, the first and second filters would have a high correlation while for a dalmatian, the first and third filters would have a high correlation}.
%Thus, neurons are connected that alone represent a certain filter but together represent a certain more complex pattern.


%\section{Rotation Invariant Convolutions}\seclbl{rotation_invariant_conv}

%TODO: Not sure if this chapter is still relevant.... -> move or delete?

%In this thesis, I show the capability of self-organisation to deal with object transformations.
%Common transformations of objects in visual scenes are translations, rotations, zooming, and deformations.
%While most state-of-the-art architectures are translation invariant\sidenote{STOA architectures for visual scene interpretation are mainly based on convolutional neural networks and vision transformers \cite{Dosovitskiy_Beyer_Kolesnikov_Weissenborn_Zhai_Unterthiner_Dehghani_Minderer_Heigold_Gelly_2021}. These architectures are by definition translation invariant.}, they suffer from the other aforementioned transformations.
%The model can be trained to be more robust against most kind of transformations through data augmentation \sidecite{Simard_Steinkraus_Platt_2003}.
%For example, dynamically increasing and decreasing image sizes works very well to obtain robustness against different object sizes.
%Robustness against deformations, on the other hand, can be learned to a great extend through data pre-processing such as random grid-based deformations \sidecite{Ronneberger_Fischer_Brox_2015, Sager_Salzmann_Burn_Stadelmann_2022}.
%However, deformations are often domain specific.
%In this thesis, I mainly focus on rotation invariance of visual perception systems as this is a challenging task and is for many applications the most important transformation invariance beyond translation.
%Although transformation invariant models can be obtain with data augmentation \cite{Simard_Steinkraus_Platt_2003, Fasel_Gatica_Perez_2006}, this approach is considered inefficient as the model has to learn many redundant parameters, independent for each rotation angle.
%In the following, we focus on methods that overcome this limitation.

%A simple approach to achieve rotation invariance is to find the main axis of an image patch and to rotate it until it is aligned with the samples from the training set \sidecite{Jafari_Khouzani_Soltanian_Zadeh_2005}.
%Another common strategy is to define features that are rotation invariant or equivariant, i.e. to use features whose output is either not affected by rotating the input image or whose output is rotated the same way as the input image by definition.
%Some well-known approaches are Local Binary Patterns \sidecite{Ojala_Pietikainen_Maenpaa_2002}, spiral resampling \sidecite{Wen_RongWu_ShiehChungWei_1996}, and steerable pyramid filters \sidecite{greenspan1994rotation}.

%Other approaches learn rotatable filters from the input data.
%Dieleman et al. \sidecite{Dieleman_DeFauw_Kavukcuoglu_2016} propose four new neural networks blocks.
%The probably most important block proposed in their work is a pooling operation that is applied over rotated feature maps to  reduce the number of parameters and to learn rotation invariance more explicitly.
%Another approach \sidecite{Laptev_Savinov_Buhmann_Pollefeys_2016} also applies convolutional filters to rotated versions of the image but aggregates the result by taking the maximum activation over the feature maps as output.

%Another category of approaches apply rotations to learned convolutional filters.
%Earlier approaches \sidecite{Schmidt_Roth_2012, Kivinen_Williams_2011, Sohn_Lee_2012} use a Convolutional Restricted Boltzmann Machine (C-RBM)\sidenote{A C-RBM is a generative stochastic network that can learn a probability distribution over its inputs. Multiple layers of C-RBM are also known as deep belief networks.} \sidecite{Lee_Grosse_Ranganath_Ng_2009} to tie the weights.
%Besides using C-RBM, it is also possible to tie the weights within several layers of a CNN to enforce rotation invariance and to reduce the number of parameters to learn. 
%Teney and Herbert \sidecite{Teney_Hebert_2016} split the filters of a CNN in orientation groups and constraint their weights.
%Such models achieve rotation covariance\sidenote{rotation covariance means that applying a rotation to the input image results in a shift of the output across the features} and only need to learn a single canonical filter per orientation group.
%This concept can also be applied to the rotation group in the final layers of a CNN to obtain invariance to global rotations \sidecite{Wu_Hu_Kong_2015}.

\end{comment}
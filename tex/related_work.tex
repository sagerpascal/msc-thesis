%% related_work.tex

TODO: Teile auf in related work (zu Beginn) + optionales Kapitel mit weiteren relevanten Themen zu Natural Intelligence, die aber nicht wirklich related work sind sondern eher ein Survey

\section{Natural Intelligence}\seclbl{natural_intelligence}
This thesis is inspired by the work ''A Theory of Natural Intelligence`` from von der Malsburg et al. \sidecite{von_der_Malsburg_Stadelmann_Grewe_2022}.
Therefore, summarize their work in detail in the following.

According \cite{von_der_Malsburg_Stadelmann_Grewe_2022}, the process of learning is influenced by ``nature'', ``nurture'', and ``emergence''\sidenote{nature refers to the influence of genes and evolution, nurture to the influence of experience and education}.
They point out that human genome (as of nature) only contain 1GB of information \sidecite{hbcrd} and humans only absorb a few GB into permanent memory over a lifetime (as of nurture) but it requires about 1PB to describe the connectivity in human brain.
Therefore, it is important to distinguish the amount of information to describe a structure from the amount of information needed to generate it.
Similar, nature and nurture only require a few GB to construct, respectively instruct the entire human brain.
Therefore, they argue that the human brain must be highly structured (i.e. nature and nurture ``generate'' the human brain by selecting from a set pre-structured patterns).
The authors call the process of generating the highly structured network in the human brain the ``Kolmogorov \sidecite{Kolmogorov_1998} Algorithm of the Brain''\sidenote{as the Kolmogorov complexity describes the number of bits required by the shortest algorithm that can generate the structure}.
Network self-organization is the only mechanism that has not yet been disproved by experiments as the brains Kolmogorov algorithm \sidecite{Willshaw_VonDerMalsburg_1976, Willshaw_VonDerMalsburg_1979}.
This mechanism loops between activity and connectivity, with activity acting back on connectivity through synaptic plasticity until a steady state, called an attractor network, is reached.
The consistency property of an attractor network means that a network has many alternative signal pathways between pairs of neurons \sidecite{Malsburg_1987}.
Thus, the brain develops as an overlay of attractor networks called net-fragments \sidecite{vonderMalsburg_2018}.
Net-fragments consist of small sets of neurons, whereby each neuron can be part of several net fragments.
The network self-organization has to start from an already established coarse global structure which is improved in a coarse-to-fine manner to avoid being caught in a local optima.

Also, von der Malsburg et al. \cite{von_der_Malsburg_Stadelmann_Grewe_2022} discuss scene representation (i.e. how a scene is represented in the brain) even tough they point out that this is a contested concept \sidecite{freeman1990representations}.
Scene representation is a organization framework to put abstract interpretation of scene layouts, elements, potential actions, and emotional responses in relation.
The details are not rendered as in photographic images but the framework supports the detailed reconstructions of narrow sectors of the scene.
The basic goal if learning is to integrate a behavioral schema into the flow of scene representations.
They propose the hypothesis that the network structure resulting from self-organization together with the neural activation in the framework of scene representation are the inductive bias that tunes the brain to the natural environment.

Finally, they discuss how net fragments can be used to implement such structures and processes using vision as an example.
They point out that a neuron is grouped in one or multiple net fragments through network self-organization.
The net fragments can be considered as filters that detect previously seen patterns in the visual input signal.
An object is represented by multiple net fragments, where each fragment responds to the surface of that object and has shared neurons and connections with other net fragments representing that object.
Thus, net fragments render the topological structure of the surfaces that dominate the environment.
Von der Malsburg et al. \cite{von_der_Malsburg_Stadelmann_Grewe_2022} propose that net fragments represent shape primitives which can adapt to the shape of actual objects\sidenote{adapt in spite of metric deformations, depth rotation, and position}.
Shifter circuits are one possible implementation of networks that enable invariant responses to the position- and shape-variant representations \sidecite{Arathorn_2002, Olshausen1995}.
They are composed of net-fragments that can be formed by network self-organization \sidecite{Fernandes_vonderMalsburg_2015}.
Ref. \cite{von_der_Malsburg_Stadelmann_Grewe_2022} also argue that net fragments are the compositional data structure used by the brain.
A hierarchy of features may be represented by nested net fragments of different size.
Complex objects, such as mental constructs, can thus be seen as larger net fragments composed as mergers of pre-existing smaller net fragments.

Von der Malsburg et al. \cite{von_der_Malsburg_Stadelmann_Grewe_2022} do not address how their interesting theoretical concepts can be implemented in a mathematical model.
However, a concrete implementation was done by Claude Lehmann in the form of a Master's thesis \sidecite{lehmann}.
He proposes a new layer called the laterally connected layer (LCL).
The LCL layer extends convolutional layers by forming lateral intra-layer connections based on the Hebbian learning rule (c.f. Section \secref{hebbian}).
Similar to a convolutional filter, the convolutional feature map is calculated.
Afterwards, the convolutional feature maps are compared and the lateral impact between feature maps is calculated (i.e. the covariance between the feature maps).
When two feature maps are similar in the same pixel locations, their connection strength is increased.
By using Hebbian learning, lateral connections are formed between the feature maps with a high lateral impact.
Thus, new filters are formed based on existing filters with a high covariance.
Lehmann found that LCL layers increase robustness for object recognition.
He shows on the MNIST dataset \cite{Lecun_Bottou_Bengio_Haffner_1998} that for a small reduction in accuracy of 1\%, the performance on corrupted images increases by up to 21\% and that it works especially well for noisy types of corruptions.
However, this layers improved performance only on very small networks.

In this thesis, the ideas from the work of von der Malsburg et al. \cite{von_der_Malsburg_Stadelmann_Grewe_2022} are incorporated into a deep learning model.
Thus, this work is considered to be \emph{the} inspiration for this thesis.
However, this thesis is based on other concepts than the thesis by Lehmann \cite{lehmann} and therefore has little in common with his work.

\section{Self-Organization}\seclbl{self_org_related}
Self-organization is the process by which systems consisting of many units spontaneously acquire their structure or function without interference from a external agent or system.
They organize their global behavior by local interactions amongst themselves.
The absence of a central control unit allow self-organizing systems to quickly adjust to new environmental conditions.
Additionally, such systems have in-built redundancy with a high degree of robustness as they  are made of many simpler individual units.
These individual units can even fail without the overall system breaking down.

In nature, groups of millions units that solve complex tasks by using only local interactions can be observed.
For example, ants can navigate difficult terrain with a local pheromone-based communication and thus form a collective type of intelligence.
Such observations inspired researchers to build algorithms which are based on local communication and self-organization, for example ant colony optimization algorithms \sidecite{dorigo1997ant}.
DeepSwarm \sidecite{Byla_Pang_2020} is a neural architecture search method that uses this algorithm to search for the best neural architecture.
This methods achieves competitive performance on rather small datasets such as MNIST \cite{Lecun_Bottou_Bengio_Haffner_1998}, Fashion-MNIST \cite{xiao2017/online}, and CIFAR-10 \cite{krizhevsky2009learning}.

%Robotic is another research area that uses ideas from collective intelligence such as self-assembly or self-organization.
%For example, swarm systems consist of multiple robots that work together to solve complex tasks \sidecite{Hamann_2018}.
%A famous example of self-assembling robots was presented in 2014 by Rubenstein et al. \sidecite{Rubenstein_Cornejo_Nagpal_2014}.
%They teach kilobots\sidenote{kilobots are 3.3cm tall low-cost swarm robots developed at Harvard University} to self-assemble into target shapes such as letters or stars solely based on local communication between robots.
%However, the kilobots still rely on hand-crafted algorithms to determine their position in the global coordinate system.

%TODO: Write more about swarm intelligence or delete thie paragraph above (does not really fit in here....)

Cellular Automata mimic developmental processes in multi-cell organisms.
They contain a grid of similar cells with an internal state which is updated periodically.
The transition from a given state to a subsequent state is defined by some update rules.
During an update, cells are only allowed to communicate with the neighbouring cells.
Thus, self-organization is enforced by the definition of the update rules.
Such automata can be used to study biological pattern formations \sidecite{Wolfram1984} or physical systems \sidecite{VICHNIAC198496}.
Neural Cellular Automata \sidecite{Wulff1992LearningCA} use neural networks to learn the update rule.
The input in such a neural network is the state of a given cell and its neighbours, the output the subsequent cell state.
Usually, the same network is applied to all cells.
In this case, a fully connected neural network which is applied to each cell and its local neighbours can be reformulated as a CNN\sidecite{PhysRevE}.
NCAs can be trained efficiently with gradient descent to grow given 2D patterns such as images\sidecite{48963, Mordvintsev_Randazzo_Fouts_2022}.
These images are grown through self-organisation (i.e. the pixels pick a color based on the color of neighboring pixels) and are surprisingly resistant to damage.
For example, large parts of the images can be removed and the system is able to rebuild these pixels\sidenote{a demo of this regeneration process is available at \cite{NCAs_distill}}.
However, the aforementioned approaches can only grow the pattern they were trained on.
A recent method called Variational Neural Cellular Automata \sidecite{Palm_GonDuque_Sudhakaran_Risi_2022} use an NCA as decoder of a Variational Autoencoder \sidecite{Kingma_Welling_2014}.
This probabilistic generative model can grow a large variety of images from a given input encoded in a vector format.
However, there is still a big gap in performance compared to state-of-the-art generative models.
Besides growing 2D patterns, NCAs can also create 3D patterns such as buildings in the popular video game Minecraft by utilizing 3D CNNs \sidecite{Sudhakaran_Grbic_Li_Katona_Najarro_Glanois_Risi_2021} or generate structures with specific function such as simulated robots able to locomote\sidecite{Horibe_Walker_Risi_2021}.
Moreover, self-assembling approaches based on NCAs are not restricted to grid-structures.
NCAs can be generalized to graph neural networks \sidecite{Grattarola_Livi_Alippi_2021}.
Graph cellular automata (GCA) use graph neural networks \sidecite{Zhou_Cui_Hu_Zhang_Yang_Liu_Wang_Li_Sun_2021} instead of CNNs to learn the state transition rules and can thus deal with more complex pattern structures than just 2D and 3D grids.
The process of growing images from cells of an NCA can also be inverted.
Randazzo et al. \sidecite{randazzo2020self_classifying} propose to use NCA to classify given structures such as images.
They apply the same network to each pixel and its neighbours of an image.
In an iterative process based on local communication, the image fragments then agree on which object they represent.
NCAs can even be used to control reinforcement learning (RL) agents.
Variengien et al. \sidecite{Variengien_Nichele_Glover_Pontes_Filho_2021} use the observations of the environment as state of the NCA, the subsequent state predicted by the NCA are used as Q-value estimates of a deep Q-learning algorithm \sidecite{Mnih_Kavukcuoglu_Silver_Graves_Antonoglou_Wierstra_Riedmiller_2013}.

Self-organization can not only be used to generate structures but also to optimize the weights of a neural networks over the agents lifetime.
For example, a Hebbian learning rule for meta-learning can be used to self-organize the weights of a RL agent over his lifetime\sidecite{NEURIPS2020_ee23e7ad}.
This means that across multiple episodes the weights of a Hebbian based model are learned.
The weights of the agents policy are reset in every episode and the Hebbian based model is used to update them.
This allows the agent to adapt better to the changed conditions within the environment.

Besides optimizing the weights, self-organization has also been used to change the learning rule itself.
The method ``Evolve and Merge`` \sidecite{Pedersen_Risi_2021} uses the so called ``ABCD'' Hebbian learning rule which updates the weights as follows:
\begin{equation}\eqlbl{McCulloch_Pitts_act}
	\Delta w_{ij} = \alpha (A o_i o_j + B o_i + C o_j + D)
\end{equation}%

$\alpha$ is the learning rate, $o_i$ and $o_j$ are the activity levels of connected neurons and $A$, $B$, $C$, and $D$ are learned constants.
For each connection in the network is one learning rule initialized and the constants are learned.
After a pre-defined number of epochs, the learning rules are clustered and the ones with similar constants are merged.
By repeating this process, the number of parameters can be reduced and robustness increases according to the authors.

Alternatively, it is also possible to initialize the network with shared parameters instead of starting with many rules and merging them over time.
Kirsch and Schmidhuber \sidecite{kirsch2021meta} use multiple tiny recurrent neural networks (RNNs) that have the same weight parameters but different internal states\sidenote{Intuitively, these tiny RNNs can be interpreted as more complex neurons.}.
By using self-organization and Hebbian learning, they show that it is possible to learn powerful learning algorithms such as backpropagation while running the network in forward-mode mode only.
However, it works only for small-scale problems as it can get stuck in local optima.
In general seem self-organizing systems to be hard to optimize and only to work for small datasets or simple problems so far.

Risi \sidecite{risi2021selfassemblingAI} describes why self-organizing systems are hard to train;
First, the system is hard to control because there is no central entity in charge but the system must still be nudges into the right direction.
Second, self-organizing systems are unpredictable (i.e. there exist no mathematical model that tells the outcome of the self-organizing process).

\subsection{Growing Networks}
Unsupervised learning techniques usually map high dimensional input data to a lower-dimensional representation.
One approach to do so are self-organizing maps (SOM) \sidecite{Kohonen_1982, Kohonen_1989}.
They map the input data to a discretized representation of the input space of the training samples, called a map.
In opposite to ANNs, they use competitive learning instead of error correction learning (i.e. back-propagation with gradient descent).
A weight vector is used to map the data to a node in the mapping field.
The datapoints ``compete'' for the weight vectors.
The weight vector of a node in the map that best matches a datapoint is moved closer to that input, as are nodes that are in the neighbourhood.
By doing so, samples that are close in the input space are also closed in the resulting maps.

However, SOM have two major limitations; First, the network structure must be pre-defined which constraints the result mapping accuracy. Second, the capacity of the map is predefined through the number of nodes.
Growing networks are able to overcome this limitations.
Growing networks add nodes or whole layers of nodes into the network structure at the positions of the map where the error is highest.
Many growing networks \sidecite{NIPS1994_d56b9fc4, Reilly_Cooper_Elbaum_1982, Fritzke_1994} add such units after a fixed number of iterations in which the error is accumulated.
After adding a unit, it takes several iterations to accumulate the error again until the next node can be added.

Grow When Required (GWR) networks \sidecite{Marsland_Shapiro_Nehmzow_2002} use a different criterion to add nodes.
Instead of adding nodes to support the node with the highest error, nodes are added when a given input samples cannot be matched with the current nodes by some pre-defined accuracy.
This allows the network to adapt the growing process rather fast; The networks stops growing when the input space is matched b the network with some accuracy and the networks starts growing again if the input distribution changes.

Such GWR networks can be used to build self-organizing architectures.
For example, Mici et al. \sidecite{Mici_Parisi_Wermter_2018} build a self-organizing architecture based on GWR to learn human-object interactions from videos.
They use two GWR in parallel, one to process feature representations of body postures and another to process manipulated objects.
A third GWR is used to combine these two streams and to create actionâ€“object mappings in a self-organized manner.
By doing so, they are able to learn human-object interactions and exhibit a model which is more robust to unseen samples than comparable deep learning approaches.


\subsection{Self-Organization in Spiking Neural Networks}\seclbl{self_org_spiking}
Spiking neural networks (SNNs) (c.f. Section \secref{spiking_networks}) communicate through binary signals known as spikes and are very efficient on special event-based hardware\sidecite{8259423}.
There exist several methods to self-organize such architectures.
For the sake of completeness, two well-known approaches are described in the following.
However, since this thesis focuses on self-organization in deep learning systems, these approaches are only roughly described and for detailed explanations please refer to the respective literature.

Similar to deep learning, there exists a multitude of different network architectures; Shallow \sidecite{masquelier2007unsupervised, 6469239} and deep networks \sidecite{kheradpisheh2018stdp, mozafari2019bio} structures, fully connected \sidecite{diehl2015unsupervised} and convolutional layers \sidecite{cao2015spiking, tavanaei2016bio}, as well as based on different learning rules such as supervised \sidecite{diehl2015fast, zenke2018superspike}, unsupervised \sidecite{diehl2015unsupervised, ferre2018unsupervised} and reinforcement learning based \sidecite{mozafari2018first}.

A representable method for self-organization in SNNs is proposed by Raghavan et al. \sidecite{Raghavan_Lin_Thomson_2020}.
They introduce a stackable tool-kit to assemble multi-layer neural networks.
This tool-kit is a dynamical system that encapsulates the dynamics of spiking neurons, their interactions as well as the plasticity rules that control the flow of information between layers.
Based on the input, spatio-temporal waves are generated that travel across multiple layers.
A dynamic learning rule tunes the connectivity between layers based on the properties of the waves tiling the layers\sidenote{for more information please refer to \cite{Raghavan_Lin_Thomson_2020}}.

An alternative method proposed by Raghavan and Thomson \sidecite{Raghavan2019NeuralNG} grows a neural network.
They start with a single computational ``cell'' and use a wiring algorithm to generate a pooling architecture in a self-organizing fashion.
The pooling architecture emerges through two processes; First, a layered neural network is grown. Second, self-organization of its inter-layer connections is used to form defined ``pools'' or receptive fields.
They us the Izikhevich neuron model \sidecite{Izhikevich_2003} in the first layer to generate spatio-temporal waves.
The units in the second layer learn the ``underlying'' pattern of activity generated in the first layer. 
Based on the learned patterns, the inter-layer connections are modified to generate a pooling architecture\sidenote{for more information please refer to \cite{Raghavan2019NeuralNG}}.

In general, SNNs have to ``convert'' static input data such as images to a dynamic signal.
For example, images are often converted to such signals by using Difference of Gaussian (DoG) convolution filters \sidecite{Vaila_Chiasson_Saxena_2019, KHERADPISHEH201856}.
Such filters subtract one Gaussian blurred version of an original image from another, less blurred version\sidenote{DoG filter can thus be used to reduce noise and to detect edges}.
This subtraction results in spikes for each pixel.
To encode the filter output into a temporal signal, bigger spikes are forwarded earlier in time than smaller spikes.
However, such approaches lose a lot of information about the input.
For example, in the process described above are all information about color and thin structures lost.
To the author of this thesis, this seems to be the reason why these SNNs can't match the performance of deep learning algorithms so far and often only work well for small gray-scale image-datasets such as MNIST \cite{Lecun_Bottou_Bengio_Haffner_1998}.

\subsection{Relevance}\seclbl{self_org_relevance}
Self-organization is, according to von der Malsburg et al. \sidecite{von_der_Malsburg_Stadelmann_Grewe_2022}, one of the key elements for natural intelligence.
Therefore, this concept of local optimization and interaction plays a very important role in this thesis.
Specifically, two types of self-organization are proposed in this thesis;
Vertical self-organisation is based on layer-wise learning. In this approach, the layers of a neural network are considered separate units and are updated independently.
The communication between these units takes place by feeding data sequentially through the layers.
The second type of self-organization is vertical self-organisation. In this approach, data is divided into patches and then analysed by independent networks. Thus, small networks form the self-organising units and communicate with each other by adding the output of one network to the input of another network.
These two types of self-organization are described in more detail in Chapter \chref*{methods}.


\section{Alternative Training Algorithms}\seclbl{alt_train_algo}
Neural networks, especially deep neural networks, are usually optimised by backpropagation of errors (c.f. Section \secref{ann}).
Soon after backpropagation was published, it was question whether this algorithm is suitable to explain the learning in the brain \sidecite{Crick_1989, Grossberg_1987}.
Besides the fact that backpropagation seems no biologically plausible and is responsible for many of the limitations described in Section \secref{limitationsDL}, it also has technical shortcomings.
First, gradients might vanish or explode when propagated through too many layers \sidecite{Zhang_He_Sra_Jadbabaie_2020}.
Second, it turns the networks into ``black boxes'' and prevents the users from getting useful insights from trained models\sidenote{training a model a layer-wise allows modularization, i.e. divide an conquer}.
Training end-to-end does not allow to control the effect of the loss function on a hidden layer because of the non-linearity in the network \sidecite{pmlr-v38-lee15a}.
Finally, the loss landscapes are non-optimal which might lead to slow convergence and the possibility that the reached local minimum is suboptimal \sidecite{Ioffe_Szegedy_2015}.

Hebbian learning (c.f. Section \secref{hebbian}), on the other hand, is widely accepted in the fields of neurocomputing, psychology, neurology, and neurobilogy \sidecite{Widrow_Kim_Park_Perin_2019}.
Hebbian mechanisms are for example sufficient for topographic mappings\sidenote{mapping input data to a discretized representation} \sidecite{Kohonen_1989, Grajski_Merzenich_1990}, neuroplasticity\sidenote{the networks ability to reorganize itself by forming new connections} \sidecite{Montague_Gally_Edelman_1991}, or recognizing non-linear patterns \sidecite{Mel_1992}, but also have limitations \sidecite{Anderson_1998}.
Hebb's learning rule is insufficient as general rule and is limited temporally as it requires (almost) synchronous stimuli \sidecite{6302929}\cite{Anderson_1998}.
However, many signals such as motor control problems, are sequential and Hebbian learning has therefore to be combined with additional memory mechanisms \sidecite{Grossberg_Schmajuk_1989}.

There exists many alternative learning functions to these two well known algorithms that might overcome the aforementioned limitations.
A group of algorithms that do not require end-to-end forward or backward pass is called \emph{Proxy Objective}.
These algorithms use a proxy objective function for each layer.
While backpropagation calculates a global loss after the forward pass, this strategy uses a proxy function for each layer.
The loss function of the first Layer $L_1$ (i.e. the proxy objective of the first layer) influence only the trainable parameters of the first layer $\theta_1$ but not the parameters of the second layer $\theta_2$.
This allows to decouple the layers.
The different instances of \emph{Proxy Objective} methods mainly differ in the proxy objective function.
However, the idea behind the proxy objective function is always the same; namely to characterize the separability of the hidden representations.
Minimizing $L_1$ encourages the first layer to improve the separability of its output representations, making the classification problem simpler for the subsequent layer.
The separability between hidden representation vectors can be measured by distance/similarity metrics as done in \sidecite{Duan_Yu_Principe_2022, Duan_Yu_Chen_Principe_2020}.
Wang et al. \sidecite{Wang_Ni_Song_Yang_Huang_2021} propose an auxiliary neural network to map the hidden representation to another feature space before computing their distance/similarity.
This allows to choose the dimensionality of the feature space in which the proxy objective function is computed regardless of the given network layer dimension.
Others \sidecite{belilovsky2019greedy, Mostafa_Ramesh_Cauwenberghs_2018, Marquez_Hare_Niranjan_2018} propose an additional network with either fixed or trainable weights as proxy function and used this auxiliary network's accuracy to quantify data separability.
Nokland and Eidnes \sidecite{pmlr-v97-nokland19a} combine the two aforementioned approaches by using two auxiliary networks; one network is trained with a ``similarity matching loss'' and the other with a cross-entropy loss (i.e. to quantify separability with an auxiliary classifier accuracy).
A major issue of the first \emph{Proxy Objective} methods is that the models are trained layer-wise\sidenote{when layer $k$ is trained, the layers $1, ..., k-1$ are frozen and the layers $k+1, ..., n$ are not updated}.
The resulting computational complexity can be reduced with synchronous or asynchronous training.
In the synchronous setting is first a forward pass through all layers performed and afterwards are all layers trained simultaneously by minimizing their own proxy objective function \cite{belilovsky2019greedy}.
In the asynchronous settings are all layers trained simultaneously by using a replay buffer for each layer that stores the layers output \sidecite{10.5555/3524938.3525007}.
Each layer receives its input from such a buffer instead of the previous layer what eliminates the need for an end-to-end forward pass.

Recently, Hinton \sidecite{ff_algo} introduced the \emph{Forward-Forward (FF) Algorithm} that uses a proxy objective functions over multiple time-steps.
This algorithm uses two forward passes, one with positive (i.e. real) data and one with negative data.
Each layer has its own objective function which is to have a high ``goodness'' for positive data and a low ``goodness'' for negative data. 
The goodness is measured by the sum of the squared neural activities.
The goal of the learning process is to make the goodness be above some threshold value for real data and below that threshold for negative data.
More specifically, the aim is to correctly classify input vectors as positive data or negative data.
The probability that an input vector is positive $p(positive)$ is given by applying the logistic function $\sigma$ to the goodness minus the threshold $\theta$

\begin{equation}\eqlbl{ff2}
	p(positive) = \sigma \biggl( \sum_j y^2_j - \theta \biggr)
\end{equation}

where $y_j$ is the output of a hidden unit.
To perform a classification task, the label is included in the input.
Positive data consists of an input vector (e.g. an image) and the correct label, negative data consists of an input vector with the wrong label.
The goal is then to have a high goodness in each layer for the positive data and a low goodness for the negative data.
The only difference between positive and negative data is the label and thus the \emph{FF} algorithm only learns features that correlate with the label.
During inference, the data is fed with each label through the network and the one with the highest goodness is kept as prediction.
One major weakness is that one layer at a time is learned and later layers cannot affect what is learned in earlier layers.
This limitation can be overcome by treating the static input vector as a sequence (i.e. use the same input for multiple steps) and by using a multi-layer recurrent network \sidecite{Hinton_2021}.
The algorithm still runs forward in time but the activity vector at each layer is determined by the activity vector of the previous layer and the activity vector of the subsequent layer at the previous time-step.

\emph{Target Propagation} methods are a group of algorithms that require an end-to-end forward pass but not an end-to-end backward pass.
These algorithms compute targets\sidenote{a target can be interpreted as the value that should be predicted by a layer} rather than gradients at each layer.
Similar to gradients, the targets are propagated backward through the network.
While the output layer obviously uses the true label as its target (i.e. the goal is to predict the label), the targets of previous layers are found sequentially (from the output layer to the input layer).
Good targets are those that minimize the loss in the output layer if they are realized in the forward pass.
The simplest solution would be to apply the inverse function of each layer to propagate the target backward.
If $h_l$ are the activations and $\theta_l$ the parameters of layer $l$, we can define the forward pass as:

\begin{equation}\eqlbl{tp1}
	h_l = f(h_{l-1}; \theta_l)
\end{equation}

The inverse function that yields the target activation $\hat{h}_l$ would then be:

\begin{equation}\eqlbl{tp2}
	\hat{h}_l = f^{-1}(\hat{h}_{l+1}; \theta_{l+1})
\end{equation}

As mentioned before, the target of the final layer $\hat{h}_L$ is the loss-optimal output activation such as the correct label distribution.
However, advanced neural networks are usually not invertible and approximate inverse transformations have to be learned with decoders

\begin{equation}\eqlbl{tp3}
	g(h_{l+1}; \lambda_{l+1}) \approx f^{-1}(h_{l+1}; \theta_{l+1})
\end{equation}

where $\lambda_l$ are the trainable parameters of the decoder at layer $l$.
Thus, the target activation can be approximated by an encoder:

\begin{equation}\eqlbl{tp4}
	\hat{h}_l \approx g(\hat{h}_{l+1}; \lambda_{l+1}) 
\end{equation}


Different instances of \emph{Target Propagation} methods mainly differ in the way the the targets are generated and/or the loss function of the decoder $L_{g}$.
Vanilla Target Propagation \sidecite{Bengio_2014} directly derives the target from the decoder $\hat{h}_l = g(\hat{h}_{l+1}; \lambda_{l+1})$ and trains the decoder $g()$ by minimizing

\begin{equation}\eqlbl{tp5}
	L_{g} = \lVert g(h_{l+1} + \epsilon; \lambda_{l+1}) - (h_{l} + \epsilon) \rVert_2^2
\end{equation}

where $\epsilon$ is some added Gaussian noise to enhance generalization.
It is obvious that this loss functions encourages the decoder $g()$ to learn the inverse of $f()$ by minimizing the difference between the activations from the forward-pass $h_{l}$ of layer $l$ and the decoder's output (based on the activations of the subsequent layer $h_{l+1}$).
The targets are predicted by feeding the \emph{target activations} $\hat{h}_{l+1}$ through the decoder\sidenote{note that these are two different kind of activations: The inverse function is learned based on the activation of the forward pass $h_l$, the target activation is predicted based on the loss-optimal output activation $\hat{h}_L$}.

Later, a major improvement was to extend the decoder $g()$ of Vanilla \emph{Target Propagation} with a correction term that enables a more robust optimality guarantee for bigger networks \sidecite{Lee_Zhang_Fischer_Bengio_2015}.
This method is known as \emph{Difference Target Propagation} and extends Equation \eqref*{tp4} as follows:

\begin{equation}\eqlbl{tp6}
	\hat{h}_l = g(\hat{h}_{l+1}; \lambda_{l+1}) + [h_l - g(h_{l+1}; \lambda_{l+1})]
\end{equation}

The extra term $[h_l - g(h_{l+1}; \lambda_{l+1})]$ is to correct errors of the decoder in estimating the inverse.
\emph{Difference Target Propagation} has been further improved by novel loss functions $L_{g}$ for the decoder.
These methods are known as \emph{Difference Target Propagation with Difference Reconstruction Loss} and \emph{Direct Difference Target Propagation} \sidecite{10.5555/3495724.3497405}.
However, since the concepts are roughly the same as for \emph{Difference Target Propagation} (despite adding some terms to the loss function), I do not summarize these methods in more detail here.
An advantage of calculating targets is that only layer-wise gradients are required what allows models to have non-differentiable operations \cite{Lee_Zhang_Fischer_Bengio_2015}.
A major drawback is that auxiliary models have to be trained to calculate the hidden targets $\hat{h}_l$ what might overweight the savings achieved by eliminating a full backward pass.

Another group of alternative learning algorithms are known as \emph{Synthetic Gradients} \sidecite{pmlr-v70-jaderberg17a, 10.5555/3305381.3305475}.
\emph{Synthetic Gradients} methods replace the gradients used in the backward pass by approximating local gradients with auxiliary models and utilize gradient-based optimization algorithms such as SGD to update the weights locally.
The auxiliary models are usually fully-connected networks that are trained to regress a layer's gradients when given the layer's activations.
End-to-end backwards passes are only performed occasionally to acquire real gradients that can be used to trained the gradient approximation models.
Thus, the frequency of end-to-end backward passes is reduced.
When auxiliary input models are used to predicts the layer's input, the frequency of the forward pass can be reduced as well in the same manner as for the backward pass \cite{pmlr-v70-jaderberg17a}.
It is also possible to get rid of the backward pass completely by training the auxiliary gradient models with local information only \sidecite{Lansdell_Prakash_Kording_2020}.
However, this works significantly worse than when real gradients are used.
An advantage of \emph{Synthetic Gradients} methods is that they can be used to approximate backpropagation through time (BTT) for an unlimited number of steps \cite{pmlr-v70-jaderberg17a}.
It has been shown that this allows more efficient training for learning long-range dependencies compared to BTT.

Many other methods are motivated purely by biological plausibility.
Some examples are \cite{10.5555.3157096.3157213, Lillicrap_Cownden_Tweed_Akerman_2016, Xiao_Chen_Liao_Poggio_2019, aaai.BalduzziVB15, 10.5555/3016100.3016156}.
However, these biological plausible methods have been significantly outperformed by backpropagation of error on meaningful benchmark datasets \sidecite{Bartunov_Santoro_Richards_Marris_Hinton_Lillicrap_2018}.
In the following \emph{Feedback Alignment} methods are discussed as they seem to be the most popular group of biological plausible alternatives to backpropagation.
From a biological point of view, one of the major criticisms of backpropagation is the ``weight transportation problem'' \sidecite{Lillicrap_Cownden_Tweed_Akerman_2016}; it is believed that the human brain doesn't have a backward pass where an error signal is passed to previous layers.
\emph{Feedback Alignment} algorithms use fixed, random weights during the backward pass \cite{Lillicrap_Cownden_Tweed_Akerman_2016}.
Thus the symmetry between the weights used during the forward pass and the backward pass is broken.
Later, the algorithm was improved by using fixed, random weights that share the signs with the actual weights of the network \sidecite{10.5555/3016100.3016156}.

Lastly, \emph{Auxiliary Variables} are another group of learning algorithms \cite{pmlr-v33-carreira-perpinan14, 10.5555/3045390.3045677, 10.5555/3294771.3294935, Lau_Zeng_Wu_Yao_2018}.
These methods use the idea of variable splitting, i.e. transform a complicated problem into a simpler one by introducing additional trainable variables.
Introducing auxiliary variables may pose scalability issues, and these methods require special, usually tailor-made solvers.
Therefore, such methods are not reviewed in more detail in this thesis.

According to \sidecite{Duan_Principe_2022} and \sidecite{Bartunov_Santoro_Richards_Marris_Hinton_Lillicrap_2018}, only proxy objective functions have achieved competitive performance with large models on large datasets such as ImageNet \cite{Deng_Dong_Socher_Li_KaiLi_LiFei_Fei_2009}.
The other approaches work only on smaller datasets such as MNIST \cite{Lecun_Bottou_Bengio_Haffner_1998} or CIFAR-10 \cite{krizhevsky2009learning} so far.

End-to-end backpropagation is not compatible with local self-organisation which is one of the key concepts of natural intelligence according to von der Malsburg et al. \sidecite{von_der_Malsburg_Stadelmann_Grewe_2022}.
With end-to-end backpropagation, systems are trained as a single unit and consequently have no independent local units that can organise themselves.
Therefore, alternative learning algorithms is very relevant to this thesis.
In fact, the approach based on vertical self-organisation uses proxy objective functions to train the layer locally as independent units (c.f. Chapter \chref*{methods}).
Proxy objective functions are used as these are the only alternative learning algorithm known that scale well on large neural networks and large datasets.

\section{Visual Representation Learning}\seclbl{visual_rep_learning}
One of the earliest methods of learning visual representations are \emph{Autoencoders}.
\emph{Autoencoders} have been introduced 1985 \sidecite{rumelhart1985learning} and learn an encoder and a decoder function \sidecite{pmlrv27baldi12a}.
The encoder $A$ maps the input from a high-dimensional space to a lower dimensional embedding space $A: \mathbb{R}^{n} \rightarrow \mathbb{R}^{e}$ and the decoder $B$ reverses this mapping $B: \mathbb{R}^{e} \rightarrow \mathbb{R}^{n}$.
Typically, neural networks are used to learn the encoder function $A$ and decoder function $B$ by minimizing a reconstruction loss \sidecite{Ranzato_Huang_Boureau_LeCun_2007}.
In order that the functions $A$ and $B$ are not just the identity operators, some regularization is needed.
One of the simplest regularization methods is to introduce a bottleneck, i.e. to compress the representation with the encoder while still being able to re-create the original input as good as possible with the decoder.
With a bottleneck layer, the number of neurons is limited.
An alternative (or a supplement) is to limit the number of activations by enforcing sparsity.
Autoencoders with such a sparsity constraint are also known as \emph{Sparse Autoencoders}.
A sparsity constraint can be imposed with $L_1$ regularization or a kullback-leibler (KL) divergence between expected average neuron activation and an ideal distribution \sidecite{10-5555-3042573-3042641}.
Other popular versions of \emph{Autoencoders} are \emph{Denoising Autoencoders} \sidecite{10-1145-1390156-1390294}, \emph{Contractive Autoencoders} \sidecite{10-5555-3104482-3104587}, and \emph{Variational Autoencoders (VAE)} \sidecite{Kingma_Welling_2014}.
In \emph{Denoising Autoencoders}, the input is disrupted by noise (e.g. by adding Gaussian noise or removing some pixels by using Dropout) and fed into the encoder $A$.
The goal of the decoding function $B$ is to reconstruct the clean version of the input without the added noise.
By doing so, the \emph{Denoising Autoencoders} learns to create more robust representation of the input or can be used for error correction.
\emph{Contractive Autoencoders}, on the other hand, try to make the feature extraction less sensitive to small perturbations.
By adding the squared Jacobian norm to the reconstruction loss, the latent representations of the input tend to be more similar to each other.
This diminishes latent representations that are not important for the reconstruction and only important variations between the inputs are kept.
The encoder of \emph{Variational Autoencoders} map the input to a probabilistic distribution; Instead of mapping the input to an encoding vector, the input is mapped to a vector representing the means $\mu$ and another vector representing the standard deviations $\sigma$.
This done by adding two fully connected layers after the encoder $A$, one layer to predict $\mu$ and the other layer to predict $\sigma$.
Afterwards, a variable is sampled from this distribution $z \sim \mathcal{N}(\mu,\,\sigma^{2})$\sidenote{this process is also called the re-parameterization trick} and fed into the decoder $B$.
By doing so, the latent space becomes by design continuous and allows random sampling and interpolation of data.

Other approaches for \emph{self-supervised learning} typically augment the visual scene and either predict the augmentation parameters, reconstruct the original version of the image, or learn consistent representations among augmented views of the image.
For example, some models use masking to learn visual representation;
A part of the input image is removed and the model predicts the missing part (image inpainting) \cite{Elharrouss_Almaadeed_Al-Maadeed_Akbari_2020}.
This not only allows to generate part of images but also to learn visual representations of the image \sidecite{Pathak_Krahenbuhl_Donahue_Darrell_Efros_2016, he2022masked, shi2022adversarial}.
Other approaches rotate images and predict the rotation angle to generate representations \sidecite{komodakis2018unsupervised, Zhai_Oliver_Kolesnikov_Beyer_2019}.
There exist many other methods that augment images and learn good visual representations based on it.
A comprehensive overview is given by \sidecite{chen2022semi}.

Another popular \emph{self-supervised learning} paradigm is \emph{contrastive learning}.
The idea of \emph{contrastive learning} is that similar images should yield similar representations.
Typically, \emph{contrastive learning} models are trained without labels.
In this case, two augmented views of the same image are created, fed through an encoder and the representation of this two views are pushed together in the embedding space by maximizing their similarity \sidecite{chen2020simple, he2020momentum, caron2020unsupervised}.
Subsequently, the encoder can be used to generate image representations that are used for other downstream tasks such as image classification.
However, \emph{contrastive learning} can also leverage information from annotations.
For example, Khosla et al. \sidecite{khosla2020supervised} use labels in a contrastive setting to pull the representations of images of the same class together in the embedding space, while simultaneously pushing apart clusters of samples from different classes.

In this thesis, representations of images are learned.
Two different principles are applied;
vertical self-organisation uses a type of contrastive learning by forcing samples from the same class to have similar representations and samples from different classes to have high diversity.
Horizontal self-organisation uses independently trained variational autoencoders to obtain good representations of input patches.


\section{Meta-Learning}
TODO: NOT SURE IF THIS CHAPTER IS STILL NEEDED -> ADD REFERENCE TO THIS CHAPTER

Meta-learning is the process of distilling experience from multiple learning cycles and using the accumulated experience to improve the future learning process \sidecite{Hospedales_Antoniou_Micaelli_Storkey_2021}.
Therefore, meta-learning is also referred to as ``learning-to-learn'' \sidecite{Thrun_Pratt_1998} and improves learning on a lifetime (i.e. single agent) and evolutionary timescale (i.e. population of agents) \sidecite{Schrier_1984}.
Typically, meta-learning includes \emph{base learning} (also referred to as \emph{inner} or \emph{lower} learning) and \emph{meta-learning} (also referred to as \emph{outer} or \emph{upper} learning).
During \emph{base learning}, the model learns a typical task such as image classification based on a dataset and a objective function.
During \emph{meta-learning}, an algorithm updates the \emph{base learning} algorithm based on a meta objective function\sidenote{Typical meta objective function aim to improve generalization or learning speed}.
Thus, meta-learning usually iterates between learning a task and improving the learning algorithm that is used to learn the task.

In the following, I use the categorisation according to Hospedales et al. \cite{Hospedales_Antoniou_Micaelli_Storkey_2021} to describe the different aspects of meta-learning algorithm.
\begin{description}
   \item[Meta-Representation] What meta-knowledge shall be learned by the outer \emph{meta-learning} algorithm.
   \item[Meta-Optimizer] How the outer \emph{meta-learning} algorithm learns, i.e. the from of the learning algorithm.
   \item[Meta-Objective] What the goal of the \emph{meta-learning} algorithm is.
\end{description}

The first dimension of the meta-learning landscape is the \textbf{meta-representation}.
Meta-representations describe which part of the learning strategy should be learned.
One possibility is to learn good \emph{initial parameters} that can be used by the model during \emph{base learning}.
Good initial parameters are only a few gradient steps away from a set of parameters that can solve a task $\mathcal{T}$ drawn from a set of tasks $p(\mathcal{T})$.
A popular algorithm to learn initial parameters is called MAML \sidecite{10-5555-3305381-3305498, 10-5555-3327546-3327622} and works well on smaller networks.
However, a major challenge is that the outer \emph{meta-learning} algorithm has to find a solution for as many parameters as the inner \emph{base learning} needs.
Therefore, many approaches focus on isolating a subset of parameters to meta-learn \cite{lee2018gradient, qiao2018few, rusu2018meta}.
\emph{Black-box models}, on the other hand, use \emph{meta-learning} to directly provide the parameters required to classify data (i.e. the \emph{base learning} algorithm maps a sample directly to class without iterative parameter optimization) \cite{heskes2000empirical, 10-5555-3454287-3455002, Ha_Dai_Le_2016}.
A special case of the black-box models are the approaches based on \emph{metric learning}\sidenote{an explanation why this approach can be considered black-box learning is provided by Hospedales et al. \cite{Hospedales_Antoniou_Micaelli_Storkey_2021}}.
The outer \emph{meta-learning} process optimizes a model to transform inputs into representations that can be used for recognition by similarity comparison (e.g. by using cosine similarity or euclidean distance) \cite{10-5555-3294996-3295163, qiao2018few, Chen_Liu_Kira_Wang_Huang_2020}.

Other approaches learn the \emph{optimizer} of the inner \emph{base learning} algorithm \cite{ravi2016optimization, Li_Malik_2016, Li_Zhou_Chen_Li_2017}.
Typically, such approaches generate each optimization step of the \emph{base learning} algorithm based on the models parameter and a given base objective function.
The trainable component can for example be simple hyper-parameters such as the learning rate \cite{Li_Zhou_Chen_Li_2017} or more sophisticated such as pre-conditioned matrices \cite{Park_Oliva_2020}.
Even the learning algorithm itself can be learned \sidecite{kirsch2021meta}.

The outer \emph{meta-learning} algorithm can also be used to learn the \emph{inner objective function} (while the outer objective function is fixed).
Such loss-learning approaches output a scalar value that is treated as loss by the inner optimizer based on relevant quantities such as prediction/ground truth pairs or model parameters.
Common goals of this approach are to obtain a loss that has less local minima \cite{NEURIPS2018_7876acb6, Sung_Zhang_Xiang_Hospedales_Yang_2017}, provides better generalization \cite{NEURIPS2018_b9a25e42, NEURIPS2019_e0e2b58d, gonzalez2020improved}, leads to more robust models \cite{li2019feature}, or to learn from unlabelled data \cite{NEURIPS2019_6018df18, Boney2018SemiSupervisedFL}.

Other approaches use the outer \emph{meta-learning} loop to learn \emph{network architectures} for the inner \emph{base learning} cycle.
Some approaches use reinforcement learning in the outer loop to learn CNN architectures \cite{zoph2017neural}, while other approaches use evolutionary algorithms to learn the topology of LSTM cells \cite{Bayer_Wierstra_Togelius_Schmidhuber_2009} or to model the network by a graph of network-blocks \cite{10-1609-aaai-v33i01-33014780}.

The outer \emph{meta-learning} loop can also meta-learn hyper-parameters such as regularization-strength \cite{pmlr-v80-franceschi18a, Micaelli_Storkey_2021}, task-relatedness for multi-task learning \cite{10-5555-3305381-3305502}, or sparsity-strength \cite{10-5555-3305381-3305502}. Furthermore, it can meta learn suitable data augmentation methods \cite{Cubuk_2019_CVPR, Li_Hu_Wang_Hospedales_Robertson_Yang_2020}, or improve the selection process for the samples of a mini-batch \cite{10-1609-aaai-v33i01-33015741, fan2018learning}.
Even the dataset itself can be learned with dataset distillation; A rather large datasets can be summarized a smaller dataset with only a few support images \cite{Wang_Zhu_Torralba_Efros_2020, pmlr-v108-lorraine20a} that still allow good generalization on real test images. In sim2real learning \cite{Andrychowicz_Baker_Chociej_JÃ³zefowicz_McGrew_Pachocki_Petron_Plappert_Powell_Ray_2020}, also the graphics engine \cite{ruiz2018learning, Vuong_Vikram_Su_Gao_Christensen_2019} can be trained in an outer loop so that the performance on real-world data is maximized.

The second dimension of the meta-learning landscape is the \textbf{meta-optimizer}.
The meta-optimizer describes how the algorithm in the outer \emph{meta-learning} loop is learned.
Many methods use backpropagation of error and gradient descent (c.f. Section \secref{ann}) to optimize the meta parameters \cite{ravi2016optimization, 10-5555-3305381-3305498, li2019feature, 10-5555-3305381-3305502, pmlr-v80-franceschi18a, Micaelli_Storkey_2021, pmlr-v108-lorraine20a}.
However, this algorithm has some well-known downsides (c.f. Section XXXXXXXXXX) and requires differentiability.
When the inner \emph{base learning} algorithm includes non-differentiable steps \cite{Cubuk_2019_CVPR} or the outer meta objective function is non-differentiable \cite{huang2019addressing}, many methods utilize the RL paradigm to optimize the outer objective \cite{Duan_Schulman_Chen_Bartlett_Sutskever_Abbeel_2016}.
However, using RL to alleviate requirement for differentiability is usually computationally very costly.
Finally, evolutionary algorithms can be used for learning the outer \emph{meta-learning} function \cite{schmidhuber-1987, Stanley_Clune_Lehman_Miikkulainen_2019, Salimans_Ho_Chen_Sidor_Sutskever_2017}.
These algorithms have no differentiability constraints, do not suffer from gradient degradation issues, are highly parallelizable, and can often avoid local minima better than gradient-based methods \cite{Salimans_Ho_Chen_Sidor_Sutskever_2017}.
The downside of evolutionary algorithms is that often a large population size is required (especially if many parameters have to be learned) and the performance is generally inferior to gradient-based methods for large models.
These three learning methods are also used in conventional machine learning.
However, meta-learning comparatively uses RL and evolutionary algorithms more frequently as some components are often non-differentiable in the meta-learning setup.

The third dimension of the meta-learning landscape is the \textbf{meta-objective}.
The meta-objective describes what the goal of the meta-learning algorithm is.
Typically, the performance of a meta-learning algorithm is evaluated with a metric on the inner loop or a meta-metric on the outer loop.
However, there are several design options within the meta-learning framework.
First, the inner base-learning episodes can either take few \cite{ravi2016optimization, 10-5555-3305381-3305498} or many \cite{10-5555-3305381-3305502, 10-5555-3305890-3306069} samples and thus the goal can be to either improve few- or -many-shot performance.
Second, the validation loss of the inner base-learning algorithm can either be calculated at the end of a learning episode to encourage a better \emph{final} performance of the base task or as sum of the loss calculated after each update step to encourage \emph{faster} learning \cite{antoniou2018how}.
Third, the goal of the meta-learning can either be to better solve any task drawn from a set of (often related) tasks (i.e. multi-task setting) \cite{10-5555-3305381-3305498, 10-5555-3294996-3295163, li2019feature}, or to solve one specific task better than when only the inner base learning algorithm is used (i.e. single task setting) \cite{10-5555-3305381-3305502}.
Finally, the meta-optimization can either be done offline (i.e. the inner and outer loop alternate) \cite{10-5555-3305381-3305498} or online (i.e. the meta-learning takes place within a base learning episode) \cite{li2019feature}.


%\section{Correlation within CNNs}

%TODO: Not sure if this chapter is still relevant.... -> move or delete?

%Self-organization in neural networks can be done based on the input data.
%If Hebbian learning is used\sidenote{``Cells that fire together wire together''}, cells are connected based on their correlation (i.e. cells with a high correlation are wired together).
%One way to capture the correlation within CNNs are Gram matrices.
%Gram matrices are essentially the dot-product between the channels of a feature map and can capture the style of given image.
%They are for example used for image style transfer\sidecite{Gatys_Ecker_Bethge_2015} or related fields such as texture synthesis \sidecite{Gatys_Ecker_Bethge_20152}.
%Appendix \chref{image_style_transfer} provides an intuitive explanation what image style transfer is and how it is related to Gram matrices.

%A Gram matrix can be calculated based on the output of a convolutional layer.
%Each filter of a convolutional layer (i.e. each channel) produces a so called convolutional map.
%A convolutional map contains information about the content of the image such as object structure and positioning as well as information about the style.
%Calculating a Gram matrix eliminates content-related information from the convolutional layer output but does not affect style information (c.f. Appendix \chref{image_style_transfer}).
%A Gram matrix calculates the correlations between the convolutional maps (i.e. between the filter responses) of a convolutional layer output.
%For a convolutional filter output $F$ of layer $l$ and two flattened convolutional maps $i$ and $j$ it is defined as \sidecite{7780634}:

%\begin{equation}\eqlbl{Gram_mat}
%		G_{ij}^{l} = \sum_k F^{l}_{ik} \cdot F^{l}_{jk}
%\end{equation}%

%Thereby, $k$ is a hyperparameter defining how many elements of the convolutional output $F$ are compared.
%Gatys et al. \sidecite{7780634} applied this formula the first time to convolutional filters but did not fully explain why it works.
%However, they found that the style is captured well in the correlation between convolutional maps.
%Later, it was shown \sidecite{10555531720773172198} that matching the Gram matrices between two convolutional filters can be reformulated as minimizing the Maximum Mean Discrepancy (MMD) \sidecite{JMLRv13gretton12a} and thus that the style information is intrinsically represented by the distribution of activations in a CNN.

%Intuitively, several filters together can describe the style of the image.
%For example, if one filter reacts to vertical white and black lines and a second filter reacts to horizontal white and black lines and the input image has a checkerboard style, then these two filters have a high correlation, which is reflected in the Gram Matrix (c.f. Appendix \chref{image_style_transfer}).

%When Hebbian learning is used for self-organization, neurons of filters that often trigger together are connected.
%A dataset usually contains specific patterns, which are represented in the Gram Matrix\sidenote{In the following the term pattern is used instead of style, because the Gram matrix can represent not only styles like photorealistic images, drawings, etc., but any non-content related information.
%For example, for an animal dataset, one filter could have high activation on white color, a second filter on black vertical lines, and a third filter on black dots.
%For a photo of a zebra, the first and second filters would have a high correlation while for a dalmatian, the first and third filters would have a high correlation}.
%Thus, neurons are connected that alone represent a certain filter but together represent a certain more complex pattern.


%\section{Rotation Invariant Convolutions}\seclbl{rotation_invariant_conv}

%TODO: Not sure if this chapter is still relevant.... -> move or delete?

%In this thesis, I show the capability of self-organization to deal with object transformations.
%Common transformations of objects in visual scenes are translations, rotations, zooming, and deformations.
%While most state-of-the-art architectures are translation invariant\sidenote{STOA architectures for visual scene interpretation are mainly based on convolutional neural networks and vision transformers \cite{Dosovitskiy_Beyer_Kolesnikov_Weissenborn_Zhai_Unterthiner_Dehghani_Minderer_Heigold_Gelly_2021}. These architectures are by definition translation invariant.}, they suffer from the other aforementioned transformations.
%The model can be trained to be more robust against most kind of transformations through data augmentation \sidecite{Simard_Steinkraus_Platt_2003}.
%For example, dynamically increasing and decreasing image sizes works very well to obtain robustness against different object sizes.
%Robustness against deformations, on the other hand, can be learned to a great extend through data pre-processing such as random grid-based deformations \sidecite{Ronneberger_Fischer_Brox_2015, Sager_Salzmann_Burn_Stadelmann_2022}.
%However, deformations are often domain specific.
%In this thesis, I mainly focus on rotation invariance of visual perception systems as this is a challenging task and is for many applications the most important transformation invariance beyond translation.
%Although transformation invariant models can be obtain with data augmentation \cite{Simard_Steinkraus_Platt_2003, Fasel_Gatica_Perez_2006}, this approach is considered inefficient as the model has to learn many redundant parameters, independent for each rotation angle.
%In the following, we focus on methods that overcome this limitation.

%A simple approach to achieve rotation invariance is to find the main axis of an image patch and to rotate it until it is aligned with the samples from the training set \sidecite{Jafari_Khouzani_Soltanian_Zadeh_2005}.
%Another common strategy is to define features that are rotation invariant or equivariant, i.e. to use features whose output is either not affected by rotating the input image or whose output is rotated the same way as the input image by definition.
%Some well-known approaches are Local Binary Patterns \sidecite{Ojala_Pietikainen_Maenpaa_2002}, spiral resampling \sidecite{Wen_RongWu_ShiehChungWei_1996}, and steerable pyramid filters \sidecite{greenspan1994rotation}.

%Other approaches learn rotatable filters from the input data.
%Dieleman et al. \sidecite{Dieleman_DeFauw_Kavukcuoglu_2016} propose four new neural networks blocks.
%The probably most important block proposed in their work is a pooling operation that is applied over rotated feature maps to  reduce the number of parameters and to learn rotation invariance more explicitly.
%Another approach \sidecite{Laptev_Savinov_Buhmann_Pollefeys_2016} also applies convolutional filters to rotated versions of the image but aggregates the result by taking the maximum activation over the feature maps as output.

%Another category of approaches apply rotations to learned convolutional filters.
%Earlier approaches \sidecite{Schmidt_Roth_2012, Kivinen_Williams_2011, Sohn_Lee_2012} use a Convolutional Restricted Boltzmann Machine (C-RBM)\sidenote{A C-RBM is a generative stochastic network that can learn a probability distribution over its inputs. Multiple layers of C-RBM are also known as deep belief networks.} \sidecite{Lee_Grosse_Ranganath_Ng_2009} to tie the weights.
%Besides using C-RBM, it is also possible to tie the weights within several layers of a CNN to enforce rotation invariance and to reduce the number of parameters to learn. 
%Teney and Herbert \sidecite{Teney_Hebert_2016} split the filters of a CNN in orientation groups and constraint their weights.
%Such models achieve rotation covariance\sidenote{rotation covariance means that applying a rotation to the input image results in a shift of the output across the features} and only need to learn a single canonical filter per orientation group.
%This concept can also be applied to the rotation group in the final layers of a CNN to obtain invariance to global rotations \sidecite{Wu_Hu_Kong_2015}.

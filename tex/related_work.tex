%% related_work.tex
This chapter presents the related work of the thesis. \secref{natural_intelligence} gives an introduction to a publication entitled ``A Theory of Natural Intelligence'' by von der Malsburg et al. \sidecite{von_der_Malsburg_Stadelmann_Grewe_2022}. This work is the main source of inspiration and is therefore summarised in detail. It identifies various concepts as the fundamentals of intelligence, including self-organisation. Therefore, the work associated with self-organisation is reviewed in \secref{self_org_related}. Self-organisation is not compatible with end-to-end backpropagation of error. Thus, alternative training algorithms are presented in \secref{alt_train_algo}. Then, \secref{visual_rep_learning} reviews different methods for learning image representations. Finally, meta-learning is summarised in \secref{meta_learning}. Meta-learning is of limited relevance to this thesis but is important for the proposed future work.

\section{Natural Intelligence}\seclbl{natural_intelligence}
According to \cite{von_der_Malsburg_Stadelmann_Grewe_2022}, the process of learning is influenced by ``nature'', ``nurture'', and ``emergence''\sidenote{nature refers to the influence of genes and evolution, nurture to the influence of experience and education, and emergence to the construction of network structures}.
They point out that the human genome (as of nature) only contains 1GB of information \sidecite{hbcrd} and humans only absorb a few GB into permanent memory over a lifetime (as of nurture), but it requires about 1PB to describe the connectivity in the human brain.
Thus, there is a massive gap between the amount of information needed to describe the brain's structure and the amount of information needed to generate it.
Therefore, they argue that the human brain must be highly structured and that the processes of nature and nurture construct the human brain by selecting from a set of pre-structured patterns.
The authors call the process of generating the highly structured network in the human brain the ``Kolmogorov \sidecite{Kolmogorov_1998} Algorithm of the Brain''\sidenote{the Kolmogorov complexity describes the number of bits required by the shortest algorithm that can generate a given structure}.
Network self-organisation is the only mechanism that experiments have not yet disproved as the brains Kolmogorov algorithm \sidecite{Willshaw_VonDerMalsburg_1976, Willshaw_VonDerMalsburg_1979}.
This mechanism loops between activity and connectivity, with activity acting back on connectivity through synaptic plasticity until a steady state, called an attractor network, is reached.
The consistency property of an attractor network means that a network has many alternative signal pathways between pairs of neurons \sidecite{Malsburg_1987}.
Thus, the brain develops as an overlay of attractor networks called net-fragments \sidecite{vonderMalsburg_2018}.
Net fragments consist of small sets of neurons, whereby each neuron can be part of several net fragments.
The network self-organisation has to start from an already established coarse global structure which is improved in a coarse-to-fine manner to avoid being caught in local optima.

Also, von der Malsburg et al. \cite{von_der_Malsburg_Stadelmann_Grewe_2022} discuss scene representation (i.e. how a scene is represented in the brain) even though they point out that this is a contested concept \sidecite{freeman1990representations}.
Scene representation is an organisational framework to put abstract interpretations of scene layouts, elements, potential actions, and emotional responses in relation.
The details are not rendered as in photographic images, but the framework supports the detailed reconstructions of narrow sectors of the scene.
The primary goal of learning is to integrate a behavioural schema into the flow of scene representations.
They propose the hypothesis that the network structure resulting from self-organisation and the neural activation in the framework of scene representation is the inductive bias that tunes the brain to the natural environment.

Finally, they discuss how net fragments can be used to implement such structures and processes using vision as an example.
They point out that network self-organisation groups neurons in one or multiple net fragments.
The net fragments can be considered filters that detect previously seen patterns in the visual input signal.
An object is represented by multiple net fragments, where each fragment responds to the surface of that object and has shared neurons and connections with other net fragments representing that object.
Thus, net fragments render the topological structure of the surfaces that dominate the environment.
Von der Malsburg et al. \cite{von_der_Malsburg_Stadelmann_Grewe_2022} propose that net fragments represent shape primitives which can adapt to the shape of actual objects\sidenote{adapt despite deformations, depth rotation, and position}.
Shifter circuits are one possible implementation of networks that enable invariant responses to the position- and shape-variant representations \sidecite{Arathorn_2002, Olshausen1995}.
They comprise net fragments that can be formed by network self-organisation \sidecite{Fernandes_vonderMalsburg_2015}.
Ref. \cite{von_der_Malsburg_Stadelmann_Grewe_2022} also argues that net fragments are the compositional data structure used by the brain.
Nested net fragments of different sizes may represent a hierarchy of features.
Complex objects, such as mental constructs, can thus be seen as larger net fragments composed of mergers of pre-existing smaller net fragments.

Von der Malsburg et al. \cite{von_der_Malsburg_Stadelmann_Grewe_2022} have not addressed how their interesting theoretical concepts can be implemented in a mathematical model.
However, a concrete implementation was done by Claude Lehmann in the form of a Master's thesis \sidecite{lehmann}.
He proposes a new layer called the laterally connected layer (LCL).
The LCL layer extends convolutional layers by forming lateral intra-layer connections based on the Hebbian learning rule (c.f. Section \secref{hebbian}).
Similar to a convolutional filter, the convolutional feature map is calculated.
Afterwards, the convolutional feature maps are compared, and the lateral impact between feature maps is calculated (i.e. the covariance between the feature maps).
When two feature maps are similar in pixel locations, their connection strength increases.
Hebbian learning forms lateral connections between the feature maps with a high lateral impact.
Thus, new filters are formed based on existing filters with a high covariance.
Lehmann found that LCL layers increase robustness for object recognition.
He shows on the MNIST dataset \cite{Lecun_Bottou_Bengio_Haffner_1998} that for a slight reduction in accuracy of 1\%, the performance on corrupted images increases by up to 21\% and that it works well for noisy types of corruptions.
However, the proposed layer has only improved performance on small networks, and its effectiveness on larger networks has yet to be demonstrated.

In this thesis, the ideas from the work of von der Malsburg et al. \cite{von_der_Malsburg_Stadelmann_Grewe_2022} are incorporated into a deep learning model.
Thus, this work is considered to be \emph{the} inspiration for this thesis.
However, this thesis is based on other concepts than the thesis by Lehmann \cite{lehmann} and therefore has little in common with his work.



\section{Self-Organisation}\seclbl{self_org_related}
\emph{Self-organisation} is the process by which systems consisting of many units spontaneously acquire their structure or function without interference from an external agent or system.
They organise their global behaviour through local interactions amongst themselves.
The absence of a central control unit allows self-organising systems to adjust to new environmental conditions quickly.
Additionally, such systems have built-in redundancy with a high degree of robustness as they consist of many simpler individual units.
These individual units can even fail without the overall system breaking down.

In nature, groups of millions of units that solve complex tasks using only local interactions can be observed.
For example, ants can navigate rugged terrain with local pheromone-based communication and thus form a collective type of intelligence.
Such observations inspired researchers to build algorithms based on local communication and self-organisation, for example, ant colony optimisation algorithms \sidecite{dorigo1997ant}.
DeepSwarm \sidecite{Byla_Pang_2020} is a neural architecture search method that uses this algorithm to search for the best neural architecture.
This method achieves competitive performance on relatively small datasets such as MNIST \cite{Lecun_Bottou_Bengio_Haffner_1998}, Fashion-MNIST \cite{xiao2017/online}, and CIFAR-10 \cite{krizhevsky2009learning}.

Cellular Automata mimic developmental processes in multi-cell organisms.
They contain a grid of similar cells with an internal state updated periodically.
Update rules define the transition from a given state to a subsequent state.
During an update, cells can only communicate with the neighbouring cells.
Thus, self-organisation is enforced by the definition of the update rules.
Such automata can be used to study biological pattern formations \sidecite{Wolfram1984} or physical systems \sidecite{VICHNIAC198496}.
Neural Cellular Automata \sidecite{Wulff1992LearningCA}.  Neural networks can be used to learn the update rule.
The input in such a neural network is the state of a given cell and its neighbours, and the output is the following cell state.
Usually, the same network is applied to all cells.
In this case, a fully connected neural network applied to each cell and its local neighbours can be reformulated as a CNN\sidecite{PhysRevE}.
NCAs can be trained efficiently with gradient descent to grow given 2D patterns such as images\sidecite{48963, Mordvintsev_Randazzo_Fouts_2022}.
These images are grown through self-organisation (i.e. the pixels pick a colour based on the colour of neighbouring pixels) and are surprisingly resistant to damage.
For example, large parts of the images can be removed, and the system can still rebuild the entire image\sidenote{a demo of this regeneration process is available at \cite{NCAs_distill}}.
However, the aforementioned approaches can only grow the pattern they were trained on.
A recent method called Variational Neural Cellular Automata \sidecite{Palm_GonDuque_Sudhakaran_Risi_2022} use an NCA as the decoder of a variational auto-encoder \sidecite{Kingma_Welling_2014}.
This probabilistic generative model can grow many images from a given input encoded in a vector format.
However, there is still a big gap in performance compared to state-of-the-art generative models.
Besides growing 2D patterns, NCAs can also create 3D patterns such as buildings in the popular video game Minecraft by utilising 3D CNNs \sidecite{Sudhakaran_Grbic_Li_Katona_Najarro_Glanois_Risi_2021} or generate structures with a specific function such as simulated robots able to locomote\sidecite{Horibe_Walker_Risi_2021}.
Moreover, self-assembling approaches based on NCAs are not restricted to grid structures.
NCAs can be generalised to graph neural networks \sidecite{Grattarola_Livi_Alippi_2021}.
Graph cellular automata (GCA) use graph neural networks \sidecite{Zhou_Cui_Hu_Zhang_Yang_Liu_Wang_Li_Sun_2021} instead of CNNs to learn the state transition rules and can thus deal with more complex pattern structures than just 2D and 3D grids.
The process of growing images from cells of an NCA can also be inverted.
Randazzo et al. \sidecite{randazzo2020self_classifying} propose to use NCA to classify given structures such as images.
They apply the same network to each pixel and its neighbours of an image.
In an iterative process based on local communication, the image fragments then agree on which object they represent.
NCAs can even be used to control reinforcement learning (RL) agents.
Variengien et al. \sidecite{Variengien_Nichele_Glover_Pontes_Filho_2021} use the observations of the environment as cell states of the NCA. The following states predicted by the NCA are used as Q-value estimates of a deep Q-learning algorithm \sidecite{Mnih_Kavukcuoglu_Silver_Graves_Antonoglou_Wierstra_Riedmiller_2013}.

Self-organisation can not only be used to generate structures, but also to optimise the weights of neural networks over the agent's lifetime.
For example, a Hebbian learning rule for meta-learning can be used to self-organise the weights of an RL agent over his lifetime\sidecite{NEURIPS2020_ee23e7ad}.
This means that the weights of a Hebbian-based model are learned across multiple episodes.
The weights of the agent's policy are reset in every episode, and the Hebbian-based model is used to update them.
This allows the agent to adapt better to the changed conditions within the environment.

Besides optimising the weights, self-organisation has also been used to change the learning rule.

The method ``evolve and merge`` \sidecite{Pedersen_Risi_2021} uses the so called ``ABCD'' Hebbian learning rule which updates the weights as follows:
\begin{equation}\eqlbl{McCulloch_Pitts_act}
	\Delta w_{ij} = \alpha (A a_i a_j + B a_i + C a_j + D)
\end{equation}%

$\alpha$ is the learning rate, $a_i$ and $a_j$ are the activity levels of connected neurons and $A$, $B$, $C$, and $D$ are learned parameters.
For each connection in the network, the four parameters are learned.
After a pre-defined number of epochs, the learning rules are clustered, and the ones with similar parameters are merged.
By repeating this process, the number of parameters can be reduced, and robustness increases, according to the authors.

Alternatively, it is also possible to initialise the network with shared parameters instead of starting with many rules and merging them over time.
Kirsch and Schmidhuber \sidecite{kirsch2021meta} use multiple tiny recurrent neural networks (RNNs) that have the same weight parameters but different internal states\sidenote{intuitively, these tiny RNNs can be interpreted as more complex neurons.}.
By using self-organisation and Hebbian learning, they show that it is possible to learn powerful learning algorithms such as backpropagation while running the network in forward-mode mode only.
However, it works only for small-scale problems as it can get stuck in local optima.
In general, self-organising systems are hard to optimise and only work for small data sets or simple problems so far.

Risi \sidecite{risi2021selfassemblingAI} describes why self-organising systems are hard to train;
First, the system is hard to control because no central entity is in charge, but the system must still be nudged in the right direction.
Second, self-organising systems are unpredictable (i.e., no mathematical model tells the outcome of the self-organising process). Therefore, he concludes that training such systems is much more complex to train than a system with central supervision, such as end-to-end backpropagation of error.


\subsection{Growing Networks}
Unsupervised learning techniques usually map high-dimensional input data to a lower-dimensional representation.
One approach implementing such a mapping is based on self-organizing maps (SOM) \sidecite{Kohonen_1982, Kohonen_1989}.
SOMs map the input data to a discretized representation space of the training samples called a map.
Unlike ANNs, they use competitive learning instead of error correction learning (i.e. backpropagation or error).
A weight vector maps the data to a node in the mapping field.
The data points ``compete'' for the weight vectors.
The weight vector of a node in the map that best matches a datapoint is moved closer to that input, as are nodes in the neighbourhood.
By doing so, samples that are close in the input space are also closed in the resulting maps.

However, SOMs have two significant limitations; First, the network structure must be pre-defined, limiting the mapping accuracy. Second, the map's capacity is pre-defined through the number of nodes.
Growing networks can overcome these limitations:
More advanced growing networks can add nodes or whole layers of nodes into the network structure at the positions of the map where the error is highest.
Many growing networks \sidecite{NIPS1994_d56b9fc4, Reilly_Cooper_Elbaum_1982, Fritzke_1994} add such units after a fixed number of iterations in which the error is accumulated.
After adding a unit, it takes several iterations to accumulate the error again until the next node can be added.

Grow When Required (GWR) networks \sidecite{Marsland_Shapiro_Nehmzow_2002} use a different criterion to add nodes.
Instead of adding nodes to support the node with the highest error, nodes are added when a given input sample cannot be matched with the current nodes by some pre-defined accuracy.
This allows the network to adapt to the growing process relatively fast; The network stops growing when the inputs can be matched with existing latent representations with a good-enough accuracy, and the network starts growing again if the input distribution changes.

Such GWR networks can be used to build self-organizing architectures.
For example, Mici et al. \sidecite{Mici_Parisi_Wermter_2018} build a self-organizing architecture based on GWR to learn human-object interactions from videos.
They use two GWR in parallel, one to process feature representations of body postures and another to process manipulated objects.
A third GWR combines these two streams and creates actionâ€“object mappings in a self-organized manner.
By doing so, they can learn human-object interactions and exhibit a model more robust to unseen samples than comparable deep learning approaches.


\subsection{Spiking Networks}\seclbl{self_org_spiking}
Spiking neural networks (SNNs) (c.f. Section \secref{spiking_networks}) communicate through binary signals known as spikes and are very efficient on special event-based hardware\sidecite{8259423}.
There exist several methods to self-organise such architectures.
For completeness, two well-known approaches are described in the following.
However, since this thesis focuses on the self-organisation of deep learning systems without time-dependent spikes, these approaches are only roughly described, and interested readers may refer to the respective literature.

Similar to deep learning, there exists a multitude of different spiking network architectures; Shallow \sidecite{masquelier2007unsupervised, 6469239} and deep \sidecite{kheradpisheh2018stdp, mozafari2019bio} network structures, networks based on fully connected layers \sidecite{diehl2015unsupervised} or convolutional layers \sidecite{cao2015spiking, tavanaei2016bio}, as well as networks that are trained with different learning principles such as supervised \sidecite{diehl2015fast, zenke2018superspike}, unsupervised \sidecite{diehl2015unsupervised, ferre2018unsupervised} and reinforcement learning \sidecite{mozafari2018first}.

One type of methodology for self-organisation of SNNs is based on a stackable toolkit to assemble multi-layer neural networks \sidecite{Raghavan_Lin_Thomson_2020}.
This toolkit is a dynamical system that encapsulates the dynamics of spiking neurons, their interactions, and the plasticity rules that control the flow of information between layers.
Based on the input, spatiotemporal waves are generated that travel across multiple layers.
A dynamic learning rule tunes the connectivity between layers based on the properties of the waves tiling the layers\sidenote{for more information, please refer to \cite{Raghavan_Lin_Thomson_2020}}.

An alternative method grows a spiking network through self-organisation \sidecite{Raghavan2019NeuralNG}; The growing process starts with a single computational ``cell'', and a wiring algorithm is used to generate a pooling architecture in a self-organising fashion.
The pooling architecture emerges through two processes; First, a network layer is grown. Second, self-organisation of its inter-layer connections forms defined receptive fields.
The first layer uses the Izikhevich neuron model \sidecite{Izhikevich_2003} to generate spatiotemporal waves.
The neurons in the following layers learn the ``underlying'' pattern of activity generated in the first layer. 
Based on the learned patterns, the inter-layer connections are modified to generate a pooling architecture\sidenote{for more information, please refer to \cite{Raghavan2019NeuralNG}}.

In general, SNNs must ``convert'' static input data such as images to a dynamic signal (c.f. \secref{spiking_networks
}). However, such approaches lose a lot of information about the input.
For example, all information about colour and thin structures is lost if DoG filters (c.f. \secref{spiking_networks}) are used to generate spikes.
Currently, this seems to be one of the limiting factors why such SNNs cannot match the performance of deep learning algorithms and often only work well for small grey-scale image data sets such as MNIST \cite{Lecun_Bottou_Bengio_Haffner_1998}.

\subsection{Relevance}\seclbl{self_org_relevance}
Self-organisation is, according to von der Malsburg et al. \sidecite{von_der_Malsburg_Stadelmann_Grewe_2022}, one of the fundamental principles for natural intelligence.
Therefore, this concept of local optimisation and interaction plays an essential role in this thesis.
Specifically, two types of self-organisation are proposed in this thesis;
Vertical self-organisation is based on layer-wise learning. In this approach, the layers of a neural network are considered separate units and are updated independently.
The communication between these units takes place by feeding data sequentially through the layers.
The second type of self-organisation is vertical self-organisation. In this approach, data is divided into patches and then analysed by independent networks. Thus, small networks form self-organising units and communicate with each other by adding the output of one network to the input of another network.
These two types of self-organisation are described in more detail in Chapter \chref*{methods}.


\section{Learning Algorithms}\seclbl{alt_train_algo}
Neural networks are usually optimised with backpropagation of errors (c.f. Section \secref{ann}).
Soon after backpropagation was published, it was questioned whether this algorithm is suitable to explain learning in the brain \sidecite{Crick_1989, Grossberg_1987}.
Besides the fact that backpropagation seems biologically not plausible and might be responsible for many of the limitations described in \secref{limitationsDL}, it also has technical shortcomings:
First, gradients might vanish or explode when propagating through too many layers \sidecite{Zhang_He_Sra_Jadbabaie_2020}.
Second, it turns the networks into ``black boxes'' and prevents users from getting valuable insights from trained models\sidenote{training a model layer-wise allows modularisation, i.e., dividing and conquering}.
Training end-to-end does not allow controlling the loss function's effect on a hidden layer because of the non-linearity in the network \sidecite{pmlr-v38-lee15a}.
Finally, the loss landscapes are non-optimal, which might lead to slow convergence, and the optimisation process can get stuck in a local minimum  \sidecite{Ioffe_Szegedy_2015}.

Hebbian learning (c.f. Section \secref{hebbian}), on the other hand, is widely accepted in the fields of neurocomputing, psychology, neurology, and neurobiology \sidecite{Widrow_Kim_Park_Perin_2019}.
Hebbian works well, for example, for topographic mappings\sidenote{mapping input data to a discretized representation} \sidecite{Kohonen_1989, Grajski_Merzenich_1990}, neuroplasticity\sidenote{the networks ability to reorganize itself by forming new connections} \sidecite{Montague_Gally_Edelman_1991}, or recognizing non-linear patterns \sidecite{Mel_1992}, but also have limitations \sidecite{Anderson_1998}.
Hebb's learning rule is insufficient as a general rule and is for time-shifted signals as it requires (almost) synchronous stimuli \sidecite{6302929}\cite{Anderson_1998}.
However, many signals, such as motor control problems, are sequential and Hebbian learning has to be combined with additional memory mechanisms \sidecite{Grossberg_Schmajuk_1989}.

\subsection{Proxy Objective Functions}
Many alternative learning functions exist to these two well-known algorithms that might overcome the aforementioned limitations.
One way to optimise locally and to overcome end-to-end backward passes is to use \emph{proxy objective functions}.
Using proxy objective functions means having a loss function for each layer instead of a global loss function for the entire model.
The loss function of the first layer (i.e. the proxy objective of the first layer) influences only the trainable parameters of the first layer but not the parameters of the second layer.
Thus, proxy objective functions allow the decoupling of layers.
The different instances of this method mainly differ in the details of the proxy objective function.
However, the aim is always the same: improving the separability of the hidden representations according to some measure.
Minimising the proxy objective function encourages a layer to improve the separability of its output representations, making the separability problem simpler for the subsequent layer.
A simple way to measure the separability between hidden representation vectors is to use distance or similarity metrics \sidecite{Duan_Yu_Principe_2022, Duan_Yu_Chen_Principe_2020}.
Instead of measuring the distance between hidden representations directly, an auxiliary neural network can be used to first map the hidden representations to another feature space before calculating their distance \sidecite{Wang_Ni_Song_Yang_Huang_2021}.
This allows to reduce the dimensionality of the feature space before calculating the proxy objective function.
Reducing the dimensionality is especially helpful for layers with large output representations, such as the activation maps of convolutional layers.
An auxiliary network can also be used to classify the latent representations in each layer and to use this auxiliary network's accuracy to quantify data separability \sidecite{belilovsky2019greedy, Mostafa_Ramesh_Cauwenberghs_2018, Marquez_Hare_Niranjan_2018}.
Nokland and Eidnes \sidecite{pmlr-v97-nokland19a} combine the two aforementioned approaches by using two auxiliary networks; one network is trained with a ``similarity matching loss'' and the other with a cross-entropy loss (i.e. to quantify separability with an auxiliary classifier accuracy).

A significant issue of the many models based on proxy objective functions is that the models are usually trained layer-wise\sidenote{when layer $k$ is trained, the layers $1, ..., k-1$ are frozen, and the layers $k+1, ..., n$ are not updated}.
The resulting computational inefficiency can be reduced with synchronous or asynchronous training.
In the synchronous setting, a forward pass through all layers is done, and afterwards, all layers are trained simultaneously by minimising their own proxy objective function \cite{belilovsky2019greedy}.
In the asynchronous setting, all layers are trained simultaneously by using a replay buffer for each layer that stores the layer's output \sidecite{10.5555/3524938.3525007}.
Each layer receives input from such a buffer instead of the previous layer, which eliminates the need for an end-to-end forward pass.

Recently, Hinton \sidecite{ff_algo} introduced the \emph{forward-forward (FF) algorithm}.
This algorithm uses two forward passes, one with positive (i.e. real) data and one with negative data.
Each layer has its objective function that aims to have a high ``goodness'' for positive data and a low ``goodness'' for negative data. 
The sum of the squared neural activities measures the goodness. Thus, having many (strongly) active neurons means having high goodness.
The goal of the learning process is to make the goodness be above some threshold value for real data and below that threshold for negative data.
The probability that an input vector is positive $P(\text{positive}
)$ is given by applying the logistic function $\sigma$ to the goodness minus the threshold $\theta$

\begin{equation}\eqlbl{ff2}
	P(\text{positive}) = \sigma \left( \sum_j y^2_j - \theta \right)
\end{equation}

where $y_j$ is the output of a hidden unit.
This algorithm can be used in different settings, for example, for supervised learning.
The label must be included in the input to conduct a supervised classification task.
Positive data consists of an input vector (e.g. an image) and the correct label, and negative data consists of an input vector with the wrong label.
The goal is to have a high goodness in each layer for the positive data and a low goodness for the negative data.
The only difference between positive and negative data is the label, and thus the FF algorithm only learns features that correlate with the label.
During inference, the data is fed with each label through the network, and the one with the highest goodness is kept as the prediction.
One major weakness is that one layer at a time is learned and later layers cannot affect what is learned in earlier layers.
This limitation can be overcome by treating the static input vector as a sequence (i.e. using the same input for multiple timesteps) and using a multi-layer recurrent network \sidecite{Hinton_2021}.
The trick is to feed the input into the network from one side and the label into the network from the other side. A layer receives as input the activity vector of the previous layer and the activity vector of the subsequent layer at the previous timestep. By enforcing high goodness when the input and label match, information must flow in both directions. Thus, hierarchical features can be learned without a backward pass or layer-wise training.


\subsection{Target Propagation}
\emph{Target propagation} methods are a group of algorithms that require an end-to-end forward pass but not an end-to-end backward pass.
These algorithms compute targets\sidenote{a target can be interpreted as the value that a layer should predict} rather than gradients at each layer.
Similar to gradients, the targets are propagated backwards through the network.
Each layer is trained to minimise the difference between its activations and its received target values.
While the output layer obviously uses the true label as its target (i.e. the goal is to predict the true label), the targets of previous layers are found sequentially (from the output layer to the input layer).
Good targets are those that minimise the loss in the output layer if they are realised in the forward pass.
Intuitively, each layer tells the previous layer the best possible input.

Let's define $h_l(\cdot)$ as a layer's function, i.e. layer $l$ calculates its activation $\boldsymbol{\hat{a}_l}$ based on the output of the previous layer $l-1$ as
\begin{equation}\eqlbl{tp0}
	\boldsymbol{\hat{a}_l} = h_l(\boldsymbol{\hat{a}_{l-1}})
\end{equation}

In the context of target propagation, $\boldsymbol{\hat{a}_l}$ is the actual layer activation and $\boldsymbol{a_l}$ is the desired layer activation (i.e. the target).
The output of a network with $L$ layers is a prediction $\boldsymbol{\hat{y}} = \boldsymbol{\hat{a}_L}$. If for each layer an inverse function $\boldsymbol{\hat{a}_{l-1}} = h_l^{-1}(\boldsymbol{\hat{a}_l})$ would exist, a layer's input could be calculated based on a layer's output. 
This idea is used by target propagation: During the forward pass, a prediction $\boldsymbol{\hat{y}}$ is calculated. However, the goal is to calculate the correct label distribution $\boldsymbol{y}$. Therefore, $\boldsymbol{y}$ is fed through the inverse function of each layer $h_l^{-1}(\cdot)$ to calculate a target for the previous layer.
Thus, if each layer predicted this target as output, the network's output would be the correct label distribution $\boldsymbol{y}$.
However, advanced neural networks are usually not invertible, and approximate inverse transformations have to be learned with decoders

\begin{equation}\eqlbl{tp3}
	d_l(\boldsymbol{\hat{a}_l}) \approx  h_l^{-1}(\boldsymbol{\hat{a}_l})
\end{equation}

Thus, the target activation can be approximated by a decoder:

\begin{equation}\eqlbl{tp4}
	\boldsymbol{a_{l}} \approx d_{l+1}(\boldsymbol{a_{l+1}}) 
\end{equation}



Different instances of target propagation methods mainly differ in the way the targets are generated or the loss function of the decoder $L_{d}$ is calculated.
Vanilla target propagation \sidecite{Bengio_2014} directly derives the target from the decoder $\boldsymbol{a_{l}} = d_{l+1}(\boldsymbol{a_{l+1}}) $ and trains the decoder $d(\cdot)$ by minimizing

\begin{equation}\eqlbl{tp5}
	L_{d} = \lVert d_{l+1}(\boldsymbol{\hat{a}_{l+1}} + \epsilon) - (\boldsymbol{\hat{a}_l} + \epsilon) \rVert_2^2
\end{equation}

where $\epsilon$ is some added Gaussian noise to enhance generalisation.
It is obvious that this loss function encourages the decoder $d_l(\cdot)$ to learn the inverse of $h_l(\cdot)$ by minimising the difference between the activations from the forward-pass $\boldsymbol{\hat{a}_l}$ of layer $l$ and the decoder's prediction of $\boldsymbol{\hat{a}_l}$ (calculated based on the activations of the subsequent layer $\boldsymbol{\hat{a}_{l+1}}$).
Afterwards, the targets are predicted by feeding the target activations $\boldsymbol{a_{l+1}}$ through the decoder\sidenote{note that these are two different kinds of activations: The inverse function is learned based on the activation of the forward pass $\boldsymbol{\hat{a}_{l+1}}$, the target activation is predicted based on the loss-optimal output activation $\boldsymbol{a_{l+1}}$}.

A major improvement of vanilla target propagation is to extend the decoder $g()$ with a correction term that enables a more robust optimality guarantee for bigger networks \sidecite{Lee_Zhang_Fischer_Bengio_2015}.
This method is known as \emph{difference target propagation} and extends Equation \eqref*{tp4} as follows:

\begin{equation}\eqlbl{tp6}
	\boldsymbol{a_{l}} \approx d_{l+1}(\boldsymbol{a_{l+1}}) + \left[ \boldsymbol{\hat{a}_l} - d_{l+1}\boldsymbol{\hat{a}_{l+1}}) \right]
\end{equation}

The additional term $\left[ \boldsymbol{\hat{a}_l} - d_{l+1}\boldsymbol{\hat{a}_{l+1}}) \right]$ is to correct errors of the decoder in estimating the inverse.
Difference target propagation has been further improved by novel loss functions $L_{d}$ for the decoder.
These methods are known as \emph{difference target propagation with difference reconstruction loss} and \emph{direct difference target propagation} \sidecite{10.5555/3495724.3497405}.
An advantage of calculating targets is that only layer-wise gradients are required, which allows models to have non-differentiable operations \cite{Lee_Zhang_Fischer_Bengio_2015}.
A significant drawback is that auxiliary decoders must be trained to approximate the hidden targets $\boldsymbol{a_{l}}$, which might outweigh the savings achieved by eliminating a complete backward pass.
















\subsection{Other Methods}
\emph{Synthetic gradients} methods \sidecite{pmlr-v70-jaderberg17a, 10.5555/3305381.3305475} replace the gradients used in the backward pass by approximating local gradients with auxiliary models and utilize gradient-based optimization algorithms such as stochastic gradient descent to update the weights locally.
The auxiliary models are usually fully-connected networks that are trained to regress a layer's gradients when given the layer's activations.
End-to-end backwards passes are only performed occasionally to acquire real gradients that can be used to trained the gradient approximation models.
Thus, the frequency of end-to-end backward passes is reduced.
When auxiliary input models are used to predicts the layer's input, the frequency of the forward pass can be reduced as well in the same manner as for the backward pass \cite{pmlr-v70-jaderberg17a}.
It is also possible to get rid of the backward pass completely by training the auxiliary gradient models with local information only \sidecite{Lansdell_Prakash_Kording_2020}.
However, this works significantly worse than when real gradients are used.
An advantage of synthetic gradient methods is that they can be used to approximate backpropagation through time (BTT) for an unlimited number of steps \cite{pmlr-v70-jaderberg17a}.
It has been shown that this allows more efficient training for learning long-range dependencies compared to BTT.

Many other methods are motivated purely by biological plausibility \cite{10.5555.3157096.3157213, Lillicrap_Cownden_Tweed_Akerman_2016, Xiao_Chen_Liao_Poggio_2019, aaai.BalduzziVB15, 10.5555/3016100.3016156}.
However, these biological plausible methods have been significantly outperformed by backpropagation of error on meaningful benchmark datasets \sidecite{Bartunov_Santoro_Richards_Marris_Hinton_Lillicrap_2018}.
Probably the most popular group of biological plausible alternatives to backpropagation are methods based on \emph{feedback alignment}.
From a biological point of view, one of the major criticisms of backpropagation is the ``weight transportation problem'' \sidecite{Lillicrap_Cownden_Tweed_Akerman_2016}; it is believed that the human brain doesn't have a backward pass that passes an error signal to previous layers.
\emph{Feedback alignment} algorithms use fixed, random weights during the backward pass \cite{Lillicrap_Cownden_Tweed_Akerman_2016}.
The gradients are calculated in the same way as when stochastic gradient descent is used but with random weights.
Thus the symmetry between the weights used during the forward pass and the backward pass is broken.
The network still learns to make the feedback useful even though the weights during forward and backward pass are different. 
Later, the algorithm was improved by using fixed, random weights that share the signs with the actual weights of the network \sidecite{10.5555/3016100.3016156}.

Lastly, another group of learning algorithms use \emph{auxiliary variables} \cite{pmlr-v33-carreira-perpinan14, 10.5555/3045390.3045677, 10.5555/3294771.3294935, Lau_Zeng_Wu_Yao_2018}.
These methods use the idea of variable splitting, i.e. transform a complicated problem into a simpler one by introducing additional trainable variables.
Introducing auxiliary variables may pose scalability issues, and these methods require special, usually tailor-made solvers.
Therefore, such methods usually scale to specific use-cases only.

\subsection{Relevance}
End-to-end backpropagation is not compatible with local self-organisation which is one of the key concepts of natural intelligence according to von der Malsburg et al. \sidecite{von_der_Malsburg_Stadelmann_Grewe_2022}.
With end-to-end backpropagation, systems are trained as a single unit and consequently have no independent local units that can organise themselves.
Therefore, alternative learning algorithms are very relevant to this thesis.
In fact, the approach based on vertical self-organisation uses proxy objective functions to train the layer locally as independent units (c.f. Chapter \chref*{methods}).
Proxy objective functions are used in this thesis as this method is the only known alternative to backpropagation of error that scales well to large neural networks and large datasets.
According to \sideciteay{Duan_Principe_2022} and \sideciteay{Bartunov_Santoro_Richards_Marris_Hinton_Lillicrap_2018}, only proxy objective functions have achieved competitive performance with large models on large datasets such as ImageNet \cite{Deng_Dong_Socher_Li_KaiLi_LiFei_Fei_2009}.
The other approaches work only on smaller datasets such as MNIST \cite{Lecun_Bottou_Bengio_Haffner_1998} or CIFAR-10 \cite{krizhevsky2009learning} so far.

\section{Representation Learning}\seclbl{visual_rep_learning}
One of the earliest methods of learning visual representations are \emph{Autoencoders}.
\emph{Autoencoders} have been introduced 1985 \sidecite{rumelhart1985learning} and learn an encoder and a decoder function \sidecite{pmlrv27baldi12a}.
The encoder $A$ maps the input from a high-dimensional space to a lower dimensional embedding space $A: \mathbb{R}^{n} \rightarrow \mathbb{R}^{e}$ and the decoder $B$ reverses this mapping $B: \mathbb{R}^{e} \rightarrow \mathbb{R}^{n}$.
Typically, neural networks are used to learn the encoder function $A$ and decoder function $B$ by minimizing a reconstruction loss \sidecite{Ranzato_Huang_Boureau_LeCun_2007}.
In order that the functions $A$ and $B$ are not just the identity operators, some regularization is needed.
One of the simplest regularization methods is to introduce a bottleneck, i.e. to compress the representation with the encoder while still being able to re-create the original input as good as possible with the decoder.
With a bottleneck layer, the number of neurons is limited.
An alternative (or a supplement) is to limit the number of activations by enforcing sparsity.
Autoencoders with such a sparsity constraint are also known as \emph{Sparse Autoencoders}.
A sparsity constraint can be imposed with $L_1$ regularization or a kullback-leibler (KL) divergence between expected average neuron activation and an ideal distribution \sidecite{10-5555-3042573-3042641}.
Other popular versions of \emph{Autoencoders} are \emph{Denoising Autoencoders} \sidecite{10-1145-1390156-1390294}, \emph{Contractive Autoencoders} \sidecite{10-5555-3104482-3104587}, and \emph{Variational Autoencoders (VAE)} \sidecite{Kingma_Welling_2014}.
In \emph{Denoising Autoencoders}, the input is disrupted by noise (e.g. by adding Gaussian noise or removing some pixels by using Dropout) and fed into the encoder $A$.
The goal of the decoding function $B$ is to reconstruct the clean version of the input without the added noise.
By doing so, the \emph{Denoising Autoencoders} learns to create more robust representation of the input or can be used for error correction.
\emph{Contractive Autoencoders}, on the other hand, try to make the feature extraction less sensitive to small perturbations.
By adding the squared Jacobian norm to the reconstruction loss, the latent representations of the input tend to be more similar to each other.
This diminishes latent representations that are not important for the reconstruction and only important variations between the inputs are kept.
The encoder of \emph{Variational Autoencoders} map the input to a probabilistic distribution; Instead of mapping the input to an encoding vector, the input is mapped to a vector representing the means $\mu$ and another vector representing the standard deviations $\sigma$.
This done by adding two fully connected layers after the encoder $A$, one layer to predict $\mu$ and the other layer to predict $\sigma$.
Afterwards, a variable is sampled from this distribution $z \sim \mathcal{N}(\mu,\,\sigma^{2})$\sidenote{this process is also called the re-parameterization trick} and fed into the decoder $B$.
By doing so, the latent space becomes by design continuous and allows random sampling and interpolation of data.

Other approaches for \emph{self-supervised learning} typically augment the visual scene and either predict the augmentation parameters, reconstruct the original version of the image, or learn consistent representations among augmented views of the image.
For example, some models use masking to learn visual representation;
A part of the input image is removed and the model predicts the missing part (image inpainting) \cite{Elharrouss_Almaadeed_Al-Maadeed_Akbari_2020}.
This not only allows to generate part of images but also to learn visual representations of the image \sidecite{Pathak_Krahenbuhl_Donahue_Darrell_Efros_2016, he2022masked, shi2022adversarial}.
Other approaches rotate images and predict the rotation angle to generate representations \sidecite{komodakis2018unsupervised, Zhai_Oliver_Kolesnikov_Beyer_2019}.
There exist many other methods that augment images and learn good visual representations based on it.
A comprehensive overview is given by \sidecite{chen2022semi}.

Another popular \emph{self-supervised learning} paradigm is \emph{contrastive learning}.
The idea of \emph{contrastive learning} is that similar images should yield similar representations.
Typically, \emph{contrastive learning} models are trained without labels.
In this case, two augmented views of the same image are created, fed through an encoder and the representation of this two views are pushed together in the embedding space by maximizing their similarity \sidecite{chen2020simple, he2020momentum, caron2020unsupervised}.
Subsequently, the encoder can be used to generate image representations that are used for other downstream tasks such as image classification.
However, \emph{contrastive learning} can also leverage information from annotations.
For example, Khosla et al. \sidecite{khosla2020supervised} use labels in a contrastive setting to pull the representations of images of the same class together in the embedding space, while simultaneously pushing apart clusters of samples from different classes.

In this thesis, representations of images are learned.
Two different principles are applied;
vertical self-organisation uses a type of contrastive learning by forcing samples from the same class to have similar representations and samples from different classes to have high diversity.
Horizontal self-organisation uses independently trained variational autoencoders to obtain good representations of input patches.


\section{Meta-Learning}\seclbl{meta_learning}
TODO: NOT SURE IF THIS SECTION IS NEEDED / SHOULD BE KEPT... -> NOT THAT RELEVANT SO FAR

Meta-learning is the process of distilling experience from multiple learning cycles and using the accumulated experience to improve the future learning process \sidecite{Hospedales_Antoniou_Micaelli_Storkey_2021}.
Therefore, meta-learning is also referred to as ``learning-to-learn'' \sidecite{Thrun_Pratt_1998} and improves learning on a lifetime (i.e. single agent) and evolutionary timescale (i.e. population of agents) \sidecite{Schrier_1984}.
Typically, meta-learning includes \emph{base learning} (also referred to as \emph{inner} or \emph{lower} learning) and \emph{meta-learning} (also referred to as \emph{outer} or \emph{upper} learning).
During \emph{base learning}, the model learns a typical task such as image classification based on a dataset and a objective function.
During \emph{meta-learning}, an algorithm updates the \emph{base learning} algorithm based on a meta objective function\sidenote{Typical meta objective function aim to improve generalization or learning speed}.
Thus, meta-learning usually iterates between learning a task and improving the learning algorithm that is used to learn the task.

In the following, I use the categorisation according to Hospedales et al. \cite{Hospedales_Antoniou_Micaelli_Storkey_2021} to describe the different aspects of meta-learning algorithm.
\begin{description}
   \item[Meta-Representation] What meta-knowledge shall be learned by the outer \emph{meta-learning} algorithm.
   \item[Meta-Optimizer] How the outer \emph{meta-learning} algorithm learns, i.e. the from of the learning algorithm.
   \item[Meta-Objective] What the goal of the \emph{meta-learning} algorithm is.
\end{description}

The first dimension of the meta-learning landscape is the \textbf{meta-representation}.
Meta-representations describe which part of the learning strategy should be learned.
One possibility is to learn good \emph{initial parameters} that can be used by the model during \emph{base learning}.
Good initial parameters are only a few gradient steps away from a set of parameters that can solve a task $\mathcal{T}$ drawn from a set of tasks $p(\mathcal{T})$.
A popular algorithm to learn initial parameters is called MAML \sidecite{10-5555-3305381-3305498, 10-5555-3327546-3327622} and works well on smaller networks.
However, a major challenge is that the outer \emph{meta-learning} algorithm has to find a solution for as many parameters as the inner \emph{base learning} needs.
Therefore, many approaches focus on isolating a subset of parameters to meta-learn \cite{lee2018gradient, qiao2018few, rusu2018meta}.
\emph{Black-box models}, on the other hand, use \emph{meta-learning} to directly provide the parameters required to classify data (i.e. the \emph{base learning} algorithm maps a sample directly to class without iterative parameter optimization) \cite{heskes2000empirical, 10-5555-3454287-3455002, Ha_Dai_Le_2016}.
A special case of the black-box models are the approaches based on \emph{metric learning}\sidenote{an explanation why this approach can be considered black-box learning is provided by Hospedales et al. \cite{Hospedales_Antoniou_Micaelli_Storkey_2021}}.
The outer \emph{meta-learning} process optimizes a model to transform inputs into representations that can be used for recognition by similarity comparison (e.g. by using cosine similarity or euclidean distance) \cite{10-5555-3294996-3295163, qiao2018few, Chen_Liu_Kira_Wang_Huang_2020}.

Other approaches learn the \emph{optimizer} of the inner \emph{base learning} algorithm \cite{ravi2016optimization, Li_Malik_2016, Li_Zhou_Chen_Li_2017}.
Typically, such approaches generate each optimization step of the \emph{base learning} algorithm based on the models parameter and a given base objective function.
The trainable component can for example be simple hyper-parameters such as the learning rate \cite{Li_Zhou_Chen_Li_2017} or more sophisticated such as pre-conditioned matrices \cite{Park_Oliva_2020}.
Even the learning algorithm itself can be learned \sidecite{kirsch2021meta}.

The outer \emph{meta-learning} algorithm can also be used to learn the \emph{inner objective function} (while the outer objective function is fixed).
Such loss-learning approaches output a scalar value that is treated as loss by the inner optimizer based on relevant quantities such as prediction/ground truth pairs or model parameters.
Common goals of this approach are to obtain a loss that has less local minima \cite{NEURIPS2018_7876acb6, Sung_Zhang_Xiang_Hospedales_Yang_2017}, provides better generalization \cite{NEURIPS2018_b9a25e42, NEURIPS2019_e0e2b58d, gonzalez2020improved}, leads to more robust models \cite{li2019feature}, or to learn from unlabelled data \cite{NEURIPS2019_6018df18, Boney2018SemiSupervisedFL}.

Other approaches use the outer \emph{meta-learning} loop to learn \emph{network architectures} for the inner \emph{base learning} cycle.
Some approaches use reinforcement learning in the outer loop to learn CNN architectures \cite{zoph2017neural}, while other approaches use evolutionary algorithms to learn the topology of LSTM cells \cite{Bayer_Wierstra_Togelius_Schmidhuber_2009} or to model the network by a graph of network-blocks \cite{10-1609-aaai-v33i01-33014780}.

The outer \emph{meta-learning} loop can also meta-learn hyper-parameters such as regularization-strength \cite{pmlr-v80-franceschi18a, Micaelli_Storkey_2021}, task-relatedness for multi-task learning \cite{10-5555-3305381-3305502}, or sparsity-strength \cite{10-5555-3305381-3305502}. Furthermore, it can meta learn suitable data augmentation methods \cite{Cubuk_2019_CVPR, Li_Hu_Wang_Hospedales_Robertson_Yang_2020}, or improve the selection process for the samples of a mini-batch \cite{10-1609-aaai-v33i01-33015741, fan2018learning}.
Even the dataset itself can be learned with dataset distillation; A rather large datasets can be summarized a smaller dataset with only a few support images \cite{Wang_Zhu_Torralba_Efros_2020, pmlr-v108-lorraine20a} that still allow good generalization on real test images. In sim2real learning \cite{Andrychowicz_Baker_Chociej_JÃ³zefowicz_McGrew_Pachocki_Petron_Plappert_Powell_Ray_2020}, also the graphics engine \cite{ruiz2018learning, Vuong_Vikram_Su_Gao_Christensen_2019} can be trained in an outer loop so that the performance on real-world data is maximized.

The second dimension of the meta-learning landscape is the \textbf{meta-optimizer}.
The meta-optimizer describes how the algorithm in the outer \emph{meta-learning} loop is learned.
Many methods use backpropagation of error and gradient descent (c.f. Section \secref{ann}) to optimize the meta parameters \cite{ravi2016optimization, 10-5555-3305381-3305498, li2019feature, 10-5555-3305381-3305502, pmlr-v80-franceschi18a, Micaelli_Storkey_2021, pmlr-v108-lorraine20a}.
However, this algorithm has some well-known downsides (c.f. Section XXXXXXXXXX) and requires differentiability.
When the inner \emph{base learning} algorithm includes non-differentiable steps \cite{Cubuk_2019_CVPR} or the outer meta objective function is non-differentiable \cite{huang2019addressing}, many methods utilize the RL paradigm to optimize the outer objective \cite{Duan_Schulman_Chen_Bartlett_Sutskever_Abbeel_2016}.
However, using RL to alleviate requirement for differentiability is usually computationally very costly.
Finally, evolutionary algorithms can be used for learning the outer \emph{meta-learning} function \cite{schmidhuber-1987, Stanley_Clune_Lehman_Miikkulainen_2019, Salimans_Ho_Chen_Sidor_Sutskever_2017}.
These algorithms have no differentiability constraints, do not suffer from gradient degradation issues, are highly parallelizable, and can often avoid local minima better than gradient-based methods \cite{Salimans_Ho_Chen_Sidor_Sutskever_2017}.
The downside of evolutionary algorithms is that often a large population size is required (especially if many parameters have to be learned) and the performance is generally inferior to gradient-based methods for large models.
These three learning methods are also used in conventional machine learning.
However, meta-learning comparatively uses RL and evolutionary algorithms more frequently as some components are often non-differentiable in the meta-learning setup.

The third dimension of the meta-learning landscape is the \textbf{meta-objective}.
The meta-objective describes what the goal of the meta-learning algorithm is.
Typically, the performance of a meta-learning algorithm is evaluated with a metric on the inner loop or a meta-metric on the outer loop.
However, there are several design options within the meta-learning framework.
First, the inner base-learning episodes can either take few \cite{ravi2016optimization, 10-5555-3305381-3305498} or many \cite{10-5555-3305381-3305502, 10-5555-3305890-3306069} samples and thus the goal can be to either improve few- or -many-shot performance.
Second, the validation loss of the inner base-learning algorithm can either be calculated at the end of a learning episode to encourage a better \emph{final} performance of the base task or as sum of the loss calculated after each update step to encourage \emph{faster} learning \cite{antoniou2018how}.
Third, the goal of the meta-learning can either be to better solve any task drawn from a set of (often related) tasks (i.e. multi-task setting) \cite{10-5555-3305381-3305498, 10-5555-3294996-3295163, li2019feature}, or to solve one specific task better than when only the inner base learning algorithm is used (i.e. single task setting) \cite{10-5555-3305381-3305502}.
Finally, the meta-optimization can either be done offline (i.e. the inner and outer loop alternate) \cite{10-5555-3305381-3305498} or online (i.e. the meta-learning takes place within a base learning episode) \cite{li2019feature}.


%\section{Correlation within CNNs}

%TODO: Not sure if this chapter is still relevant.... -> move or delete?

%Self-organization in neural networks can be done based on the input data.
%If Hebbian learning is used\sidenote{``Cells that fire together wire together''}, cells are connected based on their correlation (i.e. cells with a high correlation are wired together).
%One way to capture the correlation within CNNs are Gram matrices.
%Gram matrices are essentially the dot-product between the channels of a feature map and can capture the style of given image.
%They are for example used for image style transfer\sidecite{Gatys_Ecker_Bethge_2015} or related fields such as texture synthesis \sidecite{Gatys_Ecker_Bethge_20152}.
%Appendix \chref{image_style_transfer} provides an intuitive explanation what image style transfer is and how it is related to Gram matrices.

%A Gram matrix can be calculated based on the output of a convolutional layer.
%Each filter of a convolutional layer (i.e. each channel) produces a so called convolutional map.
%A convolutional map contains information about the content of the image such as object structure and positioning as well as information about the style.
%Calculating a Gram matrix eliminates content-related information from the convolutional layer output but does not affect style information (c.f. Appendix \chref{image_style_transfer}).
%A Gram matrix calculates the correlations between the convolutional maps (i.e. between the filter responses) of a convolutional layer output.
%For a convolutional filter output $F$ of layer $l$ and two flattened convolutional maps $i$ and $j$ it is defined as \sidecite{7780634}:

%\begin{equation}\eqlbl{Gram_mat}
%		G_{ij}^{l} = \sum_k F^{l}_{ik} \cdot F^{l}_{jk}
%\end{equation}%

%Thereby, $k$ is a hyperparameter defining how many elements of the convolutional output $F$ are compared.
%Gatys et al. \sidecite{7780634} applied this formula the first time to convolutional filters but did not fully explain why it works.
%However, they found that the style is captured well in the correlation between convolutional maps.
%Later, it was shown \sidecite{10555531720773172198} that matching the Gram matrices between two convolutional filters can be reformulated as minimizing the Maximum Mean Discrepancy (MMD) \sidecite{JMLRv13gretton12a} and thus that the style information is intrinsically represented by the distribution of activations in a CNN.

%Intuitively, several filters together can describe the style of the image.
%For example, if one filter reacts to vertical white and black lines and a second filter reacts to horizontal white and black lines and the input image has a checkerboard style, then these two filters have a high correlation, which is reflected in the Gram Matrix (c.f. Appendix \chref{image_style_transfer}).

%When Hebbian learning is used for self-organization, neurons of filters that often trigger together are connected.
%A dataset usually contains specific patterns, which are represented in the Gram Matrix\sidenote{In the following the term pattern is used instead of style, because the Gram matrix can represent not only styles like photorealistic images, drawings, etc., but any non-content related information.
%For example, for an animal dataset, one filter could have high activation on white color, a second filter on black vertical lines, and a third filter on black dots.
%For a photo of a zebra, the first and second filters would have a high correlation while for a dalmatian, the first and third filters would have a high correlation}.
%Thus, neurons are connected that alone represent a certain filter but together represent a certain more complex pattern.


%\section{Rotation Invariant Convolutions}\seclbl{rotation_invariant_conv}

%TODO: Not sure if this chapter is still relevant.... -> move or delete?

%In this thesis, I show the capability of self-organization to deal with object transformations.
%Common transformations of objects in visual scenes are translations, rotations, zooming, and deformations.
%While most state-of-the-art architectures are translation invariant\sidenote{STOA architectures for visual scene interpretation are mainly based on convolutional neural networks and vision transformers \cite{Dosovitskiy_Beyer_Kolesnikov_Weissenborn_Zhai_Unterthiner_Dehghani_Minderer_Heigold_Gelly_2021}. These architectures are by definition translation invariant.}, they suffer from the other aforementioned transformations.
%The model can be trained to be more robust against most kind of transformations through data augmentation \sidecite{Simard_Steinkraus_Platt_2003}.
%For example, dynamically increasing and decreasing image sizes works very well to obtain robustness against different object sizes.
%Robustness against deformations, on the other hand, can be learned to a great extend through data pre-processing such as random grid-based deformations \sidecite{Ronneberger_Fischer_Brox_2015, Sager_Salzmann_Burn_Stadelmann_2022}.
%However, deformations are often domain specific.
%In this thesis, I mainly focus on rotation invariance of visual perception systems as this is a challenging task and is for many applications the most important transformation invariance beyond translation.
%Although transformation invariant models can be obtain with data augmentation \cite{Simard_Steinkraus_Platt_2003, Fasel_Gatica_Perez_2006}, this approach is considered inefficient as the model has to learn many redundant parameters, independent for each rotation angle.
%In the following, we focus on methods that overcome this limitation.

%A simple approach to achieve rotation invariance is to find the main axis of an image patch and to rotate it until it is aligned with the samples from the training set \sidecite{Jafari_Khouzani_Soltanian_Zadeh_2005}.
%Another common strategy is to define features that are rotation invariant or equivariant, i.e. to use features whose output is either not affected by rotating the input image or whose output is rotated the same way as the input image by definition.
%Some well-known approaches are Local Binary Patterns \sidecite{Ojala_Pietikainen_Maenpaa_2002}, spiral resampling \sidecite{Wen_RongWu_ShiehChungWei_1996}, and steerable pyramid filters \sidecite{greenspan1994rotation}.

%Other approaches learn rotatable filters from the input data.
%Dieleman et al. \sidecite{Dieleman_DeFauw_Kavukcuoglu_2016} propose four new neural networks blocks.
%The probably most important block proposed in their work is a pooling operation that is applied over rotated feature maps to  reduce the number of parameters and to learn rotation invariance more explicitly.
%Another approach \sidecite{Laptev_Savinov_Buhmann_Pollefeys_2016} also applies convolutional filters to rotated versions of the image but aggregates the result by taking the maximum activation over the feature maps as output.

%Another category of approaches apply rotations to learned convolutional filters.
%Earlier approaches \sidecite{Schmidt_Roth_2012, Kivinen_Williams_2011, Sohn_Lee_2012} use a Convolutional Restricted Boltzmann Machine (C-RBM)\sidenote{A C-RBM is a generative stochastic network that can learn a probability distribution over its inputs. Multiple layers of C-RBM are also known as deep belief networks.} \sidecite{Lee_Grosse_Ranganath_Ng_2009} to tie the weights.
%Besides using C-RBM, it is also possible to tie the weights within several layers of a CNN to enforce rotation invariance and to reduce the number of parameters to learn. 
%Teney and Herbert \sidecite{Teney_Hebert_2016} split the filters of a CNN in orientation groups and constraint their weights.
%Such models achieve rotation covariance\sidenote{rotation covariance means that applying a rotation to the input image results in a shift of the output across the features} and only need to learn a single canonical filter per orientation group.
%This concept can also be applied to the rotation group in the final layers of a CNN to obtain invariance to global rotations \sidecite{Wu_Hu_Kong_2015}.

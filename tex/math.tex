

\section{Notation}

Depending on the context, a variable can be a scalar value, a vector, or a matrix. The following formatting is used:

\begin{tabular}{ p{5cm} p{2cm} p{7cm} }
	\textbf{Formatting} & \textbf{Example} & \textbf{Meaning}\\
	\hline
  	No formatting, lower case & $a$ & A scalar value\\
  	Bold, lower case & $\boldsymbol{a}$ & A vector\\
  	Bold, upper case & $\boldsymbol{A}$ & A matrix\\
  	Curved brackets with a point & $a(\cdot)$ & A function, where $(\cdot)$ is a placeholder for a variable\\
  	Rectangular brackets    & $a[t]$ & Depends on context, either variable $a$ at time $t$\\
							& $\boldsymbol{x}[c]$ & or variable $\boldsymbol{x}$ is from class $c$\\
  	Superscript rectangular brackets & $a^{[l]}$ & $l$ is the index of the layer, e.g. $\boldsymbol{W}^{[l]}$ is the weight matrix of layer $l$\\
  	Superscript curved brackets & $a^{(i)}$ & $i$ is the index of the sample, e.g. $\boldsymbol{x}^{(i)}$ is the$i$-th sample of a data set\\
\end{tabular}


\section{Variables}



\begin{tabular}{ p{3cm} p{11cm} }
	\textbf{Variable} & \textbf{Meaning}\\
	\hline
	$\beta$ & The weight factor of the Kullback-Leibler term in the loss of a variational autoencoder\\
	$\lambda$ & The weight factor for loss terms\\
	$\eta$ & The learning rate of an optimization algorithm\\
	$\rho$ & The desired activation probability\\
	$\hat{\rho}_i$ & The activation probability of a neuron $i$\\
	$\psi$ & An estimate of the synaptic activity, used for Hebbian learning (i.e. an estimate of $a$)\\
	$\theta$ & A threshold value that was used instead of the bias $b$ in early versions of neural networks or as goodness threshold in the forward-forward network\\
	$a$ & The output of an activation function of a single neuron\\
	$\boldsymbol{a}$ & The output of an activation function in an intermediate layer of a multi-layer network, the output of the model is typically denoted as $\boldsymbol{\hat{y}}$\\
	$b$ & The bias of a single neuron\\
	$\boldsymbol{b}$ & The bias of a network layer\\
	$C$ & The set of classes of a classification task\\
	$\boldsymbol{h}$ & The latent representations of a network that are typically used in a subsequent downstream task\\  % OK
	$k$ & The number of neurons within a layer\\
	$L$ & The number of layers of a network\\
	$l$ & The index of a layer, e.g. $\boldsymbol{W}^{[l]}$ is the weight matrix of layer $l$\\
	$m$ & The number of input samples within a (mini-)batch\\
	    & or the state of the memory in context of Hoppfield networks\\
\end{tabular}


\begin{tabular}{ p{3cm} p{11cm} }
	\textbf{Variable} & \textbf{Meaning}\\
	\hline
	$n$ & The length (size) of a vector, for example, an input sample is defined as $\boldsymbol{x} = (x_1, ..., x_n)$\\
	$w_{ij}$ & The weight between neuron $i$ and neuron $j$\\
	$\boldsymbol{w} = (w_1, ..., w_n)$ & A weight vector of a neuron\\
	$\boldsymbol{W} = (\boldsymbol{w}_1, ..., \boldsymbol{w}_k)$ & A weight matrix of a network layer\\
	$\boldsymbol{X} = \boldsymbol{x}^{(1)}, ..., \boldsymbol{x}^{(m)}$ & A dataset with $m$ samples\\
	$\boldsymbol{x} = (x_1, ..., x_n)$ & An input sample that is fed into a model or a network layer, typically a vector of length $n$\\
	$\boldsymbol{y}$ & The expected output of a model or a network layer (i.e. the ground truth), typically a vector ($\boldsymbol{y}$) for a multi-class classification task or a scalar value ($y$) for a regression or single-class classification task\\
	$\boldsymbol{\hat{y}}$ & The actual output of a model or a network layer (i.e. the prediction), typically a vector ($\boldsymbol{\hat{y}}$) for a multi-class classification task or a scalar value ($\hat{y}$) for a regression or single-class classification task (in a multi-layer network, the output of an intermediate layer is typically denoted as $\boldsymbol{a}$)\\
	$z$ & The output of the aggregation function $g(\cdot)$ for a single neuron\\
	$\boldsymbol{z}$ & The output of the aggregation function $g(\cdot)$ for a network layer\\
\end{tabular}


\section{Functions}

\begin{tabular}{ p{3cm} p{11cm} }
	\textbf{Function} & \textbf{Meaning}\\
	\hline
	$\mathcal{L}(\cdot)$ & Loss function to calculate the goodness of the model's prediction\\
	$D(\cdot)$ & A decoder (multiple network layers) to transform a latent representation to an output\\
	$d(\cdot)$ & A decoder for a single layer to approximate the inverse of a layer $h^{-1}$\\
	$E(\cdot)$ & An encoder (multiple network layers) to transform an input to a latent representation\\
	$f(\cdot)$ & An activation function such as $\sigma(\cdot)$, $(\cdot)^{+}$, or $\tanh(\cdot)$ (c.f. \eqref{act_functions})\\
	$g(\cdot)$ & The aggregation function that calculates the scalar value that is fed into the activation function $f(\cdot)$ (c.f. \eqref{McCulloch_Pitts_agg}, \eqref{Perceptron_agg}, \eqref{nn})\\
	$h_l(\cdot)$ & The function implemented by a layer $l$\\
	$G(\cdot)$ & A function to update the memory of Hopfield networks\\
	$O(\cdot)$ & An output function to calculate the output of a Hopfield network based on the input and the memory\\
	$I(\cdot)$ & The energy function of a Hopfield network\\
	$\sigma(\cdot)$ & The sigmoid activation function (c.f. \eqref{act_functions})\\
	$(\cdot)^{+}$ & The ReLU activation function (c.f. \eqref{act_functions})\\
	 $\tanh(\cdot)$ & The hyperbolic tangent activation function (c.f. \eqref{act_functions})\\
	
\end{tabular}

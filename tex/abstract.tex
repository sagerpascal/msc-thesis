In the past decade, deep learning has established itself as state-of-the-art technology in various automatic image analysis tasks.
Despite impressive results, this technology has several limitations, notably its limited robustness to noise, constrained transformation invariance during object recognition and reliance on a substantial amount of training data.
Conversely, the human does not suffer from these limitations due to its non-sequential processing of extracted image features and its ability to perceive visual scenes holistically, i.e. interpret it as more than the sum of its part, as outlined by the Gestalt psychology.
Furthermore, the brain establishes consistency at every point in the network through self-organisation and localised learning processes, i.e. each cell is able to predict the activity of neighbouring cells, and a consensus between the feature representations is achieved through mutual cell support. This mechanism solves the problem of ``early commitment'' that is inherent present in deep networks, due to the fact neural networks rely on a global error correction algorithm to establish consistency at a single point between prediction and teaching signal.

Building upon these insights, this thesis proposes a novel image-processing framework inspired by the functioning of the human brain.
Accordingly, a significant part of this thesis is devoted to identifying and interpreting neuroscientific findings.
These findings are analysed and translated into a computational framework, assigning distinct roles to each framework component and clarifying their congruence with biological learning.

The framework consists of three components: The sensor system \emph{S0},  responsible for extracting low-level features from the images; the feature-building stage \emph{S1}, which uses lateral (intra-layer) connections to form neuron groups, so-called net fragments,  fostering mutual support to stabilise known patterns; the prototype stage \emph{S2}, which maps the formed net fragments to object prototypes using projection fibres and provides feedback to \emph{S1}.
This iterative projection process lasts until consistency is achieved at every point in the network, i.e. until cells and synapses have reached a stable attractor state.

While prior research has demonstrated the efficiency of projection fibres, implementing net fragments still needs to be explored.
Consequently, this thesis analyses the implementation of this component in detail and discusses it by conducting experiments with a simple dataset based on straight lines.
The experimental findings show that lateral connections trained with Hebbian learning can effectively facilitate cell support.
The network exhibits significant robustness using cell support and can deactivate up to $91.7\%$ of unwanted cell activity triggered by noise signals. Furthermore, lateral support can restore discontinuous lines, demonstrating the network's ability to deal with occluded objects. With a range of lateral connections of $11$ pixels, interruptions of up to $8$ pixels can be reconstructed, and with additional feedback from \emph{S2}, even interruptions of up to $20$ pixels can be restored. The proposed framework has the potential to address several weaknesses of conventional neural networks and presents a promising alternative to current image processing algorithms.
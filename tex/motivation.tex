%% motivation.tex
Despite the fact that Deep Learning has achieved incredible performance on a variety of tasks it is still questionable whether the current methodology is enough to achieve real (or at least more advanced) intelligence (c.f. Section \secref*{limitationsDL}).
Many of the current limitations are tackled by approaches from the field of neurocomputing (c.f. Section \secref*{neurocomputing}).
Although many of the methods cannot yet compete with current DL models or work only on small data sets, these methods have shown promising results.
The neurocomputing methods are inspired by neuroscience (the study of the nervous system including the brain, spinal cord, and peripheral nervous system).
Neuroscience provides a source of inspiration for AI algorithms, independent of mathematical models.
Recently, von der Malsburg et al. \sidecite{von_der_Malsburg_Stadelmann_Grewe_2022} have published their hypothesis about the key building blocks for a system with general intelligence.
They point out that building self-organizing sub-networks during training may be one of the keys to fundamentally improve current systems.
Lehmann \sidecite{lehmann} worked on incorporating a few of these ideas and principles into a DL framework.
He proposes a laterally connected layer (LCL), a layer that forms lateral intra-layer connections based on Hebbian learning.
These connections are formed during training based on the input.
He shows that the LCL layer increases robustness on the popular MNIST dataset.
However, he also points out potential for improvement of the LCL layers.

Von der Malsburg et al. \cite{von_der_Malsburg_Stadelmann_Grewe_2022} propose several concepts how current problems in ANNs can be tackled.
They present systematically sound theoretical foundations, but neither the formulation of a mathematical model nor the translation of their ideas into a concrete application were in the scope of their work.
Specifically, we do the following...

TODO...




















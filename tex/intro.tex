%% intro.tex
Humanity has always tried to simplify its life through technological progress.
For example, the computer has influenced and changed every field imaginable in the last decades. 
In fact, it has been so successful that it has spawned its own scientific discipline, computer science.
Nowadays, it is impossible to imagine life without computers: Almost every household and company owns at least one.
Computers facilitate various tasks, from communication to the acquisition of knowledge.
For all these tasks, software developers have created appropriate programs.
Software development is the process of writing a script with a programming language to specify how the computer should behave for a given input.
Simply put: A software program tells the computer what to do if the user enters a command.
Writing software can be done easily if the tasks are clearly defined and can be described precisely.
However, there exist tasks that are almost impossible to program, such as writing a script that detects cats in images.
The problem is that we cannot describe to a computer how a cat looks precisely\sidenote{at least not on a pixel-level basis so that one can compare a given image with a precise description}.

Therefore, scientists came up with the idea to not just program such tasks but to let the computer learn them.
Machine learning (ML) algorithms can learn and adapt without following explicit instructions such as program code.
Instead, they use statistical models to analyse data, find patterns, and make predictions.
Machine learning has become an indispensable part of our everyday lives.
For example, we use it for machine translation, transport and logistics organisation, product recommendations, fraud detection, self-driving cars, unlocking smartphones, improving video games, speech recognition, and much more.
A sub-branch of machine learning is deep learning (DL) which achieved awe-inspiring results in the last decade and is considered \emph{the} state-of-the-art technology for many of the aforementioned tasks.

Even though this technology works very well for many tasks, such models have some crucial flaws by definition (c.f. \secref{limitationsDL}), which cannot be resolved in the current DL framework.
One of the godfathers of deep learning is Turing Award\sidenote{the Turing Award is recognised as the highest academic award in computer science and sometimes also called the ``Nobel Prize of Computing''} winner Geoffrey Hinton. 
Especially his contribution to learning with backpropagation of error (c.f. \secref{ann}) has created the foundation for modern deep learning systems.
More than 30 years later, Hinton says he is ``deeply suspicious'' about end-to-end backpropagation of error. In his opinion, we have to ``throw it all away and start again'' to improve current systems fundamentally \sidecite{axios_hinton}.
This seems rather extreme, considering what DL systems have achieved.
However, it also shows that the learning algorithm of such systems has serious flaws.

Some of the most critical limitations of deep learning systems are listed in \secref{limitationsDL}.
This thesis addresses these problems by exploring different architectures inspired by findings from neuroscience\sidenote{the field of neuroscience studies how the human nervous system and especially the brain works}.
The inspiration is drawn from neuroscience, as the human brain is a learning system that does not have these problems. Thus, incorporating findings from neuroscience into the deep learning framework might help overcome some of the current limitations.
In this thesis, the ``Theory of Natural Intelligence'' by von der Malsburg et al. \sidecite{von_der_Malsburg_Stadelmann_Grewe_2022} in particular is used as inspirational source.
The aforementioned theory focuses on visual processing in the human brain, and consequently, deep learning architectures for image processing are examined in this thesis.
Thereby, the main focus of this thesis is on \emph{finding new principles} of deep learning architectures rather than on achieving state-of-the-art performance in terms of accuracy or f-score on existing benchmarking datasets.

The thesis differs from neurocomputing\sidenote{neurocomputing is a sub-field of neuroscience that deals with the implementation of learning algorithms with a high degree of biological plausibility} in the sense that the architectures investigated are still closely related to the DL framework and, for example, do not use time-dynamic signals. 
For the completeness of this thesis, an overview of neurocomputing is given in \secref{neurocomputing}.
However, since these neurocomputing algorithms have not (yet) become established in practical applications, they are not investigated further.
Thus, the thesis is strongly oriented along the field of deep learning and focuses on incorporating neuroscience findings in the deep learning framework, while natural plausibility is considered less important.

\section{Contribution}
This thesis makes the following contributions:
\begin{itemize}
	\item First, the basics of deep learning and neurocomputing are described in detail. Together with the related work, this provides a survey of the most important research dealing with alternative learning algorithms.
	\item Second, promising concepts from the neuroscience literature that may help to build more intelligent systems are identified, and suggestions are made on integrating these concepts into modern DL frameworks.
	\item Third, two concrete DL architectures that use such neuroscience-inspired concepts are presented, and their strengths and weaknesses are analysed in detail.
	\item Fourth, this work provides an intuition of which concepts currently little used in DL settings seem promising and recommends future research directions towards more general artificial intelligence.
\end{itemize}


\section{Organisation of Thesis}\seclbl{org_thesis}
The remainder of the thesis is organised as follows: In \chref{fundamentals}, the fundamentals of deep learning and neurocomputing are explained in detail.
Experts may skip this chapter; especially the second section on neurocomputing can be omitted, as it is not fundamental for understanding this thesis but only intended for interested readers or those who want to get a better overview of the field.
\chref{rel_work} introduces the related work. Next, \chref{neuro_concepts} identifies promising findings from neuroscience that might help to improve current deep learning systems. Furthermore, it proposes how these neuroscientific concepts could be implemented in a deep learning setting.
In \chref{horizontal_self_org} and \chref*{vertical_self_org}, two different architectures incorporating these concepts are presented:
\chref{horizontal_self_org} describes a network based on horizontal self-organisation. The proposed model trains each layer in parallel using proxy objective functions and can still extract hierarchical features.
The model described in \chref{vertical_self_org} uses vertical self-organisation by dividing the data into patches and analysing them on independent but communicating sub-models.
Finally, in \chref{future_work}, future work is described, and some insights and intuitions of the author are given about which neuroscience concepts seem promising and which do not, and what the next steps for developing more intelligent systems could be.

\section{TODO}
Feedback Thilo: weniger Small-Scale in Thesis -> mehr Meta-Level in Introduction / eigene Theorie, evtl. Fundamentals -> mehr Neuroscience (1000 Brains, Capsule Networks, Christoph,Geoffrey Generation von Rules)

\section{Alternative Learning Approaches}
TODO: This does not belong here, move it to a suitable position in the text.
Mögliches Framing: Idee von Christoph in Introduction (3-staged model). Alternativen (dieses Kapitel) als erster Abschnitt in Fundamentals. Danach deep learning und neurocomputing ebenfalls in Fundamentals als die Building Blocks solcher Systeme.

Das Ziel von AI research ist die Entwicklung einer Technologie, die uns Menschen in verschiedensten Aufgaben unterstützen kann. Zurzeit sind viele der existierenden Systeme auf einen oder wenige sehr ähnliche Tasks limitiert. Zudem gibt es Tasks welche bisher mit bestehenden Methoden nicht gelöst werden konnten (zitiere Arc). Nachfolgend werden einige alternativen Ansätze für deep networks beschrieben.

Ein aktueller Trend im Bereich deep learning ist das Training von Foundation Models (zitiere GPT-3, GPT-4) und das anschliessende fine-tuning auf spezifische Tasks (zitiere HFRL). Einige dieser Applikationen wie ChatGPT (zitiere) wirken sehr intelligent, leiden aber unter den typischen drawbacks von reinem statistischem Lernen (c.f. Section). Viel research wird unternommen, um diese well-known problems zu reduzieren, beispielsweise das Reduzieren von hallucination (Zitate), das  vortlaufende Lernen (zitiere online learning), transfer learning (zitate), oder meta-lerarning (zitate). Das darunterliegende Framework bleibt aber identisch: Daten werden mit generierten oder extrahierten Labels statistisch matched und wodurch Probleme wie fehlende robustness, generalization, data efficiency, causality understaning oder common sense reasoning nicht oder nur schwer lösbar sind (sources).

Viele Ansätze zur Schaffung der next-generation AI systems nutzen das menschliche Gehirn als Inspirationsquelle. Die Furschungsgruppe von Jeff Hawking geht einen Schritt weiter und nutzen das Gehirn nicht nur als Quelle für Ideen sondern bauen ein System das, gemäss ihrer Inspiration, den Lernalgorithmus des Gehirns implementiert. Ihre Theorie basiert auf dem Proposal von Vernon Mountcastle das besagt, dass der neocortex aus vielen cortical columns besteht welche alle eine similar architecture haben und similar functions ausführen (Zitiere 1000 Brains Buch Seite 248). Deshalb argumentieren sie, dass das Gehirn nicht wie deep learning systeme ein Modell bilden sondern many models of each object (Zitiere die Papers). Every model is constructed using diverse inputs, be it from slightly distinct parts of the sensor or from entirely different sensors. These models collaborate and vote collectively to establish a consensus regarding their sensory observations. Deshalb wird diese Theorie auch 1000 brains theroy genannt, da viele Modelle parallel arbeiten. Dies ist strongly related to ensemble approaches in deep learning (cite), wobei ähnlich wie bei random forrest (cite) jedem Modell nur ein Teil der Daten zugägnlich gemacht wird. Aktuell arbeitet die Firschungsgruppe von Jeff Hawkings an der Ausarbeitung und Implementierung dieser Theorie. Aktuelle Ergebnisse sind schwer interpretierbar, da sie ihr Modell auf denselben Daten trainiert und getestet haben\sidenote{bei einem Lernsystem mit genug Kapazität können so die Trainingsdaten overfitted werden, was zu sehr guten Resultaten auf diesen Daten führt}. Das Modell kann zurzeit einige hundert Objekte erkennen und ist robust gegenüber noise. Die robustness war aufgrund der vielen decoupled models intuitiv zu erwarten. Ein aktuelles Problem ist, dass die Kapazität des Netzwerks stark limitiert ist durch die Anzahl input signals und Anzahl outputs. Rein durch das Hinzufügen von weiteren columns (erweiterung der Kapazität in hidden layers) skaliert das Modell nicht zu einem vielfachen an erkennbaren Objekten.

Aus einigen key components dieser theory hat sich auch eine zweite Art von Netzwerken entwickelt\sidenote{die treibende Kraft dahinter, Dileep George, arbeitete end mit Jeff Hawkings zusammen und war massgeblich an der Entwicklung von core-componetes der 1000 brains theory beteiligt}, das Recursive Cortical Network (RCN). It is based on the insight that human vision treats shape and appearance different and a known object with an unexpected colour is still easily recognisable.
The RCN uses separate mechanisms for dealing with contours and appearances. Furthermore, they incorporates many neuroscientific principles such as lateral connections for contour consistency.
A major advantage is that it is unsupervised, able to learn from very few examples, and has one-shot and few-shot learning capabilities. Das Prinzip scheint sehr vielversprechend, jedoch skaliert es (in der aktuellen Form) nicht zu natürlichen Bildern. Das Erstellen von Konturhierarchien scheint nur möglich, wenn das Objekt deutlich vom Hintergrund getrennt ist. Folglich hat dieses Netzwerk erst auf MNIST und text-based CAPTCHAs funktioniert.

Andere approaches attempt to more closely mimic biological neural organization. Capsule neural network (CapsNet) modelieren explizit hierarchical relationships. Diese Netzwerke gruppiert Neuronen in "capsules" und jede capsule  represents a property, such as position, size, orientation, deformation, texture, colour, or movement. Der output from several of those capsules form more stable representations for higher capsules. Dynamic routing is used to find an agreement between bottom up activations and top down concepts ``generated'' by the available evidence over multiple iterations. CapsNets are still an active area of research, and their widespread adoption and application in real-world scenarios. Besonders das dynamic routing macht den Algorithmus langsam und bisher hat dieser approach nur auf sehr kleinen dataset wie MNIST oder CIFAR-10 funktioniert. Damit dieser Algorithmus gut skaliert, müssen spatial information (information about relative relationships between features) robuster gebildet werden können. Vorteile sind hingegen robustness to affine transformations and ability to segment with overlapping images.

Eine weitere häufig verwendetes Prinzip, das die Inspiration aus dem biologischen Kontext hat, ist die Kombination von representations mit actions. It is known from the study of animals that both eye movements and the behavioural state influence the responses of neurons in the visual cortex \sidecite{Keller_Bonhoeffer_Hübener_2012}.
Thus, animals integrate their action (i.e., their movement) with currently incoming sensory signals to predict future sensory inputs.
Keurti et al. \sidecite{Keurti_Pan_Besserve_Grewe_Schölkopf_2022} argue that efference copies\sidenote{The internal copy of an outflowing movement-producing signal generated by an organism's motor system is also known as efference copy.} are helpful to learn useful latent representations perceived by the visual system.
They enforce that transformations of the real world can also be applied to latent representations, i.e. that the mental objects and the real-world objects remain consistent when similar transformations are applied to them.
Allowing an agent to interact with the world to better understand it and to create better representations seems not only important from a neuroscientific perspective, but is also in line with theories from psychology.
Piaget \sidecite{Piaget_1964} argues that perceiving an object is rather about understanding how it transforms and behaves and not creating a mental copy of it.
Die Kombination von perception mit actions für autonomous machine intelligence wird auch postulated by LeCun \sidecite{LeCun_AMI}. Er schlägt das Erstellen eines world-models vor, dessen Zustand basierend auf actions vorhergesagt werden kann. Dabei hebt er die Wichtigkeit der Nutzung vonself-supervised learning hervor, beispielsweise in Kombination mit energy functions hervor.
Die Idee, dass gute representations actionable sind, scheint vielversprechend. Aktuelle Bilddatensätze bestehen aber hauptsächlich aus 2D Bildern und nicht aus 3D Simulationen und erlauben deshalb keine actions by default. Actionable representations scheinen in einigen Bereichen wie bei der Entwicklung von autonomous agents with embodiment eine sinnvolle Idee zu sein. Computer sowie elektronische Bildsensoren (cameras) haben aber weder 3D Auflösung noch ein embodiment. Deshalb scheint fraglich, ob solche Systeme für den häufigsten use-case der Bildanalyse, nämlich für 2D Bilder oder Videos, überhaupt geeignet ist. Die Ausnutzung des sequentiellen Kontexts scheint aber äusserst hilfreich und wäre beispielsweise auch mit Videos und ohne actions möglich.

Die Nutzung von energy functions scheint zum Erlernen von generelleren und robusteren AI systems vielversprechend. In the context of active inference (source), perception emerges as the process of minimizing variational free energy concerning beliefs about hidden variables. Dies erlaubt planning und reasoning durchzuführen. Beispielsweise kann dadurch ein Netzwerk einen generativen process $R(r,o)$ (when it rains $(s)$, the street gets wet $(o)$), durch prior beliefs ($p(s)$) und der beobachtete hidden states ($p(o|s)$) die probability von verschiedensten hidden states vohersagen (die Wahrscheinlichkeit, dass die Strasse wegen dem Regen Nass ist $p(s|o)$). Dieses Prinzip wird auch von anderen bekannten Forschungsgruppen wie derjenigen von Yoshua Bengio aktiv verfolgt. Ihre GFlowNets (Generative Flow Networks) erlauben to disentangle both the explanatory causal factors and the mechanisms that relate them. Dies erlaubt to deal with out-of-distribution generalization in a more rational way. Viele dieser in diesem Paragraph beschriebenen Technologien scheinen auf den ersten Blick nicht sehr relevant für das Extrahieren von Bildrepräsentationen. Eine detailierte Betrachtung zeigt aber, dass energy functions genutzt werden könnten um gegebene Bildvektoren mit Prototypvektoren abzugleichen und gleichzeitig die certainity zu bestimmen. Folglich könnte eine entsprechende Adaptierung dieses Frameworks auch für pattern recognition Vorteile bringen.


TODO lese: https://arxiv.org/pdf/2207.04630.pdf



---
TODO: Wenn in einem späteren Kapitel probablistic neuron motiviert wird: https://cerenaut.ai/2014/12/24/sparse-distributed-representations-sdrs/
 






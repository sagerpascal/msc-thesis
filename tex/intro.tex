%% intro.tex
Humanity has always tried to simplify its life through technological progress.
For example, the computer has influenced and changed every field imaginable in the last decades. 
In fact, it has been so successful that it has spawned its own scientific discipline, computer science.
Nowadays, it is impossible to imagine life without computers: Almost every household and company owns at least one.
Computers facilitate various tasks, from communication to the acquisition of knowledge.
For all these tasks, software developers have created appropriate programs.
Software development is the process of writing a script with a programming language to specify how the computer should behave for a given input.
Simply put: A software program tells the computer what to do if the user enters a command.
Writing software can be done easily if the tasks are clearly defined and can be described precisely.
However, there exist tasks that are almost impossible to program, such as writing a script that detects cats in images.
The problem is that we cannot describe to a computer how a cat looks precisely\sidenote{at least not on a pixel-level basis so that one can compare a given image with a precise description}.

Therefore, scientists came up with the idea to not just program such tasks but to let the computer learn them.
Machine learning (ML) algorithms can learn and adapt without following explicit instructions such as program code.
Instead, they use statistical models to analyse data, find patterns, and make predictions.
Machine learning has become an indispensable part of our everyday lives.
For example, we use it for machine translation, transport and logistics organisation, product recommendations, fraud detection, self-driving cars, unlocking smartphones, improving video games, speech recognition, and much more.
A sub-branch of machine learning is deep learning (DL) which achieved awe-inspiring results in the last decade and is considered \emph{the} state-of-the-art technology for many of the aforementioned tasks.

Even though this technology works very well for many tasks, such models have some crucial flaws by definition (c.f. \secref{limitationsDL}), which cannot be resolved in the current DL framework.
One of the godfathers of deep learning is Turing Award\sidenote{the Turing Award is recognised as the highest academic award in computer science and sometimes also called the ``Nobel Prize of Computing''} winner Geoffrey Hinton. 
Especially his contribution to learning with backpropagation of error (c.f. \secref{ann}) has created the foundation for modern deep learning systems.
More than 30 years later, Hinton says he is ``deeply suspicious'' about end-to-end backpropagation of error. In his opinion, we have to ``throw it all away and start again'' to improve current systems fundamentally \sidecite{axios_hinton}.
This seems rather extreme, considering what DL systems have achieved.
However, it also shows that the learning algorithm of such systems has serious flaws.

Some of the most critical limitations of deep learning systems are listed in \secref{limitationsDL}.
This thesis addresses these problems by exploring different architectures inspired by findings from neuroscience\sidenote{the field of neuroscience studies how the human nervous system and especially the brain works}.
The inspiration is drawn from neuroscience, as the human brain is a learning system that does not have these problems. Thus, incorporating findings from neuroscience into the deep learning framework might help overcome some of the current limitations.
In this thesis, the ``Theory of Natural Intelligence'' by von der Malsburg et al. \sidecite{von_der_Malsburg_Stadelmann_Grewe_2022} in particular is used as inspirational source.
The aforementioned theory focuses on visual processing in the human brain, and consequently, deep learning architectures for image processing are examined in this thesis.
Thereby, the main focus of this thesis is on \emph{finding new principles} of deep learning architectures rather than on achieving state-of-the-art performance in terms of accuracy or f-score on existing benchmarking datasets.

The thesis differs from neurocomputing\sidenote{neurocomputing is a sub-field of neuroscience that deals with the implementation of learning algorithms with a high degree of biological plausibility} in the sense that the architectures investigated are still closely related to the DL framework and, for example, do not use time-dynamic signals. 
For the completeness of this thesis, an overview of neurocomputing is given in \secref{neurocomputing}.
However, since these neurocomputing algorithms have not (yet) become established in practical applications, they are not investigated further.
Thus, the thesis is strongly oriented along the field of deep learning and focuses on incorporating neuroscience findings in the deep learning framework, while natural plausibility is considered less important.

\section{Contribution}
This thesis makes the following contributions:
\begin{itemize}
	\item First, the basics of deep learning and neurocomputing are described in detail. Together with the related work, this provides a survey of the most important research dealing with alternative learning algorithms.
	\item Second, promising concepts from the neuroscience literature that may help to build more intelligent systems are identified, and suggestions are made on integrating these concepts into modern DL frameworks.
	\item Third, two concrete DL architectures that use such neuroscience-inspired concepts are presented, and their strengths and weaknesses are analysed in detail.
	\item Fourth, this work provides an intuition of which concepts currently little used in DL settings seem promising and recommends future research directions towards more general artificial intelligence.
\end{itemize}


\section{Organisation of Thesis}\seclbl{org_thesis}
The remainder of the thesis is organised as follows: In \chref{fundamentals}, the fundamentals of deep learning and neurocomputing are explained in detail.
Experts may skip this chapter; especially the second section on neurocomputing can be omitted, as it is not fundamental for understanding this thesis but only intended for interested readers or those who want to get a better overview of the field.
\chref{rel_work} introduces the related work. Next, \chref{neuro_concepts} identifies promising findings from neuroscience that might help to improve current deep learning systems. Furthermore, it proposes how these neuroscientific concepts could be implemented in a deep learning setting.
In \chref{horizontal_self_org} and \chref*{vertical_self_org}, two different architectures incorporating these concepts are presented:
\chref{horizontal_self_org} describes a network based on horizontal self-organisation. The proposed model trains each layer in parallel using proxy objective functions and can still extract hierarchical features.
The model described in \chref{vertical_self_org} uses vertical self-organisation by dividing the data into patches and analysing them on independent but communicating sub-models.
Finally, in \chref{future_work}, future work is described, and some insights and intuitions of the author are given about which neuroscience concepts seem promising and which do not, and what the next steps for developing more intelligent systems could be.

\section{TODO}
Feedback Thilo: weniger Small-Scale in Thesis -> mehr Meta-Level in Introduction / eigene Theorie, evtl. Fundamentals -> mehr Neuroscience (1000 Brains, Capsule Networks, Christoph,Geoffrey Generation von Rules)

\section{Alternative Learning Approaches}
TODO: This does not belong here, move it to a suitable position in the text.
Mögliches Framing: Idee von Christoph in Introduction (3-staged model). Alternativen (dieses Kapitel) als erster Abschnitt in Fundamentals. Danach deep learning und neurocomputing ebenfalls in Fundamentals als die Building Blocks solcher Systeme.

AI research aims to develop a technology that can support humans in various tasks. However, current systems have many drawbacks and are limited to a few closely related tasks. Arguably, the only existing systems that, in non-experts' eyes, behave intelligently\sidenote{which does not mean that such systems exhibit intelligence} are foundation models such as large language models (LLMs) (cite GPT-3, GPT-4) and the subsequent fine-tuning to specific tasks (cite HFRL). However, like other deep learning models, also LLMs suffer from the typical drawbacks of statistical learning (c.f. Section). Therefore, a lot of research has been conducted to reduce these known problems: For example, current deep learning research aims to reduce hallucinations (citations), to implement an ongoing learning framework (citations, online learning), to transfer knowledge between tasks and data domains (cite transfer learning), or to learn what should be learned (cite meta-learning). However, the underlying framework remains identical: Data is statistically mapped to manually or automatically generated labels, making it difficult to solve problems such as lack of robustness,  out-of-distribution generalisation, data efficiency, causal understanding or common sense reasoning (citations). In the following, alternative approaches to training deep networks are presented that attempt to alleviate these problems with new principles that do not rely solely on label prediction or data reconstruction. 

Many approaches to building next-generation AI systems use the human brain as a source of inspiration (sources). Jeff Hawking's research group goes one step further and uses the brain as the single source of ideas and builds, following their interpretation, a system that is biologically highly plausible and implements the brain's learning algorithm. The learning algorithm is an adapted version of unsupervised Hebbian learning. Their theory is based on Vernon Mountcastle's proposal that the neocortex is made up of many cortical columns, all having a similar architecture and performing similar functions (quote from the book 1000 Brains, page 248). Thus, they argue that the brain does not comprise a single learning model like current deep learning systems but many independent models for each object (cite the papers). Each model uses different inputs from different sensory system parts. These models make predictions based on the input they receive and vote to reach a consensus on the sensory observations.
This theory is also known as the 1000 brains theory, as many models work in parallel. The principle of many parallel models is closely related to ensemble approaches in deep learning (cite), whereby, similar to random forest (cite), only a portion of the data is available to each model. Current results are difficult to interpret as the proposed model has been trained and tested on the same data\sidenote{if a learning system has enough capacity, the training data can be overfitted, leading to very good results on that data}. The model, in its current form, can recognise several hundred objects and is robust to noise. A problem with the current approach is that the number of input signals severely limits the network's capacity. By adding more cortical columns (expanding the capacity in hidden layers), the model cannot scale to recognise more objects. 

Strictly enforcing biological plausibility limits performance significantly, as the biologically inspired algorithm cannot fully exploit the mathematical capabilities of computational hardware. Intuitively, however, several proposed principles appear promising: \emph{sparse coding} encourages compact and efficient representation of complex data (citation), \emph{self-organisation} of a large number of parallel models increases robustness (citation), and \emph{predicting future states} allows learning in an unsupervised manner and, similar to LLM, does not restrict the objective function to specific predefined tasks.

A second type of network that has evolved from the 1000 brains theory \sidenote{the driving force behind it, Dileep George, worked with Jeff Hawkings and was instrumental in developing the fundamentals of the 1000 brains theory} is called the recursive cortical network (RCN). It is based on the insight that human vision processes shape and appearance differently and that a familiar object with an unexpected colour can still be easily recognised.
The RCN uses separate mechanisms for processing contours and appearances. It also incorporates many neuroscience principles, such as lateral connections for contour consistency.
The training process is unsupervised. When the features cannot explain an image at a certain level $n$, the active features from the level below $n-1$ are combined to create a new feature at level $n$. Afterwards, features are pruned using a cost function considering both reconstruction and compression errors.
A significant advantage of RCN is that it can learn from few examples and has one-shot and few-shot learning capabilities.
However, in its current form, RCNs cannot be applied to natural images: The creation of contour hierarchies is possible only if the object is clearly separated from its background. Therefore, this network has only worked for MNIST and text-based CAPTCHAs so far.

Similar to the 1000 brains theory, this model has drawbacks that prevent its practical application. At the same time, it introduces many promising concepts. The separation of shape and appearance seems promising, as this limits the feature space, and these concepts can be learned independently. This allows the model to transfer knowledge better: a fist-sized ball can be an apple, an orange or a lemon, depending on its appearance. However, the shape of these fruits only needs to be learned once. Furthermore, in the case of multiple classes, the network does not make a single prediction but creates hypotheses that are evaluated by an outer loop. This principle could serve as confidence rating or even be used in a feedback loop to decide whether an object should be analysed further to build better representations of it.

Other approaches attempt to mimic biological neural organisation. For example, neuronal capsule networks (CapsNet) explicitly model hierarchical relationships. These networks group neurons into "capsules", and each capsule represents a property such as position, size, orientation, deformation, texture, colour or movement. The results of several of these capsules form more stable representations for higher capsules. Dynamic routing is used to match bottom-up activations and top-down concepts ``generated'' by the available evidence over multiple iterations. Thus, dynamic routing is used to learn ``viewpoint invariant'' knowledge that generalises to novel viewpoints. Furthermore, it can deal with highly overlapping objects and explain that one object is in front of another object. However, CapsNets are still an active area of research and are not yet adopted in real-world scenarios. In particular, dynamic routing makes the algorithm slow, and so far, this approach has only worked on very small datasets such as MNIST or CIFAR-10. For this algorithm to scale well, spatial information (information about relative relationships between features) needs to be able to be formed more robustly. Advantages, on the other hand, are robustness to affine transformations and the ability to segment overlapping images.

Some concepts of CapsNets are closely related to those of RCNs. However, CapsNets have their origins in the field of computer science, while RCNs have their origins in the field of neuroscience. This is also reflected in their implementation: CapsNets are based on typical deep learning principles, while RCNs are based on highly plausible biological concepts. CapsNets divide the features into capsules which is, as for RCNs, helpful for good generalisation. In combination with dynamic routing, capsules allow the model to generalise to new, unseen viewpoints. However, a severe problem is the accumulation of spatial information. This could also be due to the fact that the model processes independent images sequentially. This problem might be alleviated if the model is adapted to either process sequentially related images or videos, or if the model can interact with objects to obtain better spatial information.

Not only CapsNets try to mimic biological neural organisation but also other approaches. Researching biological neural organisation is promising as the brain is highly structured, and this structure could be the inductive bias of natural intelligence (cite Malsburg). The exact nature of the structuring is unknown, but many hypotheses were proposed and implemented as computer algorithms (citation Malsburg). Ma et al. (cited) suggest that appropriate data structuring can lead to the emergence of intelligence if it includes parsimony and self-consistency. The two principles of parsimony and self-consistency describe \emph{what} should be learned and \emph{how} it should be learned. Parsimony implies that an intelligent learning system should recognise low-dimensional structures in observed high-dimensional data and organise them in the most compact and structured way\sidenote(the goal is not to achieve the best possible compression but to obtain compact and structured representations in a computationally efficient way). The second principle of self-consistency states that an intelligent learning system minimises the \emph{internal} discrepancy between observed and regenerated observations. Thus, unlike an autoencoder with an encoder $E(\cdot)$ and a decoder $D(\cdot)$, it does not minimise the discrepancy between an observation $\boldsymbol{x}$ and its reconstructed version $\boldsymbol{\hat{x}} = D(E(\boldsymbol{x}))$, but their internal representations $\boldsymbol{z} = E(\boldsymbol{x})$ and $\boldsymbol{\hat{z}} = E(\boldsymbol{\hat{x}}) = E(D(E(\boldsymbol{x})))$. These representations are thus generated by self-learning through self-criticism, where the internal representations are consistent and satisfy the principle of parsimony. The described framework is very promising as it incorporates biological concepts into a framework that unifies and clarifies many practices and empirical findings of deep learning. The challenge of this framework seems to be the definition of the parsimony principles, i.e. how the latent space should be formed in detail. Recent results suggest the potential of this approach, but it is not entirely convincing, even on small datasets like CIFAR-10. For example, the framework's current implementation is inferior to simple autoencoder architectures in reconstruction or image classification.

The proposed framework unifies many promising concepts within an easily implementable computational framework. Self-consistency allows to measure the discrepancy in the latent space instead of the data space. Thus, the model only has to retain relevant information and can get rid of unnecessary details while still fulfilling the principle of parsimony. Leveraging these principles seems helpful, and storing latent representations in the most compact and structured way certainly makes sense by using sparse coding. However, it is unclear how to implement this in a way that scales to large datasets. 

Another frequently used principle inspired by biological learning is the combination of representations with actions. For example, it is known from animal research that both eye movements and behavioural states influence the reactions of neurons in the visual cortex {Keller_Bonhoeffer_Hübener_2012}.
Thus, animals integrate their actions (i.e., movements) with currently incoming sensory signals.
Keurti et al. (cite) argue that efference copies\sidenote{the internal copy of an outgoing motion-generating signal} help to learn useful latent representations of input perceived by the visual system. They allow an agent to perform actions by transforming objects and ensure that real-world transformations can also be applied to latent representations, i.e., mental objects and real-world objects remain consistent when similar transformations are applied.
Allowing an agent to interact with the world to understand it better and create better representations seems essential not only from a neuroscientific point of view but is also in line with theories from psychology:
Piaget\sidecite{Piaget_1964} argues that perceiving an object is more about understanding how it changes and behaves than creating a mental copy of the object.
The combination of perception and action for autonomous machine intelligence is also postulated by LeCun \sidecite{LeCun_AMI}. He proposes creating a world model whose state can be predicted based on planned actions. He emphasises the importance of self-supervised learning in combination with energy functions.

These works raise the question of what good representations are. Many deep neural networks optimise their internal representations for specific tasks such as classification or segmentation. However, a general system should have more general representations suitable for different tasks. Autoencoders, on the other hand, compress all the input data into a latent representation, retain most of the information despite its relevance, and do not distinguish between objects and backgrounds by default. Representations containing information about an object's shape and apperance from different viewpoints and knowing how the object behaves under different transformations is certainly helpful as this is a lot of useful information. However, learning actionable representation requires that this information is contained in the data source and suggests that environments based on 3D simulations are needed instead of 2D image datasets. I argue that current 2D image datasets are not enough, but 3D simulations are not needed either\sidenote{expect if an autonomous system is to be built.}. It is known from neuroscientific research that intelligent organisms cannot learn proper object representations if they only see uncorrelated images instead of a continuous input stream (cite). This seems reasonable because only a continuous input stream allows to learn the concept of changing viewpoints. However, having such a continuous stream and knowing how it changes corresponds to efference copies and render actions superfluous: A continuous video stream is like a 3D simulation where an external agent performs actions. Having a continuous input stream might be one of the main missing ingredients in current systems: The well-working LLMs are trained with a lot of temporal contexts (a sequence of words), which is omitted for visual systems trained on images (a single image). 

LeCun further describes that energy functions are well suited to make such predictions of the world state based on actions because they can nicely shape the latent space due to their regulating properties.
Using energy functions seems helpful to learn well-generalising and robust AI systems. In the context of active inference (source), perception presents itself as a process of minimising the free energy of variation with respect to beliefs about hidden variables. This enables planning and inference by modelling generative processes p(s,o). For example, if it rained at night (p(s)), one can infer that the grass is wet (p(o)). The model tries to model the chances of different hidden situations p(s|o) based on prior beliefs (p(s)) and the likelihood of what it already observed (p(o|s)). 
Thus, active inference aims to minimise a measure called variational free energy by using past experiences to learn how the world works. This helps to predict what might happen in the future by minimising the probability of surprising or unexpected situations.
This principle is also actively researched by other well-known research groups. For example, Bengio's research group works on GFlowNets (Generative Flow Networks), making it possible to disentangle the explanatory causal factors and the mechanisms that connect them. This allows for a more rational approach to generalisations well outside of distribution.

Energy-based models can be used for generative modelling tasks. For example, they can learn to generate new samples that resemble the training data by sampling from the energy function. This allows to predict future states and to learn concepts such as viewpoint transformations in a self-supervised manner.






---
TODO: Wenn in einem späteren Kapitel probablistic neuron motiviert wird: https://cerenaut.ai/2014/12/24/sparse-distributed-representations-sdrs/
 






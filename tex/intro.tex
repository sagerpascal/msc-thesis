%% intro.tex
Deep learning systems have been employed for decades \cite{ivakhnenko_cybernetic_1965} and pervasive influence our daily lives. This technology is utilised in diverse applications, such as machine translation \cite{dabre_survey_2021}, image analysis \cite{bhatt_cnn_2021}, natural language processing \cite{otter_survey_2021}, and speech synthesis \cite{ning_review_2019}, among others. Deep learning methodologies' remarkable success and widespread adoption have revolutionised various industries \sidecite{bertolini_machine_2021}, enabling enhanced performance and efficiency in numerous tasks and services.
Recently ChatGPT \sidecite{liu_summary_2023} was introduced, marking a pivotal moment in the widespread application of AI systems.
Since then, AI systems have transcended conventional boundaries and gained widespread applicability.

AI systems are trained to optimise a specific target function, such as predicting the subsequent word token in the case of language models \sidecite{radford_improving_2018}.
Therefore, such systems rely on statistical patterns rather than genuine understanding and still lack cognitive capabilities, reasoning ability, self-awareness, and intentionality \sidecite{rosenbloom_defining_2023, mitchell_debate_2023}.
Furthermore, deep networks suffer from various issues of statistical learning, such as missing robustness \sidecite{akhtar_threat_2018, long_survey_2022}, catastrophic forgetting \sidecite{kirkpatrick_overcoming_2017}, or requiring vast amounts of data \sidecite{smith_using_2022} (c.f. \secref{limitationsDL}).

Geoffrey E. Hinton, recipient of the Turing Award\sidenote{The Turing Award is recognised as the highest academic award in computer science and sometimes also called the ``Nobel Prize of Computing''.} and a prominent figure in the field, is considered one of the pioneers of deep learning.
His remarkable contributions, including improving deep learning's error correction algorithm called ``backpropagation of error'' \sidecite{rosenblatt_principles_1962, linnainmaa_taylor_1976}  (c.f. \secref{ann}), have laid the foundation for today's deep learning systems.
After working over three decades in the field, Hinton expresses deep scepticism about end-to-end backpropagation of errors and even suggests to ``throw it all away and start again'' to improve current systems fundamentally \sidecite{axios_media_inc_artificial_2017}.
While this view may seem extreme, it shows that the learning algorithm of such systems has significant shortcomings.
Some of the most critical limitations of deep learning systems are discussed in \secref{limitationsDL}.
To overcome these challenges, researchers have proposed diverse methods and approaches \cite{long_survey_2022, sager_unsupervised_2022, yarats_improving_2021}. Despite these efforts, progress has been moderate, primarily focusing on mitigating the symptoms rather than addressing the core issues.

This thesis introduces a novel learning framework inspired by neuroscience to address the core issues inherent in deep learning systems. Inspired by the human brain's remarkable learning capabilities, which surpass deep learning's limitations, this research aims to integrate neuroscientific insights into an image-processing framework. It seeks to overcome existing limitations and bridge the gap between artificial and biological intelligence. The thesis draws significant inspiration from the ``Theory of Natural Intelligence'' proposed by \sideciteay{von_der_malsburg_theory_2022} and provides a concrete manifestation of the theory within the framework. By anchoring the theoretical concepts in a concrete framework and exemplifying its learning capabilities through practical implementation, this thesis contributes to a comprehensive understanding and application of the principles of the theory.

\section{Motivation}\seclbl{intro_motivation}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{sdp_mountain_spirit.jpg}
    \caption[``Mountain Spirit in Winter'' by Sandro del Prete]{``Mountain Spirit in Winter'' by \citeay{del_prete_mountain_1982} demonstrating that humans immediately can observe the overall pattern (a man's face) even though local features depict a painter drawing a house.}
    \figlbl{sdp_mountain}
\end{figure}

One of the insights from the \emph{Gestalt} psychology \sidecite{ellis_source_1938, kohler_gestalt_1992, wagemans_century_2012, hamlyn_psychology_2017} is that humans are able to recognise the ``Gestalt'' (the entire structure) of an object within a very short time; The brain can immediately recognise global patterns - arrays of local features that consistently conform to a known large-scale pattern - even if those local features are buried in noise or would, on the basis of local context, be interpreted very differently. Thus, local decisions are made based on plausibility considering overall patterns, while the overall patterns can only be defined based on local features.

For example, when looking at \figref{sdp_mountain}, local and global patterns are not aligned. A painter drawing a picture from a house in a snowy landscape can be observed when looking at local features. However, a man's face is visible when looking at the overarching pattern.
This example illustrates that avoiding the ``fallacy of early commitment'' is important, as David Marr put it \sidecite{marr_vision_2010}.
Otherwise, when focusing on local features only, systems would commit to objects like trees, a painter, an image, a house, and snow and be unable to recognise the global pattern as a men's face typically comprises eyes, nose, etc. and not the aforementioned objects.

The theory of natural intelligence \sidecite{von_der_malsburg_theory_2022} and work based on self-organising projection fibres (c.f. \secref{projection_fibres}) considers the principle of preventing early commitment as a core mechanism for the effectiveness of the human visual system.
Preventing early commitment allows a system to leave multiple options open simultaneously: The system does not take decisions early in the learning process as typical deep learning models do but considers local and global features at the same time, continuously ruling out implausible hypotheses.

The most used architectures for image processing are based on convolutional neural networks (CNNs, c.f. \secref{cnns}). This type of network cannot prevent early commitment by design: The first layers extract local features from images, having access to small patches only. The extracted low-level features are combined into higher-level features in later layers \sidecite{prince_understanding_2023}. Thus, the first layers do not consider global features but steer the decision process during training and inference toward specific directions based on local features. Therefore, CNNs take local decisions without consolidating global information\sidenote{CNNs can be trained to make diverse decisions in high-level layers when appropriate labels are given. However, these decisions are still based on already taken local decisions.}.

Transformer-based \sidecite{dosovitskiy_image_2021} or fully connected \sidecite{tolstikhin_mlp-mixer_2021} vision architectures, on the other hand, might not have this design limitation since early layers can access the entire input.
However, they have a fallacy of early commitment because they process the input layer-wise. Typically, the input of a vision architecture is specific (e.g. an image) and mapped to more general information (e.g. a class label).
However, general information is not used to confirm or validate specific information and therefore, high-level decisions can be misled by the wrong and inconsistent early decisions.
Thus, the first layers make decisions on lower-level features and steer the decision process during training and inference towards a specific direction without considering higher-level features, thereby being prone to early commitment as well.

The fact that early layers in neural networks make decisions without incorporating higher-level features arises from the inherent characteristics of the learning principle.
Deep networks establish consistency at a specific point between a prediction and a teaching signal.
They employ a global error correction algorithm such as backpropagation of error \sidecite{rosenblatt_principles_1962, linnainmaa_taylor_1976} to update all cells, aiming to enhance alignment at that specific point and thereby construct feature processing chains.
The first neurons in these chains must take decision based on local features that are later used by neurons extracting higher level features.

In contrast, consistency in the brain is built at every single point, i.e. between each cell and synapse \sidecite{hebb_organization_1949}.
These cells represent features that contribute to hypotheses rather than a singular prediction.
Active cells support one or multiple hypotheses while suppressing hypotheses inconsistent with the feature they represent.
In a fast alignment process, increasing inhibition \sidecite{coombs_specific_1955} deactivates hypotheses that are not supported by sufficient cells, resulting in a consensus among brain cells regarding perceptual interpretations.
This process is based on local interaction and self-organisation \sidecite{ morris_cognitive_2006} without having an external teaching signal or global error correction \sidecite{grossberg_competitive_1987, crick_recent_1989}. This fundamental different principle prevents early commitment, as discussed in \secref{biologial_inspiration_early_commitment}.

Besides having the problem of early commitment, deep networks also lack dealing with ambiguity and object-independent transformation invariance (c.f. \secref{limitationsDL}).
Deep networks can learn to generate transformation-invariant features if they see enough samples during training.
However, they lack the ability to transfer the concept of a transformation, such as rotation from one object to another.

This thesis proposes a biologically inspired vision framework to mitigate these disadvantages. 
The thesis lies at the intersection of deep learning and neurocomputing\sidenote{Neurocomputing is a subfield of neuroscience that focuses on implementing biologically plausible learning algorithms.}. It adopts many learning paradigms from neurocomputing, leveraging the capabilities of biologically inspired algorithms. At the same time, the computational efficiency of deep learning algorithms is being exploited. By merging the strengths of both fields, this work aims to develop a hybrid approach that combines the biological plausibility of neurocomputing with the computational efficiency of deep learning to foster advances in learning algorithms.


\section{Contribution}
This thesis makes the following contributions:
\begin{enumerate}
	\item The basics of deep learning and neurocomputing are described. Together with the related work, this provides a survey of the most important research dealing with alternative learning algorithms compared to conventional deep learning methods.
	\item The ``Theory of Natural Intelligence'' proposed by \sideciteay{von_der_malsburg_theory_2022} is discussed and put into the context of the theory of self-organising projection fibres \sidecite{wiskott_face_1997, wolfrum_recurrent_2008} and the proposed vision framework. This thesis can be considered a more concrete formulation of this theory.
    \item Many neuroscientific findings that are highly relevant for visual object recognition are identified and summarised, and a vocabulary compatible with the current deep learning framework is introduced.
	\item An framework implementing the identified neuroscientific concepts is proposed based on a novel Bernoulli neuron,  a neuron firing a binary spike based on a probability distribution and self-organising synaptic connections. This conceptual work describes a novel framework that has never been implemented in this form before and originates from this thesis. 
    \item The feasibility of the proposed framework is demonstrated with experiments and the strengths and weaknesses are discussed.
    \item Directions for future research are given to improve the proposed vision framework further.
\end{enumerate}


\section{Organisation of Thesis}\seclbl{org_thesis}
The remainder of the thesis is organised as follows: In \chref{fundamentals}, deep learning and neurocomputing fundamentals related to this thesis are outlined in detail; experts may skip this chapter.
\chref{rel_work} introduces the related work. Next, \chref{biological_inspiration} identifies promising biological concepts for visual object recognition and introduces these principles with a vocabulary compatible with the deep learning framework\sidenote{Identifying and explaining neuroscientific findings is considered an important contribution, which is why this section has not been included in the chapter about related work.}.
Afterwards, \chref{probabilistic_framework} proposes a novel implementation of the identified biological concepts within a computational framework based on a new neuron called the Bernoulli neuron.
In \chref{experiments}, conducted experiments with the novel framework are described and in \chref{results} the obtained results.
Finally, \chref{future_work} concludes the thesis by discussing the advantages and disadvantages of the proposed learning framework and suggests potential directions for future research.
Thus, the Chapters \chref*{fundamentals}-\chref*{biological_inspiration} survey existing work and put it into context, while the Chapters \chref*{probabilistic_framework}-\chref*{future_work} introduce and discuss a novel vision framework.

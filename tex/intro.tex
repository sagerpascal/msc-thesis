%% intro.tex
Mankind has always tried to simplify its life through technological progress.
In the last hundred years, the computer in particular has shaped progress in every field imaginable. 
The computer has been so successful in fact that it has spawned its own scientific discipline, computer science.
Nowadays it is impossible to imagine life without computers: almost every household and company owns computers.
Computers facilitate various tasks, from communication to the acquisition of knowledge.
For all these tasks, software developers have created appropriate programs.
Software development is the process of writing a script with a programming language to specify how the computer should behave for a given input.
Simply put: A software program tells the computer what to do if the user enters a command.
This works very well if the tasks are clearly defined and can be described precisely.
However, there exist tasks that are almost impossible to program such as writing a script that detects cats in images because we cannot describe how a cat looks precisely\sidenote{at least not on a pixel-level basis so that we can simply compare a given image with our description}.

Therefore, scientists came up with the idea to not just program such tasks but to let the computer learn them instead.
Machine learning (ML) are algorithms that can learn and adapt without following explicit instructions such as program code.
Instead, they use statistical models to analyze data, find patterns in the data, and draw inferences from it.
Machine learning has become an indispensable part of our everyday lives.
For example, we use it for machine translation, transport and logistics organization, product recommendations, fraud detection, self-driving cars, unlocking smartphones, improving video games, speech recognition, and much more.
A sub-branch of machine learning is deep learning which achieved very impressive results in the last decade and is considered the state-of-the-art technology for many of the aforementioned tasks.

Even though this technology works very well for many tasks, such models have some crucial flaws by definition (c.f. Section \secref{limitationsDL}) which cannot be resolved for sure in the current DL framework.
One of the godfathers of deep learning is the Turing Award winner Geoffrey Hinton. 
Especially his contribution to backpropagation (c.f. Section \secref{ann}) has created the foundation for modern deep learning.
More than 30 years later, in 2017, Hinton says that he is ``deeply suspicious'' about end-to-end backpropagation of error, and in his view we have to ``throw it all away and start again'' to improve current systems fundamentally \sidecite{axios_hinton}.
Considering what DL systems have achieved, this seems a bit extreme.
However, it also shows that the current learning algorithm of such systems has serious flaws.

Some of the most crucial limitations of deep learning systems are listed in Section \secref{limitationsDL}.
This thesis addresses some of these problems, especially the robustness problem.
This is done by exploring different architectures inspired by findings from the field of neuroscience\sidenote{this field studies how the human nervous system and especially the brain works}.
The inspiration is drawn from neuroscience as the human brain seems to not have these problems and thus incorporating findings from neuroscience into the deep learning framework might help to overcome some of the current limitations.
In this thesis, the theory of natural intelligence by von der Malsburg et al. \sidecite{von_der_Malsburg_Stadelmann_Grewe_2022} in particular is used as an inspirational source.

The thesis differs from neurocomputing\sidenote{neurocomputing is considered a sub-field of neuroscience that deals with the implementation of algorithms with a high degree of plausibility according to neuroscientific findings on how natural nervous systems work} in the sense that the architectures investigated are still closely related to the DL framework and, for example, do not use time-dynamic signals. 
For the completeness of this thesis, an overview of neurocomputing is given in Section \secref{neurocomputing}.
However, since these types of algorithms have not (yet) become established in practical applications, they are not investigated in this thesis.
Thus, the thesis is strongly oriented along the field of deep learning and focuses on incorporating findings of neuroscience in the deep learning framework while natural plausibility is of less importance.


%Mankind is often inspired by nature when it comes to developing novel systems.
%The development of artificial intelligence (AI) and Deep Learning has been strongly influenced by, according to our perception, one of (if not the) most intelligent systems on planet Earth, the human brain.
%The brain is studied by the scientific field of neuroscience\sidenote{there exist many additional (sub-)fields studying the brain such as cognitive science, cognitive psychology, neurology, and neuropsychology}.
%From neuroscience or related fields come various insights on how the human nervous system and especially the brain works.
%Much of this knowledge has been gained through observations and experiments on humans or other living creatures.
%Often these findings are only what can be measured from the outside, the real core, the mechanism that causes intelligence, remains unknown.
%
%While Deep Learning is clearly inspired by the insights of Neuroscience, the two fields have little in common in their present form.
%The implementation of neuroscientific findings has emerged as a subfield in its own right, called Neurocomputing (c.f. Section \secref{neurocomputing}).
%In general, neurocomputing is closer related to the functionality of the human brain than Deep Learning.
%Neurocomputing has produced many promising algorithms that can overcome some weaknesses faced by deep learning systems. 
%Although neurocomputing has also achieved significant breakthroughs, most systems used in everyday life are still based on the principle of Deep Learning.


%\section{Motivation}\seclbl{motivation}
%One of the main challenges of today's AI systems is the processing of visual information.
%Visual perception is fundamentally the task of finding a suitable interpretation of a given scene.
%A scene typically consists of objects that interact with each other.
%Identifying these objects and their relationship among each other is implemented by algorithms as a non-deterministic search problem, i.e. algorithms try to identify the object within a scene.
%State-of-the-art algorithms for image processing are often based on deep learning.
%Despite the fact that deep learning has achieved incredible performance on a variety of tasks such as object recognition it is still questionable whether the current methodology is enough to achieve real (or at least more advanced) intelligence (c.f. Section \secref*{limitationsDL}).
%Algorithms from the field of neurocomputing (c.f. Section \secref*{neurocomputing}), which are more closely oriented to the findings from the field of neuroscience, can be considered as a complement to deep learning.
%However, even tough algorithms from the field of neurocomputing have interesting properties, they usually do not perform as good as deep learning according to the evaluation metrics used and often work on specific or small data only.

%Thus, various algorithms exist to solve the non-deterministic problem of finding an appropriate interpretation of a visual scene, but they have significant shortcomings.
%Interestingly, the problem of generating a scene\sidenote{the opposite task than finding an interpretation of a scene} from a clear description is deterministic.
%Computer graphics deals with the problem to generate images with the aid of computers.
%Nowadays, such systems are able to render photorealistic images based on descriptions.
%For example, video games are essentially programs that generate scenes consisting of multiple objects such as people, weapons, buildings, or vehicles based on program code.
%For each of these objects, skeletons are usually pre-defined and during rendering transformed (i.e. their size, rotation, distortion and positioning are adjusted) as well as properly illuminated (raytracing).
%This allows to generate an infinite number of scenes from a predefined set of skeletons and transformations.
%Essential in this process seems the separation between objects and object transformation.
%Therefore, incorporating these insights into current algorithms for the interpretation of visual scenes seems promising and could lead to significant improvements.
%More precisely, identifying objects and their transformations separately in preception systems in order to recognize objects independently of their transformations could improve visual scene interpretation.


%\section{Terminology}\seclbl{terminology}


\section{Contribution}
The following contributions are made in this thesis:
\begin{itemize}
	\item First, the basics of the fields of deep learning and neurocomputing are described in detail. Together with related work, this provides a survey of the related fundamentals.
	\item Second, promising concepts from the neuroscience literature that may be necessary for more intelligent systems are identified and suggestions are made on how to integrate these concepts into a modern DL framework.
	\item Third, I present two concrete DL architectures that make use of such neuroscience-inspired concepts and analyze their strengths and weaknesses in detail.
	\item Fourth, this work gives an impression of which concepts that are currently little used in DL settings seem to be promising and makes a recommendation for future work.
\end{itemize}

\section{Organization of Thesis}\seclbl{org_thesis}
The remainder of the thesis is organized as follows: In Chapter \chref*{fundamentals} the fundamentals of deep learning and neurocomputing are explained in detail.
Experts may skip this chapter, likewise, the second Section on neurocomputing can optionally be omitted, as it is not fundamental for understanding this thesis.
Chapter \chref*{rel_work} introduces the related work. Chapter \chref*{methods} describes the methods used; first, I explain my interpretation on how specific neuroscientific concepts could be implemented in a deep learning setting. Afterwards, two architectures incorporating these concepts are presented. In Chapter \chref*{results} the results and findings of this thesis are discussed. Finally, in Chapter \chref*{future_work} future work is described and some insights and intuitions of the author are given about which neuroscience concepts seem promising and which do not.



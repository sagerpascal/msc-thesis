%% intro.tex
Deep learning systems have been employed for decades \cite{ivakhnenko_cybernetic_1965} and pervasive influence our daily lives. This technology is utilized in diverse applications, such as machine translation \cite{dabre_survey_2021}, image analysis \cite{bhatt_cnn_2021}, natural language processing \cite{otter_survey_2021}, and speech synthesis \cite{ning_review_2019}, among others. Deep learning methodologies' remarkable success and widespread adoption have revolutionized various industries \cite{bertolini_machine_2021}, enabling enhanced performance and efficiency in numerous tasks and services.
However, recently ChatGPT \cite{liu_summary_2023} was introduced, marking a pivotal moment in the widespread application of AI systems.
Since then, AI systems have transcended conventional boundaries and gained widespread applicability. The emergence of the GPT-4 model \cite{openai_gpt-4_2023} has sparked discussions regarding the potential manifestation of general intelligence \cite{bubeck_sparks_2023}. While some researchers argue in favour of this perspective, I hold a different viewpoint and argue that despite their apparent intelligent behaviour, present AI systems, including ChatGPT, lack the essential characteristics necessary to qualify as true intelligence;
These systems are trained to optimize a specific target function, such as predicting the subsequent word token \sidecite{radford_improving_2018}. Thus, these systems rely on statistical patterns rather than genuine understanding. They still lack cognitive capabilities, reasoning ability, self-awareness, and intentionality \sidecite{rosenbloom_defining_2023, mitchell_debate_2023}. It might be that current learning frameworks cannot overcome these limitations, and alternative approaches are necessary (c.f. \secref{limitationsDL}).

Geoffrey Hinton, recipient of the Turing Award\sidenote{The Turing Award is recognised as the highest academic award in computer science and sometimes also called the ``Nobel Prize of Computing''.} and a prominent figure in the field, is considered one of the pioneers of deep learning.
His remarkable contributions, including developing backpropagation for learning (c.f. \secref{ann}), have laid the foundation for today's deep learning systems.
After more than three decades, Hinton expresses deep scepticism about end-to-end backpropagation of errors and even suggests to ``throw it all away and start again'' to improve current systems fundamentally \cite{axios_hinton}.
While this view may seem extreme, it shows that the learning algorithm of such systems has significant shortcomings.
Some of the most critical limitations of deep learning systems are discussed in \secref{limitationsDL}.
To overcome these challenges, researchers have proposed diverse methods and approaches \cite{long_survey_2022, sager_unsupervised_2022, yarats_improving_2021}. Despite these efforts, progress has been moderate, primarily focusing on mitigating the symptoms rather than addressing the core issues.

This thesis introduces a novel learning framework inspired by neuroscience to address the core issues inherent in deep learning systems. Inspired by the human brain's remarkable learning capabilities, which surpass deep learning's limitations, this research aims to integrate neuroscientific insights into a learning framework. It seeks to overcome existing limitations and bridge the gap between artificial and biological intelligence. The thesis draws significant inspiration from the ``Theory of Natural Intelligence" proposed by \sideciteay{von_der_malsburg_theory_2022} and provides a concrete manifestation of the theory within the framework. By anchoring the theoretical concepts in a concrete framework and exemplifying its learning capabilities through practical implementation, this thesis contributes to a comprehensive understanding and application of the principles of the theory.

This thesis lies at the intersection of deep learning and neurocomputing\sidenote{Neurocomputing is a subfield of neuroscience that focuses on implementing biologically plausible learning algorithms.}. It adopts many learning paradigms from neurocomputing, leveraging the capabilities of biologically inspired algorithms. At the same time, the computational efficiency of deep learning algorithms is being exploited. By merging the strengths of both fields, this work aims to develop a hybrid approach that combines the biological plausibility of neurocomputing with the computational efficiency of deep learning to foster advances in learning algorithms.


\section{Contribution}
This thesis makes the following contributions:
\begin{enumerate}
	\item The basics of deep learning and neurocomputing are described. Together with the related work, this provides a survey of the most important research dealing with alternative learning algorithms.
	\item The ``Theory of Natural Intelligence" proposed by \sideciteay{von_der_malsburg_theory_2022} is discussed and put into the context of the theory of self-organising projection fibres \sidecite{wiskott_face_1997, wolfrum_recurrent_2008}.
	\item The probabilistic neuron is introduced, a neuron firing a binary spike based on a learned probability distribution.
	\item A modernised version of the self-organising projection fibre framework is proposed, particularly focusing on sub-networks' emergence.
    \item The feasibility of the proposed framework is demonstrated with experiments.
    \item Directions for future research are given to develop the simplified framework into a complete vision framework.
\end{enumerate}


\section{Organisation of Thesis}\seclbl{org_thesis}
The remainder of the thesis is organised as follows: In \chref{fundamentals}, the fundamentals of deep learning and neurocomputing are explained in detail.
Experts may skip this chapter; especially the second section on neurocomputing can be omitted, as it is not fundamental for understanding this thesis but only intended for interested readers or those who want to get a better overview of the field or have inspiration for future work.
\chref{rel_work} introduces the related work. Next, \chref{probabilistic_framework} introduces the concept of the probabilistic neuron and proposes a novel vision learning framework, thereby putting it into context of existing work, the theory of natural intelligence, and findings from neuroscience.
In \chref{experiments}, conducted experiments with the novel framework are described and in \chref{results} the obtained results.
Finally, \chref{future_work} concludes the thesis by discussing advantages and disadvantages of the proposed learning framework and suggests potential directions for future research.

\section{TODO}
Feedback Thilo: weniger Small-Scale in Thesis -> mehr Meta-Level in Introduction / eigene Theorie, evtl. Fundamentals -> mehr Neuroscience (1000 Brains, Capsule Networks, Christoph,Geoffrey Generation von Rules)





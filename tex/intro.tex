%% intro.tex
Mankind has always tried to simplify its life through technological progress.
Thousands of years before Christ the wheel was invented, later the pulley, the printing press, and the steam locomotive.
At the beginning of the 19th century, the first generators were built to produce electricity, and in 1835, the first light bulb was invented by James Bowman Lindsay\sidenote{and not, as so often falsely claimed, by Thomas Alva Edision}.
In the course of time more and more complex electrical circuits were produced and finally in 1941 the first digital calculating machine, the computer was developed.
This calculating machine is able to perform various arithmetic operations on the basis of commands.
Through the development of transistors, more and more complex computers could be produced, which are able to execute various tasks.
The definition of these tasks are captured in software.
The computer has been so successful in fact that it has spawned its own scientific discipline, Computer Science.
Nowadays it is impossible to imagine life without computers: almost every household and company owns computers.
Computers facilitate various tasks, from communication to the acquisition of knowledge.
For all these tasks, software developers have created appropriate tools.
Software development is the process of writing a script with a programming language to specify how the computer should behave for a given input.
Simply put: A software program tells the computer what to do if the user enters a command.
This works very well if the tasks is clearly defined and can be described precisely.
However, there exists tasks which are almost impossible to program.
For example, writing a script that detects cats in images is almost impossible because we cannot describe how a cat looks like\sidenote{at least not on a pixel-level basis so that we can simply compare a given image with our description}.

Therefore, computer scientists came up with the idea to not just program such tasks but to let the computer learn them instead.
Machine learning (ML) are algorithms that are able to learn and adapt without following explicit instructions such as program code.
Instead, they use statistical models to analyse data, to find patterns in the data and to draw inferences out of it.
Machine learning has become an indispensable part of our everyday lives.
For example, we use it for machine translation, transport and logistics organization, product recommendations, fraud detection, self-driving cars, unlocking smartphones, improving video games, speech recognition, and much more.
A sub-branch of Machine Learning, namely Deep Learning, has made waves in the last decade.
Breakthroughs are being made with this technology are made almost monthly and allows us to execute more and more tasks.

However, such DL system are usually good at one ore a few closely related tasks.
In fact, they have they have some crucial flaws by definition (c.f. Section \secref{limitationsDL}) which cannot be resolved for sure in the current DL framework.
One of the Godfathers of Deep Learning is the Turing Award winner Geoffrey Hinton. 
Especially his contribution to back propagation (c.f. Section \secref{ann}) has shaped the field.
Back propagation is \emph{the} core learning algorithm of DL systems and was developed in 1986.
More than 30 years later, in 2017, Hinton says that he is ``deeply suspicious'' about back propagation and in his view we have to ``throw it all away and start again'' to improve current systems fundamentally \sidecite{axios_hinton}.
Considering what DL systems have achieved, this seems a bit extreme.
However, it also shows that the current learning algorithm of such systems has serious flaws.

Mankind is often inspired by nature when it comes to developing novel systems.
The development of artificial intelligence (AI) and Deep Learning has been strongly influenced by, according to our perception, one of (if not the) most intelligent systems on planet Earth, the human brain.
The brain is studied by the scientific field of neuroscience\sidenote{there exist many additional (sub-)fields studying the brain such as cognitive science, cognitive psychology, neurology, and neuropsychology}.
From neuroscience or related fields come various insights on how the human nervous system and especially the brain works.
Much of this knowledge has been gained through observations and experiments on humans or other living creatures.
Often these findings are only what can be measured from the outside, the real core, the mechanism that causes intelligence, remains unknown.

While Deep Learning is clearly inspired by the insights of Neuroscience, the two fields have little in common in their present form.
The implementation of neuroscientific findings has emerged as a subfield in its own right, called Neurocomputing (c.f. Section \secref{neurocomputing}).
In general, neurocomputing is closer related to the functionality of the human brain than Deep Learning.
Neurocomputing has produced many promising algorithms that can overcome some weaknesses faced by deep learning systems. 
Although neurocomputing has also achieved significant breakthroughs, most systems used in everyday life are still based on the principle of Deep Learning.

TODO: deep learning or Deep Learning -> make uniform

\subsection{Motivation}\seclbl{motivation}
One of the main challenges of today's AI systems is the processing of visual information.
Visual perception is fundamentally the task of finding a suitable interpretation of a given scene.
A scene typically consists of objects that interact with each other.
Identifying these objects and their relationship among each other is implemented by algorithms as a non-deterministic search problem, i.e. algorithms try to identify the object within a scene.
State-of-the-art algorithms for image processing are often based on Deep Learning.
Despite the fact that Deep Learning has achieved incredible performance on a variety of tasks such as object recognition it is still questionable whether the current methodology is enough to achieve real (or at least more advanced) intelligence (c.f. Section \secref*{limitationsDL}).
Algorithms from the field of neurocomputing (c.f. Section \secref*{neurocomputing}), which are more closely oriented to the findings from the field of neuroscience, can be considered as a complement to Deep Learning.
However, even tough algorithms from the field of neurocomputing have interesting properties, they usually do not perform as good as Deep Learning according to the evaluation metrics used and often work on specific or small data only.

Thus, various algorithms exist to solve the non-deterministic problem of finding an appropriate interpretation of a visual scene, but they have significant shortcomings.
Interestingly, the problem of generating a scene\sidenote{the opposite task than finding an interpretation of a scene} from a clear description is deterministic.
Computer graphics deals with the problem to generate images with the aid of computers.
Nowadays, such systems are able to render photorealistic images based on descriptions.
For example, video games are essentially programs that generate scenes consisting of multiple objects such as people, weapons, buildings, or vehicles based on program code.
For each of these objects, skeletons are usually pre-defined and during rendering transformed (i.e. their size, rotation, distortion and positioning are adjusted) as well as properly illuminated (raytracing).
This allows to generate an infinite number of scenes from a predefined set of skeletons and transformations.
Essential in this process seems the separation between objects and object transformation.
Therefore, incorporating these insights into current algorithms for the interpretation of visual scenes seems promising and could lead to significant improvements.
More precisely, identifying objects and their transformations separately in preception systems in order to recognize objects independently of their transformations could improve visual scene interpretation.

Renowned scientists \sidecite{von_der_Malsburg_Stadelmann_Grewe_2022} put forward the hypothesis that natural intelligent systems such as the human brain learn to interpret visual scenes through a mechanism based on self-organization.
Self-organization is the process by which individual units organize their global behavior by local interactions amongst themselves.
There is no central control unit that orchestrates the units.
In the context of the brain, self-organization primarily affects how neurons are interconnected.
The brains of living beings already have certain structures from birth, which are specified by the DNA.
However, in the course of time, these structures change.
According to von der Malsburg et al. \cite{von_der_Malsburg_Stadelmann_Grewe_2022}, neurons form net-fragments (a.k.a. sub-networks) that represent objects with different levels of abstraction within an image (c.f. section \secref{natural_intelligence}).
For example, some net-fragments may represent shapes and structures while a multitude of such net-fragments together represent objects such as persons or entire scenes.
Thus, self-organization could be the algorithm to form net-fragments that represent objects independent of their transformation.

Such a mechanism to form networks that can capture scenes in the form of net-fragment has not been successfully implemented in deep learning nor neurocomputing systems.
In deep learning system, the architecture is usually predefined and only the parameters (i.e. weight and biases, c.f. \secref{ann}) are updated.
In neurocomputing, on the other hand, the theory behind self-organization is often associated with Hebbian Learning (c.f. \secref{hebbian}).
The theory behind this learning rule can be summarized as ``cells that fire together wire together''\sidecite{science.1372754}.
However, even though promising systems based Hebbian learning could be developed, the breakthrough has not been achieved yet.
Moreover, there don't exist systems that use Hebbian learning to explicitly form net-fragment that represent objects.

This thesis aims to develop a learning paradigm that explicitly foster creation of net-fragments that represent objects independent of their transformation.
... TODO ...



\subsection{Organization of Thesis}\seclbl{org_thesis}
The remainder of the thesis is organized as follows: In chapter \chref*{fundamentals} we present the fundamentals necessary to understand this thesis. We provide an overview about how deep learning works and what the most common research areas in neurocomputing are. Chapter \chref*{rel_work} presents the work related to this thesis.

TODO....




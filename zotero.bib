
@incollection{cord_supervised_2008,
	address = {Berlin, Heidelberg},
	title = {Supervised {Learning}},
	isbn = {978-3-540-75170-0 978-3-540-75171-7},
	url = {http://link.springer.com/10.1007/978-3-540-75171-7_2},
	language = {en},
	urldate = {2023-06-26},
	booktitle = {Machine {Learning} {Techniques} for {Multimedia}},
	publisher = {Springer Berlin Heidelberg},
	author = {Cunningham, Pádraig and Cord, Matthieu and Delany, Sarah Jane},
	editor = {Cord, Matthieu and Cunningham, Pádraig},
	year = {2008},
	doi = {10.1007/978-3-540-75171-7_2},
	note = {Series Title: Cognitive Technologies},
	pages = {21--49},
}

@article{liu_self-supervised_2021,
	title = {Self-supervised {Learning}: {Generative} or {Contrastive}},
	issn = {1041-4347, 1558-2191, 2326-3865},
	shorttitle = {Self-supervised {Learning}},
	url = {https://ieeexplore.ieee.org/document/9462394/},
	doi = {10.1109/TKDE.2021.3090866},
	urldate = {2023-06-26},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Liu, Xiao and Zhang, Fanjin and Hou, Zhenyu and Mian, Li and Wang, Zhaoyu and Zhang, Jing and Tang, Jie},
	year = {2021},
	pages = {1--1},
}

@book{russell_artificial_2021,
	address = {Hoboken},
	edition = {Fourth edition},
	series = {Pearson series in artificial intelligence},
	title = {Artificial intelligence: a modern approach},
	isbn = {978-0-13-461099-3},
	shorttitle = {Artificial intelligence},
	abstract = {"Updated edition of popular textbook on Artificial Intelligence. This edition specific looks at ways of keeping artificial intelligence under control"--},
	publisher = {Pearson},
	author = {Russell, Stuart J. and Norvig, Peter},
	year = {2021},
	keywords = {Artificial intelligence},
}

@article{van_engelen_survey_2020,
	title = {A survey on semi-supervised learning},
	volume = {109},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-019-05855-6},
	doi = {10.1007/s10994-019-05855-6},
	abstract = {Abstract
            Semi-supervised learning is the branch of machine learning concerned with using labelled as well as unlabelled data to perform certain learning tasks. Conceptually situated between supervised and unsupervised learning, it permits harnessing the large amounts of unlabelled data available in many use cases in combination with typically smaller sets of labelled data. In recent years, research in this area has followed the general trends observed in machine learning, with much attention directed at neural network-based models and generative learning. The literature on the topic has also expanded in volume and scope, now encompassing a broad spectrum of theory, algorithms and applications. However, no recent surveys exist to collect and organize this knowledge, impeding the ability of researchers and engineers alike to utilize it. Filling this void, we present an up-to-date overview of semi-supervised learning methods, covering earlier work as well as more recent advances. We focus primarily on semi-supervised classification, where the large majority of semi-supervised learning research takes place. Our survey aims to provide researchers and practitioners new to the field as well as more advanced readers with a solid understanding of the main approaches and algorithms developed over the past two decades, with an emphasis on the most prominent and currently relevant work. Furthermore, we propose a new taxonomy of semi-supervised classification algorithms, which sheds light on the different conceptual and methodological approaches for incorporating unlabelled data into the training process. Lastly, we show how the fundamental assumptions underlying most semi-supervised learning algorithms are closely connected to each other, and how they relate to the well-known semi-supervised clustering assumption.},
	language = {en},
	number = {2},
	urldate = {2023-06-26},
	journal = {Machine Learning},
	author = {Van Engelen, Jesper E. and Hoos, Holger H.},
	month = feb,
	year = {2020},
	pages = {373--440},
}

@article{arulkumaran_deep_2017,
	title = {Deep {Reinforcement} {Learning}: {A} {Brief} {Survey}},
	volume = {34},
	issn = {1053-5888},
	shorttitle = {Deep {Reinforcement} {Learning}},
	url = {http://ieeexplore.ieee.org/document/8103164/},
	doi = {10.1109/MSP.2017.2743240},
	number = {6},
	urldate = {2023-06-26},
	journal = {IEEE Signal Processing Magazine},
	author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
	month = nov,
	year = {2017},
	pages = {26--38},
}

@book{rosenblatt_principles_1962,
	series = {Cornell {Aeronautical} {Laboratory}. {Report} no. {VG}-1196-{G}-8},
	title = {Principles of {Neurodynamics}: {Perceptrons} and the {Theory} of {Brain} {Mechanisms}},
	url = {https://books.google.ch/books?id=7FhRAAAAMAAJ},
	publisher = {Spartan Books},
	author = {Rosenblatt, Frank},
	year = {1962},
	lccn = {62012882},
}

@article{linnainmaa_taylor_1976,
	title = {Taylor expansion of the accumulated rounding error},
	volume = {16},
	issn = {0006-3835, 1572-9125},
	url = {http://link.springer.com/10.1007/BF01931367},
	doi = {10.1007/BF01931367},
	language = {en},
	number = {2},
	urldate = {2023-06-26},
	journal = {BIT},
	author = {Linnainmaa, Seppo},
	month = jun,
	year = {1976},
	pages = {146--160},
}

@article{linnainmaa_taylor_1976-1,
	title = {Taylor expansion of the accumulated rounding error},
	volume = {16},
	issn = {0006-3835, 1572-9125},
	url = {http://link.springer.com/10.1007/BF01931367},
	doi = {10.1007/BF01931367},
	language = {en},
	number = {2},
	urldate = {2023-06-26},
	journal = {BIT},
	author = {Linnainmaa, Seppo},
	month = jun,
	year = {1976},
	pages = {146--160},
}

@inproceedings{bengio_deep_2012,
	address = {Bellevue, Washington, USA},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Deep {Learning} of {Representations} for {Unsupervised} and {Transfer} {Learning}},
	volume = {27},
	url = {https://proceedings.mlr.press/v27/bengio12a.html},
	abstract = {Deep learning algorithms seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels, with higher-level learned features defined in terms of lower-level features. The objective is to make these higher-level representations more abstract, with their individual features more invariant to most of the variations that are typically present in the training distribution, while collectively preserving as much as possible of the information in the input. Ideally, we would like these representations to disentangle the unknown factors of variation that underlie the training distribution. Such unsupervised learning of representations can be exploited usefully under the hypothesis that the input distribution P(x) is structurally related to some task of interest, say predicting P(y{\textbar}x). This paper focuses on the context of the Unsupervised and Transfer Learning Challenge, on why unsupervised pre-training of representations can be useful, and how it can be exploited in the transfer learning scenario, where we care about predictions on examples that are not from the same distribution as the training distribution.},
	booktitle = {Proceedings of {ICML} {Workshop} on {Unsupervised} and {Transfer} {Learning}},
	publisher = {PMLR},
	author = {Bengio, Yoshua},
	editor = {Guyon, Isabelle and Dror, Gideon and Lemaire, Vincent and Taylor, Graham and Silver, Daniel},
	month = jul,
	year = {2012},
	pages = {17--36},
}

@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {0932-4194, 1435-568X},
	url = {http://link.springer.com/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	language = {en},
	number = {4},
	urldate = {2023-06-26},
	journal = {Mathematics of Control, Signals, and Systems},
	author = {Cybenko, G.},
	month = dec,
	year = {1989},
	pages = {303--314},
}

@article{fukushima_visual_1969,
	title = {Visual {Feature} {Extraction} by a {Multilayered} {Network} of {Analog} {Threshold} {Elements}},
	volume = {5},
	issn = {0536-1567},
	url = {http://ieeexplore.ieee.org/document/4082265/},
	doi = {10.1109/TSSC.1969.300225},
	number = {4},
	urldate = {2023-06-26},
	journal = {IEEE Transactions on Systems Science and Cybernetics},
	author = {Fukushima, Kunihiko},
	year = {1969},
	pages = {322--333},
}

@book{hebb_organization_1949,
	address = {Oxford,  England},
	title = {The {Organization} of {Behavior}; {A} {Neuropsychological} {Theory}},
	publisher = {Psychology Press},
	author = {Hebb, Donald O.},
	year = {1949},
}

@book{costandi_neuroplasticity_2016,
	address = {Cambridge, MA},
	series = {The {MIT} {Press} essential knowledge series},
	title = {Neuroplasticity},
	isbn = {978-0-262-52933-4},
	publisher = {The MIT Press},
	author = {Costandi, Moheb},
	year = {2016},
	keywords = {Nervous system, Neural transmission, Neuroplasticity, Physiology},
}

@article{coombs_specific_1955,
	title = {The specific ionic conductances and the ionic movements across the motoneuronal membrane that produce the inhibitory post-synaptic potential},
	volume = {130},
	issn = {00223751},
	url = {https://onlinelibrary.wiley.com/doi/10.1113/jphysiol.1955.sp005412},
	doi = {10.1113/jphysiol.1955.sp005412},
	language = {en},
	number = {2},
	urldate = {2023-06-26},
	journal = {The Journal of Physiology},
	author = {Coombs, Jo S. and Eccles, John C. and Fatt, Paul},
	month = nov,
	year = {1955},
	pages = {326--373},
}

@article{takagi_roles_2000,
	title = {Roles of ion channels in {EPSP} integration at neuronal dendrites},
	volume = {37},
	issn = {01680102},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168010200001206},
	doi = {10.1016/S0168-0102(00)00120-6},
	language = {en},
	number = {3},
	urldate = {2023-06-26},
	journal = {Neuroscience Research},
	author = {Takagi, Hiroshi},
	month = jul,
	year = {2000},
	pages = {167--171},
}

@article{felleman_distributed_1991,
	title = {Distributed {Hierarchical} {Processing} in the {Primate} {Cerebral} {Cortex}},
	volume = {1},
	issn = {1047-3211, 1460-2199},
	url = {https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/1.1.1},
	doi = {10.1093/cercor/1.1.1},
	language = {en},
	number = {1},
	urldate = {2023-06-26},
	journal = {Cerebral Cortex},
	author = {Felleman, Daniel J. and Van Essen, David C.},
	month = jan,
	year = {1991},
	pages = {1--47},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	language = {en},
	number = {6088},
	urldate = {2023-06-26},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	pages = {533--536},
}

@article{herculano-houzel_human_2009,
	title = {The human brain in numbers: a linearly scaled-up primate brain},
	volume = {3},
	issn = {16625161},
	shorttitle = {The human brain in numbers},
	url = {http://journal.frontiersin.org/article/10.3389/neuro.09.031.2009/abstract},
	doi = {10.3389/neuro.09.031.2009},
	urldate = {2023-06-26},
	journal = {Frontiers in Human Neuroscience},
	author = {Herculano-Houzel, Suzana},
	year = {2009},
}

@article{zeng_neuronal_2017,
	title = {Neuronal cell-type classification: challenges, opportunities and the path forward},
	volume = {18},
	issn = {1471-003X, 1471-0048},
	shorttitle = {Neuronal cell-type classification},
	url = {https://www.nature.com/articles/nrn.2017.85},
	doi = {10.1038/nrn.2017.85},
	language = {en},
	number = {9},
	urldate = {2023-06-26},
	journal = {Nature Reviews Neuroscience},
	author = {Zeng, Hongkui and Sanes, Joshua R.},
	month = sep,
	year = {2017},
	pages = {530--546},
}

@book{james_principles_1890,
	title = {The principles of psychology},
	publisher = {Henry Holt and Company},
	author = {James, William},
	year = {1890},
}

@book{bain_mind_1873,
	address = {New York},
	title = {Mind and body: {The} theories of their relation},
	volume = {1},
	publisher = {D. Appleton and Company},
	author = {Bain, Alexander},
	year = {1873},
}

@article{hopfield_neural_1982,
	title = {Neural networks and physical systems with emergent collective computational abilities.},
	volume = {79},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.79.8.2554},
	doi = {10.1073/pnas.79.8.2554},
	abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
	language = {en},
	number = {8},
	urldate = {2023-06-26},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Hopfield, J J},
	month = apr,
	year = {1982},
	pages = {2554--2558},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	language = {en},
	number = {7553},
	urldate = {2023-06-26},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
}

@book{mitchell_machine_1997,
	address = {New York},
	series = {{McGraw}-{Hill} series in computer science},
	title = {Machine {Learning}},
	isbn = {978-0-07-042807-2},
	publisher = {McGraw-Hill},
	author = {Mitchell, Tom M.},
	year = {1997},
	keywords = {Computer algorithms, Machine learning},
}

@misc{radford_improving_2018,
	title = {Improving language understanding by generative pre-training},
	url = {https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf},
	urldate = {2023-06-24},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	year = {2018},
}

@misc{noauthor_improving_nodate,
	title = {Improving language understanding by generative pre-training},
}

@article{wiskott_face_1997,
	title = {Face recognition by elastic bunch graph matching},
	volume = {19},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/598235/},
	doi = {10.1109/34.598235},
	number = {7},
	urldate = {2023-06-14},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wiskott, Laurenz and Fellous, Jean-Marc and Kuiger, Norbert and Von Der Malsburg, Christoph},
	month = jul,
	year = {1997},
	pages = {775--779},
}

@book{ivakhnenko_cybernetic_1965,
	address = {New York},
	title = {Cybernetic {Predicting} {Devices}},
	publisher = {CCM Information Corporation},
	author = {Ivakhnenko, Aleksei G. and Lapa, Valentin G.},
	year = {1965},
}

@article{wolfrum_recurrent_2008,
	title = {A recurrent dynamic model for correspondence-based face recognition},
	volume = {8},
	issn = {1534-7362},
	url = {http://jov.arvojournals.org/article.aspx?doi=10.1167/8.7.34},
	doi = {10.1167/8.7.34},
	language = {en},
	number = {7},
	urldate = {2023-03-06},
	journal = {Journal of Vision},
	author = {Wolfrum, Philipp and Wolff, Christian and Lücke, Jörg and von der Malsburg, Christoph},
	month = dec,
	year = {2008},
	pages = {34},
}

@misc{von_der_malsburg_theory_2022,
	title = {A {Theory} of {Natural} {Intelligence}},
	url = {http://arxiv.org/abs/2205.00002},
	doi = {10.48550/arXiv.2205.00002},
	abstract = {Introduction: In contrast to current AI technology, natural intelligence -- the kind of autonomous intelligence that is realized in the brains of animals and humans to attain in their natural environment goals defined by a repertoire of innate behavioral schemata -- is far superior in terms of learning speed, generalization capabilities, autonomy and creativity. How are these strengths, by what means are ideas and imagination produced in natural neural networks? Methods: Reviewing the literature, we put forward the argument that both our natural environment and the brain are of low complexity, that is, require for their generation very little information and are consequently both highly structured. We further argue that the structures of brain and natural environment are closely related. Results: We propose that the structural regularity of the brain takes the form of net fragments (self-organized network patterns) and that these serve as the powerful inductive bias that enables the brain to learn quickly, generalize from few examples and bridge the gap between abstractly defined general goals and concrete situations. Conclusions: Our results have important bearings on open problems in artificial neural network research.},
	urldate = {2022-07-11},
	publisher = {arXiv},
	author = {von der Malsburg, Christoph and Stadelmann, Thilo and Grewe, Benjamin F.},
	month = apr,
	year = {2022},
	note = {arXiv:2205.00002 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, I.2, Quantitative Biology - Neurons and Cognition},
}

@article{yarats_improving_2021,
	title = {Improving {Sample} {Efficiency} in {Model}-{Free} {Reinforcement} {Learning} from {Images}},
	volume = {35},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17276},
	doi = {10.1609/aaai.v35i12.17276},
	abstract = {Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. A promising approach is to learn a latent representation together with the control policy. However, fitting a high-capacity encoder using a scarce reward signal is sample inefficient and leads to poor performance.
Prior work has shown that auxiliary losses, such as image reconstruction, can aid efficient representation learning.  
However, incorporating reconstruction loss into an off-policy learning algorithm often leads to training instability. We explore the underlying reasons and 
identify variational autoencoders, used by previous investigations, as the cause of the divergence.   
Following these findings, we propose effective techniques to improve training stability. 
This results in a simple approach capable of
matching state-of-the-art model-free and model-based algorithms on MuJoCo control tasks. Furthermore, our approach demonstrates robustness to observational noise, surpassing existing approaches in this setting. Code, results, and videos are anonymously available at https://sites.google.com/view/sac-ae/home.},
	number = {12},
	urldate = {2023-06-24},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Yarats, Denis and Zhang, Amy and Kostrikov, Ilya and Amos, Brandon and Pineau, Joelle and Fergus, Rob},
	month = may,
	year = {2021},
	pages = {10674--10681},
}

@article{sager_unsupervised_2022,
	title = {Unsupervised {Domain} {Adaptation} for {Vertebrae} {Detection} and {Identification} in {3D} {CT} {Volumes} {Using} a {Domain} {Sanity} {Loss}},
	volume = {8},
	copyright = {All rights reserved},
	issn = {2313-433X},
	url = {https://www.mdpi.com/2313-433X/8/8/222},
	doi = {10.3390/jimaging8080222},
	abstract = {A variety of medical computer vision applications analyze 2D slices of computed tomography (CT) scans, whereas axial slices from the body trunk region are usually identified based on their relative position to the spine. A limitation of such systems is that either the correct slices must be extracted manually or labels of the vertebrae are required for each CT scan to develop an automated extraction system. In this paper, we propose an unsupervised domain adaptation (UDA) approach for vertebrae detection and identification based on a novel Domain Sanity Loss (DSL) function. With UDA the model’s knowledge learned on a publicly available (source) data set can be transferred to the target domain without using target labels, where the target domain is defined by the specific setup (CT modality, study protocols, applied pre- and processing) at the point of use (e.g., a specific clinic with its specific CT study protocols). With our approach, a model is trained on the source and target data set in parallel. The model optimizes a supervised loss for labeled samples from the source domain and the DSL loss function based on domain-specific “sanity checks” for samples from the unlabeled target domain. Without using labels from the target domain, we are able to identify vertebra centroids with an accuracy of 72.8\%. By adding only ten target labels during training the accuracy increases to 89.2\%, which is on par with the current state-of-the-art for full supervised learning, while using about 20 times less labels. Thus, our model can be used to extract 2D slices from 3D CT scans on arbitrary data sets fully automatically without requiring an extensive labeling effort, contributing to the clinical adoption of medical imaging by hospitals.},
	language = {en},
	number = {8},
	urldate = {2022-09-01},
	journal = {Journal of Imaging},
	author = {Sager, Pascal and Salzmann, Sebastian and Burn, Felice and Stadelmann, Thilo},
	month = aug,
	year = {2022},
	pages = {222},
}

@article{long_survey_2022,
	title = {A survey on adversarial attacks in computer vision: {Taxonomy}, visualization and future directions},
	volume = {121},
	issn = {01674048},
	shorttitle = {A survey on adversarial attacks in computer vision},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167404822002413},
	doi = {10.1016/j.cose.2022.102847},
	language = {en},
	urldate = {2023-06-24},
	journal = {Computers \& Security},
	author = {Long, Teng and Gao, Qi and Xu, Lili and Zhou, Zhangbing},
	month = oct,
	year = {2022},
	pages = {102847},
}

@misc{rosenbloom_defining_2023,
	title = {Defining and {Explorting} the {Intelligence} {Space}},
	url = {http://arxiv.org/abs/2306.06499},
	doi = {10.48550/arXiv.2306.06499},
	abstract = {Intelligence is a difficult concept to define, despite many attempts at doing so. Rather than trying to settle on a single definition, this article introduces a broad perspective on what intelligence is, by laying out a cascade of definitions that induces both a nested hierarchy of three levels of intelligence and a wider-ranging space that is built around them and approximations to them. Within this intelligence space, regions are identified that correspond to both natural -- most particularly, human -- intelligence and artificial intelligence (AI), along with the crossover notion of humanlike intelligence. These definitions are then exploited in early explorations of four more advanced, and likely more controversial, topics: the singularity, generative AI, ethics, and intellectual property.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {Rosenbloom, Paul S.},
	month = jun,
	year = {2023},
	note = {arXiv:2306.06499 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{mitchell_debate_2023,
	title = {The debate over understanding in {AI}’s large language models},
	volume = {120},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/10.1073/pnas.2215907120},
	doi = {10.1073/pnas.2215907120},
	abstract = {We survey a current, heated debate in the artificial intelligence (AI) research community on whether large pretrained language models can be said to understand language—and the physical and social situations language encodes—in any humanlike sense. We describe arguments that have been made for and against such understanding and key questions for the broader sciences of intelligence that have arisen in light of these arguments. We contend that an extended science of intelligence can be developed that will provide insight into distinct modes of understanding, their strengths and limitations, and the challenge of integrating diverse forms of cognition.},
	language = {en},
	number = {13},
	urldate = {2023-06-24},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mitchell, Melanie and Krakauer, David C.},
	month = mar,
	year = {2023},
	pages = {e2215907120},
}

@article{bertolini_machine_2021,
	title = {Machine {Learning} for industrial applications: {A} comprehensive literature review},
	volume = {175},
	issn = {09574174},
	shorttitle = {Machine {Learning} for industrial applications},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095741742100261X},
	doi = {10.1016/j.eswa.2021.114820},
	language = {en},
	urldate = {2023-06-24},
	journal = {Expert Systems with Applications},
	author = {Bertolini, Massimo and Mezzogori, Davide and Neroni, Mattia and Zammori, Francesco},
	month = aug,
	year = {2021},
	pages = {114820},
}

@article{ning_review_2019,
	title = {A {Review} of {Deep} {Learning} {Based} {Speech} {Synthesis}},
	volume = {9},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/9/19/4050},
	doi = {10.3390/app9194050},
	abstract = {Speech synthesis, also known as text-to-speech (TTS), has attracted increasingly more attention. Recent advances on speech synthesis are overwhelmingly contributed by deep learning or even end-to-end techniques which have been utilized to enhance a wide range of application scenarios such as intelligent speech interaction, chatbot or conversational artificial intelligence (AI). For speech synthesis, deep learning based techniques can leverage a large scale of {\textless}text, speech{\textgreater} pairs to learn effective feature representations to bridge the gap between text and speech, thus better characterizing the properties of events. To better understand the research dynamics in the speech synthesis field, this paper firstly introduces the traditional speech synthesis methods and highlights the importance of the acoustic modeling from the composition of the statistical parametric speech synthesis (SPSS) system. It then gives an overview of the advances on deep learning based speech synthesis, including the end-to-end approaches which have achieved start-of-the-art performance in recent years. Finally, it discusses the problems of the deep learning methods for speech synthesis, and also points out some appealing research directions that can bring the speech synthesis research into a new frontier.},
	language = {en},
	number = {19},
	urldate = {2023-06-24},
	journal = {Applied Sciences},
	author = {Ning, Yishuang and He, Sheng and Wu, Zhiyong and Xing, Chunxiao and Zhang, Liang-Jie},
	month = sep,
	year = {2019},
	pages = {4050},
}

@article{otter_survey_2021,
	title = {A {Survey} of the {Usages} of {Deep} {Learning} for {Natural} {Language} {Processing}},
	volume = {32},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/9075398/},
	doi = {10.1109/TNNLS.2020.2979670},
	number = {2},
	urldate = {2023-06-24},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Otter, Daniel W. and Medina, Julian R. and Kalita, Jugal K.},
	month = feb,
	year = {2021},
	pages = {604--624},
}

@article{bhatt_cnn_2021,
	title = {{CNN} {Variants} for {Computer} {Vision}: {History}, {Architecture}, {Application}, {Challenges} and {Future} {Scope}},
	volume = {10},
	issn = {2079-9292},
	shorttitle = {{CNN} {Variants} for {Computer} {Vision}},
	url = {https://www.mdpi.com/2079-9292/10/20/2470},
	doi = {10.3390/electronics10202470},
	abstract = {Computer vision is becoming an increasingly trendy word in the area of image processing. With the emergence of computer vision applications, there is a significant demand to recognize objects automatically. Deep CNN (convolution neural network) has benefited the computer vision community by producing excellent results in video processing, object recognition, picture classification and segmentation, natural language processing, speech recognition, and many other fields. Furthermore, the introduction of large amounts of data and readily available hardware has opened new avenues for CNN study. Several inspirational concepts for the progress of CNN have been investigated, including alternative activation functions, regularization, parameter optimization, and architectural advances. Furthermore, achieving innovations in architecture results in a tremendous enhancement in the capacity of the deep CNN. Significant emphasis has been given to leveraging channel and spatial information, with a depth of architecture and information processing via multi-path. This survey paper focuses mainly on the primary taxonomy and newly released deep CNN architectures, and it divides numerous recent developments in CNN architectures into eight groups. Spatial exploitation, multi-path, depth, breadth, dimension, channel boosting, feature-map exploitation, and attention-based CNN are the eight categories. The main contribution of this manuscript is in comparing various architectural evolutions in CNN by its architectural change, strengths, and weaknesses. Besides, it also includes an explanation of the CNN’s components, the strengths and weaknesses of various CNN variants, research gap or open challenges, CNN applications, and the future research direction.},
	language = {en},
	number = {20},
	urldate = {2023-06-24},
	journal = {Electronics},
	author = {Bhatt, Dulari and Patel, Chirag and Talsania, Hardik and Patel, Jigar and Vaghela, Rasmika and Pandya, Sharnil and Modi, Kirit and Ghayvat, Hemant},
	month = oct,
	year = {2021},
	pages = {2470},
}

@article{dabre_survey_2021,
	title = {A {Survey} of {Multilingual} {Neural} {Machine} {Translation}},
	volume = {53},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3406095},
	doi = {10.1145/3406095},
	abstract = {We present a survey on multilingual neural machine translation (MNMT), which has gained a lot of traction in recent years. MNMT has been useful in improving translation quality as a result of translation knowledge transfer (transfer learning). MNMT is more promising and interesting than its statistical machine translation counterpart, because end-to-end modeling and distributed representations open new avenues for research on machine translation. Many approaches have been proposed to exploit multilingual parallel corpora for improving translation quality. However, the lack of a comprehensive survey makes it difficult to determine which approaches are promising and, hence, deserve further exploration. In this article, we present an in-depth survey of existing literature on MNMT. We first categorize various approaches based on their central use-case and then further categorize them based on resource scenarios, underlying modeling principles, core-issues, and challenges. Wherever possible, we address the strengths and weaknesses of several techniques by comparing them with each other. We also discuss the future directions for MNMT. This article is aimed towards both beginners and experts in NMT. We hope this article will serve as a starting point as well as a source of new ideas for researchers and engineers interested in MNMT.},
	language = {en},
	number = {5},
	urldate = {2023-06-24},
	journal = {ACM Computing Surveys},
	author = {Dabre, Raj and Chu, Chenhui and Kunchukuttan, Anoop},
	month = sep,
	year = {2021},
	pages = {1--38},
}

@misc{liu_summary_2023,
	title = {Summary of {ChatGPT}/{GPT}-4 {Research} and {Perspective} {Towards} the {Future} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2304.01852},
	doi = {10.48550/arXiv.2304.01852},
	abstract = {This paper presents a comprehensive survey of ChatGPT and GPT-4, state-of-the-art large language models (LLM) from the GPT series, and their prospective applications across diverse domains. Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability and performance. We performed an in-depth analysis of 194 relevant papers on arXiv, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains. The findings reveal a significant and increasing interest in ChatGPT/GPT-4 research, predominantly centered on direct natural language processing applications, while also demonstrating considerable potential in areas ranging from education and history to mathematics, medicine, and physics. This study endeavors to furnish insights into ChatGPT's capabilities, potential implications, ethical concerns, and offer direction for future advancements in this field.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {Liu, Yiheng and Han, Tianle and Ma, Siyuan and Zhang, Jiayue and Yang, Yuanyuan and Tian, Jiaming and He, Hao and Li, Antong and He, Mengshen and Liu, Zhengliang and Wu, Zihao and Zhu, Dajiang and Li, Xiang and Qiang, Ning and Shen, Dingang and Liu, Tianming and Ge, Bao},
	month = may,
	year = {2023},
	note = {arXiv:2304.01852 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{bubeck_sparks_2023,
	title = {Sparks of {Artificial} {General} {Intelligence}: {Early} experiments with {GPT}-4},
	shorttitle = {Sparks of {Artificial} {General} {Intelligence}},
	url = {http://arxiv.org/abs/2303.12712},
	doi = {10.48550/arXiv.2303.12712},
	abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
	month = apr,
	year = {2023},
	note = {arXiv:2303.12712 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

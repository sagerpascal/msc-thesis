
@misc{lau_proximal_2018,
	title = {A {Proximal} {Block} {Coordinate} {Descent} {Algorithm} for {Deep} {Neural} {Network} {Training}},
	url = {http://arxiv.org/abs/1803.09082},
	doi = {10.48550/arXiv.1803.09082},
	abstract = {Training deep neural networks (DNNs) efficiently is a challenge due to the associated highly nonconvex optimization. The backpropagation (backprop) algorithm has long been the most widely used algorithm for gradient computation of parameters of DNNs and is used along with gradient descent-type algorithms for this optimization task. Recent work have shown the efficiency of block coordinate descent (BCD) type methods empirically for training DNNs. In view of this, we propose a novel algorithm based on the BCD method for training DNNs and provide its global convergence results built upon the powerful framework of the Kurdyka-Lojasiewicz (KL) property. Numerical experiments on standard datasets demonstrate its competitive efficiency against standard optimizers with backprop.},
	urldate = {2023-07-04},
	publisher = {arXiv},
	author = {Lau, Tim Tsz-Kit and Zeng, Jinshan and Wu, Baoyuan and Yao, Yuan},
	month = mar,
	year = {2018},
	note = {arXiv:1803.09082 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@inproceedings{zhang_convergent_2017,
	title = {Convergent {Block} {Coordinate} {Descent} for {Training} {Tikhonov} {Regularized} {Deep} {Neural} {Networks}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6a2feef8ed6a9fe76d6b3f30f02150b4-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Ziming and Brand, Matthew},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@inproceedings{taylor_training_2016,
	series = {{ICML}'16},
	title = {Training {Neural} {Networks} without {Gradients}: {A} {Scalable} {ADMM} {Approach}},
	abstract = {With the growing importance of large network models and enormous training datasets, GPUs have become increasingly necessary to train neural networks. This is largely because conventional optimization algorithms rely on stochastic gradient methods that don't scale well to large numbers of cores in a cluster setting. Furthermore, the convergence of all gradient methods, including batch methods, suffers from common problems like saturation effects, poor conditioning, and saddle points. This paper explores an unconventional training method that uses alternating direction methods and Bregman iteration to train networks without gradient descent steps. The proposed method reduces the network training problem to a sequence of minimization substeps that can each be solved globally in closed form. The proposed method is advantageous because it avoids many of the caveats that make gradient methods slow on highly nonconvex problems. The method exhibits strong scaling in the distributed setting, yielding linear speedups even when split over thousands of cores.},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 48},
	publisher = {JMLR.org},
	author = {Taylor, Gavin and Burmeister, Ryan and Xu, Zheng and Singh, Bharat and Patel, Ankit and Goldstein, Tom},
	year = {2016},
	note = {Place: New York, NY, USA},
	pages = {2722--2731},
}

@inproceedings{carreira-perpinan_distributed_2014,
	address = {Reykjavik, Iceland},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Distributed optimization of deeply nested systems},
	volume = {33},
	url = {https://proceedings.mlr.press/v33/carreira-perpinan14.html},
	abstract = {Intelligent processing of complex signals such as images is often performed by a hierarchy of nonlinear processing layers, such as a deep net or an object recognition cascade. Joint estimation of the parameters of all the layers is a difficult nonconvex optimization. We describe a general strategy to learn the parameters and, to some extent, the architecture of nested systems, which we call the method of auxiliary coordinates (MAC). This replaces the original problem involving a deeply nested function with a constrained problem involving a different function in an augmented space without nesting. The constrained problem may be solved with penalty-based methods using alternating optimization over the parameters and the auxiliary coordinates. MAC has provable convergence, is easy to implement reusing existing algorithms for single layers, can be parallelized trivially and massively, applies even when parameter derivatives are not available or not desirable, can perform some model selection on the fly, and is competitive with state-of-the-art nonlinear optimizers even in the serial computation setting, often providing reasonable models within a few iterations.},
	booktitle = {Proceedings of the {Seventeenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Carreira-Perpinan, Miguel and Wang, Weiran},
	editor = {Kaski, Samuel and Corander, Jukka},
	month = apr,
	year = {2014},
	pages = {10--19},
}

@inproceedings{liao_how_2016,
	series = {{AAAI}'16},
	title = {How {Important} is {Weight} {Symmetry} in {Backpropagation}?},
	abstract = {Gradient backpropagation (BP) requires symmetric feedforward and feedback connections—the same weights must be used for forward and backward passes. This "weight transport problem" (Grossberg 1987) is thought to be one of the main reasons to doubt BP's biologically plausibility. Using 15 different classification datasets, we systematically investigate to what extent BP really depends on weight symmetry. In a study that turned out to be surprisingly similar in spirit to Lillicrap et al.'s demonstration (Lillicrap et al. 2014) but orthogonal in its results, our experiments indicate that: (1) the magnitudes of feedback weights do not matter to performance (2) the signs of feedback weights do matter—the more concordant signs between feedforward and their corresponding feedback connections, the better (3) with feedback weights having random magnitudes and 100\% concordant signs, we were able to achieve the same or even better performance than SGD. (4) some normalizations/stabilizations are indispensable for such asymmetric BP to work, namely Batch Normalization (BN) (Ioffe and Szegedy 2015) and/or a "Batch Manhattan" (BM) update rule.},
	booktitle = {Proceedings of the {Thirtieth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Liao, Qianli and Leibo, Joel Z. and Poggio, Tomaso},
	year = {2016},
	note = {Place: Phoenix, Arizona},
	pages = {1837--1844},
}

@article{lillicrap_random_2016,
	title = {Random synaptic feedback weights support error backpropagation for deep learning},
	volume = {7},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/ncomms13276},
	doi = {10.1038/ncomms13276},
	abstract = {Abstract
            The brain processes information through multiple layers of neurons. This deep architecture is representationally powerful, but complicates learning because it is difficult to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame by multiplying error signals with all the synaptic weights on each neuron’s axon and further downstream. However, this involves a precise, symmetric backward connectivity pattern, which is thought to be impossible in the brain. Here we demonstrate that this strong architectural constraint is not required for effective error propagation. We present a surprisingly simple mechanism that assigns blame by multiplying errors by even random synaptic weights. This mechanism can transmit teaching signals across multiple layers of neurons and performs as effectively as backpropagation on a variety of tasks. Our results help reopen questions about how the brain could use error signals and dispel long-held assumptions about algorithmic constraints on learning.},
	language = {en},
	number = {1},
	urldate = {2023-07-04},
	journal = {Nature Communications},
	author = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
	month = nov,
	year = {2016},
	pages = {13276},
}

@misc{lansdell_learning_2020,
	title = {Learning to solve the credit assignment problem},
	url = {http://arxiv.org/abs/1906.00889},
	doi = {10.48550/arXiv.1906.00889},
	abstract = {Backpropagation is driving today's artificial neural networks (ANNs). However, despite extensive research, it remains unclear if the brain implements this algorithm. Among neuroscientists, reinforcement learning (RL) algorithms are often seen as a realistic alternative: neurons can randomly introduce change, and use unspecific feedback signals to observe their effect on the cost and thus approximate their gradient. However, the convergence rate of such learning scales poorly with the number of involved neurons. Here we propose a hybrid learning approach. Each neuron uses an RL-type strategy to learn how to approximate the gradients that backpropagation would provide. We provide proof that our approach converges to the true gradient for certain classes of networks. In both feedforward and convolutional networks, we empirically show that our approach learns to approximate the gradient, and can match or the performance of exact gradient-based learning. Learning feedback weights provides a biologically plausible mechanism of achieving good performance, without the need for precise, pre-specified learning rules.},
	urldate = {2023-07-04},
	publisher = {arXiv},
	author = {Lansdell, Benjamin James and Prakash, Prashanth Ravi and Kording, Konrad Paul},
	month = apr,
	year = {2020},
	note = {arXiv:1906.00889 [cs, q-bio]},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
}

@inproceedings{czarnecki_understanding_2017,
	series = {{ICML}'17},
	title = {Understanding {Synthetic} {Gradients} and {Decoupled} {Neural} {Interfaces}},
	abstract = {When training neural networks, the use of Synthetic Gradients (SG) allows layers or modules to be trained without update locking - without waiting for a true error gradient to be backpropagated - resulting in Decoupled Neural Interfaces (DNIs). This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al. (2016). However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view. In this paper, we study DNIs through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation. We show that the incorporation of SGs does not affect the representational strength of the learning system for a neural network, and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approx-imate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations. Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison.},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning} - {Volume} 70},
	publisher = {JMLR.org},
	author = {Czarnecki, Wojciech Marian and Swirszcz, Grzegorz and Jaderberg, Max and Osindero, Simon and Vinyals, Oriol and Kavukcuoglu, Koray},
	year = {2017},
	note = {Place: Sydney, NSW, Australia},
	pages = {904--912},
}

@inproceedings{jaderberg_decoupled_2017,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Decoupled {Neural} {Interfaces} using {Synthetic} {Gradients}},
	volume = {70},
	url = {https://proceedings.mlr.press/v70/jaderberg17a.html},
	abstract = {Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled {\textless}em{\textgreater}synthetic gradient{\textless}/em{\textgreater} in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise {\textless}em{\textgreater}decoupled neural interfaces{\textless}/em{\textgreater}. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one’s future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass – amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
	editor = {Precup, Doina and Teh, Yee Whye},
	month = aug,
	year = {2017},
	pages = {1627--1635},
}

@incollection{wagner_robustness_2013,
	title = {Robustness in {Natural} {Systems} and {Self}-{Organization}},
	isbn = {978-1-4008-4938-3},
	url = {https://www.degruyter.com/document/doi/10.1515/9781400849383.297/html},
	urldate = {2023-07-03},
	booktitle = {Robustness and {Evolvability} in {Living} {Systems}},
	publisher = {Princeton University Press},
	collaborator = {Wagner, Andreas},
	month = dec,
	year = {2013},
	doi = {10.1515/9781400849383.297},
	pages = {297--309},
}

@inproceedings{meulemans_theoretical_2020,
	title = {A {Theoretical} {Framework} for {Target} {Propagation}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/e7a425c6ece20cbc9056f98699b53c6f-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Meulemans, Alexander and Carzaniga, Francesco and Suykens, Johan and Sacramento, João and Grewe, Benjamin F.},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {20024--20036},
}

@incollection{appice_difference_2015,
	address = {Cham},
	title = {Difference {Target} {Propagation}},
	volume = {9284},
	isbn = {978-3-319-23527-1 978-3-319-23528-8},
	url = {http://link.springer.com/10.1007/978-3-319-23528-8_31},
	language = {en},
	urldate = {2023-07-04},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer International Publishing},
	author = {Lee, Dong-Hyun and Zhang, Saizheng and Fischer, Asja and Bengio, Yoshua},
	editor = {Appice, Annalisa and Rodrigues, Pedro Pereira and Santos Costa, Vítor and Soares, Carlos and Gama, João and Jorge, Alípio},
	year = {2015},
	doi = {10.1007/978-3-319-23528-8_31},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {498--515},
}

@misc{bengio_how_2014,
	title = {How {Auto}-{Encoders} {Could} {Provide} {Credit} {Assignment} in {Deep} {Networks} via {Target} {Propagation}},
	url = {http://arxiv.org/abs/1407.7906},
	doi = {10.48550/arXiv.1407.7906},
	abstract = {We propose to exploit \{{\textbackslash}em reconstruction\} as a layer-local training signal for deep learning. Reconstructions can be propagated in a form of target propagation playing a role similar to back-propagation but helping to reduce the reliance on derivatives in order to perform credit assignment across many levels of possibly strong non-linearities (which is difficult for back-propagation). A regularized auto-encoder tends produce a reconstruction that is a more likely version of its input, i.e., a small move in the direction of higher likelihood. By generalizing gradients, target propagation may also allow to train deep networks with discrete hidden units. If the auto-encoder takes both a representation of input and target (or of any side information) in input, then its reconstruction of input representation provides a target towards a representation that is more likely, conditioned on all the side information. A deep auto-encoder decoding path generalizes gradient propagation in a learned way that can could thus handle not just infinitesimal changes but larger, discrete changes, hopefully allowing credit assignment through a long chain of non-linear operations. In addition to each layer being a good auto-encoder, the encoder also learns to please the upper layers by transforming the data into a space where it is easier to model by them, flattening manifolds and disentangling factors. The motivations and theoretical justifications for this approach are laid down in this paper, along with conjectures that will have to be verified either mathematically or experimentally, including a hypothesis stating that such auto-encoder mediated target propagation could play in brains the role of credit assignment through many non-linear, noisy and discrete transformations.},
	urldate = {2023-07-04},
	publisher = {arXiv},
	author = {Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv:1407.7906 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{bartunov_assessing_2018,
	title = {Assessing the {Scalability} of {Biologically}-{Motivated} {Deep} {Learning} {Algorithms} and {Architectures}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/63c3ddcc7b23daa1e42dc41f9a44a873-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bartunov, Sergey and Santoro, Adam and Richards, Blake and Marris, Luke and Hinton, Geoffrey E and Lillicrap, Timothy},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
}

@misc{hinton_forward-forward_2022,
	title = {The {Forward}-{Forward} {Algorithm}: {Some} {Preliminary} {Investigations}},
	url = {https://www.cs.toronto.edu/~hinton/FFA13.pdf},
	urldate = {2022-12-17},
	author = {Hinton, Geoffrey E.},
	year = {2022},
}

@inproceedings{belilovsky_decoupled_2020,
	series = {{ICML}'20},
	title = {Decoupled {Greedy} {Learning} of {CNNs}},
	abstract = {A commonly cited inefficiency of neural network training by back-propagation is the update locking problem: each layer must wait for the signal to propagate through the full network before updating. Several alternatives that can alleviate this issue have been proposed. In this context, we consider a simpler, but more effective, substitute that uses minimal feedback, which we call Decoupled Greedy Learning (DGL). It is based on a greedy relaxation of the joint training objective, recently shown to be effective in the context of Convolutional Neural Networks (CNNs) on large-scale image classification. We consider an optimization of this objective that permits us to decouple the layer training, allowing for layers or modules in networks to be trained with a potentially linear parallelization in layers. With the use of a replay buffer we show this approach can be extended to asynchronous settings, where modules can operate with possibly large communication delays. We show theoretically and empirically that this approach converges. Then, we empirically find that it can lead to better generalization than sequential greedy optimization. We demonstrate the effectiveness of DGL against alternative approaches on the CIFAR-10 dataset and on the large-scale ImageNet dataset.},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	publisher = {JMLR.org},
	author = {Belilovsky, Eugene and Eickenberg, Michael and Oyallon, Edouard},
	year = {2020},
	pages = {736--745},
}

@inproceedings{nokland_training_2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Training {Neural} {Networks} with {Local} {Error} {Signals}},
	volume = {97},
	url = {https://proceedings.mlr.press/v97/nokland19a.html},
	abstract = {Supervised training of neural networks for classification is typically performed with a global loss function. The loss function provides a gradient for the output layer, and this gradient is back-propagated to hidden layers to dictate an update direction for the weights. An alternative approach is to train the network with layer-wise loss functions. In this paper we demonstrate, for the first time, that layer-wise training can approach the state-of-the-art on a variety of image datasets. We use single-layer sub-networks and two different supervised loss functions to generate local error signals for the hidden layers, and we show that the combination of these losses help with optimization in the context of local learning. Using local errors could be a step towards more biologically plausible deep learning because the global error does not have to be transported back to hidden layers. A completely backprop free variant outperforms previously reported results among methods aiming for higher biological plausibility.},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Nøkland, Arild and Eidnes, Lars Hiller},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	month = jun,
	year = {2019},
	pages = {4839--4850},
}

@article{marquez_deep_2018,
	title = {Deep {Cascade} {Learning}},
	volume = {29},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/8307262/},
	doi = {10.1109/TNNLS.2018.2805098},
	number = {11},
	urldate = {2023-07-04},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Marquez, Enrique S. and Hare, Jonathon S. and Niranjan, Mahesan},
	month = nov,
	year = {2018},
	pages = {5475--5485},
}

@article{mostafa_deep_2018,
	title = {Deep {Supervised} {Learning} {Using} {Local} {Errors}},
	volume = {12},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/article/10.3389/fnins.2018.00608/full},
	doi = {10.3389/fnins.2018.00608},
	urldate = {2023-07-04},
	journal = {Frontiers in Neuroscience},
	author = {Mostafa, Hesham and Ramesh, Vishwajith and Cauwenberghs, Gert},
	month = aug,
	year = {2018},
	pages = {608},
}

@inproceedings{belilovsky_greedy_2019,
	title = {Greedy {Layerwise} {Learning} {Can} {Scale} to {ImageNet}},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	publisher = {PMLR},
	author = {Belilovsky, Eugene and Eickenberg, Michael and Oyallon, Edouard},
	year = {2019},
	pages = {583--593},
}

@misc{wang_revisiting_2021,
	title = {Revisiting {Locally} {Supervised} {Learning}: an {Alternative} to {End}-to-end {Training}},
	shorttitle = {Revisiting {Locally} {Supervised} {Learning}},
	url = {http://arxiv.org/abs/2101.10832},
	doi = {10.48550/arXiv.2101.10832},
	abstract = {Due to the need to store the intermediate activations for back-propagation, end-to-end (E2E) training of deep networks usually suffers from high GPUs memory footprint. This paper aims to address this problem by revisiting the locally supervised learning, where a network is split into gradient-isolated modules and trained with local supervision. We experimentally show that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, we propose an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. As InfoPro loss is difficult to compute in its original form, we derive a feasible upper bound as a surrogate optimization objective, yielding a simple but effective algorithm. In fact, we show that the proposed method boils down to minimizing the combination of a reconstruction loss and a normal cross-entropy/contrastive term. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate that InfoPro is capable of achieving competitive performance with less than 40\% memory footprint compared to E2E training, while allowing using training data with higher-resolution or larger batch sizes under the same GPU memory constraint. Our method also enables training local modules asynchronously for potential training acceleration. Code is available at: https://github.com/blackfeather-wang/InfoPro-Pytorch.},
	urldate = {2023-07-04},
	publisher = {arXiv},
	author = {Wang, Yulin and Ni, Zanlin and Song, Shiji and Yang, Le and Huang, Gao},
	month = jan,
	year = {2021},
	note = {arXiv:2101.10832 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{duan_kernel_2020,
	title = {On {Kernel} {Method}–{Based} {Connectionist} {Models} and {Supervised} {Deep} {Learning} {Without} {Backpropagation}},
	volume = {32},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/32/1/97-135/95570},
	doi = {10.1162/neco_a_01250},
	abstract = {We propose a novel family of connectionist models based on kernel machines and consider the problem of learning layer by layer a compositional hypothesis class (i.e., a feedforward, multilayer architecture) in a supervised setting. In terms of the models, we present a principled method to “kernelize” (partly or completely) any neural network (NN). With this method, we obtain a counterpart of any given NN that is powered by kernel machines instead of neurons. In terms of learning, when learning a feedforward deep architecture in a supervised setting, one needs to train all the components simultaneously using backpropagation (BP) since there are no explicit targets for the hidden layers (Rumelhart, Hinton, \& Williams, 1986 ). We consider without loss of generality the two-layer case and present a general framework that explicitly characterizes a target for the hidden layer that is optimal for minimizing the objective function of the network. This characterization then makes possible a purely greedy training scheme that learns one layer at a time, starting from the input layer. We provide instantiations of the abstract framework under certain architectures and objective functions. Based on these instantiations, we present a layer-wise training algorithm for an [Formula: see text]-layer feedforward network for classification, where [Formula: see text] can be arbitrary. This algorithm can be given an intuitive geometric interpretation that makes the learning dynamics transparent. Empirical results are provided to complement our theory. We show that the kernelized networks, trained layer-wise, compare favorably with classical kernel machines as well as other connectionist models trained by BP. We also visualize the inner workings of the greedy kernelized models to validate our claim on the transparency of the layer-wise algorithm.},
	language = {en},
	number = {1},
	urldate = {2023-07-04},
	journal = {Neural Computation},
	author = {Duan, Shiyu and Yu, Shujian and Chen, Yunmei and Principe, Jose C.},
	month = jan,
	year = {2020},
	pages = {97--135},
}

@article{duan_modularizing_2022,
	title = {Modularizing {Deep} {Learning} via {Pairwise} {Learning} {With} {Kernels}},
	volume = {33},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/9314071/},
	doi = {10.1109/TNNLS.2020.3042346},
	number = {4},
	urldate = {2023-07-04},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Duan, Shiyu and Yu, Shujian and Principe, Jose C.},
	month = apr,
	year = {2022},
	pages = {1441--1451},
}

@inproceedings{ho_random_1995,
	address = {Montreal, Que., Canada},
	title = {Random decision forests},
	volume = {1},
	isbn = {978-0-8186-7128-9},
	url = {http://ieeexplore.ieee.org/document/598994/},
	doi = {10.1109/ICDAR.1995.598994},
	urldate = {2023-07-01},
	booktitle = {Proceedings of 3rd {International} {Conference} on {Document} {Analysis} and {Recognition}},
	publisher = {IEEE Comput. Soc. Press},
	author = {Ho, Tin Kam},
	year = {1995},
	pages = {278--282},
}

@article{poljak_experimental_1927,
	title = {An experimental study of the association callosal, and projection fibers of the cerebral cortex of the cat},
	volume = {44},
	issn = {0021-9967, 1096-9861},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/cne.900440202},
	doi = {10.1002/cne.900440202},
	language = {en},
	number = {2},
	urldate = {2023-07-02},
	journal = {The Journal of Comparative Neurology},
	author = {Poljak, Stjepan},
	month = dec,
	year = {1927},
	pages = {197--258},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {00189219},
	url = {http://ieeexplore.ieee.org/document/726791/},
	doi = {10.1109/5.726791},
	number = {11},
	urldate = {2023-06-30},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
	month = nov,
	year = {1998},
	pages = {2278--2324},
}

@article{kolmogorov_tables_1998,
	title = {On tables of random numbers},
	volume = {207},
	issn = {03043975},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304397598000759},
	doi = {10.1016/S0304-3975(98)00075-9},
	language = {en},
	number = {2},
	urldate = {2023-06-30},
	journal = {Theoretical Computer Science},
	author = {Kolmogorov, Andrei N.},
	month = nov,
	year = {1998},
	pages = {387--395},
}

@article{friston_active_2016,
	title = {Active inference and learning},
	volume = {68},
	issn = {01497634},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0149763416301336},
	doi = {10.1016/j.neubiorev.2016.06.022},
	language = {en},
	urldate = {2023-07-01},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Friston, Karl and FitzGerald, Thomas and Rigoli, Francesco and Schwartenbeck, Philipp and O'Doherty, John and Pezzulo, Giovanni},
	month = sep,
	year = {2016},
	pages = {862--879},
}

@article{duan_kernel_2020-1,
	title = {On {Kernel} {Method}–{Based} {Connectionist} {Models} and {Supervised} {Deep} {Learning} {Without} {Backpropagation}},
	volume = {32},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/32/1/97-135/95570},
	doi = {10.1162/neco_a_01250},
	abstract = {We propose a novel family of connectionist models based on kernel machines and consider the problem of learning layer by layer a compositional hypothesis class (i.e., a feedforward, multilayer architecture) in a supervised setting. In terms of the models, we present a principled method to “kernelize” (partly or completely) any neural network (NN). With this method, we obtain a counterpart of any given NN that is powered by kernel machines instead of neurons. In terms of learning, when learning a feedforward deep architecture in a supervised setting, one needs to train all the components simultaneously using backpropagation (BP) since there are no explicit targets for the hidden layers (Rumelhart, Hinton, \& Williams, 1986 ). We consider without loss of generality the two-layer case and present a general framework that explicitly characterizes a target for the hidden layer that is optimal for minimizing the objective function of the network. This characterization then makes possible a purely greedy training scheme that learns one layer at a time, starting from the input layer. We provide instantiations of the abstract framework under certain architectures and objective functions. Based on these instantiations, we present a layer-wise training algorithm for an [Formula: see text]-layer feedforward network for classification, where [Formula: see text] can be arbitrary. This algorithm can be given an intuitive geometric interpretation that makes the learning dynamics transparent. Empirical results are provided to complement our theory. We show that the kernelized networks, trained layer-wise, compare favorably with classical kernel machines as well as other connectionist models trained by BP. We also visualize the inner workings of the greedy kernelized models to validate our claim on the transparency of the layer-wise algorithm.},
	language = {en},
	number = {1},
	urldate = {2023-07-04},
	journal = {Neural Computation},
	author = {Duan, Shiyu and Yu, Shujian and Chen, Yunmei and Principe, Jose C.},
	month = jan,
	year = {2020},
	pages = {97--135},
}

@article{grossberg_neural_1989,
	title = {Neural dynamics of adaptive timing and temporal discrimination during associative learning},
	volume = {2},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0893608089900269},
	doi = {10.1016/0893-6080(89)90026-9},
	language = {en},
	number = {2},
	urldate = {2023-07-03},
	journal = {Neural Networks},
	author = {Grossberg, Stephen and Schmajuk, Nestor A.},
	month = jan,
	year = {1989},
	pages = {79--102},
}

@incollection{rumelhart_learning_1986,
	address = {Cambridge, MA, USA},
	title = {Learning {Internal} {Representations} by {Error} {Propagation}},
	isbn = {0-262-68053-X},
	booktitle = {Parallel {Distributed} {Processing}: {Explorations} in the {Microstructure} of {Cognition}, {Vol}. 1: {Foundations}},
	publisher = {MIT Press},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	year = {1986},
	pages = {318--362},
}

@incollection{anderson_biased_1998,
	title = {Biased {Random}-{Walk} {Learning}: {A} {Neurobiological} {Correlate} to {Trial}-and-{Error}},
	isbn = {978-0-12-526420-4},
	shorttitle = {Biased {Random}-{Walk} {Learning}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780125264204500082},
	language = {en},
	urldate = {2023-07-03},
	booktitle = {Neural {Networks} and {Pattern} {Recognition}},
	publisher = {Elsevier},
	author = {Anderson, Russell W.},
	year = {1998},
	doi = {10.1016/B978-012526420-4/50008-2},
	pages = {221--244},
}

@article{mel_nmda-based_1992,
	title = {{NMDA}-{Based} {Pattern} {Discrimination} in a {Modeled} {Cortical} {Neuron}},
	volume = {4},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/4/4/502-517/5650},
	doi = {10.1162/neco.1992.4.4.502},
	abstract = {Compartmental simulations of an anatomically characterized cortical pyramidal cell were carried out to study the integrative behavior of a complex dendritic tree. Previous theoretical (Feldman and Ballard 1982; Durbin and Rumelhart 1989; Mel 1990; Mel and Koch 1990; Poggio and Girosi 1990) and compartmental modeling (Koch et al. 1983; Shepherd et al. 1985; Koch and Poggio 1987; Rall and Segev 1987; Shepherd and Brayton 1987; Shepherd et al. 1989; Brown et al. 1991) work had suggested that multiplicative interactions among groups of neighboring synapses could greatly enhance the processing power of a neuron relative to a unit with only a single global firing threshold. This issue was investigated here, with a particular focus on the role of voltage-dependent N-methyl-D-asparate (NMDA) channels in the generation of cell responses. First, it was found that when a large proportion of the excitatory synaptic input to dendritic spines is carried by NMDA channels, the pyramidal cell responds preferentially to spatially clustered, rather than random, distributions of activated synapses. Second, based on this mechanism, the NMDA-rich neuron is shown to be capable of solving a nonlinear pattern discrimination task. We propose that manipulation of the spatial ordering of afferent synaptic connections onto the dendritic arbor is a possible biological strategy for pattern information storage during learning.},
	language = {en},
	number = {4},
	urldate = {2023-07-03},
	journal = {Neural Computation},
	author = {Mel, Bartlett W.},
	month = jul,
	year = {1992},
	pages = {502--517},
}

@article{montague_spatial_1991,
	title = {Spatial {Signaling} in the {Development} and {Function} of {Neural} {Connections}},
	volume = {1},
	issn = {1047-3211, 1460-2199},
	url = {https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/1.3.199},
	doi = {10.1093/cercor/1.3.199},
	language = {en},
	number = {3},
	urldate = {2023-07-03},
	journal = {Cerebral Cortex},
	author = {Montague, P. Read and Gally, Joseph A. and Edelman, Gerald M.},
	year = {1991},
	pages = {199--220},
}

@article{grajski_hebb-type_1990,
	title = {Hebb-{Type} {Dynamics} is {Sufficient} to {Account} for the {Inverse} {Magnification} {Rule} in {Cortical} {Somatotopy}},
	volume = {2},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/2/1/71-84/5512},
	doi = {10.1162/neco.1990.2.1.71},
	abstract = {The inverse magnification rule in cortical somatotopy is the experimentally derived inverse relationship between cortical magnification (area of somatotopic map representing a unit area of skin surface) and receptive field size (area of restricted skin surface driving a cortical neuron). We show by computer simulation of a simple, multilayer model that Hebb-type synaptic modification subject to competitive constraints is sufficient to account for the inverse magnification rule.},
	language = {en},
	number = {1},
	urldate = {2023-07-03},
	journal = {Neural Computation},
	author = {Grajski, Kamil A. and Merzenich, Michael M.},
	month = mar,
	year = {1990},
	pages = {71--84},
}

@incollection{widrow_natures_2019,
	title = {Nature's {Learning} {Rule}},
	isbn = {978-0-12-815480-9},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780128154809000013},
	language = {en},
	urldate = {2023-07-03},
	booktitle = {Artificial {Intelligence} in the {Age} of {Neural} {Networks} and {Brain} {Computing}},
	publisher = {Elsevier},
	author = {Widrow, Bernard and Kim, Youngsik and Park, Dookun and Perin, Jose Krause},
	year = {2019},
	doi = {10.1016/B978-0-12-815480-9.00001-3},
	pages = {1--30},
}

@inproceedings{ioffe_batch_2015,
	series = {{ICML}'15},
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 37},
	publisher = {JMLR.org},
	author = {Ioffe, Sergey and Szegedy, Christian},
	year = {2015},
	note = {Place: Lille, France},
	pages = {448--456},
}

@inproceedings{lee_deeply-supervised_2015,
	address = {San Diego, California, USA},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Deeply-{Supervised} {Nets}},
	volume = {38},
	url = {https://proceedings.mlr.press/v38/lee15a.html},
	abstract = {We propose deeply-supervised nets (DSN), a method that simultaneously minimizes classification error and improves the directness and transparency of the hidden layer learning process. We focus our attention on three aspects of traditional convolutional-neural-network-type (CNN-type) architectures: (1) transparency in the effect intermediate layers have on overall classification; (2) discriminativeness and robustness of learned features, especially in early layers; (3) training effectiveness in the face of “vanishing” gradients. To combat these issues, we introduce “companion” objective functions at each hidden layer, in addition to the overall objective function at the output layer (an integrated strategy distinct from layer-wise pre-training). We also analyze our algorithm using techniques extended from stochastic gradient methods. The advantages provided by our method are evident in our experimental results, showing state-of-the-art performance on MNIST, CIFAR-10, CIFAR-100, and SVHN.},
	booktitle = {Proceedings of the {Eighteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Lee, Chen-Yu and Xie, Saining and Gallagher, Patrick and Zhang, Zhengyou and Tu, Zhuowen},
	editor = {Lebanon, Guy and Vishwanathan, S. V. N.},
	month = may,
	year = {2015},
	pages = {562--570},
}

@misc{zhang_why_2020,
	title = {Why gradient clipping accelerates training: {A} theoretical justification for adaptivity},
	shorttitle = {Why gradient clipping accelerates training},
	url = {http://arxiv.org/abs/1905.11881},
	doi = {10.48550/arXiv.1905.11881},
	abstract = {We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, {\textbackslash}emph\{gradient clipping\} and {\textbackslash}emph\{normalized gradient\}, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Zhang, Jingzhao and He, Tianxing and Sra, Suvrit and Jadbabaie, Ali},
	month = feb,
	year = {2020},
	note = {arXiv:1905.11881 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@article{grossberg_competitive_1987,
	title = {Competitive {Learning}: {From} {Interactive} {Activation} to {Adaptive} {Resonance}},
	volume = {11},
	issn = {03640213},
	shorttitle = {Competitive {Learning}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1551-6708.1987.tb00862.x},
	doi = {10.1111/j.1551-6708.1987.tb00862.x},
	language = {en},
	number = {1},
	urldate = {2023-07-03},
	journal = {Cognitive Science},
	author = {Grossberg, Stephen},
	month = jan,
	year = {1987},
	pages = {23--63},
}

@article{crick_recent_1989,
	title = {The recent excitement about neural networks},
	volume = {337},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/337129a0},
	doi = {10.1038/337129a0},
	language = {en},
	number = {6203},
	urldate = {2023-07-03},
	journal = {Nature},
	author = {Crick, Francis},
	month = jan,
	year = {1989},
	pages = {129--132},
}

@inproceedings{raghavan_neural_2019,
	title = {Neural networks grown and self-organized by noise},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Raghavan, Guruprasad and Thomson, Matt},
	editor = {Wallach, Hannah and Larochelle, Hugo and Beygelzimer, Alina and d'Alché-Buc, Florence and Fox, Emily and Garnett, Roman},
	year = {2019},
}

@misc{raghavan_self-organization_2020,
	title = {Self-organization of multi-layer spiking neural networks},
	url = {http://arxiv.org/abs/2006.06902},
	doi = {10.48550/arXiv.2006.06902},
	abstract = {Living neural networks in our brains autonomously self-organize into large, complex architectures during early development to result in an organized and functional organic computational device. A key mechanism that enables the formation of complex architecture in the developing brain is the emergence of traveling spatio-temporal waves of neuronal activity across the growing brain. Inspired by this strategy, we attempt to efficiently self-organize large neural networks with an arbitrary number of layers into a wide variety of architectures. To achieve this, we propose a modular tool-kit in the form of a dynamical system that can be seamlessly stacked to assemble multi-layer neural networks. The dynamical system encapsulates the dynamics of spiking units, their inter/intra layer interactions as well as the plasticity rules that control the flow of information between layers. The key features of our tool-kit are (1) autonomous spatio-temporal waves across multiple layers triggered by activity in the preceding layer and (2) Spike-timing dependent plasticity (STDP) learning rules that update the inter-layer connectivity based on wave activity in the connecting layers. Our framework leads to the self-organization of a wide variety of architectures, ranging from multi-layer perceptrons to autoencoders. We also demonstrate that emergent waves can self-organize spiking network architecture to perform unsupervised learning, and networks can be coupled with a linear classifier to perform classification on classic image datasets like MNIST. Broadly, our work shows that a dynamical systems framework for learning can be used to self-organize large computational devices.},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Raghavan, Guruprasad and Lin, Cong and Thomson, Matt},
	month = jun,
	year = {2020},
	note = {arXiv:2006.06902 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
}

@article{marsland_self-organising_2002,
	title = {A self-organising network that grows when required},
	volume = {15},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608002000783},
	doi = {10.1016/S0893-6080(02)00078-3},
	language = {en},
	number = {8-9},
	urldate = {2023-07-03},
	journal = {Neural Networks},
	author = {Marsland, Stephen and Shapiro, Jonathan and Nehmzow, Ulrich},
	month = oct,
	year = {2002},
	pages = {1041--1058},
}

@article{fritzke_growing_1994,
	title = {Growing cell structures—{A} self-organizing network for unsupervised and supervised learning},
	volume = {7},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0893608094900914},
	doi = {10.1016/0893-6080(94)90091-4},
	language = {en},
	number = {9},
	urldate = {2023-07-03},
	journal = {Neural Networks},
	author = {Fritzke, Bernd},
	month = jan,
	year = {1994},
	pages = {1441--1460},
}

@article{reilly_neural_1982,
	title = {A neural model for category learning},
	volume = {45},
	issn = {0340-1200, 1432-0770},
	url = {http://link.springer.com/10.1007/BF00387211},
	doi = {10.1007/BF00387211},
	language = {en},
	number = {1},
	urldate = {2023-07-03},
	journal = {Biological Cybernetics},
	author = {Reilly, Douglas L. and Cooper, Leon N. and Elbaum, Charles},
	month = aug,
	year = {1982},
	pages = {35--41},
}

@inproceedings{fritzke_growing_1994-1,
	title = {A {Growing} {Neural} {Gas} {Network} {Learns} {Topologies}},
	volume = {7},
	url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Fritzke, Bernd},
	editor = {Tesauro, G. and Touretzky, D. and Leen, T.},
	year = {1994},
}

@book{kohonen_self-organization_1989,
	address = {Berlin, Heidelberg},
	edition = {Third edition},
	title = {Self-{Organization} and {Associative} {Memory}},
	isbn = {978-3-642-88163-3},
	abstract = {This monograph gives a tutorial treatment of new approaches to self-organization, adaptation, learning and memory. It is based on recent research results, both mathematical and computer simulations, and lends itself to graduate and postgraduate courses in the natural sciences. The book presents new formalisms of pattern processing: orthogonal projectors, optimal associative mappings, novelty filters, subspace methods, feature-sensitive units, and self-organization of topological maps, with all their computable algorithms. The main objective is to provide an understanding of the properties of information representations from a general point of view and of their use in pattern information processing, as well as an understanding of many functions of the brain. In the third edition two new discussions have been added and a proof has been revised. The author has developed this book from Associative Memory - A System-Theoretical Approach (Volume 17 of Springer Series in Communication and Cybernetics, 1977), the first ever monograph on distributed associative memories},
	language = {eng},
	publisher = {Springer Berlin Heidelberg},
	author = {Kohonen, Teuvo},
	year = {1989},
	note = {OCLC: 851372105},
}

@article{kohonen_self-organized_1982,
	title = {Self-organized formation of topologically correct feature maps},
	volume = {43},
	issn = {0340-1200, 1432-0770},
	url = {http://link.springer.com/10.1007/BF00337288},
	doi = {10.1007/BF00337288},
	language = {en},
	number = {1},
	urldate = {2023-07-03},
	journal = {Biological Cybernetics},
	author = {Kohonen, Teuvo},
	year = {1982},
	pages = {59--69},
}

@misc{risi_future_2021,
	title = {The {Future} of {Artificial} {Intelligence} is {Self}-{Organizing} and {Self}-{Assembling}},
	url = {https://sebastianrisi.com/self\%5Fassembling\%5Fai},
	journal = {sebastianrisi.com},
	author = {Risi, Sebastian},
	year = {2021},
}

@misc{variengien_towards_2021,
	title = {Towards self-organized control: {Using} neural cellular automata to robustly control a cart-pole agent},
	shorttitle = {Towards self-organized control},
	url = {http://arxiv.org/abs/2106.15240},
	doi = {10.48550/arXiv.2106.15240},
	abstract = {Neural cellular automata (Neural CA) are a recent framework used to model biological phenomena emerging from multicellular organisms. In these systems, artificial neural networks are used as update rules for cellular automata. Neural CA are end-to-end differentiable systems where the parameters of the neural network can be learned to achieve a particular task. In this work, we used neural CA to control a cart-pole agent. The observations of the environment are transmitted in input cells, while the values of output cells are used as a readout of the system. We trained the model using deep-Q learning, where the states of the output cells were used as the Q-value estimates to be optimized. We found that the computing abilities of the cellular automata were maintained over several hundreds of thousands of iterations, producing an emergent stable behavior in the environment it controls for thousands of steps. Moreover, the system demonstrated life-like phenomena such as a developmental phase, regeneration after damage, stability despite a noisy environment, and robustness to unseen disruption such as input deletion.},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Variengien, Alexandre and Nichele, Stefano and Glover, Tom and Pontes-Filho, Sidney},
	month = jul,
	year = {2021},
	note = {arXiv:2106.15240 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{kirsch_meta_2021,
	title = {Meta {Learning} {Backpropagation} {And} {Improving} {It}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/7608de7a475c0c878f60960d72a92654-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kirsch, Louis and Schmidhuber, Jürgen},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {14122--14134},
}

@inproceedings{pedersen_evolving_2021,
	title = {Evolving and {Merging} {Hebbian} {Learning} {Rules}: {Increasing} {Generalization} by {Decreasing} the {Number} of {Rules}},
	shorttitle = {Evolving and {Merging} {Hebbian} {Learning} {Rules}},
	url = {http://arxiv.org/abs/2104.07959},
	doi = {10.1145/3449639.3459317},
	abstract = {Generalization to out-of-distribution (OOD) circumstances after training remains a challenge for artificial agents. To improve the robustness displayed by plastic Hebbian neural networks, we evolve a set of Hebbian learning rules, where multiple connections are assigned to a single rule. Inspired by the biological phenomenon of the genomic bottleneck, we show that by allowing multiple connections in the network to share the same local learning rule, it is possible to drastically reduce the number of trainable parameters, while obtaining a more robust agent. During evolution, by iteratively using simple K-Means clustering to combine rules, our Evolve and Merge approach is able to reduce the number of trainable parameters from 61,440 to 1,920, while at the same time improving robustness, all without increasing the number of generations used. While optimization of the agents is done on a standard quadruped robot morphology, we evaluate the agents' performances on slight morphology modifications in a total of 30 unseen morphologies. Our results add to the discussion on generalization, overfitting and OOD adaptation. To create agents that can adapt to a wider array of unexpected situations, Hebbian learning combined with a regularising "genomic bottleneck" could be a promising research direction.},
	urldate = {2023-07-03},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference}},
	author = {Pedersen, Joachim Winther and Risi, Sebastian},
	month = jun,
	year = {2021},
	note = {arXiv:2104.07959 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	pages = {892--900},
}

@inproceedings{najarro_meta-learning_2020,
	title = {Meta-{Learning} through {Hebbian} {Plasticity} in {Random} {Networks}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/ee23e7ad9b473ad072d57aaa9b2a5222-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Najarro, Elias and Risi, Sebastian},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {20719--20731},
}

@article{randazzo_self-classifying_2020,
	title = {Self-classifying {MNIST} {Digits}},
	volume = {5},
	issn = {2476-0757},
	url = {https://distill.pub/2020/selforg/mnist},
	doi = {10.23915/distill.00027.002},
	number = {8},
	urldate = {2023-07-03},
	journal = {Distill},
	author = {Randazzo, Ettore and Mordvintsev, Alexander and Niklasson, Eyvind and Levin, Michael and Greydanus, Sam},
	month = aug,
	year = {2020},
	pages = {10.23915/distill.00027.002},
}

@inproceedings{grattarola_learning_2021,
	title = {Learning {Graph} {Cellular} {Automata}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/af87f7cdcda223c41c3f3ef05a3aaeea-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Grattarola, Daniele and Livi, Lorenzo and Alippi, Cesare},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {20983--20994},
}

@incollection{hu_regenerating_2021,
	address = {Cham},
	title = {Regenerating {Soft} {Robots} {Through} {Neural} {Cellular} {Automata}},
	volume = {12691},
	isbn = {978-3-030-72811-3 978-3-030-72812-0},
	url = {http://link.springer.com/10.1007/978-3-030-72812-0_3},
	language = {en},
	urldate = {2023-07-03},
	booktitle = {Genetic {Programming}},
	publisher = {Springer International Publishing},
	author = {Horibe, Kazuya and Walker, Kathryn and Risi, Sebastian},
	editor = {Hu, Ting and Lourenço, Nuno and Medvet, Eric},
	year = {2021},
	doi = {10.1007/978-3-030-72812-0_3},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {36--50},
}

@misc{sudhakaran_growing_2021,
	title = {Growing {3D} {Artefacts} and {Functional} {Machines} with {Neural} {Cellular} {Automata}},
	url = {http://arxiv.org/abs/2103.08737},
	doi = {10.48550/arXiv.2103.08737},
	abstract = {Neural Cellular Automata (NCAs) have been proven effective in simulating morphogenetic processes, the continuous construction of complex structures from very few starting cells. Recent developments in NCAs lie in the 2D domain, namely reconstructing target images from a single pixel or infinitely growing 2D textures. In this work, we propose an extension of NCAs to 3D, utilizing 3D convolutions in the proposed neural network architecture. Minecraft is selected as the environment for our automaton since it allows the generation of both static structures and moving machines. We show that despite their simplicity, NCAs are capable of growing complex entities such as castles, apartment blocks, and trees, some of which are composed of over 3,000 blocks. Additionally, when trained for regeneration, the system is able to regrow parts of simple functional machines, significantly expanding the capabilities of simulated morphogenetic systems. The code for the experiment in this paper can be found at: https://github.com/real-itu/3d-artefacts-nca.},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Sudhakaran, Shyam and Grbic, Djordje and Li, Siyan and Katona, Adam and Najarro, Elias and Glanois, Claire and Risi, Sebastian},
	month = jun,
	year = {2021},
	note = {arXiv:2103.08737 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{palm_variational_2022,
	title = {Variational {Neural} {Cellular} {Automata}},
	url = {http://arxiv.org/abs/2201.12360},
	doi = {10.48550/arXiv.2201.12360},
	abstract = {In nature, the process of cellular growth and differentiation has lead to an amazing diversity of organisms -- algae, starfish, giant sequoia, tardigrades, and orcas are all created by the same generative process. Inspired by the incredible diversity of this biological generative process, we propose a generative model, the Variational Neural Cellular Automata (VNCA), which is loosely inspired by the biological processes of cellular growth and differentiation. Unlike previous related works, the VNCA is a proper probabilistic generative model, and we evaluate it according to best practices. We find that the VNCA learns to reconstruct samples well and that despite its relatively few parameters and simple local-only communication, the VNCA can learn to generate a large variety of output from information encoded in a common vector format. While there is a significant gap to the current state-of-the-art in terms of generative modeling performance, we show that the VNCA can learn a purely self-organizing generative process of data. Additionally, we show that the VNCA can learn a distribution of stable attractors that can recover from significant damage.},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Palm, Rasmus Berg and González-Duque, Miguel and Sudhakaran, Shyam and Risi, Sebastian},
	month = feb,
	year = {2022},
	note = {arXiv:2201.12360 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{mordvintsev_growing_2022,
	address = {Online},
	title = {Growing {Isotropic} {Neural} {Cellular} {Automata}},
	url = {https://direct.mit.edu/isal/article-abstract/doi/10.1162/isal_a_00552},
	doi = {10.1162/isal_a_00552},
	language = {en},
	urldate = {2023-07-03},
	booktitle = {The 2022 {Conference} on {Artificial} {Life}},
	publisher = {MIT Press},
	author = {Mordvintsev, Alexander and Randazzo, Ettore and Fouts, Craig},
	year = {2022},
}

@article{mordvintsev_growing_2020,
	title = {Growing {Neural} {Cellular} {Automata}},
	volume = {5},
	issn = {2476-0757},
	url = {https://distill.pub/2020/growing-ca},
	doi = {10.23915/distill.00023},
	number = {2},
	urldate = {2023-07-03},
	journal = {Distill},
	author = {Mordvintsev, Alexander and Randazzo, Ettore and Niklasson, Eyvind and Levin, Michael},
	month = feb,
	year = {2020},
	pages = {10.23915/distill.00023},
}

@article{mordvintsev_growing_2020-1,
	title = {Growing {Neural} {Cellular} {Automata}},
	volume = {5},
	issn = {2476-0757},
	url = {https://distill.pub/2020/growing-ca},
	doi = {10.23915/distill.00023},
	number = {2},
	urldate = {2023-07-03},
	journal = {Distill},
	author = {Mordvintsev, Alexander and Randazzo, Ettore and Niklasson, Eyvind and Levin, Michael},
	month = feb,
	year = {2020},
	pages = {10.23915/distill.00023},
}

@article{gilpin_cellular_2019,
	title = {Cellular automata as convolutional neural networks},
	volume = {100},
	issn = {2470-0045, 2470-0053},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.100.032402},
	doi = {10.1103/PhysRevE.100.032402},
	language = {en},
	number = {3},
	urldate = {2023-07-03},
	journal = {Physical Review E},
	author = {Gilpin, William},
	month = sep,
	year = {2019},
	pages = {032402},
}

@inproceedings{wulff_learning_1992,
	title = {Learning {Cellular} {Automaton} {Dynamics} with {Neural} {Networks}},
	volume = {5},
	url = {https://proceedings.neurips.cc/paper_files/paper/1992/file/d6c651ddcd97183b2e40bc464231c962-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Morgan-Kaufmann},
	author = {Wulff, N. and Hertz, J A},
	editor = {Hanson, S. and Cowan, J. and Giles, C.},
	year = {1992},
}

@article{vichniac_simulating_1984,
	title = {Simulating physics with cellular automata},
	volume = {10},
	issn = {01672789},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0167278984902537},
	doi = {10.1016/0167-2789(84)90253-7},
	language = {en},
	number = {1-2},
	urldate = {2023-07-03},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Vichniac, Gérard Y.},
	month = jan,
	year = {1984},
	pages = {96--116},
}

@article{wolfram_cellular_1984,
	title = {Cellular automata as models of complexity},
	volume = {311},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/311419a0},
	doi = {10.1038/311419a0},
	language = {en},
	number = {5985},
	urldate = {2023-07-03},
	journal = {Nature},
	author = {Wolfram, Stephen},
	month = oct,
	year = {1984},
	pages = {419--424},
}

@book{morris_cognitive_2006,
	title = {Cognitive {Systems} - {Information} {Processing} {Meets} {Brain} {Science}},
	isbn = {978-0-12-088566-4},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780120885664X50004},
	language = {en},
	urldate = {2023-07-03},
	publisher = {Elsevier},
	author = {Morris, Richard and Tarassenko, Lionel and Kenward, Michael},
	year = {2006},
	doi = {10.1016/B978-0-12-088566-4.X5000-4},
}

@article{wolfrum_recurrent_2008,
	title = {A recurrent dynamic model for correspondence-based face recognition},
	volume = {8},
	issn = {1534-7362},
	url = {http://jov.arvojournals.org/article.aspx?doi=10.1167/8.7.34},
	doi = {10.1167/8.7.34},
	language = {en},
	number = {7},
	urldate = {2023-07-03},
	journal = {Journal of Vision},
	author = {Wolfrum, Philipp and Wolff, Christian and Lücke, Jörg and Von Der Malsburg, Christoph},
	month = dec,
	year = {2008},
	pages = {34},
}

@book{wiskott_face_1996,
	title = {Face {Recognition} by {Dynamic} {Link} {Matching}},
	publisher = {Ruhr-Univ., Inst. für Neuroinformatik},
	author = {Wiskott, Laurenz and von der Malsburg, Christoph},
	year = {1996},
}

@article{bienenstock_neural_1987,
	title = {A {Neural} {Network} for {Invariant} {Pattern} {Recognition}},
	volume = {4},
	issn = {0295-5075, 1286-4854},
	url = {https://iopscience.iop.org/article/10.1209/0295-5075/4/1/020},
	doi = {10.1209/0295-5075/4/1/020},
	number = {1},
	urldate = {2023-07-03},
	journal = {Europhysics Letters (EPL)},
	author = {Bienenstock, Ellie and von der Malsburg, Christoph},
	month = jul,
	year = {1987},
	pages = {121--126},
}

@article{lades_distortion_1993,
	title = {Distortion invariant object recognition in the dynamic link architecture},
	volume = {42},
	issn = {00189340},
	url = {http://ieeexplore.ieee.org/document/210173/},
	doi = {10.1109/12.210173},
	number = {3},
	urldate = {2023-07-03},
	journal = {IEEE Transactions on Computers},
	author = {Lades, Martin and Vorbruggen, Jan C. and Buhmann, Joachim and Lange, Jörg and von der Malsburg, Christoph and Wurtz, Rolf P. and Konen, Wolfgang},
	month = mar,
	year = {1993},
	pages = {300--311},
}

@article{romanski_information_1993,
	title = {Information {Cascade} from {Primary} {Auditory} {Cortex} to the {Amygdala}: {Corticocortical} and {Corticoamygdaloid} {Projections} of {Temporal} {Cortex} in the {Rat}},
	volume = {3},
	issn = {1047-3211, 1460-2199},
	shorttitle = {Information {Cascade} from {Primary} {Auditory} {Cortex} to the {Amygdala}},
	url = {https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/3.6.515},
	doi = {10.1093/cercor/3.6.515},
	language = {en},
	number = {6},
	urldate = {2023-07-02},
	journal = {Cerebral Cortex},
	author = {Romanski, Lizabeth M. and LeDoux, Joseph E.},
	year = {1993},
	pages = {515--532},
}

@article{tanigawa_organization_2005,
	title = {Organization of {Horizontal} {Axons} in the {Inferior} {Temporal} {Cortex} and {Primary} {Visual} {Cortex} of the {Macaque} {Monkey}},
	volume = {15},
	issn = {1460-2199, 1047-3211},
	url = {http://academic.oup.com/cercor/article/15/12/1887/339721/Organization-of-Horizontal-Axons-in-the-Inferior},
	doi = {10.1093/cercor/bhi067},
	language = {en},
	number = {12},
	urldate = {2023-07-02},
	journal = {Cerebral Cortex},
	author = {Tanigawa, Hisashi and Wang, QuanXin and Fujita, Ichiro},
	month = dec,
	year = {2005},
	pages = {1887--1899},
}

@article{yamada_somatotopic_2007,
	title = {Somatotopic {Organization} of {Thalamocortical} {Projection} {Fibers} as {Assessed} with {MR} {Tractography}},
	volume = {242},
	issn = {0033-8419, 1527-1315},
	url = {http://pubs.rsna.org/doi/10.1148/radiol.2423060297},
	doi = {10.1148/radiol.2423060297},
	language = {en},
	number = {3},
	urldate = {2023-07-02},
	journal = {Radiology},
	author = {Yamada, Kei and Nagakane, Yoshinari and Yoshikawa, Kenji and Kizu, Osamu and Ito, Hirotoshi and Kubota, Takao and Akazawa, Kentaro and Oouchi, Hiroyuki and Matsushima, Shigenori and Nakagawa, Masanori and Nishimura, Tsunehiko},
	month = mar,
	year = {2007},
	pages = {840--845},
}

@article{yasui_subparafascicular_1990,
	title = {The subparafascicular thalamic nucleus of the rat receives projection fibers from the inferior colliculus and auditory cortex},
	volume = {537},
	issn = {00068993},
	url = {https://linkinghub.elsevier.com/retrieve/pii/000689939090378O},
	doi = {10.1016/0006-8993(90)90378-O},
	language = {en},
	number = {1-2},
	urldate = {2023-07-02},
	journal = {Brain Research},
	author = {Yasui, Yukihiko and Kayahara, Tetsuro and Nakano, Katsuma and Mizuno, Noboru},
	month = dec,
	year = {1990},
	pages = {323--327},
}

@article{mumuni_cnn_2021,
	title = {{CNN} {Architectures} for {Geometric} {Transformation}-{Invariant} {Feature} {Representation} in {Computer} {Vision}: {A} {Review}},
	volume = {2},
	issn = {2662-995X, 2661-8907},
	shorttitle = {{CNN} {Architectures} for {Geometric} {Transformation}-{Invariant} {Feature} {Representation} in {Computer} {Vision}},
	url = {https://link.springer.com/10.1007/s42979-021-00735-0},
	doi = {10.1007/s42979-021-00735-0},
	language = {en},
	number = {5},
	urldate = {2023-07-02},
	journal = {SN Computer Science},
	author = {Mumuni, Alhassan and Mumuni, Fuseini},
	month = sep,
	year = {2021},
	pages = {340},
}

@inproceedings{poulenard_effective_2019,
	address = {Québec City, QC, Canada},
	title = {Effective {Rotation}-{Invariant} {Point} {CNN} with {Spherical} {Harmonics} {Kernels}},
	isbn = {978-1-72813-131-3},
	url = {https://ieeexplore.ieee.org/document/8886010/},
	doi = {10.1109/3DV.2019.00015},
	urldate = {2023-07-02},
	booktitle = {2019 {International} {Conference} on {3D} {Vision} ({3DV})},
	publisher = {IEEE},
	author = {Poulenard, Adrien and Rakotosaona, Marie-Julie and Ponty, Yann and Ovsjanikov, Maks},
	month = sep,
	year = {2019},
	pages = {47--56},
}

@book{marr_vision_2010,
	title = {Vision: {A} {Computational} {Investigation} into the {Human} {Representation} and {Processing} of {Visual} {Information}},
	isbn = {978-0-262-28961-0},
	shorttitle = {Vision},
	url = {https://direct.mit.edu/books/book/3299},
	language = {en},
	urldate = {2023-07-02},
	publisher = {The MIT Press},
	author = {Marr, David},
	year = {2010},
	doi = {10.7551/mitpress/9780262514620.001.0001},
}

@misc{ahmad_properties_2015,
	title = {Properties of {Sparse} {Distributed} {Representations} and their {Application} to {Hierarchical} {Temporal} {Memory}},
	url = {http://arxiv.org/abs/1503.07469},
	doi = {10.48550/arXiv.1503.07469},
	abstract = {Empirical evidence demonstrates that every region of the neocortex represents information using sparse activity patterns. This paper examines Sparse Distributed Representations (SDRs), the primary information representation strategy in Hierarchical Temporal Memory (HTM) systems and the neocortex. We derive a number of properties that are core to scaling, robustness, and generalization. We use the theory to provide practical guidelines and illustrate the power of SDRs as the basis of HTM. Our goal is to help create a unified mathematical and practical framework for SDRs as it relates to cortical function.},
	urldate = {2023-07-02},
	publisher = {arXiv},
	author = {Ahmad, Subutai and Hawkins, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv:1503.07469 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Quantitative Biology - Neurons and Cognition},
}

@misc{du_implicit_2020,
	title = {Implicit {Generation} and {Generalization} in {Energy}-{Based} {Models}},
	url = {http://arxiv.org/abs/1903.08689},
	doi = {10.48550/arXiv.1903.08689},
	abstract = {Energy based models (EBMs) are appealing due to their generality and simplicity in likelihood modeling, but have been traditionally difficult to train. We present techniques to scale MCMC based EBM training on continuous neural networks, and we show its success on the high-dimensional data domains of ImageNet32x32, ImageNet128x128, CIFAR-10, and robotic hand trajectories, achieving better samples than other likelihood models and nearing the performance of contemporary GAN approaches, while covering all modes of the data. We highlight some unique capabilities of implicit generation such as compositionality and corrupt image reconstruction and inpainting. Finally, we show that EBMs are useful models across a wide variety of tasks, achieving state-of-the-art out-of-distribution classification, adversarially robust classification, state-of-the-art continual online class learning, and coherent long term predicted trajectory rollouts.},
	urldate = {2023-07-01},
	publisher = {arXiv},
	author = {Du, Yilun and Mordatch, Igor},
	month = jun,
	year = {2020},
	note = {arXiv:1903.08689 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{bengio_gflownet_2022,
	title = {{GFlowNet} {Foundations}},
	url = {http://arxiv.org/abs/2111.09266},
	doi = {10.48550/arXiv.2111.09266},
	abstract = {Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of GFlowNets. They can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. GFlowNets amortize the work typically done by computationally expensive MCMC methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entropy and mutual information, sampling from a Pareto frontier, connections to reward-maximizing policies, and extensions to stochastic environments, continuous actions and modular energy functions.},
	urldate = {2023-07-01},
	publisher = {arXiv},
	author = {Bengio, Yoshua and Lahlou, Salem and Deleu, Tristan and Hu, Edward J. and Tiwari, Mo and Bengio, Emmanuel},
	month = aug,
	year = {2022},
	note = {arXiv:2111.09266 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{bengio_flow_2021,
	title = {Flow {Network} based {Generative} {Models} for {Non}-{Iterative} {Diverse} {Candidate} {Generation}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/e614f646836aaed9f89ce58e837e2310-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bengio, Emmanuel and Jain, Moksh and Korablyov, Maksym and Precup, Doina and Bengio, Yoshua},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {27381--27394},
}

@book{parr_active_2022,
	title = {Active {Inference}: {The} {Free} {Energy} {Principle} in {Mind}, {Brain}, and {Behavior}},
	isbn = {978-0-262-36997-8},
	shorttitle = {Active {Inference}},
	url = {https://direct.mit.edu/books/book/5299/Active-InferenceThe-Free-Energy-Principle-in-Mind},
	abstract = {The first comprehensive treatment of active inference, an integrative perspective on brain, cognition, and behavior used across multiple disciplines.
            Active inference is a way of understanding sentient behavior—a theory that characterizes perception, planning, and action in terms of probabilistic inference. Developed by theoretical neuroscientist Karl Friston over years of groundbreaking research, active inference provides an integrated perspective on brain, cognition, and behavior that is increasingly used across multiple disciplines including neuroscience, psychology, and philosophy. Active inference puts the action into perception. This book offers the first comprehensive treatment of active inference, covering theory, applications, and cognitive domains.
            Active inference is a “first principles” approach to understanding behavior and the brain, framed in terms of a single imperative to minimize free energy. The book emphasizes the implications of the free energy principle for understanding how the brain works. It first introduces active inference both conceptually and formally, contextualizing it within current theories of cognition. It then provides specific examples of computational models that use active inference to explain such cognitive phenomena as perception, attention, memory, and planning.},
	language = {en},
	urldate = {2023-07-01},
	publisher = {The MIT Press},
	author = {Parr, Thomas and Pezzulo, Giovanni and Friston, Karl J.},
	month = mar,
	year = {2022},
	doi = {10.7551/mitpress/12441.001.0001},
}

@article{lecun_path_2022,
	title = {A {Path} {Towards} {Autonomous} {Machine} {Intelligence}},
	volume = {62},
	journal = {Open Review},
	author = {LeCun, Yann},
	month = jun,
	year = {2022},
}

@article{piaget_part_1964,
	title = {Part {I}: {Cognitive} development in children: {Piaget} development and learning},
	volume = {2},
	issn = {0022-4308, 1098-2736},
	shorttitle = {Part {I}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/tea.3660020306},
	doi = {10.1002/tea.3660020306},
	language = {en},
	number = {3},
	urldate = {2023-07-01},
	journal = {Journal of Research in Science Teaching},
	author = {Piaget, Jean},
	month = sep,
	year = {1964},
	pages = {176--186},
}

@misc{keurti_homomorphism_2023,
	title = {Homomorphism {Autoencoder} -- {Learning} {Group} {Structured} {Representations} from {Observed} {Transitions}},
	url = {http://arxiv.org/abs/2207.12067},
	doi = {10.48550/arXiv.2207.12067},
	abstract = {How can agents learn internal models that veridically represent interactions with the real world is a largely open question. As machine learning is moving towards representations containing not just observational but also interventional knowledge, we study this problem using tools from representation learning and group theory. We propose methods enabling an agent acting upon the world to learn internal representations of sensory information that are consistent with actions that modify it. We use an autoencoder equipped with a group representation acting on its latent space, trained using an equivariance-derived loss in order to enforce a suitable homomorphism property on the group representation. In contrast to existing work, our approach does not require prior knowledge of the group and does not restrict the set of actions the agent can perform. We motivate our method theoretically, and show empirically that it can learn a group representation of the actions, thereby capturing the structure of the set of transformations applied to the environment. We further show that this allows agents to predict the effect of sequences of future actions with improved accuracy.},
	urldate = {2023-07-01},
	publisher = {arXiv},
	author = {Keurti, Hamza and Pan, Hsiao-Ru and Besserve, Michel and Grewe, Benjamin F. and Schölkopf, Bernhard},
	month = jun,
	year = {2023},
	note = {arXiv:2207.12067 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Group Theory, Statistics - Machine Learning},
}

@article{keller_sensorimotor_2012,
	title = {Sensorimotor {Mismatch} {Signals} in {Primary} {Visual} {Cortex} of the {Behaving} {Mouse}},
	volume = {74},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627312003844},
	doi = {10.1016/j.neuron.2012.03.040},
	language = {en},
	number = {5},
	urldate = {2023-07-01},
	journal = {Neuron},
	author = {Keller, Georg B. and Bonhoeffer, Tobias and Hübener, Mark},
	month = jun,
	year = {2012},
	pages = {809--815},
}

@article{knoblich_social_2006,
	title = {The {Social} {Nature} of {Perception} and {Action}},
	volume = {15},
	issn = {0963-7214, 1467-8721},
	url = {http://journals.sagepub.com/doi/10.1111/j.0963-7214.2006.00415.x},
	doi = {10.1111/j.0963-7214.2006.00415.x},
	abstract = {Humans engage in a wide range of social activities. Previous research has focused on the role of higher cognitive functions, such as mentalizing (the ability to infer others' mental states) and language processing, in social exchange. This article reviews recent studies on action perception and joint action suggesting that basic perception–action links are crucial for many social interactions. Mapping perceived actions to one's own action repertoire enables direct understanding of others' actions and supports action identification. Joint action relies on shared action representations and involves modeling of others' performance in relation to one's own. Taking the social nature of perception and action seriously not only contributes to the understanding of dedicated social processes but has the potential to create a new perspective on the individual mind and brain.},
	language = {en},
	number = {3},
	urldate = {2023-07-01},
	journal = {Current Directions in Psychological Science},
	author = {Knoblich, Günther and Sebanz, Natalie},
	month = jun,
	year = {2006},
	pages = {99--104},
}

@article{zhou_does_2019,
	title = {Does computer vision matter for action?},
	volume = {4},
	issn = {2470-9476},
	url = {https://www.science.org/doi/10.1126/scirobotics.aaw6661},
	doi = {10.1126/scirobotics.aaw6661},
	abstract = {Controlled experiments indicate that explicit intermediate representations help action.
          , 
            Controlled experiments indicate that explicit intermediate representations help action.},
	language = {en},
	number = {30},
	urldate = {2023-07-01},
	journal = {Science Robotics},
	author = {Zhou, Brady and Krähenbühl, Philipp and Koltun, Vladlen},
	month = may,
	year = {2019},
	pages = {eaaw6661},
}

@inproceedings{rennie_self-critical_2017,
	title = {Self-{Critical} {Sequence} {Training} for {Image} {Captioning}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Rennie, Steven J. and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
	month = jul,
	year = {2017},
}

@article{ma_principles_2022,
	title = {On the principles of {Parsimony} and {Self}-consistency for the emergence of intelligence},
	volume = {23},
	issn = {2095-9184, 2095-9230},
	url = {https://link.springer.com/10.1631/FITEE.2200297},
	doi = {10.1631/FITEE.2200297},
	language = {en},
	number = {9},
	urldate = {2023-07-01},
	journal = {Frontiers of Information Technology \& Electronic Engineering},
	author = {Ma, Yi and Tsao, Doris and Shum, Heung-Yeung},
	month = sep,
	year = {2022},
	pages = {1298--1323},
}

@techreport{krizhevsky_learning_2009,
	title = {Learning {Multiple} {Layers} of {Features} from {Tiny} {Images}},
	institution = {Canadian Institute for Advanced Research},
	author = {Krizhevsky, Alex},
	year = {2009},
}

@inproceedings{sabour_dynamic_2017,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'17},
	title = {Dynamic {Routing} between {Capsules}},
	isbn = {978-1-5108-6096-4},
	abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
	year = {2017},
	note = {event-place: Long Beach, California, USA},
	pages = {3859--3869},
}

@article{george_generative_2017,
	title = {A generative vision model that trains with high data efficiency and breaks text-based {CAPTCHAs}},
	volume = {358},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.aag2612},
	doi = {10.1126/science.aag2612},
	abstract = {Computer or human?
            
              Proving that we are human is now part of many tasks that we do on the internet, such as creating an email account, voting in an online poll, or even downloading a scientific paper. One of the most popular tests is text-based CAPTCHA, where would-be users are asked to decipher letters that may be distorted, partially obscured, or shown against a busy background. This test is used because computers find it tricky, but (most) humans do not. George
              et al.
              developed a hierarchical model for computer vision that was able to solve CAPTCHAs with a high accuracy rate using comparatively little training data. The results suggest that moving away from text-based CAPTCHAs, as some online services have done, may be a good idea.
            
            
              Science
              , this issue p.
              eaag2612
            
          , 
            A hierarchical computer vision model solves CAPTCHAs with a high accuracy rate using relatively little training data.
          , 
            
              INTRODUCTION
              Compositionality, generalization, and learning from a few examples are among the hallmarks of human intelligence. CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart), images used by websites to block automated interactions, are examples of problems that are easy for people but difficult for computers. CAPTCHAs add clutter and crowd letters together to create a chicken-and-egg problem for algorithmic classifiers—the classifiers work well for characters that have been segmented out, but segmenting requires an understanding of the characters, which may be rendered in a combinatorial number of ways. CAPTCHAs also demonstrate human data efficiency: A recent deep-learning approach for parsing one specific CAPTCHA style required millions of labeled examples, whereas humans solve new styles without explicit training.
              By drawing inspiration from systems neuroscience, we introduce recursive cortical network (RCN), a probabilistic generative model for vision in which message-passing–based inference handles recognition, segmentation, and reasoning in a unified manner. RCN learns with very little training data and fundamentally breaks the defense of modern text-based CAPTCHAs by generatively segmenting characters. In addition, RCN outperforms deep neural networks on a variety of benchmarks while being orders of magnitude more data-efficient.
            
            
              RATIONALE
              Modern deep neural networks resemble the feed-forward hierarchy of simple and complex cells in the neocortex. Neuroscience has postulated computational roles for lateral and feedback connections, segregated contour and surface representations, and border-ownership coding observed in the visual cortex, yet these features are not commonly used by deep neural nets. We hypothesized that systematically incorporating these findings into a new model could lead to higher data efficiency and generalization. Structured probabilistic models provide a natural framework for incorporating prior knowledge, and belief propagation (BP) is an inference algorithm that can match the cortical computational speed. The representational choices in RCN were determined by investigating the computational underpinnings of neuroscience data under the constraint that accurate inference should be possible using BP.
            
            
              RESULTS
              RCN was effective in breaking a wide variety of CAPTCHAs with very little training data and without using CAPTCHA-specific heuristics. By comparison, a convolutional neural network required a 50,000-fold larger training set and was less robust to perturbations to the input. Similar results are shown on one- and few-shot MNIST (modified National Institute of Standards and Technology handwritten digit data set) classification, where RCN was significantly more robust to clutter introduced during testing. As a generative model, RCN outperformed neural network models when tested on noisy and cluttered examples and generated realistic samples from one-shot training of handwritten characters. RCN also proved to be effective at an occlusion reasoning task that required identifying the precise relationships between characters at multiple points of overlap. On a standard benchmark for parsing text in natural scenes, RCN outperformed state-of-the-art deep-learning methods while requiring 300-fold less training data.
            
            
              CONCLUSION
              
                Our work demonstrates that structured probabilistic models that incorporate inductive biases from neuroscience can lead to robust, generalizable machine learning models that learn with high data efficiency. In addition, our model’s effectiveness in breaking text-based CAPTCHAs with very little training data suggests that websites should seek more robust mechanisms for detecting automated interactions.
                
              
              
                
                  Breaking CAPTCHAs using a generative vision model.
                  Text-based CAPTCHAs exploit the data efficiency and generative aspects of human vision to create a challenging task for machines. By handling recognition and segmentation in a unified way, our model fundamentally breaks the defense of text-based CAPTCHAs. Shown are the parses by our model for a variety of CAPTCHAs .
                
                
              
            
          , 
            Learning from a few examples and generalizing to markedly different situations are capabilities of human visual intelligence that are yet to be matched by leading machine learning models. By drawing inspiration from systems neuroscience, we introduce a probabilistic generative model for vision in which message-passing–based inference handles recognition, segmentation, and reasoning in a unified way. The model demonstrates excellent generalization and occlusion-reasoning capabilities and outperforms deep neural networks on a challenging scene text recognition benchmark while being 300-fold more data efficient. In addition, the model fundamentally breaks the defense of modern text-based CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart) by generatively segmenting characters without CAPTCHA-specific heuristics. Our model emphasizes aspects such as data efficiency and compositionality that may be important in the path toward general artificial intelligence.},
	language = {en},
	number = {6368},
	urldate = {2023-07-01},
	journal = {Science},
	author = {George, Dileep and Lehrach, Wolfgang and Kansky, Ken and Lázaro-Gredilla, Miguel and Laan, Christopher and Marthi, Bhaskara and Lou, Xinghua and Meng, Zhaoshi and Liu, Yi and Wang, Huayan and Lavin, Alex and Phoenix, D. Scott},
	month = dec,
	year = {2017},
	pages = {eaag2612},
}

@misc{ferrier_toward_2014,
	title = {Toward a {Universal} {Cortical} {Algorithm}: {Examining} {Hierarchical} {Temporal} {Memory} in {Light} of {Frontal} {Cortical} {Function}},
	shorttitle = {Toward a {Universal} {Cortical} {Algorithm}},
	url = {http://arxiv.org/abs/1411.4702},
	doi = {10.48550/arXiv.1411.4702},
	abstract = {A wide range of evidence points toward the existence of a common algorithm underlying the processing of information throughout the cerebral cortex. Several hypothesized features of this cortical algorithm are reviewed, including sparse distributed representation, Bayesian inference, hierarchical organization composed of alternating template matching and pooling layers, temporal slowness and predictive coding. Hierarchical Temporal Memory (HTM) is a family of learning algorithms and corresponding theories of cortical function that embodies these principles. HTM has previously been applied mainly to perceptual tasks typical of posterior cortex. In order to evaluate HTM as a candidate model of cortical function, it is necessary also to investigate its compatibility with the requirements of frontal cortical function. To this end, a variety of models of frontal cortical function are reviewed and integrated, to arrive at the hypothesis that frontal functions including attention, working memory and action selection depend largely upon the same basic algorithms as do posterior functions, with the notable additions of a mechanism for the active maintenance of representations and of multiple cortico-striato-thalamo-cortical loops that allow communication between regions of frontal cortex to be gated in an adaptive manner. Computational models of this system are reviewed. Finally, there is a discussion of how HTM can contribute to the understanding of frontal cortical function, and of what the requirements of frontal cortical function mean for the future development of HTM.},
	urldate = {2023-07-01},
	publisher = {arXiv},
	author = {Ferrier, Michael R.},
	month = nov,
	year = {2014},
	note = {arXiv:1411.4702 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, I.2.6, Quantitative Biology - Neurons and Cognition},
}

@article{yang_survey_2023,
	title = {A {Survey} on ensemble learning under the era of deep learning},
	volume = {56},
	issn = {0269-2821, 1573-7462},
	url = {https://link.springer.com/10.1007/s10462-022-10283-5},
	doi = {10.1007/s10462-022-10283-5},
	language = {en},
	number = {6},
	urldate = {2023-07-01},
	journal = {Artificial Intelligence Review},
	author = {Yang, Yongquan and Lv, Haijun and Chen, Ning},
	month = jun,
	year = {2023},
	pages = {5545--5589},
}

@article{lewis_locations_2019,
	title = {Locations in the {Neocortex}: {A} {Theory} of {Sensorimotor} {Object} {Recognition} {Using} {Cortical} {Grid} {Cells}},
	volume = {13},
	issn = {1662-5110},
	shorttitle = {Locations in the {Neocortex}},
	url = {https://www.frontiersin.org/article/10.3389/fncir.2019.00022/full},
	doi = {10.3389/fncir.2019.00022},
	urldate = {2023-07-01},
	journal = {Frontiers in Neural Circuits},
	author = {Lewis, Marcus and Purdy, Scott and Ahmad, Subutai and Hawkins, Jeff},
	month = apr,
	year = {2019},
	pages = {22},
}

@article{lewis_locations_2019-1,
	title = {Locations in the {Neocortex}: {A} {Theory} of {Sensorimotor} {Object} {Recognition} {Using} {Cortical} {Grid} {Cells}},
	volume = {13},
	issn = {1662-5110},
	doi = {10.3389/fncir.2019.00022},
	journal = {Frontiers in Neural Circuits},
	author = {Lewis, Marcus and Purdy, Scott and Ahmad, Subutai and Hawkins, Jeff},
	month = apr,
	year = {2019},
	pages = {22},
}

@article{mountcastle_columnar_1997,
	title = {The columnar organization of the neocortex},
	volume = {120},
	issn = {14602156},
	url = {https://academic.oup.com/brain/article-lookup/doi/10.1093/brain/120.4.701},
	doi = {10.1093/brain/120.4.701},
	number = {4},
	urldate = {2023-07-01},
	journal = {Brain},
	author = {Mountcastle, V.},
	month = apr,
	year = {1997},
	pages = {701--722},
}

@incollection{mountcastle_organizing_1978,
	address = {Cambridge, MA},
	title = {An {Organizing} {Principle} for {Cerebral} {Function}: {The} {Unit} {Model} and the {Distributed} {System}},
	language = {en},
	booktitle = {The {Mindful} {Brain}},
	publisher = {MIT Press},
	author = {Mountcastle, Vernon},
	year = {1978},
	pages = {7--50},
}

@article{hawkins_framework_2019,
	title = {A {Framework} for {Intelligence} and {Cortical} {Function} {Based} on {Grid} {Cells} in the {Neocortex}},
	volume = {12},
	issn = {1662-5110},
	url = {https://www.frontiersin.org/article/10.3389/fncir.2018.00121/full},
	doi = {10.3389/fncir.2018.00121},
	urldate = {2023-07-01},
	journal = {Frontiers in Neural Circuits},
	author = {Hawkins, Jeff and Lewis, Marcus and Klukas, Mirko and Purdy, Scott and Ahmad, Subutai},
	month = jan,
	year = {2019},
	pages = {121},
}

@article{hospedales_meta-learning_2021,
	title = {Meta-{Learning} in {Neural} {Networks}: {A} {Survey}},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Meta-{Learning} in {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/document/9428530/},
	doi = {10.1109/TPAMI.2021.3079209},
	urldate = {2023-07-01},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Hospedales, Timothy M and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos J.},
	year = {2021},
	pages = {1--1},
}

@incollection{thrun_learning_1998,
	address = {Boston, MA},
	title = {Learning to {Learn}: {Introduction} and {Overview}},
	isbn = {978-1-4613-7527-2 978-1-4615-5529-2},
	shorttitle = {Learning to {Learn}},
	url = {http://link.springer.com/10.1007/978-1-4615-5529-2_1},
	language = {en},
	urldate = {2023-07-01},
	booktitle = {Learning to {Learn}},
	publisher = {Springer US},
	author = {Thrun, Sebastian and Pratt, Lorien},
	editor = {Thrun, Sebastian and Pratt, Lorien},
	year = {1998},
	doi = {10.1007/978-1-4615-5529-2_1},
	pages = {3--17},
}

@article{sager_unsupervised_2022,
	title = {Unsupervised {Domain} {Adaptation} for {Vertebrae} {Detection} and {Identification} in {3D} {CT} {Volumes} {Using} a {Domain} {Sanity} {Loss}},
	volume = {8},
	issn = {2313-433X},
	url = {https://www.mdpi.com/2313-433X/8/8/222},
	doi = {10.3390/jimaging8080222},
	abstract = {A variety of medical computer vision applications analyze 2D slices of computed tomography (CT) scans, whereas axial slices from the body trunk region are usually identified based on their relative position to the spine. A limitation of such systems is that either the correct slices must be extracted manually or labels of the vertebrae are required for each CT scan to develop an automated extraction system. In this paper, we propose an unsupervised domain adaptation (UDA) approach for vertebrae detection and identification based on a novel Domain Sanity Loss (DSL) function. With UDA the model’s knowledge learned on a publicly available (source) data set can be transferred to the target domain without using target labels, where the target domain is defined by the specific setup (CT modality, study protocols, applied pre- and processing) at the point of use (e.g., a specific clinic with its specific CT study protocols). With our approach, a model is trained on the source and target data set in parallel. The model optimizes a supervised loss for labeled samples from the source domain and the DSL loss function based on domain-specific “sanity checks” for samples from the unlabeled target domain. Without using labels from the target domain, we are able to identify vertebra centroids with an accuracy of 72.8\%. By adding only ten target labels during training the accuracy increases to 89.2\%, which is on par with the current state-of-the-art for full supervised learning, while using about 20 times less labels. Thus, our model can be used to extract 2D slices from 3D CT scans on arbitrary data sets fully automatically without requiring an extensive labeling effort, contributing to the clinical adoption of medical imaging by hospitals.},
	language = {en},
	number = {8},
	urldate = {2023-07-01},
	journal = {Journal of Imaging},
	author = {Sager, Pascal and Salzmann, Sebastian and Burn, Felice and Stadelmann, Thilo},
	month = aug,
	year = {2022},
	pages = {222},
}

@article{zhuang_comprehensive_2021,
	title = {A {Comprehensive} {Survey} on {Transfer} {Learning}},
	volume = {109},
	issn = {0018-9219, 1558-2256},
	url = {https://ieeexplore.ieee.org/document/9134370/},
	doi = {10.1109/JPROC.2020.3004555},
	number = {1},
	urldate = {2023-07-01},
	journal = {Proceedings of the IEEE},
	author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
	month = jan,
	year = {2021},
	pages = {43--76},
}

@article{hoi_online_2021,
	title = {Online learning: {A} comprehensive survey},
	volume = {459},
	issn = {09252312},
	shorttitle = {Online learning},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231221006706},
	doi = {10.1016/j.neucom.2021.04.112},
	language = {en},
	urldate = {2023-07-01},
	journal = {Neurocomputing},
	author = {Hoi, Steven C.H. and Sahoo, Doyen and Lu, Jing and Zhao, Peilin},
	month = oct,
	year = {2021},
	pages = {249--289},
}

@inproceedings{sahoo_online_2018,
	address = {Stockholm, Sweden},
	title = {Online {Deep} {Learning}: {Learning} {Deep} {Neural} {Networks} on the {Fly}},
	isbn = {978-0-9992411-2-7},
	shorttitle = {Online {Deep} {Learning}},
	url = {https://www.ijcai.org/proceedings/2018/369},
	doi = {10.24963/ijcai.2018/369},
	abstract = {Deep Neural Networks (DNNs) are typically trained by backpropagation in a batch setting, requiring the entire training data to be made available prior to the learning task. This is not scalable for many real-world scenarios where new data arrives sequentially in a stream. We aim to address an open challenge of ``Online Deep Learning" (ODL) for learning DNNs on the fly in an online setting. Unlike traditional online learning that often optimizes some convex objective function with respect to a shallow model (e.g., a linear/kernel-based hypothesis), ODL is more challenging as the optimization objective is non-convex, and regular DNN with standard backpropagation does not work well in practice for online settings. We present a new ODL framework that attempts to tackle the challenges by learning DNN models which dynamically adapt depth from a sequence of training data in an online learning setting. Specifically, we propose a novel Hedge Backpropagation (HBP) method for online updating the parameters of DNN effectively, and validate the efficacy on large data sets (both stationary and concept drifting scenarios).},
	language = {en},
	urldate = {2023-07-01},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Sahoo, Doyen and Pham, Quang and Lu, Jing and Hoi, Steven C. H.},
	month = jul,
	year = {2018},
	pages = {2660--2666},
}

@misc{manakul_selfcheckgpt_2023,
	title = {{SelfCheckGPT}: {Zero}-{Resource} {Black}-{Box} {Hallucination} {Detection} for {Generative} {Large} {Language} {Models}},
	shorttitle = {{SelfCheckGPT}},
	url = {http://arxiv.org/abs/2303.08896},
	doi = {10.48550/arXiv.2303.08896},
	abstract = {Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose "SelfCheckGPT", a simple sampling-based approach that can be used to fact-check black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if a LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that in sentence hallucination detection, our approach has AUC-PR scores comparable to or better than grey-box methods, while SelfCheckGPT is best at passage factuality assessment.},
	urldate = {2023-07-01},
	publisher = {arXiv},
	author = {Manakul, Potsawee and Liusie, Adian and Gales, Mark J. F.},
	month = may,
	year = {2023},
	note = {arXiv:2303.08896 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{feldman_trapping_2023,
	title = {Trapping {LLM} {Hallucinations} {Using} {Tagged} {Context} {Prompts}},
	url = {http://arxiv.org/abs/2306.06085},
	doi = {10.48550/arXiv.2306.06085},
	abstract = {Recent advances in large language models (LLMs), such as ChatGPT, have led to highly sophisticated conversation agents. However, these models suffer from "hallucinations," where the model generates false or fabricated information. Addressing this challenge is crucial, particularly with AI-driven platforms being adopted across various sectors. In this paper, we propose a novel method to recognize and flag instances when LLMs perform outside their domain knowledge, and ensuring users receive accurate information. We find that the use of context combined with embedded tags can successfully combat hallucinations within generative language models. To do this, we baseline hallucination frequency in no-context prompt-response pairs using generated URLs as easily-tested indicators of fabricated data. We observed a significant reduction in overall hallucination when context was supplied along with question prompts for tested generative engines. Lastly, we evaluated how placing tags within contexts impacted model responses and were able to eliminate hallucinations in responses with 98.88\% effectiveness.},
	urldate = {2023-07-01},
	publisher = {arXiv},
	author = {Feldman, Philip and Foulds, James R. and Pan, Shimei},
	month = jun,
	year = {2023},
	note = {arXiv:2306.06085 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, I.2.7, K.4.2},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	doi = {10.48550/arXiv.2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	urldate = {2023-07-01},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{touvron_llama_2023,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	shorttitle = {{LLaMA}},
	url = {http://arxiv.org/abs/2302.13971},
	doi = {10.48550/arXiv.2302.13971},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
	urldate = {2023-07-01},
	publisher = {arXiv},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13971 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {1877--1901},
}

@article{kelso_selforganizing_1995,
	title = {Self‐organizing dynamics of the human brain: {Critical} instabilities and Šil’nikov chaos},
	volume = {5},
	issn = {1054-1500, 1089-7682},
	shorttitle = {Self‐organizing dynamics of the human brain},
	url = {https://pubs.aip.org/aip/cha/article/5/1/64-69/136508},
	doi = {10.1063/1.166087},
	language = {en},
	number = {1},
	urldate = {2023-06-30},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Kelso, J. A. Scott and Fuchs, Armin},
	month = mar,
	year = {1995},
	pages = {64--69},
}

@article{singer_brain_1986,
	title = {The brain as a self-organizing system},
	volume = {236},
	issn = {0175-758X, 1433-8491},
	url = {http://link.springer.com/10.1007/BF00641050},
	doi = {10.1007/BF00641050},
	language = {en},
	number = {1},
	urldate = {2023-06-30},
	journal = {European Archives of Psychiatry and Neurological Sciences},
	author = {Singer, Wolf},
	year = {1986},
	pages = {4--9},
}

@book{kelso_dynamic_1999,
	address = {Cambridge, Mass.},
	edition = {3.print},
	series = {A {Bradford} book},
	title = {Dynamic patterns: the self-organization of brain and behavior},
	isbn = {978-0-262-61131-2 978-0-262-11200-0},
	shorttitle = {Dynamic patterns},
	language = {eng},
	publisher = {MIT Press},
	author = {Kelso, J. A. Scott},
	year = {1999},
}

@article{mountcastle_columnar_1997,
	title = {The columnar organization of the neocortex},
	volume = {120},
	issn = {14602156},
	url = {https://academic.oup.com/brain/article-lookup/doi/10.1093/brain/120.4.701},
	doi = {10.1093/brain/120.4.701},
	number = {4},
	urldate = {2023-06-30},
	journal = {Brain},
	author = {Mountcastle, Vernon},
	month = apr,
	year = {1997},
	pages = {701--722},
}

@incollection{mountcastle_organizing_1978-1,
	address = {Cambridge, MA},
	title = {An {Organizing} {Principle} for {Cerebral} {Function}: {The} {Unit} {Model} and the {Distributed} {System}},
	language = {en},
	booktitle = {The {Mindful} {Brain}},
	publisher = {MIT Press},
	author = {Mountcastle, Vernon},
	year = {1978},
	pages = {7--50},
}

@article{bassett_understanding_2011,
	title = {Understanding complexity in the human brain},
	volume = {15},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661311000416},
	doi = {10.1016/j.tics.2011.03.006},
	language = {en},
	number = {5},
	urldate = {2023-06-30},
	journal = {Trends in Cognitive Sciences},
	author = {Bassett, Danielle S. and Gazzaniga, Michael S.},
	month = may,
	year = {2011},
	pages = {200--209},
}

@article{gazzaniga_organization_1989,
	title = {Organization of the {Human} {Brain}},
	volume = {245},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.2672334},
	doi = {10.1126/science.2672334},
	language = {en},
	number = {4921},
	urldate = {2023-06-30},
	journal = {Science},
	author = {Gazzaniga, Michael S.},
	month = sep,
	year = {1989},
	pages = {947--952},
}

@book{ackerman_discovering_1992,
	address = {Washington, D.C},
	title = {Discovering the brain},
	isbn = {978-0-309-04529-2},
	publisher = {National Academy Press},
	author = {Ackerman, Sandra},
	collaborator = {Institute of Medicine (U.S.) and National Institute of Mental Health (U.S.)},
	year = {1992},
	keywords = {Brain, Congresses, Neurobiology, Neurology, United States},
}

@mastersthesis{lehmann_leveraging_2022,
	title = {Leveraging {Neuroscience} for {Deep} {Learning} {Based} {Object} {Recognition}},
	school = {Zurich University of Applied Sciences},
	author = {Lehmann, Claude},
	year = {2022},
}

@article{fernandes_self-organization_2015,
	title = {Self-{Organization} of {Control} {Circuits} for {Invariant} {Fiber} {Projections}},
	volume = {27},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/27/5/1005-1032/8081},
	doi = {10.1162/NECO_a_00725},
	abstract = {Assuming that patterns in memory are represented as two-dimensional arrays of local features, just as they are in primary visual cortices, pattern recognition can take the form of elastic graph matching (Lades et al., 1993 ). Neural implementation of this may be based on preorganized fiber projections that can be activated rapidly with the help of control units (Wolfrum, Wolff, Lücke, \& von der Malsburg, 2008 ). Each control unit governs a set of projection fibers that form part of a coherent mapping. We describe a mathematical model for the ontogenesis of the underlying connectivity based on a principle of network self-organization as described by the Häussler system (Häussler \& von der Malsburg, 1983 ), modified to be sensitive to pattern similarity and to support formation of multiple mappings, each under the command of a control unit. The process takes the form of a soft-winner-take-all, where units compete for the representation of maps. We show simulations for invariant point-to-point and feature-to-feature mappings.},
	language = {en},
	number = {5},
	urldate = {2023-06-30},
	journal = {Neural Computation},
	author = {Fernandes, Tomas and von der Malsburg, Christoph},
	month = may,
	year = {2015},
	pages = {1005--1032},
}

@article{olshausen_multiscale_1995,
	title = {A multiscale dynamic routing circuit for forming size- and position-invariant object representations},
	volume = {2},
	issn = {0929-5313, 1573-6873},
	url = {http://link.springer.com/10.1007/BF00962707},
	doi = {10.1007/BF00962707},
	language = {en},
	number = {1},
	urldate = {2023-06-30},
	journal = {Journal of Computational Neuroscience},
	author = {Olshausen, Bruno A. and Anderson, Charles H. and Van Essen, David C.},
	month = mar,
	year = {1995},
	pages = {45--62},
}

@book{arathorn_map-seeking_2002,
	address = {Stanford, Calif},
	title = {Map-seeking circuits in visual cognition: a computational mechanism for biological and machine vision},
	isbn = {978-0-8047-4277-1},
	shorttitle = {Map-seeking circuits in visual cognition},
	publisher = {Stanford University Press},
	author = {Arathorn, David W.},
	year = {2002},
	keywords = {Cognitive maps (Psychology), Computer vision, Neural circuitry, Vision, Visual cortex},
}

@incollection{freeman_representations_1990,
	address = {Oxford, NY},
	title = {Representations: {Who} needs them?},
	booktitle = {Brain {Organization} and {Memory}: {Cells}, {Systems} and {Circuits}.},
	publisher = {Oxford Guilford Press},
	author = {Freeman, Walter J. and Skarda, Christine A.},
	editor = {McGaugh, J. and Weinberger, Jerry and Lynch, G.},
	year = {1990},
	note = {Publisher: Guilford Press},
	pages = {375--380},
}

@article{von_der_malsburg_concerning_2018,
	title = {Concerning the {Neuronal} {Code}},
	volume = {19},
	url = {https://doi.org/10.17791/JCS.2018.19.4.511},
	doi = {10.17791/JCS.2018.19.4.511},
	number = {4},
	urldate = {2023-06-30},
	journal = {Journal of Cognitive Science},
	author = {von der Malsburg, Christoph},
	month = dec,
	year = {2018},
	pages = {511--550},
}

@article{von_der_malsburg_neural_1987,
	title = {A {Neural} {Network} for the {Retrieval} of {Superimposed} {Connection} {Patterns}},
	volume = {3},
	issn = {0295-5075, 1286-4854},
	url = {https://iopscience.iop.org/article/10.1209/0295-5075/3/11/015},
	doi = {10.1209/0295-5075/3/11/015},
	number = {11},
	urldate = {2023-06-30},
	journal = {Europhysics Letters (EPL)},
	author = {Von Der Malsburg, Christoph and Bienenstock, Elie L.},
	month = jun,
	year = {1987},
	pages = {1243--1249},
}

@article{willshaw_how_1976,
	title = {How patterned neural connections can be set up by self-organization},
	volume = {194},
	issn = {0080-4649, 2053-9193},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspb.1976.0087},
	doi = {10.1098/rspb.1976.0087},
	abstract = {An important problem in biology is to explain how patterned neural connections are set up during ontogenesis. Topographically ordered mappings, found widely in nervous systems, are those in which neighbouring elements in one sheet of cells project to neighbouring elements in a second sheet. Exploiting this neighbourhood property leads to a new theory for the establishment of topographical mappings, in which the distance between two cells is expressed in terms of their similarity with respect to certain physical properties assigned to them. This topographical code can be realized in a model employing either synchronization of nervous activity or exchange of specific molecules between neighbouring cells. By means of modifiable synapses the code is used to set up a topographical mapping between two sheets with the same internal structure. We have investigated the neural activity version. Without needing to make any elaborate assumptions about its structure or about the operations its elements are to carry out we have shown that the mappings are set up in a system-to-system rather than a cell-to-cell fashion. The pattern of connections develops in a step-by-step and orderly fashion, the orientation of the mappings being laid down in the earliest stages of development.},
	language = {en},
	number = {1117},
	urldate = {2023-06-30},
	journal = {Proceedings of the Royal Society of London. Series B. Biological Sciences},
	author = {Willshaw, David J. and Von Der Malsburg, Christoph},
	month = nov,
	year = {1976},
	pages = {431--445},
}

@article{willshaw_marker_1979,
	title = {A marker induction mechanism for the establishment of ordered neural mappings: its application to the retinotectal problem},
	volume = {287},
	issn = {0080-4622, 2054-0280},
	shorttitle = {A marker induction mechanism for the establishment of ordered neural mappings},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.1979.0056},
	doi = {10.1098/rstb.1979.0056},
	abstract = {This paper examines the idea that ordered patterns of nerve connections are set up by means of markers carried by the individual cells. The case of the ordered retinotectal projection in amphibia and fishes is discussed in great detail. It is suggested that retinotectal mappings are the result of two mechanisms acting in concert. One mechanism induces a set of retinal markers into the tectum. By this means, an initially haphazard pattern of synapses is transformed into a continuous or piece-wise continuous projection. The other mechanism places the individual pieces of the map in the correct orientation. The machinery necessary for this inductive scheme has been expressed in terms of a set of differential equations, which have been solved numerically for a number of cases. Straightforward assumptions are made as to how markers are distributed in the retina; how they are induced into the tectum; and how the induced markers bring about alterations in the pattern of synaptic contacts. A detailed physiological interpretation of the model is given. The inductive mechanism has been formulated at the level of the individual synaptic interactions. Therefore, it is possible to specify, in a given situation, not only the nature of the end state of the mapping but also how the mapping develops over time. The role of the modes of growth of retina and tectum in shaping the developing projection becomes clear. Since, on this model, the tectum is initially devoid of markers, there is an important difference between the development and the regeneration of ordered mappings. In the development of duplicate maps from various types of compound-eyes, it is suggested that the tectum, rather than the retina, contains an abnormal distribution of markers. An important parameter in these experiments, and also in the regeneration experiments where part-duplication has been found, is the range of interaction amongst the retinal cells. It is suggested that the results of many of the regeneration experiments (including apparently contradictory ones) are manifestations of a conflict between the two alternative ways of specifying the orientation of the map: through the information carried by the markers previously induced into the tectum and through the orientation mechanism itself.},
	language = {en},
	number = {1021},
	urldate = {2023-06-30},
	journal = {Philosophical Transactions of the Royal Society of London. B, Biological Sciences},
	author = {Willshaw, David J. and Von Der Malsburg, Christoph},
	month = nov,
	year = {1979},
	pages = {203--243},
}

@article{mcpherson_physical_2001,
	title = {A {Physical} {Map} of the {Human} {Genome}},
	volume = {409},
	issn = {1476-4687},
	url = {https://doi.org/10.1038/35057157},
	doi = {10.1038/35057157},
	number = {6822},
	journal = {Nature},
	author = {McPherson, John D. and Marra, Marco and Hillier, LaDeana and Waterston, Robert H. and Chinwalla, Asif and Wallis, John and Sekhon, Mandeep and Wylie, Kristine and Mardis, Elaine R. and Wilson, Richard K. and Fulton, Robert and Kucaba, Tamara A. and Wagner-McPherson, Caryn and Barbazuk, William B. and Gregory, Simon G. and Humphray, Sean J. and French, Lisa and Evans, Richard S. and Bethel, Graeme and Whittaker, Adam and Holden, Jane L. and McCann, Owen T. and Dunham, Andrew and Soderlund, Carol and Scott, Carol E. and Bentley, David R. and Schuler, Gregory and Chen, Hsiu-Chuan and Jang, Wonhee and Green, Eric D. and Idol, Jacquelyn R. and Maduro, Valerie V. Braden and Montgomery, Kate T. and Lee, Eunice and Miller, Ashley and Emerling, Suzanne and Kucherlapati, Raju and Gibbs, Richard and Scherer, Steve and Gorrell, J. Harley and Sodergren, Erica and Clerc-Blankenburg, Kerstin and Tabor, Paul and Naylor, Susan and Garcia, Dawn and de Jong, Pieter J. and Catanese, Joseph J. and Nowak, Norma and Osoegawa, Kazutoyo and Qin, Shizhen and Rowen, Lee and Madan, Anuradha and Dors, Monica and Hood, Leroy and Trask, Barbara and Friedman, Cynthia and Massa, Hillary and Cheung, Vivian G. and Kirsch, Ilan R. and Reid, Thomas and Yonescu, Raluca and Weissenbach, Jean and Bruls, Thomas and Heilig, Roland and Branscomb, Elbert and Olsen, Anne and Doggett, Norman and Cheng, Jan-Fang and Hawkins, Trevor and Myers, Richard M. and Shang, Jin and Ramirez, Lucia and Schmutz, Jeremy and Velasquez, Olivia and Dixon, Kami and Stone, Nancy E. and Cox, David R. and Haussler, David and Kent, W. James and Furey, Terrence and Rogic, Sanja and Kennedy, Scot and Jones, Steven and Rosenthal, Andre and Wen, Gaiping and Schilhabel, Markus and Gloeckner, Gernot and Nyakatura, Gerald and Siebert, Reiner and Schlegelberger, Brigitte and Korenberg, Julie and Chen, Xiao-Ning and Fujiyama, Asao and Hattori, Masahira and Toyoda, Atsushi and Yada, Tetsushi and Park, Hong-Seok and Sakaki, Yoshiyuki and Shimizu, Nobuyoshi and Asakawa, Shuichi and Kawasaki, Kazuhiko and Sasaki, Takashi and Shintani, Ai and Shimizu, Atsushi and Shibuya, Kazunori and Kudoh, Jun and Minoshima, Shinsei and Ramser, Juliane and Seranski, Peter and Hoff, Celine and Poustka, Annemarie and Reinhardt, Richard and Lehrach, Hans},
	month = feb,
	year = {2001},
	pages = {934--941},
}

@article{the_international_human_genome_mapping_consortium_physical_2001,
	title = {A physical map of the human genome},
	volume = {409},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/35057157},
	doi = {10.1038/35057157},
	language = {en},
	number = {6822},
	urldate = {2023-06-30},
	journal = {Nature},
	author = {{The International Human Genome Mapping Consortium} and {Washington University School of Medicine, Genome Sequencing Center:} and McPherson, John D. and Marra, Marco and Hillier, LaDeana and Waterston, Robert H. and Chinwalla, Asif and Wallis, John and Sekhon, Mandeep and Wylie, Kristine and Mardis, Elaine R. and Wilson, Richard K. and Fulton, Robert and Kucaba, Tamara A. and Wagner-McPherson, Caryn and Barbazuk, William B. and {Wellcome Trust Genome Campus:} and Gregory, Simon G. and Humphray, Sean J. and French, Lisa and Evans, Richard S. and Bethel, Graeme and Whittaker, Adam and Holden, Jane L. and McCann, Owen T. and Dunham, Andrew and Soderlund, Carol and Scott, Carol E. and Bentley, David R. and {National Center for Biotechnology Information:} and Schuler, Gregory and Chen, Hsiu-Chuan and Jang, Wonhee and {National Human Genome Research Insititute:} and Green, Eric D. and Idol, Jacquelyn R. and Maduro, Valerie V. Braden and {Albert Einstein College of Medicine:} and Montgomery, Kate T. and Lee, Eunice and Miller, Ashley and Emerling, Suzanne and Kucherlapati, Raju and {Baylor College of Medicine, Human Genome Sequencing Center:} and Gibbs, Richard and Scherer, Steve and Gorrell, J. Harley and Sodergren, Erica and Clerc-Blankenburg, Kerstin and Tabor, Paul and Naylor, Susan and Garcia, Dawn and {Roswell Park Cancer Institute:} and De Jong, Pieter J. and Catanese, Joseph J. and Nowak, Norma and Osoegawa, Kazutoyo and {Multimegabase Sequencing Center:} and Qin, Shizhen and Rowen, Lee and Madan, Anuradha and Dors, Monica and Hood, Leroy and {Fred Hutchinson Cancer Research Institute:} and Trask, Barbara and Friedman, Cynthia and Massa, Hillary and {The Children's Hospital of Philadelphia:} and Cheung, Vivian G. and Kirsch, Ilan R. and Reid, Thomas and Yonescu, Raluca and {Genoscope:} and Weissenbach, Jean and Bruls, Thomas and Heilig, Roland and {US DOE Joint Genome Institute:} and Branscomb, Elbert and Olsen, Anne and Doggett, Norman and Cheng, Jan-Fang and Hawkins, Trevor and {Stanford Human Genome Center and Department of Genetics:} and Myers, Richard M. and Shang, Jin and Ramirez, Lucia and Schmutz, Jeremy and Velasquez, Olivia and Dixon, Kami and Stone, Nancy E. and Cox, David R. and {University of California, Santa Cruz:} and Haussler, David and Kent, W. James and Furey, Terrence and Rogic, Sanja and Kennedy, Scot and {British Columbia Cancer Research Centre:} and Jones, Steven and {Department of Genome Analysis, Institute of Molecular Biotechnology:} and Rosenthal, André and Wen, Gaiping and Schilhabel, Markus and Gloeckner, Gernot and Nyakatura, Gerald and Siebert, Reiner and Schlegelberger, Brigitte and {Departments of Human Genetics and Pediatrics, University of California:} and Korenberg, Julie and Chen, Xiao-Ning and {RIKEN Genomic Sciences Center:} and Fujiyama, Asao and Hattori, Masahira and Toyoda, Atsushi and Yada, Tetsushi and Park, Hong-Seok and Sakaki, Yoshiyuki and {Department of Molecular Biology, Keio University School of Medicine:} and Shimizu, Nobuyoshi and Asakawa, Shuichi and Kawasaki, Kazuhiko and Sasaki, Takashi and Shintani, Ai and Shimizu, Atsushi and Shibuya, Kazunori and Kudoh, Jun and Minoshima, Shinsei and {Max-Planck-Institute for Molecular Genetics:} and Ramser, Juliane and Seranski, Peter and Hoff, Celine and Poustka, Annemarie and Reinhardt, Richard and Lehrach, Hans},
	month = feb,
	year = {2001},
	pages = {934--941},
}

@article{vlachas_backpropagation_2020,
	title = {Backpropagation algorithms and {Reservoir} {Computing} in {Recurrent} {Neural} {Networks} for the forecasting of complex spatiotemporal dynamics},
	volume = {126},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608020300708},
	doi = {10.1016/j.neunet.2020.02.016},
	language = {en},
	urldate = {2023-06-30},
	journal = {Neural Networks},
	author = {Vlachas, Pantelis R. and Pathak, Jaideep and Hunt, Brian R. and Sapsis, Themistoklis P. and Girvan, Michelle and Ott, Edward and Koumoutsakos, Petros},
	month = jun,
	year = {2020},
	pages = {191--217},
}

@article{krishnagopal_separation_2020,
	title = {Separation of chaotic signals by reservoir computing},
	volume = {30},
	issn = {1054-1500, 1089-7682},
	url = {https://pubs.aip.org/aip/cha/article/1078551},
	doi = {10.1063/1.5132766},
	language = {en},
	number = {2},
	urldate = {2023-06-30},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Krishnagopal, Sanjukta and Girvan, Michelle and Ott, Edward and Hunt, Brian R.},
	month = feb,
	year = {2020},
	pages = {023123},
}

@article{ghosh_quantum_2019,
	title = {Quantum reservoir processing},
	volume = {5},
	issn = {2056-6387},
	url = {https://www.nature.com/articles/s41534-019-0149-8},
	doi = {10.1038/s41534-019-0149-8},
	abstract = {Abstract
            The concurrent rise of artificial intelligence and quantum information poses an opportunity for creating interdisciplinary technologies like quantum neural networks. Quantum reservoir processing, introduced here, is a platform for quantum information processing developed on the principle of reservoir computing that is a form of an artificial neural network. A quantum reservoir processor can perform qualitative tasks like recognizing quantum states that are entangled as well as quantitative tasks like estimating a nonlinear function of an input quantum state (e.g., entropy, purity, or logarithmic negativity). In this way, experimental schemes that require measurements of multiple observables can be simplified to measurement of one observable on a trained quantum reservoir processor.},
	language = {en},
	number = {1},
	urldate = {2023-06-30},
	journal = {npj Quantum Information},
	author = {Ghosh, Sanjib and Opala, Andrzej and Matuszewski, Michał and Paterek, Tomasz and Liew, Timothy C. H.},
	month = apr,
	year = {2019},
	pages = {35},
}

@article{chen_temporal_2020,
	title = {Temporal {Information} {Processing} on {Noisy} {Quantum} {Computers}},
	volume = {14},
	issn = {2331-7019},
	url = {https://link.aps.org/doi/10.1103/PhysRevApplied.14.024065},
	doi = {10.1103/PhysRevApplied.14.024065},
	language = {en},
	number = {2},
	urldate = {2023-06-30},
	journal = {Physical Review Applied},
	author = {Chen, Jiayin and Nurdin, Hendra I. and Yamamoto, Naoki},
	month = aug,
	year = {2020},
	pages = {024065},
}

@article{tanaka_recent_2019,
	title = {Recent advances in physical reservoir computing: {A} review},
	volume = {115},
	issn = {08936080},
	shorttitle = {Recent advances in physical reservoir computing},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608019300784},
	doi = {10.1016/j.neunet.2019.03.005},
	language = {en},
	urldate = {2023-06-30},
	journal = {Neural Networks},
	author = {Tanaka, Gouhei and Yamane, Toshiyuki and Héroux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
	month = jul,
	year = {2019},
	pages = {100--123},
}

@article{bi_synaptic_2001,
	title = {Synaptic {Modification} by {Correlated} {Activity}: {Hebb}'s {Postulate} {Revisited}},
	volume = {24},
	issn = {0147-006X, 1545-4126},
	shorttitle = {Synaptic {Modification} by {Correlated} {Activity}},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.neuro.24.1.139},
	doi = {10.1146/annurev.neuro.24.1.139},
	abstract = {▪ Abstract  Correlated spiking of pre- and postsynaptic neurons can result in strengthening or weakening of synapses, depending on the temporal order of spiking. Recent findings indicate that there are narrow and cell type–specific temporal windows for such synaptic modification and that the generally accepted input- (or synapse-) specific rule for modification appears not to be strictly adhered to. Spike timing–dependent modifications, together with selective spread of synaptic changes, provide a set of cellular mechanisms that are likely to be important for the development and functioning of neural networks.
            When an axon of cell A is near enough to excite cell B or repeatedly or consistently takes part in firing it, some growth or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased.      Donald Hebb (1949)},
	language = {en},
	number = {1},
	urldate = {2023-06-29},
	journal = {Annual Review of Neuroscience},
	author = {Bi, Guo-qiang and Poo, Mu-ming},
	month = mar,
	year = {2001},
	pages = {139--166},
}

@incollection{adamatzky_reservoir_2018,
	address = {New York, NY},
	title = {Reservoir {Computing}},
	isbn = {978-1-4939-6882-4 978-1-4939-6883-1},
	url = {http://link.springer.com/10.1007/978-1-4939-6883-1_683},
	language = {en},
	urldate = {2023-06-29},
	booktitle = {Unconventional {Computing}},
	publisher = {Springer US},
	author = {Konkoli, Zoran},
	editor = {Adamatzky, Andrew},
	year = {2018},
	doi = {10.1007/978-1-4939-6883-1_683},
	pages = {619--629},
}

@incollection{montavon_practical_2012,
	address = {Berlin, Heidelberg},
	title = {A {Practical} {Guide} to {Applying} {Echo} {State} {Networks}},
	volume = {7700},
	isbn = {978-3-642-35288-1 978-3-642-35289-8},
	url = {http://link.springer.com/10.1007/978-3-642-35289-8_36},
	language = {en},
	urldate = {2023-06-29},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}},
	publisher = {Springer Berlin Heidelberg},
	author = {Lukoševičius, Mantas},
	editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
	year = {2012},
	doi = {10.1007/978-3-642-35289-8_36},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {659--686},
}

@article{erdos_random_1959,
	title = {On {Random} {Graphs} {I}},
	volume = {6},
	journal = {Publicationes Mathematicae Debrecen},
	author = {Erdös, Paul and Rényi, Alfred},
	year = {1959},
	keywords = {graph sna},
	pages = {290},
}

@article{tanaka_recent_2019-1,
	title = {Recent advances in physical reservoir computing: {A} review},
	volume = {115},
	issn = {08936080},
	shorttitle = {Recent advances in physical reservoir computing},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608019300784},
	doi = {10.1016/j.neunet.2019.03.005},
	language = {en},
	urldate = {2023-06-29},
	journal = {Neural Networks},
	author = {Tanaka, Gouhei and Yamane, Toshiyuki and Héroux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
	month = jul,
	year = {2019},
	pages = {100--123},
}

@incollection{adamatzky_reservoir_2018-1,
	address = {New York, NY},
	title = {Reservoir {Computing}},
	isbn = {978-1-4939-6882-4 978-1-4939-6883-1},
	url = {http://link.springer.com/10.1007/978-1-4939-6883-1_683},
	language = {en},
	urldate = {2023-06-29},
	booktitle = {Unconventional {Computing}},
	publisher = {Springer US},
	author = {Konkoli, Zoran},
	editor = {Adamatzky, Andrew},
	year = {2018},
	doi = {10.1007/978-1-4939-6883-1_683},
	pages = {619--629},
}

@article{maass_real-time_2002,
	title = {Real-{Time} {Computing} {Without} {Stable} {States}: {A} {New} {Framework} for {Neural} {Computation} {Based} on {Perturbations}},
	volume = {14},
	issn = {0899-7667, 1530-888X},
	shorttitle = {Real-{Time} {Computing} {Without} {Stable} {States}},
	url = {https://direct.mit.edu/neco/article/14/11/2531-2560/6650},
	doi = {10.1162/089976602760407955},
	abstract = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.},
	language = {en},
	number = {11},
	urldate = {2023-06-29},
	journal = {Neural Computation},
	author = {Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
	month = nov,
	year = {2002},
	pages = {2531--2560},
}

@article{jager_echo_2001,
	title = {The "{Echo} {State}" {Approach} to {Analysing} and {Training} {Recurrent} {Neural} {Networks}-with an {Erratum} {Note}'},
	volume = {148},
	journal = {German National Research Institute for Computer Science GMD: Technical Report},
	author = {Jäger, Herbert},
	month = jan,
	year = {2001},
}

@article{kheradpisheh_stdp-based_2018,
	title = {{STDP}-based spiking deep convolutional neural networks for object recognition},
	volume = {99},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608017302903},
	doi = {10.1016/j.neunet.2017.12.005},
	language = {en},
	urldate = {2023-06-29},
	journal = {Neural Networks},
	author = {Kheradpisheh, Saeed Reza and Ganjtabesh, Mohammad and Thorpe, Simon J. and Masquelier, Timothée},
	month = mar,
	year = {2018},
	pages = {56--67},
}

@article{nunes_spiking_2022,
	title = {Spiking {Neural} {Networks}: {A} {Survey}},
	volume = {10},
	issn = {2169-3536},
	shorttitle = {Spiking {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/document/9787485/},
	doi = {10.1109/ACCESS.2022.3179968},
	urldate = {2023-06-29},
	journal = {IEEE Access},
	author = {Nunes, Joao D. and Carvalho, Marcelo and Carneiro, Diogo and Cardoso, Jaime S.},
	year = {2022},
	pages = {60738--60764},
}

@article{brette_adaptive_2005,
	title = {Adaptive {Exponential} {Integrate}-and-{Fire} {Model} as an {Effective} {Description} of {Neuronal} {Activity}},
	volume = {94},
	issn = {0022-3077, 1522-1598},
	url = {https://www.physiology.org/doi/10.1152/jn.00686.2005},
	doi = {10.1152/jn.00686.2005},
	abstract = {We introduce a two-dimensional integrate-and-fire model that combines an exponential spike mechanism with an adaptation equation, based on recent theoretical findings. We describe a systematic method to estimate its parameters with simple electrophysiological protocols (current-clamp injection of pulses and ramps) and apply it to a detailed conductance-based model of a regular spiking neuron. Our simple model predicts correctly the timing of 96\% of the spikes (±2 ms) of the detailed model in response to injection of noisy synaptic conductances. The model is especially reliable in high-conductance states, typical of cortical activity in vivo, in which intrinsic conductances were found to have a reduced role in shaping spike trains. These results are promising because this simple model has enough expressive power to reproduce qualitatively several electrophysiological classes described in vitro.},
	language = {en},
	number = {5},
	urldate = {2023-06-29},
	journal = {Journal of Neurophysiology},
	author = {Brette, Romain and Gerstner, Wulfram},
	month = nov,
	year = {2005},
	pages = {3637--3642},
}

@article{abbott_lapicques_1999,
	title = {Lapicque’s introduction of the integrate-and-fire model neuron (1907)},
	volume = {50},
	issn = {03619230},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0361923099001616},
	doi = {10.1016/S0361-9230(99)00161-6},
	language = {en},
	number = {5-6},
	urldate = {2023-06-29},
	journal = {Brain Research Bulletin},
	author = {Abbott, Larry F.},
	month = nov,
	year = {1999},
	pages = {303--304},
}

@article{izhikevich_simple_2003,
	title = {Simple model of spiking neurons},
	volume = {14},
	issn = {1045-9227},
	url = {http://ieeexplore.ieee.org/document/1257420/},
	doi = {10.1109/TNN.2003.820440},
	language = {en},
	number = {6},
	urldate = {2023-06-29},
	journal = {IEEE Transactions on Neural Networks},
	author = {Izhikevich, Eeugene M.},
	month = nov,
	year = {2003},
	pages = {1569--1572},
}

@incollection{eckmiller_spike_1990,
	address = {Amsterdam ; New York : New York, N.Y., U.S.A},
	title = {Spike arrival times: {A} highly efficient coding scheme for neural networks},
	isbn = {978-0-444-88390-2},
	booktitle = {Parallel processing in neural systems and computers},
	publisher = {North-Holland ; Distributors for the U.S. and Canada, Elsevier Science Pub. Co},
	author = {Thorpe, Simon J.},
	editor = {Eckmiller, Rolf and Hartmann, Georg and Hauske, Gert},
	year = {1990},
	note = {Meeting Name: International Conference on Parallel Processing in Neural Systems and Computers},
	keywords = {Congresses, Neural computers, Parallel processing (Electronic computers)},
	pages = {91--94},
}

@article{maass_networks_1997,
	title = {Networks of spiking neurons: {The} third generation of neural network models},
	volume = {10},
	issn = {08936080},
	shorttitle = {Networks of spiking neurons},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608097000117},
	doi = {10.1016/S0893-6080(97)00011-7},
	language = {en},
	number = {9},
	urldate = {2023-06-29},
	journal = {Neural Networks},
	author = {Maass, Wolfgang},
	month = dec,
	year = {1997},
	pages = {1659--1671},
}

@book{noauthor_information_1997,
	address = {Boston, MA},
	series = {The {Information} {Retrieval} {Series}},
	title = {Information {Retrieval} {Systems}},
	volume = {1},
	isbn = {978-0-7923-9926-1},
	url = {http://link.springer.com/10.1007/b102478},
	language = {en},
	urldate = {2023-06-29},
	publisher = {Springer US},
	year = {1997},
	doi = {10.1007/b102478},
}

@misc{ramsauer_hopfield_2021,
	title = {Hopfield {Networks} is {All} {You} {Need}},
	url = {http://arxiv.org/abs/2008.02217},
	doi = {10.48550/arXiv.2008.02217},
	abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers},
	urldate = {2023-06-29},
	publisher = {arXiv},
	author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
	month = apr,
	year = {2021},
	note = {arXiv:2008.02217 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{demircigil_model_2017,
	title = {On a {Model} of {Associative} {Memory} with {Huge} {Storage} {Capacity}},
	volume = {168},
	issn = {0022-4715, 1572-9613},
	url = {http://link.springer.com/10.1007/s10955-017-1806-y},
	doi = {10.1007/s10955-017-1806-y},
	language = {en},
	number = {2},
	urldate = {2023-06-29},
	journal = {Journal of Statistical Physics},
	author = {Demircigil, Mete and Heusel, Judith and Löwe, Matthias and Upgang, Sven and Vermet, Franck},
	month = jul,
	year = {2017},
	pages = {288--299},
}

@inproceedings{krotov_dense_2016,
	address = {Red Hook, NY, USA},
	title = {Dense {Associative} {Memory} for {Pattern} {Recognition}},
	volume = {29},
	isbn = {978-1-5108-3881-9},
	abstract = {A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions - the higher rectified polynomials which until now have not been used in deep learning. The utility of the dense memories is illustrated for two test cases: the logical gate XOR and the recognition of handwritten digits from the MNIST data set.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	publisher = {Curran Associates Inc.},
	author = {Krotov, Dmitry and Hopfield, John J.},
	editor = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},
	year = {2016},
	note = {event-place: Barcelona, Spain},
	pages = {1180--1188},
}

@book{lee_advances_2017,
	address = {Red Hook, NY},
	title = {Advances in neural information processing systems 29: 30th {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2016: {Barcelona}, {Spain}, 5-10 {December} 2016},
	isbn = {978-1-5108-3881-9},
	shorttitle = {Advances in neural information processing systems 29},
	language = {eng},
	publisher = {Curran Associates, Inc},
	editor = {Lee, Daniel D. and Luxburg, Ulrike von and Garnett, Roman and Sugiyama, Masashi and Guyon, Isabelle and Neural Information Processing Systems Foundation},
	year = {2017},
	note = {Meeting Name: Annual Conference on Neural Information Processing Systems},
}

@article{mceliece_capacity_1987,
	title = {The capacity of the {Hopfield} associative memory},
	volume = {33},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1057328/},
	doi = {10.1109/TIT.1987.1057328},
	language = {en},
	number = {4},
	urldate = {2023-06-29},
	journal = {IEEE Transactions on Information Theory},
	author = {McEliece, Robert J. and Posner, Edward C. and Rodemich, Eugener and Venkatesh, Santoshs},
	month = jul,
	year = {1987},
	pages = {461--482},
}

@article{hopfield_unlearning_1983,
	title = {‘{Unlearning}’ has a stabilizing effect in collective memories},
	volume = {304},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/304158a0},
	doi = {10.1038/304158a0},
	language = {en},
	number = {5922},
	urldate = {2023-06-29},
	journal = {Nature},
	author = {Hopfield, John J. and Feinstein, David I. and Palmer, Richard G.},
	month = jul,
	year = {1983},
	pages = {158--159},
}

@misc{weston_memory_2015,
	title = {Memory {Networks}},
	url = {http://arxiv.org/abs/1410.3916},
	doi = {10.48550/arXiv.1410.3916},
	abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
	urldate = {2023-06-29},
	publisher = {arXiv},
	author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
	month = nov,
	year = {2015},
	note = {arXiv:1410.3916 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Statistics - Machine Learning},
}

@article{fix_discriminatory_1989,
	title = {Discriminatory {Analysis}. {Nonparametric} {Discrimination}: {Consistency} {Properties}},
	volume = {57},
	issn = {03067734},
	shorttitle = {Discriminatory {Analysis}. {Nonparametric} {Discrimination}},
	url = {https://www.jstor.org/stable/1403797?origin=crossref},
	doi = {10.2307/1403797},
	number = {3},
	urldate = {2023-06-29},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	author = {Fix, Evelyn and Hodges, Joseph L.},
	month = dec,
	year = {1989},
	pages = {238},
}

@article{hopfield_neural_1982,
	title = {Neural networks and physical systems with emergent collective computational abilities.},
	volume = {79},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.79.8.2554},
	doi = {10.1073/pnas.79.8.2554},
	abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
	language = {en},
	number = {8},
	urldate = {2023-06-29},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Hopfield, John J.},
	month = apr,
	year = {1982},
	pages = {2554--2558},
}

@inproceedings{teichmann_intrinsic_2015,
	title = {Intrinsic {Plasticity}: {A} {Simple} {Mechanism} to {Stabilize} {Hebbian} {Learning} in {Multilayer} {Neural} {Networks}},
	booktitle = {Workshop {New} {Challenges} in {Neural} {Computation}},
	publisher = {Citeseer},
	author = {Teichmann, Michael and Hamker, Fred},
	month = mar,
	year = {2015},
	pages = {103--111},
}

@inproceedings{joshi_rules_2009,
	address = {Atlanta, Ga, USA},
	title = {Rules for information maximization in spiking neurons using intrinsic plasticity},
	isbn = {978-1-4244-3548-7},
	url = {http://ieeexplore.ieee.org/document/5178625/},
	doi = {10.1109/IJCNN.2009.5178625},
	urldate = {2023-06-29},
	booktitle = {2009 {International} {Joint} {Conference} on {Neural} {Networks}},
	publisher = {IEEE},
	author = {Joshi, Prashant and Triesch, Jochen},
	month = jun,
	year = {2009},
	pages = {1456--1461},
}

@article{vogels_inhibitory_2011,
	title = {Inhibitory {Plasticity} {Balances} {Excitation} and {Inhibition} in {Sensory} {Pathways} and {Memory} {Networks}},
	volume = {334},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.1211095},
	doi = {10.1126/science.1211095},
	abstract = {Plasticity at inhibitory synapses maintains balanced excitatory and inhibitory synaptic inputs at cortical neurons.
          , 
            Cortical neurons receive balanced excitatory and inhibitory synaptic currents. Such a balance could be established and maintained in an experience-dependent manner by synaptic plasticity at inhibitory synapses. We show that this mechanism provides an explanation for the sparse firing patterns observed in response to natural stimuli and fits well with a recently observed interaction of excitatory and inhibitory receptive field plasticity. The introduction of inhibitory plasticity in suitable recurrent networks provides a homeostatic mechanism that leads to asynchronous irregular network states. Further, it can accommodate synaptic memories with activity patterns that become indiscernible from the background state but can be reactivated by external stimuli. Our results suggest an essential role of inhibitory plasticity in the formation and maintenance of functional cortical circuitry.},
	language = {en},
	number = {6062},
	urldate = {2023-06-29},
	journal = {Science},
	author = {Vogels, T. P. and Sprekeler, H. and Zenke, F. and Clopath, C. and Gerstner, W.},
	month = dec,
	year = {2011},
	pages = {1569--1573},
}

@article{simoncelli_natural_2001,
	title = {Natural {Image} {Statistics} and {Neural} {Representation}},
	volume = {24},
	issn = {0147-006X, 1545-4126},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.neuro.24.1.1193},
	doi = {10.1146/annurev.neuro.24.1.1193},
	abstract = {▪ Abstract  It has long been assumed that sensory neurons are adapted, through both evolutionary and developmental processes, to the statistical properties of the signals to which they are exposed. Attneave (1954) , Barlow (1961) proposed that information theory could provide a link between environmental statistics and neural responses through the concept of coding efficiency. Recent developments in statistical modeling, along with powerful computational tools, have enabled researchers to study more sophisticated statistical models for visual images, to validate these models empirically against large sets of data, and to begin experimentally testing the efficient coding hypothesis for both individual neurons and populations of neurons.},
	language = {en},
	number = {1},
	urldate = {2023-06-29},
	journal = {Annual Review of Neuroscience},
	author = {Simoncelli, Eero P. and Olshausen, Bruno A.},
	month = mar,
	year = {2001},
	pages = {1193--1216},
}

@article{intrator_objective_1992,
	title = {Objective function formulation of the {BCM} theory of visual cortical plasticity: {Statistical} connections, stability conditions},
	volume = {5},
	issn = {08936080},
	shorttitle = {Objective function formulation of the {BCM} theory of visual cortical plasticity},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608005800036},
	doi = {10.1016/S0893-6080(05)80003-6},
	language = {en},
	number = {1},
	urldate = {2023-06-29},
	journal = {Neural Networks},
	author = {Intrator, Nathan and Cooper, Leon N.},
	month = jan,
	year = {1992},
	pages = {3--17},
}

@article{bienenstock_theory_1982,
	title = {Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex},
	volume = {2},
	issn = {0270-6474, 1529-2401},
	shorttitle = {Theory for the development of neuron selectivity},
	url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.02-01-00032.1982},
	doi = {10.1523/JNEUROSCI.02-01-00032.1982},
	abstract = {The development of stimulus selectivity in the primary sensory cortex of higher vertebrates is considered in a general mathematical framework. A synaptic evolution scheme of a new kind is proposed in which incoming patterns rather than converging afferents compete. The change in the efficacy of a given synapse depends not only on instantaneous pre- and postsynaptic activities but also on a slowly varying time-averaged value of the postsynaptic activity. Assuming an appropriate nonlinear form for this dependence, development of selectivity is obtained under quite general conditions on the sensory environment. One does not require nonlinearity of the neuron's integrative power nor does one need to assume any particular form for intracortical circuitry. This is first illustrated in simple cases, e.g., when the environment consists of only two different stimuli presented alternately in a random manner. The following formal statement then holds: the state of the system converges with probability 1 to points of maximum selectivity in the state space. We next consider the problem of early development of orientation selectivity and binocular interaction in primary visual cortex. Giving the environment an appropriate form, we obtain orientation tuning curves and ocular dominance comparable to what is observed in normally reared adult cats or monkeys. Simulations with binocular input and various types of normal or altered environments show good agreement with the relevant experimental data. Experiments are suggested that could test our theory further.},
	language = {en},
	number = {1},
	urldate = {2023-06-29},
	journal = {The Journal of Neuroscience},
	author = {Bienenstock, Elie L. and Cooper, Leon N. and Munro, Paul W.},
	month = jan,
	year = {1982},
	pages = {32--48},
}

@article{oja_simplified_1982,
	title = {Simplified neuron model as a principal component analyzer},
	volume = {15},
	issn = {0303-6812, 1432-1416},
	url = {http://link.springer.com/10.1007/BF00275687},
	doi = {10.1007/BF00275687},
	language = {en},
	number = {3},
	urldate = {2023-06-29},
	journal = {Journal of Mathematical Biology},
	author = {Oja, Erkki},
	month = nov,
	year = {1982},
	pages = {267--273},
}

@book{newman_current_1985,
	address = {Edinburgh ; New York},
	title = {Current perspectives in dysphasia},
	isbn = {978-0-443-03039-0},
	publisher = {Churchill Livingstone},
	editor = {Newman, Simon P. and Epstein, Ruth},
	year = {1985},
	keywords = {Aphasia},
}

@article{lillicrap_backpropagation_2020,
	title = {Backpropagation and the brain},
	volume = {21},
	issn = {1471-003X, 1471-0048},
	url = {https://www.nature.com/articles/s41583-020-0277-3},
	doi = {10.1038/s41583-020-0277-3},
	language = {en},
	number = {6},
	urldate = {2023-06-28},
	journal = {Nature Reviews Neuroscience},
	author = {Lillicrap, Timothy P. and Santoro, Adam and Marris, Luke and Akerman, Colin J. and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	pages = {335--346},
}

@inproceedings{glasmachers_limits_2017,
	address = {Yonsei University, Seoul, Republic of Korea},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Limits of {End}-to-{End} {Learning}},
	volume = {77},
	url = {https://proceedings.mlr.press/v77/glasmachers17a.html},
	abstract = {End-to-end learning refers to training a possibly complex learning system by applying gradient-based learning to the system as a whole. End-to-end learning systems are specifically designed so that all modules are differentiable. In effect, not only a central learning machine, but also all “peripheral” modules like representation learning and memory formation are covered by a holistic learning process. The power of end-to-end learning has been demonstrated on many tasks, like playing a whole array of Atari video games with a single architecture. While pushing for solutions to more challenging tasks, network architectures keep growing more and more complex. In this paper we ask the question whether and to what extent end-to-end learning is a future-proof technique in the sense of {\textbackslash}emphscaling to complex and diverse data processing architectures. We point out potential inefficiencies, and we argue in particular that end-to-end learning does not make optimal use of the modular design of present neural networks. Our surprisingly simple experiments demonstrate these inefficiencies, up to the complete breakdown of learning.},
	booktitle = {Proceedings of the {Ninth} {Asian} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Glasmachers, Tobias},
	editor = {Zhang, Min-Ling and Noh, Yung-Kyun},
	month = nov,
	year = {2017},
	pages = {17--32},
}

@article{diamond_identifying_2019,
	title = {Identifying what makes a neuron fire},
	volume = {597},
	issn = {0022-3751, 1469-7793},
	url = {https://onlinelibrary.wiley.com/doi/10.1113/JP278049},
	doi = {10.1113/JP278049},
	language = {en},
	number = {10},
	urldate = {2023-06-28},
	journal = {The Journal of Physiology},
	author = {Diamond, Mathew E.},
	month = may,
	year = {2019},
	pages = {2607--2608},
}

@article{wilson_spontaneous_1981,
	title = {Spontaneous firing patterns of identified spiny neurons in the rat neostriatum},
	volume = {220},
	issn = {00068993},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0006899381902110},
	doi = {10.1016/0006-8993(81)90211-0},
	language = {en},
	number = {1},
	urldate = {2023-06-28},
	journal = {Brain Research},
	author = {Wilson, Charles J. and Groves, Philip M.},
	month = sep,
	year = {1981},
	pages = {67--80},
}

@book{moravec_mind_1995,
	address = {Cambridge},
	title = {Mind {Children}: {The} {Future} of {Robot} and {Human} {Intelligence}},
	isbn = {978-0-674-57618-6},
	language = {en},
	publisher = {Harvard University Press},
	author = {Moravec, Hans},
	year = {1995},
}

@book{dong_deep_2020,
	address = {Singapore},
	title = {Deep {Reinforcement} {Learning}: {Fundamentals}, {Research} and {Applications}},
	isbn = {9789811540943 9789811540950},
	shorttitle = {Deep {Reinforcement} {Learning}},
	url = {http://link.springer.com/10.1007/978-981-15-4095-0},
	language = {en},
	urldate = {2023-06-28},
	publisher = {Springer Singapore},
	editor = {Dong, Hao and Ding, Zihan and Zhang, Shanghang},
	year = {2020},
	doi = {10.1007/978-981-15-4095-0},
}

@article{garcia-martin_estimation_2019,
	title = {Estimation of energy consumption in machine learning},
	volume = {134},
	issn = {07437315},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0743731518308773},
	doi = {10.1016/j.jpdc.2019.07.007},
	language = {en},
	urldate = {2023-06-28},
	journal = {Journal of Parallel and Distributed Computing},
	author = {García-Martín, Eva and Rodrigues, Crefeda Faviola and Riley, Graham and Grahn, Håkan},
	month = dec,
	year = {2019},
	pages = {75--88},
}

@book{koller_probabilistic_2009,
	address = {Massachusetts, USA},
	title = {Probabilistic graphical models: principles and techniques},
	isbn = {978-0-262-01319-2},
	publisher = {MIT Press},
	author = {Koller, Daphne and Friedman, Nir},
	year = {2009},
}

@article{allenby_hierarchical_2005,
	title = {Hierarchical {Bayes} {Models}: {A} {Practitioners} {Guide}},
	issn = {1556-5068},
	shorttitle = {Hierarchical {Bayes} {Models}},
	url = {http://www.ssrn.com/abstract=655541},
	doi = {10.2139/ssrn.655541},
	language = {en},
	urldate = {2023-06-28},
	journal = {SSRN Electronic Journal},
	author = {Allenby, Greg M. and Rossi, Peter E. and McCulloch, Robert E.},
	year = {2005},
}

@misc{marcus_deep_2018,
	title = {Deep {Learning}: {A} {Critical} {Appraisal}},
	shorttitle = {Deep {Learning}},
	url = {http://arxiv.org/abs/1801.00631},
	doi = {10.48550/arXiv.1801.00631},
	abstract = {Although deep learning has historical roots going back decades, neither the term "deep learning" nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Marcus, Gary},
	month = jan,
	year = {2018},
	note = {arXiv:1801.00631 [cs, stat]},
	keywords = {97R40, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.0, I.2.6, Statistics - Machine Learning},
}

@article{madan_when_2022,
	title = {When and how convolutional neural networks generalize to out-of-distribution category–viewpoint combinations},
	volume = {4},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-021-00437-5},
	doi = {10.1038/s42256-021-00437-5},
	language = {en},
	number = {2},
	urldate = {2023-06-28},
	journal = {Nature Machine Intelligence},
	author = {Madan, Spandan and Henry, Timothy and Dozier, Jamell and Ho, Helen and Bhandari, Nishchal and Sasaki, Tomotake and Durand, Frédo and Pfister, Hanspeter and Boix, Xavier},
	month = feb,
	year = {2022},
	pages = {146--153},
}

@article{parisi_continual_2019,
	title = {Continual lifelong learning with neural networks: {A} review},
	volume = {113},
	issn = {08936080},
	shorttitle = {Continual lifelong learning with neural networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608019300231},
	doi = {10.1016/j.neunet.2019.01.012},
	language = {en},
	urldate = {2023-06-28},
	journal = {Neural Networks},
	author = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
	month = may,
	year = {2019},
	pages = {54--71},
}

@inproceedings{sahoo_online_2018-1,
	address = {Stockholm, Sweden},
	title = {Online {Deep} {Learning}: {Learning} {Deep} {Neural} {Networks} on the {Fly}},
	isbn = {978-0-9992411-2-7},
	shorttitle = {Online {Deep} {Learning}},
	url = {https://www.ijcai.org/proceedings/2018/369},
	doi = {10.24963/ijcai.2018/369},
	abstract = {Deep Neural Networks (DNNs) are typically trained by backpropagation in a batch setting, requiring the entire training data to be made available prior to the learning task. This is not scalable for many real-world scenarios where new data arrives sequentially in a stream. We aim to address an open challenge of ``Online Deep Learning" (ODL) for learning DNNs on the fly in an online setting. Unlike traditional online learning that often optimizes some convex objective function with respect to a shallow model (e.g., a linear/kernel-based hypothesis), ODL is more challenging as the optimization objective is non-convex, and regular DNN with standard backpropagation does not work well in practice for online settings. We present a new ODL framework that attempts to tackle the challenges by learning DNN models which dynamically adapt depth from a sequence of training data in an online learning setting. Specifically, we propose a novel Hedge Backpropagation (HBP) method for online updating the parameters of DNN effectively, and validate the efficacy on large data sets (both stationary and concept drifting scenarios).},
	language = {en},
	urldate = {2023-06-28},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Sahoo, Doyen and Pham, Quang and Lu, Jing and Hoi, Steven C. H.},
	month = jul,
	year = {2018},
	pages = {2660--2666},
}

@article{zhang_survey_2022,
	title = {A {Survey} on {Multi}-{Task} {Learning}},
	volume = {34},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {https://ieeexplore.ieee.org/document/9392366/},
	doi = {10.1109/TKDE.2021.3070203},
	number = {12},
	urldate = {2023-06-28},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhang, Yu and Yang, Qiang},
	month = dec,
	year = {2022},
	pages = {5586--5609},
}

@article{kirkpatrick_overcoming_2017,
	title = {Overcoming catastrophic forgetting in neural networks},
	volume = {114},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1611835114},
	doi = {10.1073/pnas.1611835114},
	abstract = {Significance
            Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially. In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by synaptic consolidation in neuroscience, enables state of the art results on multiple reinforcement learning problems experienced sequentially.
          , 
            The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.},
	language = {en},
	number = {13},
	urldate = {2023-06-28},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
	month = mar,
	year = {2017},
	pages = {3521--3526},
}

@article{liu_overcoming_2021,
	title = {Overcoming {Catastrophic} {Forgetting} in {Graph} {Neural} {Networks}},
	volume = {35},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17049},
	doi = {10.1609/aaai.v35i10.17049},
	abstract = {Catastrophic forgetting refers to the tendency that a neural network ``forgets'' the previous learned knowledge upon learning new tasks. Prior methods have been focused on overcoming this problem on convolutional neural networks (CNNs), where the input samples like images lie in a grid domain, but have largely overlooked graph neural networks (GNNs) that handle non-grid data. In this paper, we propose a novel scheme dedicated to overcoming catastrophic forgetting problem and hence strengthen continual learning in GNNs. At the heart of our approach is a generic module, termed as topology-aware weight preserving (TWP), applicable to arbitrary form of GNNs in a plug-and-play fashion. Unlike the main stream of CNN-based continual learning methods that rely on solely slowing down the updates of parameters important to the downstream task, TWP explicitly explores the local structures of the input graph, and attempts to stabilize the parameters playing pivotal roles in the topological aggregation. We evaluate TWP on different GNN backbones over several datasets, and demonstrate that it yields performances superior to the state of the art. Code is publicly available at https://github.com/hhliu79/TWP.},
	number = {10},
	urldate = {2023-06-28},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Liu, Huihui and Yang, Yiding and Wang, Xinchao},
	month = may,
	year = {2021},
	pages = {8653--8661},
}

@inproceedings{zhou_distilling_2021,
	title = {Distilling {Holistic} {Knowledge} {With} {Graph} {Neural} {Networks}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Zhou, Sheng and Wang, Yucheng and Chen, Defang and Chen, Jiawei and Wang, Xin and Wang, Can and Bu, Jiajun},
	month = oct,
	year = {2021},
	pages = {10387--10396},
}

@article{choudhary_comprehensive_2020,
	title = {A comprehensive survey on model compression and acceleration},
	volume = {53},
	issn = {0269-2821, 1573-7462},
	url = {http://link.springer.com/10.1007/s10462-020-09816-7},
	doi = {10.1007/s10462-020-09816-7},
	language = {en},
	number = {7},
	urldate = {2023-06-28},
	journal = {Artificial Intelligence Review},
	author = {Choudhary, Tejalal and Mishra, Vipul and Goswami, Anurag and Sarangapani, Jagannathan},
	month = oct,
	year = {2020},
	pages = {5113--5155},
}

@misc{wu_integer_2020,
	title = {Integer {Quantization} for {Deep} {Learning} {Inference}: {Principles} and {Empirical} {Evaluation}},
	shorttitle = {Integer {Quantization} for {Deep} {Learning} {Inference}},
	url = {http://arxiv.org/abs/2004.09602},
	doi = {10.48550/arXiv.2004.09602},
	abstract = {Quantization techniques can reduce the size of Deep Neural Networks and improve inference latency and throughput by taking advantage of high throughput integer instructions. In this paper we review the mathematical aspects of quantization parameters and evaluate their choices on a wide range of neural network models for different application domains, including vision, speech, and language. We focus on quantization techniques that are amenable to acceleration by processors with high-throughput integer math pipelines. We also present a workflow for 8-bit quantization that is able to maintain accuracy within 1\% of the floating-point baseline on all networks studied, including models that are more difficult to quantize, such as MobileNets and BERT-large.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Wu, Hao and Judd, Patrick and Zhang, Xiaojie and Isaev, Mikhail and Micikevicius, Paulius},
	month = apr,
	year = {2020},
	note = {arXiv:2004.09602 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{berthelier_deep_2021,
	title = {Deep {Model} {Compression} and {Architecture} {Optimization} for {Embedded} {Systems}: {A} {Survey}},
	volume = {93},
	issn = {1939-8018, 1939-8115},
	shorttitle = {Deep {Model} {Compression} and {Architecture} {Optimization} for {Embedded} {Systems}},
	url = {https://link.springer.com/10.1007/s11265-020-01596-1},
	doi = {10.1007/s11265-020-01596-1},
	language = {en},
	number = {8},
	urldate = {2023-06-28},
	journal = {Journal of Signal Processing Systems},
	author = {Berthelier, Anthony and Chateau, Thierry and Duffner, Stefan and Garcia, Christophe and Blanc, Christophe},
	month = aug,
	year = {2021},
	pages = {863--878},
}

@misc{smith_using_2022,
	title = {Using {DeepSpeed} and {Megatron} to {Train} {Megatron}-{Turing} {NLG} {530B}, {A} {Large}-{Scale} {Generative} {Language} {Model}},
	url = {http://arxiv.org/abs/2201.11990},
	doi = {10.48550/arXiv.2201.11990},
	abstract = {Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and Zhang, Elton and Child, Rewon and Aminabadi, Reza Yazdani and Bernauer, Julie and Song, Xia and Shoeybi, Mohammad and He, Yuxiong and Houston, Michael and Tiwary, Saurabh and Catanzaro, Bryan},
	month = feb,
	year = {2022},
	note = {arXiv:2201.11990 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{brown_language_2020-1,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {1877--1901},
}

@inproceedings{peters_deep_2018,
	address = {New Orleans, Louisiana},
	title = {Deep {Contextualized} {Word} {Representations}},
	url = {http://aclweb.org/anthology/N18-1202},
	doi = {10.18653/v1/N18-1202},
	language = {en},
	urldate = {2023-06-27},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of           the {Association} for {Computational} {Linguistics}: {Human} {Language}           {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	year = {2018},
	pages = {2227--2237},
}

@misc{open_ai_ai_2018,
	title = {{AI} and {Compute}},
	url = {https://openai.com/blog/ai-and-compute/},
	urldate = {2022-08-19},
	author = {{Open AI}},
	year = {2018},
}

@misc{kumar_fundamental_2015,
	title = {Fundamental {Limits} to {Moore}'s {Law}},
	url = {http://arxiv.org/abs/1511.05956},
	doi = {10.48550/arXiv.1511.05956},
	abstract = {The theoretical and practical aspects of the fundamental, ultimate, physical limits to scaling, or Moore-s law, is presented.},
	urldate = {2023-06-27},
	publisher = {arXiv},
	author = {Kumar, Suhas},
	month = nov,
	year = {2015},
	note = {arXiv:1511.05956 [cond-mat]},
	keywords = {Condensed Matter - Mesoscale and Nanoscale Physics},
}

@article{moore_cramming_1965,
	title = {Cramming {More} {Components} onto {Integrated} {Circuits}},
	volume = {38},
	number = {8},
	journal = {Electronics},
	author = {Moore, Gordon E.},
	month = apr,
	year = {1965},
	note = {Publisher: McGraw-Hill New York},
	pages = {114--1116},
}

@article{asgari_taghanaki_deep_2021,
	title = {Deep semantic segmentation of natural and medical images: a review},
	volume = {54},
	issn = {0269-2821, 1573-7462},
	shorttitle = {Deep semantic segmentation of natural and medical images},
	url = {https://link.springer.com/10.1007/s10462-020-09854-1},
	doi = {10.1007/s10462-020-09854-1},
	language = {en},
	number = {1},
	urldate = {2023-06-27},
	journal = {Artificial Intelligence Review},
	author = {Asgari Taghanaki, Saeid and Abhishek, Kumar and Cohen, Joseph Paul and Cohen-Adad, Julien and Hamarneh, Ghassan},
	month = jan,
	year = {2021},
	pages = {137--178},
}

@article{zou_object_2023,
	title = {Object {Detection} in 20 {Years}: {A} {Survey}},
	volume = {111},
	issn = {0018-9219, 1558-2256},
	shorttitle = {Object {Detection} in 20 {Years}},
	url = {https://ieeexplore.ieee.org/document/10028728/},
	doi = {10.1109/JPROC.2023.3238524},
	number = {3},
	urldate = {2023-06-27},
	journal = {Proceedings of the IEEE},
	author = {Zou, Zhengxia and Chen, Keyan and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
	month = mar,
	year = {2023},
	pages = {257--276},
}

@inproceedings{simmler_survey_2021,
	address = {Lucerne, Switzerland},
	title = {A {Survey} of {Un}-, {Weakly}-, and {Semi}-{Supervised} {Learning} {Methods} for {Noisy}, {Missing} and {Partial} {Labels} in {Industrial} {Vision} {Applications}},
	isbn = {978-1-66543-874-2},
	url = {https://ieeexplore.ieee.org/document/9474624/},
	doi = {10.1109/SDS51136.2021.00012},
	urldate = {2023-06-27},
	booktitle = {2021 8th {Swiss} {Conference} on {Data} {Science} ({SDS})},
	publisher = {IEEE},
	author = {Simmler, Niclas and Sager, Pascal and Andermatt, Philipp and Chavarriaga, Ricardo and Schilling, Frank-Peter and Rosenthal, Matthias and Stadelmann, Thilo},
	month = jun,
	year = {2021},
	pages = {26--31},
}

@article{schmarje_survey_2021,
	title = {A {Survey} on {Semi}-, {Self}- and {Unsupervised} {Learning} for {Image} {Classification}},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9442775/},
	doi = {10.1109/ACCESS.2021.3084358},
	urldate = {2023-06-27},
	journal = {IEEE Access},
	author = {Schmarje, Lars and Santarossa, Monty and Schroder, Simon-Martin and Koch, Reinhard},
	year = {2021},
	pages = {82146--82168},
}

@misc{venture_beat_new_2023,
	title = {New {Deep} {Learning} {Model} {Brings} {Image} {Segmentation} to {Edge} {Devices}},
	url = {https://venturebeat.com/ai/new-deep-learning-model-brings-image-segmentation-to-edge-devices/},
	urldate = {2023-02-19},
	author = {{Venture Beat}},
	year = {2023},
}

@article{sharma_implications_2019,
	title = {Implications of {Pooling} {Strategies} in {Convolutional} {Neural} {Networks}: {A} {Deep} {Insight}},
	volume = {44},
	issn = {2300-3405},
	shorttitle = {Implications of {Pooling} {Strategies} in {Convolutional} {Neural} {Networks}},
	url = {https://www.sciendo.com/article/10.2478/fcds-2019-0016},
	doi = {10.2478/fcds-2019-0016},
	abstract = {Abstract
            Convolutional neural networks (CNN) is a contemporary technique for computer vision applications, where pooling implies as an integral part of the deep CNN. Besides, pooling provides the ability to learn invariant features and also acts as a regularizer to further reduce the problem of overfitting. Additionally, the pooling techniques significantly reduce the computational cost and training time of networks which are equally important to consider. Here, the performances of pooling strategies on different datasets are analyzed and discussed qualitatively. This study presents a detailed review of the conventional and the latest strategies which would help in appraising the readers with the upsides and downsides of each strategy. Also, we have identified four fundamental factors namely network architecture, activation function, overlapping and regularization approaches which immensely affect the performance of pooling operations. It is believed that this work would help in extending the scope of understanding the significance of CNN along with pooling regimes for solving computer vision problems.},
	language = {en},
	number = {3},
	urldate = {2023-06-27},
	journal = {Foundations of Computing and Decision Sciences},
	author = {Sharma, Shallu and Mehra, Rajesh},
	month = sep,
	year = {2019},
	pages = {303--330},
}

@article{sharma_implications_2019-1,
	title = {Implications of {Pooling} {Strategies} in {Convolutional} {Neural} {Networks}: {A} {Deep} {Insight}},
	volume = {44},
	issn = {2300-3405},
	shorttitle = {Implications of {Pooling} {Strategies} in {Convolutional} {Neural} {Networks}},
	url = {https://www.sciendo.com/article/10.2478/fcds-2019-0016},
	doi = {10.2478/fcds-2019-0016},
	abstract = {Abstract
            Convolutional neural networks (CNN) is a contemporary technique for computer vision applications, where pooling implies as an integral part of the deep CNN. Besides, pooling provides the ability to learn invariant features and also acts as a regularizer to further reduce the problem of overfitting. Additionally, the pooling techniques significantly reduce the computational cost and training time of networks which are equally important to consider. Here, the performances of pooling strategies on different datasets are analyzed and discussed qualitatively. This study presents a detailed review of the conventional and the latest strategies which would help in appraising the readers with the upsides and downsides of each strategy. Also, we have identified four fundamental factors namely network architecture, activation function, overlapping and regularization approaches which immensely affect the performance of pooling operations. It is believed that this work would help in extending the scope of understanding the significance of CNN along with pooling regimes for solving computer vision problems.},
	language = {en},
	number = {3},
	urldate = {2023-06-27},
	journal = {Foundations of Computing and Decision Sciences},
	author = {Sharma, Shallu and Mehra, Rajesh},
	month = sep,
	year = {2019},
	pages = {303--330},
}

@inproceedings{tolstikhin_mlp-mixer_2021,
	title = {{MLP}-{Mixer}: {An} all-{MLP} {Architecture} for {Vision}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {24261--24272},
}

@inproceedings{dosovitskiy_image_2021,
	address = {Austria},
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	booktitle = {9th {International} {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	year = {2021},
}

@inproceedings{kolesnikov_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	author = {Kolesnikov, Alexander and Dosovitskiy, Alexey and Weissenborn, Dirk and Heigold, Georg and Uszkoreit, Jakob and Beyer, Lucas and Minderer, Matthias and Dehghani, Mostafa and Houlsby, Neil and Gelly, Sylvain and Unterthiner, Thomas and Zhai, Xiaohua},
	year = {2021},
}

@inproceedings{he_deep_2016,
	address = {Las Vegas, NV, USA},
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	doi = {10.1109/CVPR.2016.90},
	urldate = {2023-06-27},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	pages = {770--778},
}

@inproceedings{ioffe_batch_2015,
	series = {{ICML}'15},
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	volume = {37},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {JMLR.org},
	author = {Ioffe, Sergey and Szegedy, Christian},
	year = {2015},
	note = {Place: Lille, France},
	pages = {448--456},
}

@inproceedings{szegedy_going_2015,
	address = {Boston, MA, USA},
	title = {Going deeper with convolutions},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298594/},
	doi = {10.1109/CVPR.2015.7298594},
	urldate = {2023-06-27},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = jun,
	year = {2015},
	pages = {1--9},
}

@inproceedings{ciresan_flexible_2011,
	address = {Barcelona, Spain},
	series = {{IJCAI}'11},
	title = {Flexible, {High} {Performance} {Convolutional} {Neural} {Networks} for {Image} {Classification}},
	volume = {2},
	isbn = {978-1-57735-514-4},
	abstract = {We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53\%, 19.51\%, 0.35\%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42\%, 0.97\% and 0.48\% after 1, 3 and 17 epochs, respectively.},
	booktitle = {Proceedings of the {Twenty}-{Second} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Cireşan, Dan C. and Meier, Ueli and Masci, Jonathan and Gambardella, Luca M. and Schmidhuber, Jürgen},
	year = {2011},
	pages = {1237--1242},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	url = {http://www.deeplearningbook.org},
	urldate = {2023-06-27},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
}

@article{zhang_parallel_1990,
	title = {Parallel distributed processing model with local space-invariant interconnections and its optical architecture},
	volume = {29},
	issn = {0003-6935, 1539-4522},
	url = {https://opg.optica.org/abstract.cfm?URI=ao-29-32-4790},
	doi = {10.1364/AO.29.004790},
	language = {en},
	number = {32},
	urldate = {2023-06-27},
	journal = {Applied Optics},
	author = {Zhang, Wei and Itoh, Kazuyoshi and Tanida, Jun and Ichioka, Yoshiki},
	month = nov,
	year = {1990},
	pages = {4790},
}

@incollection{gerber_stride_2020,
	address = {Cham},
	title = {Stride and {Translation} {Invariance} in {CNNs}},
	volume = {1342},
	isbn = {978-3-030-66150-2 978-3-030-66151-9},
	url = {https://link.springer.com/10.1007/978-3-030-66151-9_17},
	language = {en},
	urldate = {2023-06-27},
	booktitle = {Artificial {Intelligence} {Research}},
	publisher = {Springer International Publishing},
	author = {Mouton, Coenraad and Myburgh, Johannes C. and Davel, Marelie H.},
	editor = {Gerber, Aurona},
	year = {2020},
	doi = {10.1007/978-3-030-66151-9_17},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {267--281},
}

@inproceedings{waibel_phoneme_1987,
	address = {Tokyo, Japan},
	title = {Phoneme {Recognition} {Using} {Time}-{Delay} {Neural} {Networks}},
	language = {en},
	booktitle = {Meeting of the {Institute} of {Electrical}},
	author = {Waibel, Alex},
	year = {1987},
}

@misc{noauthor_notitle_nodate,
}

@article{lecun_backpropagation_1989,
	title = {Backpropagation {Applied} to {Handwritten} {Zip} {Code} {Recognition}},
	volume = {1},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/1/4/541-551/5515},
	doi = {10.1162/neco.1989.1.4.541},
	abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
	language = {en},
	number = {4},
	urldate = {2023-06-27},
	journal = {Neural Computation},
	author = {LeCun, Yann and Boser, Bernhard and Denker, John S. and Henderson, Donnie and Howard, Richard E. and Hubbard, Wayne and Jackel, Lawrence D.},
	month = dec,
	year = {1989},
	pages = {541--551},
}

@article{waibel_phoneme_1989,
	title = {Phoneme recognition using time-delay neural networks},
	volume = {37},
	issn = {00963518},
	url = {http://ieeexplore.ieee.org/document/21701/},
	doi = {10.1109/29.21701},
	number = {3},
	urldate = {2023-06-27},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Waibel, Alexander and Hanazawa, Toshiyuki and Hinton, Geoffrey E. and Shikano, Kiyohiro and Lang, Kevin J.},
	month = mar,
	year = {1989},
	pages = {328--339},
}

@article{fukushima_neocognitron_1980,
	title = {Neocognitron: {A} self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
	volume = {36},
	issn = {0340-1200, 1432-0770},
	shorttitle = {Neocognitron},
	url = {http://link.springer.com/10.1007/BF00344251},
	doi = {10.1007/BF00344251},
	language = {en},
	number = {4},
	urldate = {2023-06-27},
	journal = {Biological Cybernetics},
	author = {Fukushima, Kunihiko},
	month = apr,
	year = {1980},
	pages = {193--202},
}

@article{fukushima_neocognitron_2007,
	title = {Neocognitron},
	volume = {2},
	issn = {1941-6016},
	url = {http://www.scholarpedia.org/article/Neocognitron},
	doi = {10.4249/scholarpedia.1717},
	language = {en},
	number = {1},
	urldate = {2023-06-27},
	journal = {Scholarpedia},
	author = {Fukushima, Kunihiko},
	year = {2007},
	pages = {1717},
}

@article{hubel_receptive_1968,
	title = {Receptive fields and functional architecture of monkey striate cortex},
	volume = {195},
	issn = {00223751},
	url = {https://onlinelibrary.wiley.com/doi/10.1113/jphysiol.1968.sp008455},
	doi = {10.1113/jphysiol.1968.sp008455},
	language = {en},
	number = {1},
	urldate = {2023-06-27},
	journal = {The Journal of Physiology},
	author = {Hubel, D. H. and Wiesel, T. N.},
	month = mar,
	year = {1968},
	pages = {215--243},
}

@book{prince_understanding_2023,
	address = {S.l.},
	title = {Understanding {Deep} {Learning}},
	isbn = {978-0-262-04864-4},
	language = {eng},
	publisher = {MIT PRESS},
	author = {Prince, Simon J. D.},
	year = {2023},
	note = {OCLC: 1372277824},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2023-06-27},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{wang_comprehensive_2022,
	title = {A {Comprehensive} {Survey} of {Loss} {Functions} in {Machine} {Learning}},
	volume = {9},
	issn = {2198-5804, 2198-5812},
	url = {https://link.springer.com/10.1007/s40745-020-00253-5},
	doi = {10.1007/s40745-020-00253-5},
	language = {en},
	number = {2},
	urldate = {2023-06-27},
	journal = {Annals of Data Science},
	author = {Wang, Qi and Ma, Yue and Zhao, Kun and Tian, Yingjie},
	month = apr,
	year = {2022},
	pages = {187--212},
}

@incollection{cord_supervised_2008,
	address = {Berlin, Heidelberg},
	title = {Supervised {Learning}},
	isbn = {978-3-540-75170-0 978-3-540-75171-7},
	url = {http://link.springer.com/10.1007/978-3-540-75171-7_2},
	language = {en},
	urldate = {2023-06-26},
	booktitle = {Machine {Learning} {Techniques} for {Multimedia}},
	publisher = {Springer Berlin Heidelberg},
	author = {Cunningham, Pádraig and Cord, Matthieu and Delany, Sarah Jane},
	editor = {Cord, Matthieu and Cunningham, Pádraig},
	year = {2008},
	doi = {10.1007/978-3-540-75171-7_2},
	note = {Series Title: Cognitive Technologies},
	pages = {21--49},
}

@article{liu_self-supervised_2021,
	title = {Self-supervised {Learning}: {Generative} or {Contrastive}},
	issn = {1041-4347, 1558-2191, 2326-3865},
	shorttitle = {Self-supervised {Learning}},
	url = {https://ieeexplore.ieee.org/document/9462394/},
	doi = {10.1109/TKDE.2021.3090866},
	urldate = {2023-06-26},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Liu, Xiao and Zhang, Fanjin and Hou, Zhenyu and Mian, Li and Wang, Zhaoyu and Zhang, Jing and Tang, Jie},
	year = {2021},
	pages = {1--1},
}

@book{russell_artificial_2021,
	address = {Hoboken},
	edition = {Fourth edition},
	series = {Pearson series in artificial intelligence},
	title = {Artificial intelligence: a modern approach},
	isbn = {978-0-13-461099-3},
	shorttitle = {Artificial intelligence},
	abstract = {"Updated edition of popular textbook on Artificial Intelligence. This edition specific looks at ways of keeping artificial intelligence under control"--},
	publisher = {Pearson},
	author = {Russell, Stuart J. and Norvig, Peter},
	year = {2021},
	keywords = {Artificial intelligence},
}

@article{van_engelen_survey_2020,
	title = {A survey on semi-supervised learning},
	volume = {109},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-019-05855-6},
	doi = {10.1007/s10994-019-05855-6},
	abstract = {Abstract
            Semi-supervised learning is the branch of machine learning concerned with using labelled as well as unlabelled data to perform certain learning tasks. Conceptually situated between supervised and unsupervised learning, it permits harnessing the large amounts of unlabelled data available in many use cases in combination with typically smaller sets of labelled data. In recent years, research in this area has followed the general trends observed in machine learning, with much attention directed at neural network-based models and generative learning. The literature on the topic has also expanded in volume and scope, now encompassing a broad spectrum of theory, algorithms and applications. However, no recent surveys exist to collect and organize this knowledge, impeding the ability of researchers and engineers alike to utilize it. Filling this void, we present an up-to-date overview of semi-supervised learning methods, covering earlier work as well as more recent advances. We focus primarily on semi-supervised classification, where the large majority of semi-supervised learning research takes place. Our survey aims to provide researchers and practitioners new to the field as well as more advanced readers with a solid understanding of the main approaches and algorithms developed over the past two decades, with an emphasis on the most prominent and currently relevant work. Furthermore, we propose a new taxonomy of semi-supervised classification algorithms, which sheds light on the different conceptual and methodological approaches for incorporating unlabelled data into the training process. Lastly, we show how the fundamental assumptions underlying most semi-supervised learning algorithms are closely connected to each other, and how they relate to the well-known semi-supervised clustering assumption.},
	language = {en},
	number = {2},
	urldate = {2023-06-26},
	journal = {Machine Learning},
	author = {Van Engelen, Jesper E. and Hoos, Holger H.},
	month = feb,
	year = {2020},
	pages = {373--440},
}

@article{arulkumaran_deep_2017,
	title = {Deep {Reinforcement} {Learning}: {A} {Brief} {Survey}},
	volume = {34},
	issn = {1053-5888},
	shorttitle = {Deep {Reinforcement} {Learning}},
	url = {http://ieeexplore.ieee.org/document/8103164/},
	doi = {10.1109/MSP.2017.2743240},
	number = {6},
	urldate = {2023-06-26},
	journal = {IEEE Signal Processing Magazine},
	author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
	month = nov,
	year = {2017},
	pages = {26--38},
}

@book{rosenblatt_principles_1962,
	series = {Cornell {Aeronautical} {Laboratory}. {Report} no. {VG}-1196-{G}-8},
	title = {Principles of {Neurodynamics}: {Perceptrons} and the {Theory} of {Brain} {Mechanisms}},
	url = {https://books.google.ch/books?id=7FhRAAAAMAAJ},
	publisher = {Spartan Books},
	author = {Rosenblatt, Frank},
	year = {1962},
	lccn = {62012882},
}

@article{linnainmaa_taylor_1976,
	title = {Taylor expansion of the accumulated rounding error},
	volume = {16},
	issn = {0006-3835, 1572-9125},
	url = {http://link.springer.com/10.1007/BF01931367},
	doi = {10.1007/BF01931367},
	language = {en},
	number = {2},
	urldate = {2023-06-26},
	journal = {BIT},
	author = {Linnainmaa, Seppo},
	month = jun,
	year = {1976},
	pages = {146--160},
}

@article{linnainmaa_taylor_1976-1,
	title = {Taylor expansion of the accumulated rounding error},
	volume = {16},
	issn = {0006-3835, 1572-9125},
	url = {http://link.springer.com/10.1007/BF01931367},
	doi = {10.1007/BF01931367},
	language = {en},
	number = {2},
	urldate = {2023-06-26},
	journal = {BIT},
	author = {Linnainmaa, Seppo},
	month = jun,
	year = {1976},
	pages = {146--160},
}

@inproceedings{bengio_deep_2012,
	address = {Bellevue, Washington, USA},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Deep {Learning} of {Representations} for {Unsupervised} and {Transfer} {Learning}},
	volume = {27},
	url = {https://proceedings.mlr.press/v27/bengio12a.html},
	abstract = {Deep learning algorithms seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels, with higher-level learned features defined in terms of lower-level features. The objective is to make these higher-level representations more abstract, with their individual features more invariant to most of the variations that are typically present in the training distribution, while collectively preserving as much as possible of the information in the input. Ideally, we would like these representations to disentangle the unknown factors of variation that underlie the training distribution. Such unsupervised learning of representations can be exploited usefully under the hypothesis that the input distribution P(x) is structurally related to some task of interest, say predicting P(y{\textbar}x). This paper focuses on the context of the Unsupervised and Transfer Learning Challenge, on why unsupervised pre-training of representations can be useful, and how it can be exploited in the transfer learning scenario, where we care about predictions on examples that are not from the same distribution as the training distribution.},
	booktitle = {Proceedings of {ICML} {Workshop} on {Unsupervised} and {Transfer} {Learning}},
	publisher = {PMLR},
	author = {Bengio, Yoshua},
	editor = {Guyon, Isabelle and Dror, Gideon and Lemaire, Vincent and Taylor, Graham and Silver, Daniel},
	month = jul,
	year = {2012},
	pages = {17--36},
}

@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {0932-4194, 1435-568X},
	url = {http://link.springer.com/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	language = {en},
	number = {4},
	urldate = {2023-06-26},
	journal = {Mathematics of Control, Signals, and Systems},
	author = {Cybenko, G.},
	month = dec,
	year = {1989},
	pages = {303--314},
}

@article{fukushima_visual_1969,
	title = {Visual {Feature} {Extraction} by a {Multilayered} {Network} of {Analog} {Threshold} {Elements}},
	volume = {5},
	issn = {0536-1567},
	url = {http://ieeexplore.ieee.org/document/4082265/},
	doi = {10.1109/TSSC.1969.300225},
	number = {4},
	urldate = {2023-06-26},
	journal = {IEEE Transactions on Systems Science and Cybernetics},
	author = {Fukushima, Kunihiko},
	year = {1969},
	pages = {322--333},
}

@book{hebb_organization_1949,
	address = {Oxford,  England},
	title = {The {Organization} of {Behavior}; {A} {Neuropsychological} {Theory}},
	publisher = {Psychology Press},
	author = {Hebb, Donald O.},
	year = {1949},
}

@book{costandi_neuroplasticity_2016,
	address = {Cambridge, MA},
	series = {The {MIT} {Press} essential knowledge series},
	title = {Neuroplasticity},
	isbn = {978-0-262-52933-4},
	publisher = {The MIT Press},
	author = {Costandi, Moheb},
	year = {2016},
	keywords = {Nervous system, Neural transmission, Neuroplasticity, Physiology},
}

@article{coombs_specific_1955,
	title = {The specific ionic conductances and the ionic movements across the motoneuronal membrane that produce the inhibitory post-synaptic potential},
	volume = {130},
	issn = {00223751},
	url = {https://onlinelibrary.wiley.com/doi/10.1113/jphysiol.1955.sp005412},
	doi = {10.1113/jphysiol.1955.sp005412},
	language = {en},
	number = {2},
	urldate = {2023-06-26},
	journal = {The Journal of Physiology},
	author = {Coombs, Jo S. and Eccles, John C. and Fatt, Paul},
	month = nov,
	year = {1955},
	pages = {326--373},
}

@article{takagi_roles_2000,
	title = {Roles of ion channels in {EPSP} integration at neuronal dendrites},
	volume = {37},
	issn = {01680102},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168010200001206},
	doi = {10.1016/S0168-0102(00)00120-6},
	language = {en},
	number = {3},
	urldate = {2023-06-26},
	journal = {Neuroscience Research},
	author = {Takagi, Hiroshi},
	month = jul,
	year = {2000},
	pages = {167--171},
}

@article{felleman_distributed_1991,
	title = {Distributed {Hierarchical} {Processing} in the {Primate} {Cerebral} {Cortex}},
	volume = {1},
	issn = {1047-3211, 1460-2199},
	url = {https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/1.1.1},
	doi = {10.1093/cercor/1.1.1},
	language = {en},
	number = {1},
	urldate = {2023-06-26},
	journal = {Cerebral Cortex},
	author = {Felleman, Daniel J. and Van Essen, David C.},
	month = jan,
	year = {1991},
	pages = {1--47},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	language = {en},
	number = {6088},
	urldate = {2023-06-26},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	pages = {533--536},
}

@article{herculano-houzel_human_2009,
	title = {The human brain in numbers: a linearly scaled-up primate brain},
	volume = {3},
	issn = {16625161},
	shorttitle = {The human brain in numbers},
	url = {http://journal.frontiersin.org/article/10.3389/neuro.09.031.2009/abstract},
	doi = {10.3389/neuro.09.031.2009},
	urldate = {2023-06-26},
	journal = {Frontiers in Human Neuroscience},
	author = {Herculano-Houzel, Suzana},
	year = {2009},
}

@article{zeng_neuronal_2017,
	title = {Neuronal cell-type classification: challenges, opportunities and the path forward},
	volume = {18},
	issn = {1471-003X, 1471-0048},
	shorttitle = {Neuronal cell-type classification},
	url = {https://www.nature.com/articles/nrn.2017.85},
	doi = {10.1038/nrn.2017.85},
	language = {en},
	number = {9},
	urldate = {2023-06-26},
	journal = {Nature Reviews Neuroscience},
	author = {Zeng, Hongkui and Sanes, Joshua R.},
	month = sep,
	year = {2017},
	pages = {530--546},
}

@book{james_principles_1890,
	title = {The principles of psychology},
	publisher = {Henry Holt and Company},
	author = {James, William},
	year = {1890},
}

@book{bain_mind_1873,
	address = {New York},
	title = {Mind and body: {The} theories of their relation},
	volume = {1},
	publisher = {D. Appleton and Company},
	author = {Bain, Alexander},
	year = {1873},
}

@article{hopfield_neural_1982,
	title = {Neural networks and physical systems with emergent collective computational abilities.},
	volume = {79},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.79.8.2554},
	doi = {10.1073/pnas.79.8.2554},
	abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
	language = {en},
	number = {8},
	urldate = {2023-06-26},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Hopfield, J J},
	month = apr,
	year = {1982},
	pages = {2554--2558},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	language = {en},
	number = {7553},
	urldate = {2023-06-26},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
}

@book{mitchell_machine_1997,
	address = {New York},
	series = {{McGraw}-{Hill} series in computer science},
	title = {Machine {Learning}},
	isbn = {978-0-07-042807-2},
	publisher = {McGraw-Hill},
	author = {Mitchell, Tom M.},
	year = {1997},
	keywords = {Computer algorithms, Machine learning},
}

@misc{radford_improving_2018,
	title = {Improving language understanding by generative pre-training},
	url = {https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf},
	urldate = {2023-06-24},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	year = {2018},
}

@misc{noauthor_improving_nodate,
	title = {Improving language understanding by generative pre-training},
}

@article{wiskott_face_1997,
	title = {Face recognition by elastic bunch graph matching},
	volume = {19},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/598235/},
	doi = {10.1109/34.598235},
	number = {7},
	urldate = {2023-06-14},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wiskott, Laurenz and Fellous, Jean-Marc and Kuiger, Norbert and Von Der Malsburg, Christoph},
	month = jul,
	year = {1997},
	pages = {775--779},
}

@book{ivakhnenko_cybernetic_1965,
	address = {New York},
	title = {Cybernetic {Predicting} {Devices}},
	publisher = {CCM Information Corporation},
	author = {Ivakhnenko, Aleksei G. and Lapa, Valentin G.},
	year = {1965},
}

@article{wolfrum_recurrent_2008,
	title = {A recurrent dynamic model for correspondence-based face recognition},
	volume = {8},
	issn = {1534-7362},
	url = {http://jov.arvojournals.org/article.aspx?doi=10.1167/8.7.34},
	doi = {10.1167/8.7.34},
	language = {en},
	number = {7},
	urldate = {2023-03-06},
	journal = {Journal of Vision},
	author = {Wolfrum, Philipp and Wolff, Christian and Lücke, Jörg and von der Malsburg, Christoph},
	month = dec,
	year = {2008},
	pages = {34},
}

@misc{von_der_malsburg_theory_2022,
	title = {A {Theory} of {Natural} {Intelligence}},
	url = {http://arxiv.org/abs/2205.00002},
	doi = {10.48550/arXiv.2205.00002},
	abstract = {Introduction: In contrast to current AI technology, natural intelligence -- the kind of autonomous intelligence that is realized in the brains of animals and humans to attain in their natural environment goals defined by a repertoire of innate behavioral schemata -- is far superior in terms of learning speed, generalization capabilities, autonomy and creativity. How are these strengths, by what means are ideas and imagination produced in natural neural networks? Methods: Reviewing the literature, we put forward the argument that both our natural environment and the brain are of low complexity, that is, require for their generation very little information and are consequently both highly structured. We further argue that the structures of brain and natural environment are closely related. Results: We propose that the structural regularity of the brain takes the form of net fragments (self-organized network patterns) and that these serve as the powerful inductive bias that enables the brain to learn quickly, generalize from few examples and bridge the gap between abstractly defined general goals and concrete situations. Conclusions: Our results have important bearings on open problems in artificial neural network research.},
	urldate = {2022-07-11},
	publisher = {arXiv},
	author = {von der Malsburg, Christoph and Stadelmann, Thilo and Grewe, Benjamin F.},
	month = apr,
	year = {2022},
	note = {arXiv:2205.00002 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, I.2, Quantitative Biology - Neurons and Cognition},
}

@article{yarats_improving_2021,
	title = {Improving {Sample} {Efficiency} in {Model}-{Free} {Reinforcement} {Learning} from {Images}},
	volume = {35},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17276},
	doi = {10.1609/aaai.v35i12.17276},
	abstract = {Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. A promising approach is to learn a latent representation together with the control policy. However, fitting a high-capacity encoder using a scarce reward signal is sample inefficient and leads to poor performance.
Prior work has shown that auxiliary losses, such as image reconstruction, can aid efficient representation learning.  
However, incorporating reconstruction loss into an off-policy learning algorithm often leads to training instability. We explore the underlying reasons and 
identify variational autoencoders, used by previous investigations, as the cause of the divergence.   
Following these findings, we propose effective techniques to improve training stability. 
This results in a simple approach capable of
matching state-of-the-art model-free and model-based algorithms on MuJoCo control tasks. Furthermore, our approach demonstrates robustness to observational noise, surpassing existing approaches in this setting. Code, results, and videos are anonymously available at https://sites.google.com/view/sac-ae/home.},
	number = {12},
	urldate = {2023-06-24},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Yarats, Denis and Zhang, Amy and Kostrikov, Ilya and Amos, Brandon and Pineau, Joelle and Fergus, Rob},
	month = may,
	year = {2021},
	pages = {10674--10681},
}

@article{sager_unsupervised_2022,
	title = {Unsupervised {Domain} {Adaptation} for {Vertebrae} {Detection} and {Identification} in {3D} {CT} {Volumes} {Using} a {Domain} {Sanity} {Loss}},
	volume = {8},
	copyright = {All rights reserved},
	issn = {2313-433X},
	url = {https://www.mdpi.com/2313-433X/8/8/222},
	doi = {10.3390/jimaging8080222},
	abstract = {A variety of medical computer vision applications analyze 2D slices of computed tomography (CT) scans, whereas axial slices from the body trunk region are usually identified based on their relative position to the spine. A limitation of such systems is that either the correct slices must be extracted manually or labels of the vertebrae are required for each CT scan to develop an automated extraction system. In this paper, we propose an unsupervised domain adaptation (UDA) approach for vertebrae detection and identification based on a novel Domain Sanity Loss (DSL) function. With UDA the model’s knowledge learned on a publicly available (source) data set can be transferred to the target domain without using target labels, where the target domain is defined by the specific setup (CT modality, study protocols, applied pre- and processing) at the point of use (e.g., a specific clinic with its specific CT study protocols). With our approach, a model is trained on the source and target data set in parallel. The model optimizes a supervised loss for labeled samples from the source domain and the DSL loss function based on domain-specific “sanity checks” for samples from the unlabeled target domain. Without using labels from the target domain, we are able to identify vertebra centroids with an accuracy of 72.8\%. By adding only ten target labels during training the accuracy increases to 89.2\%, which is on par with the current state-of-the-art for full supervised learning, while using about 20 times less labels. Thus, our model can be used to extract 2D slices from 3D CT scans on arbitrary data sets fully automatically without requiring an extensive labeling effort, contributing to the clinical adoption of medical imaging by hospitals.},
	language = {en},
	number = {8},
	urldate = {2022-09-01},
	journal = {Journal of Imaging},
	author = {Sager, Pascal and Salzmann, Sebastian and Burn, Felice and Stadelmann, Thilo},
	month = aug,
	year = {2022},
	pages = {222},
}

@article{long_survey_2022,
	title = {A survey on adversarial attacks in computer vision: {Taxonomy}, visualization and future directions},
	volume = {121},
	issn = {01674048},
	shorttitle = {A survey on adversarial attacks in computer vision},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167404822002413},
	doi = {10.1016/j.cose.2022.102847},
	language = {en},
	urldate = {2023-06-24},
	journal = {Computers \& Security},
	author = {Long, Teng and Gao, Qi and Xu, Lili and Zhou, Zhangbing},
	month = oct,
	year = {2022},
	pages = {102847},
}

@misc{rosenbloom_defining_2023,
	title = {Defining and {Explorting} the {Intelligence} {Space}},
	url = {http://arxiv.org/abs/2306.06499},
	doi = {10.48550/arXiv.2306.06499},
	abstract = {Intelligence is a difficult concept to define, despite many attempts at doing so. Rather than trying to settle on a single definition, this article introduces a broad perspective on what intelligence is, by laying out a cascade of definitions that induces both a nested hierarchy of three levels of intelligence and a wider-ranging space that is built around them and approximations to them. Within this intelligence space, regions are identified that correspond to both natural -- most particularly, human -- intelligence and artificial intelligence (AI), along with the crossover notion of humanlike intelligence. These definitions are then exploited in early explorations of four more advanced, and likely more controversial, topics: the singularity, generative AI, ethics, and intellectual property.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {Rosenbloom, Paul S.},
	month = jun,
	year = {2023},
	note = {arXiv:2306.06499 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{mitchell_debate_2023,
	title = {The debate over understanding in {AI}’s large language models},
	volume = {120},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/10.1073/pnas.2215907120},
	doi = {10.1073/pnas.2215907120},
	abstract = {We survey a current, heated debate in the artificial intelligence (AI) research community on whether large pretrained language models can be said to understand language—and the physical and social situations language encodes—in any humanlike sense. We describe arguments that have been made for and against such understanding and key questions for the broader sciences of intelligence that have arisen in light of these arguments. We contend that an extended science of intelligence can be developed that will provide insight into distinct modes of understanding, their strengths and limitations, and the challenge of integrating diverse forms of cognition.},
	language = {en},
	number = {13},
	urldate = {2023-06-24},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mitchell, Melanie and Krakauer, David C.},
	month = mar,
	year = {2023},
	pages = {e2215907120},
}

@article{bertolini_machine_2021,
	title = {Machine {Learning} for industrial applications: {A} comprehensive literature review},
	volume = {175},
	issn = {09574174},
	shorttitle = {Machine {Learning} for industrial applications},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095741742100261X},
	doi = {10.1016/j.eswa.2021.114820},
	language = {en},
	urldate = {2023-06-24},
	journal = {Expert Systems with Applications},
	author = {Bertolini, Massimo and Mezzogori, Davide and Neroni, Mattia and Zammori, Francesco},
	month = aug,
	year = {2021},
	pages = {114820},
}

@article{ning_review_2019,
	title = {A {Review} of {Deep} {Learning} {Based} {Speech} {Synthesis}},
	volume = {9},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/9/19/4050},
	doi = {10.3390/app9194050},
	abstract = {Speech synthesis, also known as text-to-speech (TTS), has attracted increasingly more attention. Recent advances on speech synthesis are overwhelmingly contributed by deep learning or even end-to-end techniques which have been utilized to enhance a wide range of application scenarios such as intelligent speech interaction, chatbot or conversational artificial intelligence (AI). For speech synthesis, deep learning based techniques can leverage a large scale of {\textless}text, speech{\textgreater} pairs to learn effective feature representations to bridge the gap between text and speech, thus better characterizing the properties of events. To better understand the research dynamics in the speech synthesis field, this paper firstly introduces the traditional speech synthesis methods and highlights the importance of the acoustic modeling from the composition of the statistical parametric speech synthesis (SPSS) system. It then gives an overview of the advances on deep learning based speech synthesis, including the end-to-end approaches which have achieved start-of-the-art performance in recent years. Finally, it discusses the problems of the deep learning methods for speech synthesis, and also points out some appealing research directions that can bring the speech synthesis research into a new frontier.},
	language = {en},
	number = {19},
	urldate = {2023-06-24},
	journal = {Applied Sciences},
	author = {Ning, Yishuang and He, Sheng and Wu, Zhiyong and Xing, Chunxiao and Zhang, Liang-Jie},
	month = sep,
	year = {2019},
	pages = {4050},
}

@article{otter_survey_2021,
	title = {A {Survey} of the {Usages} of {Deep} {Learning} for {Natural} {Language} {Processing}},
	volume = {32},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/9075398/},
	doi = {10.1109/TNNLS.2020.2979670},
	number = {2},
	urldate = {2023-06-24},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Otter, Daniel W. and Medina, Julian R. and Kalita, Jugal K.},
	month = feb,
	year = {2021},
	pages = {604--624},
}

@article{bhatt_cnn_2021,
	title = {{CNN} {Variants} for {Computer} {Vision}: {History}, {Architecture}, {Application}, {Challenges} and {Future} {Scope}},
	volume = {10},
	issn = {2079-9292},
	shorttitle = {{CNN} {Variants} for {Computer} {Vision}},
	url = {https://www.mdpi.com/2079-9292/10/20/2470},
	doi = {10.3390/electronics10202470},
	abstract = {Computer vision is becoming an increasingly trendy word in the area of image processing. With the emergence of computer vision applications, there is a significant demand to recognize objects automatically. Deep CNN (convolution neural network) has benefited the computer vision community by producing excellent results in video processing, object recognition, picture classification and segmentation, natural language processing, speech recognition, and many other fields. Furthermore, the introduction of large amounts of data and readily available hardware has opened new avenues for CNN study. Several inspirational concepts for the progress of CNN have been investigated, including alternative activation functions, regularization, parameter optimization, and architectural advances. Furthermore, achieving innovations in architecture results in a tremendous enhancement in the capacity of the deep CNN. Significant emphasis has been given to leveraging channel and spatial information, with a depth of architecture and information processing via multi-path. This survey paper focuses mainly on the primary taxonomy and newly released deep CNN architectures, and it divides numerous recent developments in CNN architectures into eight groups. Spatial exploitation, multi-path, depth, breadth, dimension, channel boosting, feature-map exploitation, and attention-based CNN are the eight categories. The main contribution of this manuscript is in comparing various architectural evolutions in CNN by its architectural change, strengths, and weaknesses. Besides, it also includes an explanation of the CNN’s components, the strengths and weaknesses of various CNN variants, research gap or open challenges, CNN applications, and the future research direction.},
	language = {en},
	number = {20},
	urldate = {2023-06-24},
	journal = {Electronics},
	author = {Bhatt, Dulari and Patel, Chirag and Talsania, Hardik and Patel, Jigar and Vaghela, Rasmika and Pandya, Sharnil and Modi, Kirit and Ghayvat, Hemant},
	month = oct,
	year = {2021},
	pages = {2470},
}

@article{dabre_survey_2021,
	title = {A {Survey} of {Multilingual} {Neural} {Machine} {Translation}},
	volume = {53},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3406095},
	doi = {10.1145/3406095},
	abstract = {We present a survey on multilingual neural machine translation (MNMT), which has gained a lot of traction in recent years. MNMT has been useful in improving translation quality as a result of translation knowledge transfer (transfer learning). MNMT is more promising and interesting than its statistical machine translation counterpart, because end-to-end modeling and distributed representations open new avenues for research on machine translation. Many approaches have been proposed to exploit multilingual parallel corpora for improving translation quality. However, the lack of a comprehensive survey makes it difficult to determine which approaches are promising and, hence, deserve further exploration. In this article, we present an in-depth survey of existing literature on MNMT. We first categorize various approaches based on their central use-case and then further categorize them based on resource scenarios, underlying modeling principles, core-issues, and challenges. Wherever possible, we address the strengths and weaknesses of several techniques by comparing them with each other. We also discuss the future directions for MNMT. This article is aimed towards both beginners and experts in NMT. We hope this article will serve as a starting point as well as a source of new ideas for researchers and engineers interested in MNMT.},
	language = {en},
	number = {5},
	urldate = {2023-06-24},
	journal = {ACM Computing Surveys},
	author = {Dabre, Raj and Chu, Chenhui and Kunchukuttan, Anoop},
	month = sep,
	year = {2021},
	pages = {1--38},
}

@misc{liu_summary_2023,
	title = {Summary of {ChatGPT}/{GPT}-4 {Research} and {Perspective} {Towards} the {Future} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2304.01852},
	doi = {10.48550/arXiv.2304.01852},
	abstract = {This paper presents a comprehensive survey of ChatGPT and GPT-4, state-of-the-art large language models (LLM) from the GPT series, and their prospective applications across diverse domains. Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability and performance. We performed an in-depth analysis of 194 relevant papers on arXiv, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains. The findings reveal a significant and increasing interest in ChatGPT/GPT-4 research, predominantly centered on direct natural language processing applications, while also demonstrating considerable potential in areas ranging from education and history to mathematics, medicine, and physics. This study endeavors to furnish insights into ChatGPT's capabilities, potential implications, ethical concerns, and offer direction for future advancements in this field.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {Liu, Yiheng and Han, Tianle and Ma, Siyuan and Zhang, Jiayue and Yang, Yuanyuan and Tian, Jiaming and He, Hao and Li, Antong and He, Mengshen and Liu, Zhengliang and Wu, Zihao and Zhu, Dajiang and Li, Xiang and Qiang, Ning and Shen, Dingang and Liu, Tianming and Ge, Bao},
	month = may,
	year = {2023},
	note = {arXiv:2304.01852 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{bubeck_sparks_2023,
	title = {Sparks of {Artificial} {General} {Intelligence}: {Early} experiments with {GPT}-4},
	shorttitle = {Sparks of {Artificial} {General} {Intelligence}},
	url = {http://arxiv.org/abs/2303.12712},
	doi = {10.48550/arXiv.2303.12712},
	abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
	month = apr,
	year = {2023},
	note = {arXiv:2303.12712 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}


@article{bi_synaptic_2001,
	title = {Synaptic {Modification} by {Correlated} {Activity}: {Hebb}'s {Postulate} {Revisited}},
	volume = {24},
	issn = {0147-006X, 1545-4126},
	shorttitle = {Synaptic {Modification} by {Correlated} {Activity}},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.neuro.24.1.139},
	doi = {10.1146/annurev.neuro.24.1.139},
	abstract = {▪ Abstract  Correlated spiking of pre- and postsynaptic neurons can result in strengthening or weakening of synapses, depending on the temporal order of spiking. Recent findings indicate that there are narrow and cell type–specific temporal windows for such synaptic modification and that the generally accepted input- (or synapse-) specific rule for modification appears not to be strictly adhered to. Spike timing–dependent modifications, together with selective spread of synaptic changes, provide a set of cellular mechanisms that are likely to be important for the development and functioning of neural networks.
            When an axon of cell A is near enough to excite cell B or repeatedly or consistently takes part in firing it, some growth or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased.      Donald Hebb (1949)},
	language = {en},
	number = {1},
	urldate = {2023-06-29},
	journal = {Annual Review of Neuroscience},
	author = {Bi, Guo-qiang and Poo, Mu-ming},
	month = mar,
	year = {2001},
	pages = {139--166},
}

@incollection{adamatzky_reservoir_2018,
	address = {New York, NY},
	title = {Reservoir {Computing}},
	isbn = {978-1-4939-6882-4 978-1-4939-6883-1},
	url = {http://link.springer.com/10.1007/978-1-4939-6883-1_683},
	language = {en},
	urldate = {2023-06-29},
	booktitle = {Unconventional {Computing}},
	publisher = {Springer US},
	author = {Konkoli, Zoran},
	editor = {Adamatzky, Andrew},
	year = {2018},
	doi = {10.1007/978-1-4939-6883-1_683},
	pages = {619--629},
}

@incollection{montavon_practical_2012,
	address = {Berlin, Heidelberg},
	title = {A {Practical} {Guide} to {Applying} {Echo} {State} {Networks}},
	volume = {7700},
	isbn = {978-3-642-35288-1 978-3-642-35289-8},
	url = {http://link.springer.com/10.1007/978-3-642-35289-8_36},
	language = {en},
	urldate = {2023-06-29},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}},
	publisher = {Springer Berlin Heidelberg},
	author = {Lukoševičius, Mantas},
	editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
	year = {2012},
	doi = {10.1007/978-3-642-35289-8_36},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {659--686},
}

@article{erdos_random_1959,
	title = {On {Random} {Graphs} {I}},
	volume = {6},
	journal = {Publicationes Mathematicae Debrecen},
	author = {Erdös, Paul and Rényi, Alfred},
	year = {1959},
	keywords = {graph sna},
	pages = {290},
}

@article{tanaka_recent_2019,
	title = {Recent advances in physical reservoir computing: {A} review},
	volume = {115},
	issn = {08936080},
	shorttitle = {Recent advances in physical reservoir computing},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608019300784},
	doi = {10.1016/j.neunet.2019.03.005},
	language = {en},
	urldate = {2023-06-29},
	journal = {Neural Networks},
	author = {Tanaka, Gouhei and Yamane, Toshiyuki and Héroux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
	month = jul,
	year = {2019},
	pages = {100--123},
}

@incollection{adamatzky_reservoir_2018-1,
	address = {New York, NY},
	title = {Reservoir {Computing}},
	isbn = {978-1-4939-6882-4 978-1-4939-6883-1},
	url = {http://link.springer.com/10.1007/978-1-4939-6883-1_683},
	language = {en},
	urldate = {2023-06-29},
	booktitle = {Unconventional {Computing}},
	publisher = {Springer US},
	author = {Konkoli, Zoran},
	editor = {Adamatzky, Andrew},
	year = {2018},
	doi = {10.1007/978-1-4939-6883-1_683},
	pages = {619--629},
}

@article{maass_real-time_2002,
	title = {Real-{Time} {Computing} {Without} {Stable} {States}: {A} {New} {Framework} for {Neural} {Computation} {Based} on {Perturbations}},
	volume = {14},
	issn = {0899-7667, 1530-888X},
	shorttitle = {Real-{Time} {Computing} {Without} {Stable} {States}},
	url = {https://direct.mit.edu/neco/article/14/11/2531-2560/6650},
	doi = {10.1162/089976602760407955},
	abstract = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.},
	language = {en},
	number = {11},
	urldate = {2023-06-29},
	journal = {Neural Computation},
	author = {Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
	month = nov,
	year = {2002},
	pages = {2531--2560},
}

@article{jager_echo_2001,
	title = {The "{Echo} {State}" {Approach} to {Analysing} and {Training} {Recurrent} {Neural} {Networks}-with an {Erratum} {Note}'},
	volume = {148},
	journal = {German National Research Institute for Computer Science GMD: Technical Report},
	author = {Jäger, Herbert},
	month = jan,
	year = {2001},
}

@article{kheradpisheh_stdp-based_2018,
	title = {{STDP}-based spiking deep convolutional neural networks for object recognition},
	volume = {99},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608017302903},
	doi = {10.1016/j.neunet.2017.12.005},
	language = {en},
	urldate = {2023-06-29},
	journal = {Neural Networks},
	author = {Kheradpisheh, Saeed Reza and Ganjtabesh, Mohammad and Thorpe, Simon J. and Masquelier, Timothée},
	month = mar,
	year = {2018},
	pages = {56--67},
}

@article{nunes_spiking_2022,
	title = {Spiking {Neural} {Networks}: {A} {Survey}},
	volume = {10},
	issn = {2169-3536},
	shorttitle = {Spiking {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/document/9787485/},
	doi = {10.1109/ACCESS.2022.3179968},
	urldate = {2023-06-29},
	journal = {IEEE Access},
	author = {Nunes, Joao D. and Carvalho, Marcelo and Carneiro, Diogo and Cardoso, Jaime S.},
	year = {2022},
	pages = {60738--60764},
}

@article{brette_adaptive_2005,
	title = {Adaptive {Exponential} {Integrate}-and-{Fire} {Model} as an {Effective} {Description} of {Neuronal} {Activity}},
	volume = {94},
	issn = {0022-3077, 1522-1598},
	url = {https://www.physiology.org/doi/10.1152/jn.00686.2005},
	doi = {10.1152/jn.00686.2005},
	abstract = {We introduce a two-dimensional integrate-and-fire model that combines an exponential spike mechanism with an adaptation equation, based on recent theoretical findings. We describe a systematic method to estimate its parameters with simple electrophysiological protocols (current-clamp injection of pulses and ramps) and apply it to a detailed conductance-based model of a regular spiking neuron. Our simple model predicts correctly the timing of 96\% of the spikes (±2 ms) of the detailed model in response to injection of noisy synaptic conductances. The model is especially reliable in high-conductance states, typical of cortical activity in vivo, in which intrinsic conductances were found to have a reduced role in shaping spike trains. These results are promising because this simple model has enough expressive power to reproduce qualitatively several electrophysiological classes described in vitro.},
	language = {en},
	number = {5},
	urldate = {2023-06-29},
	journal = {Journal of Neurophysiology},
	author = {Brette, Romain and Gerstner, Wulfram},
	month = nov,
	year = {2005},
	pages = {3637--3642},
}

@article{abbott_lapicques_1999,
	title = {Lapicque’s introduction of the integrate-and-fire model neuron (1907)},
	volume = {50},
	issn = {03619230},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0361923099001616},
	doi = {10.1016/S0361-9230(99)00161-6},
	language = {en},
	number = {5-6},
	urldate = {2023-06-29},
	journal = {Brain Research Bulletin},
	author = {Abbott, Larry F.},
	month = nov,
	year = {1999},
	pages = {303--304},
}

@article{izhikevich_simple_2003,
	title = {Simple model of spiking neurons},
	volume = {14},
	issn = {1045-9227},
	url = {http://ieeexplore.ieee.org/document/1257420/},
	doi = {10.1109/TNN.2003.820440},
	language = {en},
	number = {6},
	urldate = {2023-06-29},
	journal = {IEEE Transactions on Neural Networks},
	author = {Izhikevich, Eeugene M.},
	month = nov,
	year = {2003},
	pages = {1569--1572},
}

@incollection{eckmiller_spike_1990,
	address = {Amsterdam ; New York : New York, N.Y., U.S.A},
	title = {Spike arrival times: {A} highly efficient coding scheme for neural networks},
	isbn = {978-0-444-88390-2},
	booktitle = {Parallel processing in neural systems and computers},
	publisher = {North-Holland ; Distributors for the U.S. and Canada, Elsevier Science Pub. Co},
	author = {Thorpe, Simon J.},
	editor = {Eckmiller, Rolf and Hartmann, Georg and Hauske, Gert},
	year = {1990},
	note = {Meeting Name: International Conference on Parallel Processing in Neural Systems and Computers},
	keywords = {Congresses, Neural computers, Parallel processing (Electronic computers)},
	pages = {91--94},
}

@article{maass_networks_1997,
	title = {Networks of spiking neurons: {The} third generation of neural network models},
	volume = {10},
	issn = {08936080},
	shorttitle = {Networks of spiking neurons},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608097000117},
	doi = {10.1016/S0893-6080(97)00011-7},
	language = {en},
	number = {9},
	urldate = {2023-06-29},
	journal = {Neural Networks},
	author = {Maass, Wolfgang},
	month = dec,
	year = {1997},
	pages = {1659--1671},
}

@book{noauthor_information_1997,
	address = {Boston, MA},
	series = {The {Information} {Retrieval} {Series}},
	title = {Information {Retrieval} {Systems}},
	volume = {1},
	isbn = {978-0-7923-9926-1},
	url = {http://link.springer.com/10.1007/b102478},
	language = {en},
	urldate = {2023-06-29},
	publisher = {Springer US},
	year = {1997},
	doi = {10.1007/b102478},
}

@misc{ramsauer_hopfield_2021,
	title = {Hopfield {Networks} is {All} {You} {Need}},
	url = {http://arxiv.org/abs/2008.02217},
	doi = {10.48550/arXiv.2008.02217},
	abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers},
	urldate = {2023-06-29},
	publisher = {arXiv},
	author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
	month = apr,
	year = {2021},
	note = {arXiv:2008.02217 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{demircigil_model_2017,
	title = {On a {Model} of {Associative} {Memory} with {Huge} {Storage} {Capacity}},
	volume = {168},
	issn = {0022-4715, 1572-9613},
	url = {http://link.springer.com/10.1007/s10955-017-1806-y},
	doi = {10.1007/s10955-017-1806-y},
	language = {en},
	number = {2},
	urldate = {2023-06-29},
	journal = {Journal of Statistical Physics},
	author = {Demircigil, Mete and Heusel, Judith and Löwe, Matthias and Upgang, Sven and Vermet, Franck},
	month = jul,
	year = {2017},
	pages = {288--299},
}

@inproceedings{krotov_dense_2016,
	address = {Red Hook, NY, USA},
	title = {Dense {Associative} {Memory} for {Pattern} {Recognition}},
	volume = {29},
	isbn = {978-1-5108-3881-9},
	abstract = {A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions - the higher rectified polynomials which until now have not been used in deep learning. The utility of the dense memories is illustrated for two test cases: the logical gate XOR and the recognition of handwritten digits from the MNIST data set.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	publisher = {Curran Associates Inc.},
	author = {Krotov, Dmitry and Hopfield, John J.},
	editor = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},
	year = {2016},
	note = {event-place: Barcelona, Spain},
	pages = {1180--1188},
}

@book{lee_advances_2017,
	address = {Red Hook, NY},
	title = {Advances in neural information processing systems 29: 30th {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2016: {Barcelona}, {Spain}, 5-10 {December} 2016},
	isbn = {978-1-5108-3881-9},
	shorttitle = {Advances in neural information processing systems 29},
	language = {eng},
	publisher = {Curran Associates, Inc},
	editor = {Lee, Daniel D. and Luxburg, Ulrike von and Garnett, Roman and Sugiyama, Masashi and Guyon, Isabelle and Neural Information Processing Systems Foundation},
	year = {2017},
	note = {Meeting Name: Annual Conference on Neural Information Processing Systems},
}

@article{mceliece_capacity_1987,
	title = {The capacity of the {Hopfield} associative memory},
	volume = {33},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1057328/},
	doi = {10.1109/TIT.1987.1057328},
	language = {en},
	number = {4},
	urldate = {2023-06-29},
	journal = {IEEE Transactions on Information Theory},
	author = {McEliece, Robert J. and Posner, Edward C. and Rodemich, Eugener and Venkatesh, Santoshs},
	month = jul,
	year = {1987},
	pages = {461--482},
}

@article{hopfield_unlearning_1983,
	title = {‘{Unlearning}’ has a stabilizing effect in collective memories},
	volume = {304},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/304158a0},
	doi = {10.1038/304158a0},
	language = {en},
	number = {5922},
	urldate = {2023-06-29},
	journal = {Nature},
	author = {Hopfield, John J. and Feinstein, David I. and Palmer, Richard G.},
	month = jul,
	year = {1983},
	pages = {158--159},
}

@misc{weston_memory_2015,
	title = {Memory {Networks}},
	url = {http://arxiv.org/abs/1410.3916},
	doi = {10.48550/arXiv.1410.3916},
	abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
	urldate = {2023-06-29},
	publisher = {arXiv},
	author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
	month = nov,
	year = {2015},
	note = {arXiv:1410.3916 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Statistics - Machine Learning},
}

@article{fix_discriminatory_1989,
	title = {Discriminatory {Analysis}. {Nonparametric} {Discrimination}: {Consistency} {Properties}},
	volume = {57},
	issn = {03067734},
	shorttitle = {Discriminatory {Analysis}. {Nonparametric} {Discrimination}},
	url = {https://www.jstor.org/stable/1403797?origin=crossref},
	doi = {10.2307/1403797},
	number = {3},
	urldate = {2023-06-29},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	author = {Fix, Evelyn and Hodges, Joseph L.},
	month = dec,
	year = {1989},
	pages = {238},
}

@article{hopfield_neural_1982,
	title = {Neural networks and physical systems with emergent collective computational abilities.},
	volume = {79},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.79.8.2554},
	doi = {10.1073/pnas.79.8.2554},
	abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
	language = {en},
	number = {8},
	urldate = {2023-06-29},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Hopfield, John J.},
	month = apr,
	year = {1982},
	pages = {2554--2558},
}

@inproceedings{teichmann_intrinsic_2015,
	title = {Intrinsic {Plasticity}: {A} {Simple} {Mechanism} to {Stabilize} {Hebbian} {Learning} in {Multilayer} {Neural} {Networks}},
	booktitle = {Workshop {New} {Challenges} in {Neural} {Computation}},
	publisher = {Citeseer},
	author = {Teichmann, Michael and Hamker, Fred},
	month = mar,
	year = {2015},
	pages = {103--111},
}

@inproceedings{joshi_rules_2009,
	address = {Atlanta, Ga, USA},
	title = {Rules for information maximization in spiking neurons using intrinsic plasticity},
	isbn = {978-1-4244-3548-7},
	url = {http://ieeexplore.ieee.org/document/5178625/},
	doi = {10.1109/IJCNN.2009.5178625},
	urldate = {2023-06-29},
	booktitle = {2009 {International} {Joint} {Conference} on {Neural} {Networks}},
	publisher = {IEEE},
	author = {Joshi, Prashant and Triesch, Jochen},
	month = jun,
	year = {2009},
	pages = {1456--1461},
}

@article{vogels_inhibitory_2011,
	title = {Inhibitory {Plasticity} {Balances} {Excitation} and {Inhibition} in {Sensory} {Pathways} and {Memory} {Networks}},
	volume = {334},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.1211095},
	doi = {10.1126/science.1211095},
	abstract = {Plasticity at inhibitory synapses maintains balanced excitatory and inhibitory synaptic inputs at cortical neurons.
          , 
            Cortical neurons receive balanced excitatory and inhibitory synaptic currents. Such a balance could be established and maintained in an experience-dependent manner by synaptic plasticity at inhibitory synapses. We show that this mechanism provides an explanation for the sparse firing patterns observed in response to natural stimuli and fits well with a recently observed interaction of excitatory and inhibitory receptive field plasticity. The introduction of inhibitory plasticity in suitable recurrent networks provides a homeostatic mechanism that leads to asynchronous irregular network states. Further, it can accommodate synaptic memories with activity patterns that become indiscernible from the background state but can be reactivated by external stimuli. Our results suggest an essential role of inhibitory plasticity in the formation and maintenance of functional cortical circuitry.},
	language = {en},
	number = {6062},
	urldate = {2023-06-29},
	journal = {Science},
	author = {Vogels, T. P. and Sprekeler, H. and Zenke, F. and Clopath, C. and Gerstner, W.},
	month = dec,
	year = {2011},
	pages = {1569--1573},
}

@article{simoncelli_natural_2001,
	title = {Natural {Image} {Statistics} and {Neural} {Representation}},
	volume = {24},
	issn = {0147-006X, 1545-4126},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.neuro.24.1.1193},
	doi = {10.1146/annurev.neuro.24.1.1193},
	abstract = {▪ Abstract  It has long been assumed that sensory neurons are adapted, through both evolutionary and developmental processes, to the statistical properties of the signals to which they are exposed. Attneave (1954) , Barlow (1961) proposed that information theory could provide a link between environmental statistics and neural responses through the concept of coding efficiency. Recent developments in statistical modeling, along with powerful computational tools, have enabled researchers to study more sophisticated statistical models for visual images, to validate these models empirically against large sets of data, and to begin experimentally testing the efficient coding hypothesis for both individual neurons and populations of neurons.},
	language = {en},
	number = {1},
	urldate = {2023-06-29},
	journal = {Annual Review of Neuroscience},
	author = {Simoncelli, Eero P. and Olshausen, Bruno A.},
	month = mar,
	year = {2001},
	pages = {1193--1216},
}

@article{intrator_objective_1992,
	title = {Objective function formulation of the {BCM} theory of visual cortical plasticity: {Statistical} connections, stability conditions},
	volume = {5},
	issn = {08936080},
	shorttitle = {Objective function formulation of the {BCM} theory of visual cortical plasticity},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608005800036},
	doi = {10.1016/S0893-6080(05)80003-6},
	language = {en},
	number = {1},
	urldate = {2023-06-29},
	journal = {Neural Networks},
	author = {Intrator, Nathan and Cooper, Leon N.},
	month = jan,
	year = {1992},
	pages = {3--17},
}

@article{bienenstock_theory_1982,
	title = {Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex},
	volume = {2},
	issn = {0270-6474, 1529-2401},
	shorttitle = {Theory for the development of neuron selectivity},
	url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.02-01-00032.1982},
	doi = {10.1523/JNEUROSCI.02-01-00032.1982},
	abstract = {The development of stimulus selectivity in the primary sensory cortex of higher vertebrates is considered in a general mathematical framework. A synaptic evolution scheme of a new kind is proposed in which incoming patterns rather than converging afferents compete. The change in the efficacy of a given synapse depends not only on instantaneous pre- and postsynaptic activities but also on a slowly varying time-averaged value of the postsynaptic activity. Assuming an appropriate nonlinear form for this dependence, development of selectivity is obtained under quite general conditions on the sensory environment. One does not require nonlinearity of the neuron's integrative power nor does one need to assume any particular form for intracortical circuitry. This is first illustrated in simple cases, e.g., when the environment consists of only two different stimuli presented alternately in a random manner. The following formal statement then holds: the state of the system converges with probability 1 to points of maximum selectivity in the state space. We next consider the problem of early development of orientation selectivity and binocular interaction in primary visual cortex. Giving the environment an appropriate form, we obtain orientation tuning curves and ocular dominance comparable to what is observed in normally reared adult cats or monkeys. Simulations with binocular input and various types of normal or altered environments show good agreement with the relevant experimental data. Experiments are suggested that could test our theory further.},
	language = {en},
	number = {1},
	urldate = {2023-06-29},
	journal = {The Journal of Neuroscience},
	author = {Bienenstock, Elie L. and Cooper, Leon N. and Munro, Paul W.},
	month = jan,
	year = {1982},
	pages = {32--48},
}

@article{oja_simplified_1982,
	title = {Simplified neuron model as a principal component analyzer},
	volume = {15},
	issn = {0303-6812, 1432-1416},
	url = {http://link.springer.com/10.1007/BF00275687},
	doi = {10.1007/BF00275687},
	language = {en},
	number = {3},
	urldate = {2023-06-29},
	journal = {Journal of Mathematical Biology},
	author = {Oja, Erkki},
	month = nov,
	year = {1982},
	pages = {267--273},
}

@book{newman_current_1985,
	address = {Edinburgh ; New York},
	title = {Current perspectives in dysphasia},
	isbn = {978-0-443-03039-0},
	publisher = {Churchill Livingstone},
	editor = {Newman, Simon P. and Epstein, Ruth},
	year = {1985},
	keywords = {Aphasia},
}

@article{lillicrap_backpropagation_2020,
	title = {Backpropagation and the brain},
	volume = {21},
	issn = {1471-003X, 1471-0048},
	url = {https://www.nature.com/articles/s41583-020-0277-3},
	doi = {10.1038/s41583-020-0277-3},
	language = {en},
	number = {6},
	urldate = {2023-06-28},
	journal = {Nature Reviews Neuroscience},
	author = {Lillicrap, Timothy P. and Santoro, Adam and Marris, Luke and Akerman, Colin J. and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	pages = {335--346},
}

@inproceedings{glasmachers_limits_2017,
	address = {Yonsei University, Seoul, Republic of Korea},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Limits of {End}-to-{End} {Learning}},
	volume = {77},
	url = {https://proceedings.mlr.press/v77/glasmachers17a.html},
	abstract = {End-to-end learning refers to training a possibly complex learning system by applying gradient-based learning to the system as a whole. End-to-end learning systems are specifically designed so that all modules are differentiable. In effect, not only a central learning machine, but also all “peripheral” modules like representation learning and memory formation are covered by a holistic learning process. The power of end-to-end learning has been demonstrated on many tasks, like playing a whole array of Atari video games with a single architecture. While pushing for solutions to more challenging tasks, network architectures keep growing more and more complex. In this paper we ask the question whether and to what extent end-to-end learning is a future-proof technique in the sense of {\textbackslash}emphscaling to complex and diverse data processing architectures. We point out potential inefficiencies, and we argue in particular that end-to-end learning does not make optimal use of the modular design of present neural networks. Our surprisingly simple experiments demonstrate these inefficiencies, up to the complete breakdown of learning.},
	booktitle = {Proceedings of the {Ninth} {Asian} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Glasmachers, Tobias},
	editor = {Zhang, Min-Ling and Noh, Yung-Kyun},
	month = nov,
	year = {2017},
	pages = {17--32},
}

@article{diamond_identifying_2019,
	title = {Identifying what makes a neuron fire},
	volume = {597},
	issn = {0022-3751, 1469-7793},
	url = {https://onlinelibrary.wiley.com/doi/10.1113/JP278049},
	doi = {10.1113/JP278049},
	language = {en},
	number = {10},
	urldate = {2023-06-28},
	journal = {The Journal of Physiology},
	author = {Diamond, Mathew E.},
	month = may,
	year = {2019},
	pages = {2607--2608},
}

@article{wilson_spontaneous_1981,
	title = {Spontaneous firing patterns of identified spiny neurons in the rat neostriatum},
	volume = {220},
	issn = {00068993},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0006899381902110},
	doi = {10.1016/0006-8993(81)90211-0},
	language = {en},
	number = {1},
	urldate = {2023-06-28},
	journal = {Brain Research},
	author = {Wilson, Charles J. and Groves, Philip M.},
	month = sep,
	year = {1981},
	pages = {67--80},
}

@book{moravec_mind_1995,
	address = {Cambridge},
	title = {Mind {Children}: {The} {Future} of {Robot} and {Human} {Intelligence}},
	isbn = {978-0-674-57618-6},
	language = {en},
	publisher = {Harvard University Press},
	author = {Moravec, Hans},
	year = {1995},
}

@book{dong_deep_2020,
	address = {Singapore},
	title = {Deep {Reinforcement} {Learning}: {Fundamentals}, {Research} and {Applications}},
	isbn = {9789811540943 9789811540950},
	shorttitle = {Deep {Reinforcement} {Learning}},
	url = {http://link.springer.com/10.1007/978-981-15-4095-0},
	language = {en},
	urldate = {2023-06-28},
	publisher = {Springer Singapore},
	editor = {Dong, Hao and Ding, Zihan and Zhang, Shanghang},
	year = {2020},
	doi = {10.1007/978-981-15-4095-0},
}

@article{garcia-martin_estimation_2019,
	title = {Estimation of energy consumption in machine learning},
	volume = {134},
	issn = {07437315},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0743731518308773},
	doi = {10.1016/j.jpdc.2019.07.007},
	language = {en},
	urldate = {2023-06-28},
	journal = {Journal of Parallel and Distributed Computing},
	author = {García-Martín, Eva and Rodrigues, Crefeda Faviola and Riley, Graham and Grahn, Håkan},
	month = dec,
	year = {2019},
	pages = {75--88},
}

@book{koller_probabilistic_2009,
	address = {Massachusetts, USA},
	title = {Probabilistic graphical models: principles and techniques},
	isbn = {978-0-262-01319-2},
	publisher = {MIT Press},
	author = {Koller, Daphne and Friedman, Nir},
	year = {2009},
}

@article{allenby_hierarchical_2005,
	title = {Hierarchical {Bayes} {Models}: {A} {Practitioners} {Guide}},
	issn = {1556-5068},
	shorttitle = {Hierarchical {Bayes} {Models}},
	url = {http://www.ssrn.com/abstract=655541},
	doi = {10.2139/ssrn.655541},
	language = {en},
	urldate = {2023-06-28},
	journal = {SSRN Electronic Journal},
	author = {Allenby, Greg M. and Rossi, Peter E. and McCulloch, Robert E.},
	year = {2005},
}

@misc{marcus_deep_2018,
	title = {Deep {Learning}: {A} {Critical} {Appraisal}},
	shorttitle = {Deep {Learning}},
	url = {http://arxiv.org/abs/1801.00631},
	doi = {10.48550/arXiv.1801.00631},
	abstract = {Although deep learning has historical roots going back decades, neither the term "deep learning" nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Marcus, Gary},
	month = jan,
	year = {2018},
	note = {arXiv:1801.00631 [cs, stat]},
	keywords = {97R40, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.0, I.2.6, Statistics - Machine Learning},
}

@article{madan_when_2022,
	title = {When and how convolutional neural networks generalize to out-of-distribution category–viewpoint combinations},
	volume = {4},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-021-00437-5},
	doi = {10.1038/s42256-021-00437-5},
	language = {en},
	number = {2},
	urldate = {2023-06-28},
	journal = {Nature Machine Intelligence},
	author = {Madan, Spandan and Henry, Timothy and Dozier, Jamell and Ho, Helen and Bhandari, Nishchal and Sasaki, Tomotake and Durand, Frédo and Pfister, Hanspeter and Boix, Xavier},
	month = feb,
	year = {2022},
	pages = {146--153},
}

@article{parisi_continual_2019,
	title = {Continual lifelong learning with neural networks: {A} review},
	volume = {113},
	issn = {08936080},
	shorttitle = {Continual lifelong learning with neural networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608019300231},
	doi = {10.1016/j.neunet.2019.01.012},
	language = {en},
	urldate = {2023-06-28},
	journal = {Neural Networks},
	author = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
	month = may,
	year = {2019},
	pages = {54--71},
}

@inproceedings{sahoo_online_2018,
	address = {Stockholm, Sweden},
	title = {Online {Deep} {Learning}: {Learning} {Deep} {Neural} {Networks} on the {Fly}},
	isbn = {978-0-9992411-2-7},
	shorttitle = {Online {Deep} {Learning}},
	url = {https://www.ijcai.org/proceedings/2018/369},
	doi = {10.24963/ijcai.2018/369},
	abstract = {Deep Neural Networks (DNNs) are typically trained by backpropagation in a batch setting, requiring the entire training data to be made available prior to the learning task. This is not scalable for many real-world scenarios where new data arrives sequentially in a stream. We aim to address an open challenge of ``Online Deep Learning" (ODL) for learning DNNs on the fly in an online setting. Unlike traditional online learning that often optimizes some convex objective function with respect to a shallow model (e.g., a linear/kernel-based hypothesis), ODL is more challenging as the optimization objective is non-convex, and regular DNN with standard backpropagation does not work well in practice for online settings. We present a new ODL framework that attempts to tackle the challenges by learning DNN models which dynamically adapt depth from a sequence of training data in an online learning setting. Specifically, we propose a novel Hedge Backpropagation (HBP) method for online updating the parameters of DNN effectively, and validate the efficacy on large data sets (both stationary and concept drifting scenarios).},
	language = {en},
	urldate = {2023-06-28},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Sahoo, Doyen and Pham, Quang and Lu, Jing and Hoi, Steven C. H.},
	month = jul,
	year = {2018},
	pages = {2660--2666},
}

@article{zhang_survey_2022,
	title = {A {Survey} on {Multi}-{Task} {Learning}},
	volume = {34},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {https://ieeexplore.ieee.org/document/9392366/},
	doi = {10.1109/TKDE.2021.3070203},
	number = {12},
	urldate = {2023-06-28},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhang, Yu and Yang, Qiang},
	month = dec,
	year = {2022},
	pages = {5586--5609},
}

@article{kirkpatrick_overcoming_2017,
	title = {Overcoming catastrophic forgetting in neural networks},
	volume = {114},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1611835114},
	doi = {10.1073/pnas.1611835114},
	abstract = {Significance
            Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially. In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by synaptic consolidation in neuroscience, enables state of the art results on multiple reinforcement learning problems experienced sequentially.
          , 
            The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.},
	language = {en},
	number = {13},
	urldate = {2023-06-28},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
	month = mar,
	year = {2017},
	pages = {3521--3526},
}

@article{liu_overcoming_2021,
	title = {Overcoming {Catastrophic} {Forgetting} in {Graph} {Neural} {Networks}},
	volume = {35},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17049},
	doi = {10.1609/aaai.v35i10.17049},
	abstract = {Catastrophic forgetting refers to the tendency that a neural network ``forgets'' the previous learned knowledge upon learning new tasks. Prior methods have been focused on overcoming this problem on convolutional neural networks (CNNs), where the input samples like images lie in a grid domain, but have largely overlooked graph neural networks (GNNs) that handle non-grid data. In this paper, we propose a novel scheme dedicated to overcoming catastrophic forgetting problem and hence strengthen continual learning in GNNs. At the heart of our approach is a generic module, termed as topology-aware weight preserving (TWP), applicable to arbitrary form of GNNs in a plug-and-play fashion. Unlike the main stream of CNN-based continual learning methods that rely on solely slowing down the updates of parameters important to the downstream task, TWP explicitly explores the local structures of the input graph, and attempts to stabilize the parameters playing pivotal roles in the topological aggregation. We evaluate TWP on different GNN backbones over several datasets, and demonstrate that it yields performances superior to the state of the art. Code is publicly available at https://github.com/hhliu79/TWP.},
	number = {10},
	urldate = {2023-06-28},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Liu, Huihui and Yang, Yiding and Wang, Xinchao},
	month = may,
	year = {2021},
	pages = {8653--8661},
}

@inproceedings{zhou_distilling_2021,
	title = {Distilling {Holistic} {Knowledge} {With} {Graph} {Neural} {Networks}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Zhou, Sheng and Wang, Yucheng and Chen, Defang and Chen, Jiawei and Wang, Xin and Wang, Can and Bu, Jiajun},
	month = oct,
	year = {2021},
	pages = {10387--10396},
}

@article{choudhary_comprehensive_2020,
	title = {A comprehensive survey on model compression and acceleration},
	volume = {53},
	issn = {0269-2821, 1573-7462},
	url = {http://link.springer.com/10.1007/s10462-020-09816-7},
	doi = {10.1007/s10462-020-09816-7},
	language = {en},
	number = {7},
	urldate = {2023-06-28},
	journal = {Artificial Intelligence Review},
	author = {Choudhary, Tejalal and Mishra, Vipul and Goswami, Anurag and Sarangapani, Jagannathan},
	month = oct,
	year = {2020},
	pages = {5113--5155},
}

@misc{wu_integer_2020,
	title = {Integer {Quantization} for {Deep} {Learning} {Inference}: {Principles} and {Empirical} {Evaluation}},
	shorttitle = {Integer {Quantization} for {Deep} {Learning} {Inference}},
	url = {http://arxiv.org/abs/2004.09602},
	doi = {10.48550/arXiv.2004.09602},
	abstract = {Quantization techniques can reduce the size of Deep Neural Networks and improve inference latency and throughput by taking advantage of high throughput integer instructions. In this paper we review the mathematical aspects of quantization parameters and evaluate their choices on a wide range of neural network models for different application domains, including vision, speech, and language. We focus on quantization techniques that are amenable to acceleration by processors with high-throughput integer math pipelines. We also present a workflow for 8-bit quantization that is able to maintain accuracy within 1\% of the floating-point baseline on all networks studied, including models that are more difficult to quantize, such as MobileNets and BERT-large.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Wu, Hao and Judd, Patrick and Zhang, Xiaojie and Isaev, Mikhail and Micikevicius, Paulius},
	month = apr,
	year = {2020},
	note = {arXiv:2004.09602 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{berthelier_deep_2021,
	title = {Deep {Model} {Compression} and {Architecture} {Optimization} for {Embedded} {Systems}: {A} {Survey}},
	volume = {93},
	issn = {1939-8018, 1939-8115},
	shorttitle = {Deep {Model} {Compression} and {Architecture} {Optimization} for {Embedded} {Systems}},
	url = {https://link.springer.com/10.1007/s11265-020-01596-1},
	doi = {10.1007/s11265-020-01596-1},
	language = {en},
	number = {8},
	urldate = {2023-06-28},
	journal = {Journal of Signal Processing Systems},
	author = {Berthelier, Anthony and Chateau, Thierry and Duffner, Stefan and Garcia, Christophe and Blanc, Christophe},
	month = aug,
	year = {2021},
	pages = {863--878},
}

@misc{smith_using_2022,
	title = {Using {DeepSpeed} and {Megatron} to {Train} {Megatron}-{Turing} {NLG} {530B}, {A} {Large}-{Scale} {Generative} {Language} {Model}},
	url = {http://arxiv.org/abs/2201.11990},
	doi = {10.48550/arXiv.2201.11990},
	abstract = {Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and Zhang, Elton and Child, Rewon and Aminabadi, Reza Yazdani and Bernauer, Julie and Song, Xia and Shoeybi, Mohammad and He, Yuxiong and Houston, Michael and Tiwary, Saurabh and Catanzaro, Bryan},
	month = feb,
	year = {2022},
	note = {arXiv:2201.11990 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {1877--1901},
}

@inproceedings{peters_deep_2018,
	address = {New Orleans, Louisiana},
	title = {Deep {Contextualized} {Word} {Representations}},
	url = {http://aclweb.org/anthology/N18-1202},
	doi = {10.18653/v1/N18-1202},
	language = {en},
	urldate = {2023-06-27},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of           the {Association} for {Computational} {Linguistics}: {Human} {Language}           {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	year = {2018},
	pages = {2227--2237},
}

@misc{open_ai_ai_2018,
	title = {{AI} and {Compute}},
	url = {https://openai.com/blog/ai-and-compute/},
	urldate = {2022-08-19},
	author = {{Open AI}},
	year = {2018},
}

@misc{kumar_fundamental_2015,
	title = {Fundamental {Limits} to {Moore}'s {Law}},
	url = {http://arxiv.org/abs/1511.05956},
	doi = {10.48550/arXiv.1511.05956},
	abstract = {The theoretical and practical aspects of the fundamental, ultimate, physical limits to scaling, or Moore-s law, is presented.},
	urldate = {2023-06-27},
	publisher = {arXiv},
	author = {Kumar, Suhas},
	month = nov,
	year = {2015},
	note = {arXiv:1511.05956 [cond-mat]},
	keywords = {Condensed Matter - Mesoscale and Nanoscale Physics},
}

@article{moore_cramming_1965,
	title = {Cramming {More} {Components} onto {Integrated} {Circuits}},
	volume = {38},
	number = {8},
	journal = {Electronics},
	author = {Moore, Gordon E.},
	month = apr,
	year = {1965},
	note = {Publisher: McGraw-Hill New York},
	pages = {114--1116},
}

@article{asgari_taghanaki_deep_2021,
	title = {Deep semantic segmentation of natural and medical images: a review},
	volume = {54},
	issn = {0269-2821, 1573-7462},
	shorttitle = {Deep semantic segmentation of natural and medical images},
	url = {https://link.springer.com/10.1007/s10462-020-09854-1},
	doi = {10.1007/s10462-020-09854-1},
	language = {en},
	number = {1},
	urldate = {2023-06-27},
	journal = {Artificial Intelligence Review},
	author = {Asgari Taghanaki, Saeid and Abhishek, Kumar and Cohen, Joseph Paul and Cohen-Adad, Julien and Hamarneh, Ghassan},
	month = jan,
	year = {2021},
	pages = {137--178},
}

@article{zou_object_2023,
	title = {Object {Detection} in 20 {Years}: {A} {Survey}},
	volume = {111},
	issn = {0018-9219, 1558-2256},
	shorttitle = {Object {Detection} in 20 {Years}},
	url = {https://ieeexplore.ieee.org/document/10028728/},
	doi = {10.1109/JPROC.2023.3238524},
	number = {3},
	urldate = {2023-06-27},
	journal = {Proceedings of the IEEE},
	author = {Zou, Zhengxia and Chen, Keyan and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
	month = mar,
	year = {2023},
	pages = {257--276},
}

@inproceedings{simmler_survey_2021,
	address = {Lucerne, Switzerland},
	title = {A {Survey} of {Un}-, {Weakly}-, and {Semi}-{Supervised} {Learning} {Methods} for {Noisy}, {Missing} and {Partial} {Labels} in {Industrial} {Vision} {Applications}},
	isbn = {978-1-66543-874-2},
	url = {https://ieeexplore.ieee.org/document/9474624/},
	doi = {10.1109/SDS51136.2021.00012},
	urldate = {2023-06-27},
	booktitle = {2021 8th {Swiss} {Conference} on {Data} {Science} ({SDS})},
	publisher = {IEEE},
	author = {Simmler, Niclas and Sager, Pascal and Andermatt, Philipp and Chavarriaga, Ricardo and Schilling, Frank-Peter and Rosenthal, Matthias and Stadelmann, Thilo},
	month = jun,
	year = {2021},
	pages = {26--31},
}

@article{schmarje_survey_2021,
	title = {A {Survey} on {Semi}-, {Self}- and {Unsupervised} {Learning} for {Image} {Classification}},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9442775/},
	doi = {10.1109/ACCESS.2021.3084358},
	urldate = {2023-06-27},
	journal = {IEEE Access},
	author = {Schmarje, Lars and Santarossa, Monty and Schroder, Simon-Martin and Koch, Reinhard},
	year = {2021},
	pages = {82146--82168},
}

@misc{venture_beat_new_2023,
	title = {New {Deep} {Learning} {Model} {Brings} {Image} {Segmentation} to {Edge} {Devices}},
	url = {https://venturebeat.com/ai/new-deep-learning-model-brings-image-segmentation-to-edge-devices/},
	urldate = {2023-02-19},
	author = {{Venture Beat}},
	year = {2023},
}

@article{sharma_implications_2019,
	title = {Implications of {Pooling} {Strategies} in {Convolutional} {Neural} {Networks}: {A} {Deep} {Insight}},
	volume = {44},
	issn = {2300-3405},
	shorttitle = {Implications of {Pooling} {Strategies} in {Convolutional} {Neural} {Networks}},
	url = {https://www.sciendo.com/article/10.2478/fcds-2019-0016},
	doi = {10.2478/fcds-2019-0016},
	abstract = {Abstract
            Convolutional neural networks (CNN) is a contemporary technique for computer vision applications, where pooling implies as an integral part of the deep CNN. Besides, pooling provides the ability to learn invariant features and also acts as a regularizer to further reduce the problem of overfitting. Additionally, the pooling techniques significantly reduce the computational cost and training time of networks which are equally important to consider. Here, the performances of pooling strategies on different datasets are analyzed and discussed qualitatively. This study presents a detailed review of the conventional and the latest strategies which would help in appraising the readers with the upsides and downsides of each strategy. Also, we have identified four fundamental factors namely network architecture, activation function, overlapping and regularization approaches which immensely affect the performance of pooling operations. It is believed that this work would help in extending the scope of understanding the significance of CNN along with pooling regimes for solving computer vision problems.},
	language = {en},
	number = {3},
	urldate = {2023-06-27},
	journal = {Foundations of Computing and Decision Sciences},
	author = {Sharma, Shallu and Mehra, Rajesh},
	month = sep,
	year = {2019},
	pages = {303--330},
}

@article{sharma_implications_2019-1,
	title = {Implications of {Pooling} {Strategies} in {Convolutional} {Neural} {Networks}: {A} {Deep} {Insight}},
	volume = {44},
	issn = {2300-3405},
	shorttitle = {Implications of {Pooling} {Strategies} in {Convolutional} {Neural} {Networks}},
	url = {https://www.sciendo.com/article/10.2478/fcds-2019-0016},
	doi = {10.2478/fcds-2019-0016},
	abstract = {Abstract
            Convolutional neural networks (CNN) is a contemporary technique for computer vision applications, where pooling implies as an integral part of the deep CNN. Besides, pooling provides the ability to learn invariant features and also acts as a regularizer to further reduce the problem of overfitting. Additionally, the pooling techniques significantly reduce the computational cost and training time of networks which are equally important to consider. Here, the performances of pooling strategies on different datasets are analyzed and discussed qualitatively. This study presents a detailed review of the conventional and the latest strategies which would help in appraising the readers with the upsides and downsides of each strategy. Also, we have identified four fundamental factors namely network architecture, activation function, overlapping and regularization approaches which immensely affect the performance of pooling operations. It is believed that this work would help in extending the scope of understanding the significance of CNN along with pooling regimes for solving computer vision problems.},
	language = {en},
	number = {3},
	urldate = {2023-06-27},
	journal = {Foundations of Computing and Decision Sciences},
	author = {Sharma, Shallu and Mehra, Rajesh},
	month = sep,
	year = {2019},
	pages = {303--330},
}

@inproceedings{tolstikhin_mlp-mixer_2021,
	title = {{MLP}-{Mixer}: {An} all-{MLP} {Architecture} for {Vision}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {24261--24272},
}

@inproceedings{dosovitskiy_image_2021,
	address = {Austria},
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	booktitle = {9th {International} {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	year = {2021},
}

@inproceedings{kolesnikov_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	author = {Kolesnikov, Alexander and Dosovitskiy, Alexey and Weissenborn, Dirk and Heigold, Georg and Uszkoreit, Jakob and Beyer, Lucas and Minderer, Matthias and Dehghani, Mostafa and Houlsby, Neil and Gelly, Sylvain and Unterthiner, Thomas and Zhai, Xiaohua},
	year = {2021},
}

@inproceedings{he_deep_2016,
	address = {Las Vegas, NV, USA},
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	doi = {10.1109/CVPR.2016.90},
	urldate = {2023-06-27},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	pages = {770--778},
}

@inproceedings{ioffe_batch_2015,
	series = {{ICML}'15},
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	volume = {37},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {JMLR.org},
	author = {Ioffe, Sergey and Szegedy, Christian},
	year = {2015},
	note = {Place: Lille, France},
	pages = {448--456},
}

@inproceedings{szegedy_going_2015,
	address = {Boston, MA, USA},
	title = {Going deeper with convolutions},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298594/},
	doi = {10.1109/CVPR.2015.7298594},
	urldate = {2023-06-27},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = jun,
	year = {2015},
	pages = {1--9},
}

@inproceedings{ciresan_flexible_2011,
	address = {Barcelona, Spain},
	series = {{IJCAI}'11},
	title = {Flexible, {High} {Performance} {Convolutional} {Neural} {Networks} for {Image} {Classification}},
	volume = {2},
	isbn = {978-1-57735-514-4},
	abstract = {We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53\%, 19.51\%, 0.35\%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42\%, 0.97\% and 0.48\% after 1, 3 and 17 epochs, respectively.},
	booktitle = {Proceedings of the {Twenty}-{Second} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Cireşan, Dan C. and Meier, Ueli and Masci, Jonathan and Gambardella, Luca M. and Schmidhuber, Jürgen},
	year = {2011},
	pages = {1237--1242},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	url = {http://www.deeplearningbook.org},
	urldate = {2023-06-27},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
}

@article{zhang_parallel_1990,
	title = {Parallel distributed processing model with local space-invariant interconnections and its optical architecture},
	volume = {29},
	issn = {0003-6935, 1539-4522},
	url = {https://opg.optica.org/abstract.cfm?URI=ao-29-32-4790},
	doi = {10.1364/AO.29.004790},
	language = {en},
	number = {32},
	urldate = {2023-06-27},
	journal = {Applied Optics},
	author = {Zhang, Wei and Itoh, Kazuyoshi and Tanida, Jun and Ichioka, Yoshiki},
	month = nov,
	year = {1990},
	pages = {4790},
}

@incollection{gerber_stride_2020,
	address = {Cham},
	title = {Stride and {Translation} {Invariance} in {CNNs}},
	volume = {1342},
	isbn = {978-3-030-66150-2 978-3-030-66151-9},
	url = {https://link.springer.com/10.1007/978-3-030-66151-9_17},
	language = {en},
	urldate = {2023-06-27},
	booktitle = {Artificial {Intelligence} {Research}},
	publisher = {Springer International Publishing},
	author = {Mouton, Coenraad and Myburgh, Johannes C. and Davel, Marelie H.},
	editor = {Gerber, Aurona},
	year = {2020},
	doi = {10.1007/978-3-030-66151-9_17},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {267--281},
}

@inproceedings{waibel_phoneme_1987,
	address = {Tokyo, Japan},
	title = {Phoneme {Recognition} {Using} {Time}-{Delay} {Neural} {Networks}},
	language = {en},
	booktitle = {Meeting of the {Institute} of {Electrical}},
	author = {Waibel, Alex},
	year = {1987},
}

@misc{noauthor_notitle_nodate,
}

@article{lecun_backpropagation_1989,
	title = {Backpropagation {Applied} to {Handwritten} {Zip} {Code} {Recognition}},
	volume = {1},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/1/4/541-551/5515},
	doi = {10.1162/neco.1989.1.4.541},
	abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
	language = {en},
	number = {4},
	urldate = {2023-06-27},
	journal = {Neural Computation},
	author = {LeCun, Yann and Boser, Bernhard and Denker, John S. and Henderson, Donnie and Howard, Richard E. and Hubbard, Wayne and Jackel, Lawrence D.},
	month = dec,
	year = {1989},
	pages = {541--551},
}

@article{waibel_phoneme_1989,
	title = {Phoneme recognition using time-delay neural networks},
	volume = {37},
	issn = {00963518},
	url = {http://ieeexplore.ieee.org/document/21701/},
	doi = {10.1109/29.21701},
	number = {3},
	urldate = {2023-06-27},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Waibel, Alexander and Hanazawa, Toshiyuki and Hinton, Geoffrey E. and Shikano, Kiyohiro and Lang, Kevin J.},
	month = mar,
	year = {1989},
	pages = {328--339},
}

@article{fukushima_neocognitron_1980,
	title = {Neocognitron: {A} self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
	volume = {36},
	issn = {0340-1200, 1432-0770},
	shorttitle = {Neocognitron},
	url = {http://link.springer.com/10.1007/BF00344251},
	doi = {10.1007/BF00344251},
	language = {en},
	number = {4},
	urldate = {2023-06-27},
	journal = {Biological Cybernetics},
	author = {Fukushima, Kunihiko},
	month = apr,
	year = {1980},
	pages = {193--202},
}

@article{fukushima_neocognitron_2007,
	title = {Neocognitron},
	volume = {2},
	issn = {1941-6016},
	url = {http://www.scholarpedia.org/article/Neocognitron},
	doi = {10.4249/scholarpedia.1717},
	language = {en},
	number = {1},
	urldate = {2023-06-27},
	journal = {Scholarpedia},
	author = {Fukushima, Kunihiko},
	year = {2007},
	pages = {1717},
}

@article{hubel_receptive_1968,
	title = {Receptive fields and functional architecture of monkey striate cortex},
	volume = {195},
	issn = {00223751},
	url = {https://onlinelibrary.wiley.com/doi/10.1113/jphysiol.1968.sp008455},
	doi = {10.1113/jphysiol.1968.sp008455},
	language = {en},
	number = {1},
	urldate = {2023-06-27},
	journal = {The Journal of Physiology},
	author = {Hubel, D. H. and Wiesel, T. N.},
	month = mar,
	year = {1968},
	pages = {215--243},
}

@book{prince_understanding_2023,
	address = {S.l.},
	title = {Understanding {Deep} {Learning}},
	isbn = {978-0-262-04864-4},
	language = {eng},
	publisher = {MIT PRESS},
	author = {Prince, Simon J. D.},
	year = {2023},
	note = {OCLC: 1372277824},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2023-06-27},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{wang_comprehensive_2022,
	title = {A {Comprehensive} {Survey} of {Loss} {Functions} in {Machine} {Learning}},
	volume = {9},
	issn = {2198-5804, 2198-5812},
	url = {https://link.springer.com/10.1007/s40745-020-00253-5},
	doi = {10.1007/s40745-020-00253-5},
	language = {en},
	number = {2},
	urldate = {2023-06-27},
	journal = {Annals of Data Science},
	author = {Wang, Qi and Ma, Yue and Zhao, Kun and Tian, Yingjie},
	month = apr,
	year = {2022},
	pages = {187--212},
}

@incollection{cord_supervised_2008,
	address = {Berlin, Heidelberg},
	title = {Supervised {Learning}},
	isbn = {978-3-540-75170-0 978-3-540-75171-7},
	url = {http://link.springer.com/10.1007/978-3-540-75171-7_2},
	language = {en},
	urldate = {2023-06-26},
	booktitle = {Machine {Learning} {Techniques} for {Multimedia}},
	publisher = {Springer Berlin Heidelberg},
	author = {Cunningham, Pádraig and Cord, Matthieu and Delany, Sarah Jane},
	editor = {Cord, Matthieu and Cunningham, Pádraig},
	year = {2008},
	doi = {10.1007/978-3-540-75171-7_2},
	note = {Series Title: Cognitive Technologies},
	pages = {21--49},
}

@article{liu_self-supervised_2021,
	title = {Self-supervised {Learning}: {Generative} or {Contrastive}},
	issn = {1041-4347, 1558-2191, 2326-3865},
	shorttitle = {Self-supervised {Learning}},
	url = {https://ieeexplore.ieee.org/document/9462394/},
	doi = {10.1109/TKDE.2021.3090866},
	urldate = {2023-06-26},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Liu, Xiao and Zhang, Fanjin and Hou, Zhenyu and Mian, Li and Wang, Zhaoyu and Zhang, Jing and Tang, Jie},
	year = {2021},
	pages = {1--1},
}

@book{russell_artificial_2021,
	address = {Hoboken},
	edition = {Fourth edition},
	series = {Pearson series in artificial intelligence},
	title = {Artificial intelligence: a modern approach},
	isbn = {978-0-13-461099-3},
	shorttitle = {Artificial intelligence},
	abstract = {"Updated edition of popular textbook on Artificial Intelligence. This edition specific looks at ways of keeping artificial intelligence under control"--},
	publisher = {Pearson},
	author = {Russell, Stuart J. and Norvig, Peter},
	year = {2021},
	keywords = {Artificial intelligence},
}

@article{van_engelen_survey_2020,
	title = {A survey on semi-supervised learning},
	volume = {109},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-019-05855-6},
	doi = {10.1007/s10994-019-05855-6},
	abstract = {Abstract
            Semi-supervised learning is the branch of machine learning concerned with using labelled as well as unlabelled data to perform certain learning tasks. Conceptually situated between supervised and unsupervised learning, it permits harnessing the large amounts of unlabelled data available in many use cases in combination with typically smaller sets of labelled data. In recent years, research in this area has followed the general trends observed in machine learning, with much attention directed at neural network-based models and generative learning. The literature on the topic has also expanded in volume and scope, now encompassing a broad spectrum of theory, algorithms and applications. However, no recent surveys exist to collect and organize this knowledge, impeding the ability of researchers and engineers alike to utilize it. Filling this void, we present an up-to-date overview of semi-supervised learning methods, covering earlier work as well as more recent advances. We focus primarily on semi-supervised classification, where the large majority of semi-supervised learning research takes place. Our survey aims to provide researchers and practitioners new to the field as well as more advanced readers with a solid understanding of the main approaches and algorithms developed over the past two decades, with an emphasis on the most prominent and currently relevant work. Furthermore, we propose a new taxonomy of semi-supervised classification algorithms, which sheds light on the different conceptual and methodological approaches for incorporating unlabelled data into the training process. Lastly, we show how the fundamental assumptions underlying most semi-supervised learning algorithms are closely connected to each other, and how they relate to the well-known semi-supervised clustering assumption.},
	language = {en},
	number = {2},
	urldate = {2023-06-26},
	journal = {Machine Learning},
	author = {Van Engelen, Jesper E. and Hoos, Holger H.},
	month = feb,
	year = {2020},
	pages = {373--440},
}

@article{arulkumaran_deep_2017,
	title = {Deep {Reinforcement} {Learning}: {A} {Brief} {Survey}},
	volume = {34},
	issn = {1053-5888},
	shorttitle = {Deep {Reinforcement} {Learning}},
	url = {http://ieeexplore.ieee.org/document/8103164/},
	doi = {10.1109/MSP.2017.2743240},
	number = {6},
	urldate = {2023-06-26},
	journal = {IEEE Signal Processing Magazine},
	author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
	month = nov,
	year = {2017},
	pages = {26--38},
}

@book{rosenblatt_principles_1962,
	series = {Cornell {Aeronautical} {Laboratory}. {Report} no. {VG}-1196-{G}-8},
	title = {Principles of {Neurodynamics}: {Perceptrons} and the {Theory} of {Brain} {Mechanisms}},
	url = {https://books.google.ch/books?id=7FhRAAAAMAAJ},
	publisher = {Spartan Books},
	author = {Rosenblatt, Frank},
	year = {1962},
	lccn = {62012882},
}

@article{linnainmaa_taylor_1976,
	title = {Taylor expansion of the accumulated rounding error},
	volume = {16},
	issn = {0006-3835, 1572-9125},
	url = {http://link.springer.com/10.1007/BF01931367},
	doi = {10.1007/BF01931367},
	language = {en},
	number = {2},
	urldate = {2023-06-26},
	journal = {BIT},
	author = {Linnainmaa, Seppo},
	month = jun,
	year = {1976},
	pages = {146--160},
}

@article{linnainmaa_taylor_1976-1,
	title = {Taylor expansion of the accumulated rounding error},
	volume = {16},
	issn = {0006-3835, 1572-9125},
	url = {http://link.springer.com/10.1007/BF01931367},
	doi = {10.1007/BF01931367},
	language = {en},
	number = {2},
	urldate = {2023-06-26},
	journal = {BIT},
	author = {Linnainmaa, Seppo},
	month = jun,
	year = {1976},
	pages = {146--160},
}

@inproceedings{bengio_deep_2012,
	address = {Bellevue, Washington, USA},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Deep {Learning} of {Representations} for {Unsupervised} and {Transfer} {Learning}},
	volume = {27},
	url = {https://proceedings.mlr.press/v27/bengio12a.html},
	abstract = {Deep learning algorithms seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels, with higher-level learned features defined in terms of lower-level features. The objective is to make these higher-level representations more abstract, with their individual features more invariant to most of the variations that are typically present in the training distribution, while collectively preserving as much as possible of the information in the input. Ideally, we would like these representations to disentangle the unknown factors of variation that underlie the training distribution. Such unsupervised learning of representations can be exploited usefully under the hypothesis that the input distribution P(x) is structurally related to some task of interest, say predicting P(y{\textbar}x). This paper focuses on the context of the Unsupervised and Transfer Learning Challenge, on why unsupervised pre-training of representations can be useful, and how it can be exploited in the transfer learning scenario, where we care about predictions on examples that are not from the same distribution as the training distribution.},
	booktitle = {Proceedings of {ICML} {Workshop} on {Unsupervised} and {Transfer} {Learning}},
	publisher = {PMLR},
	author = {Bengio, Yoshua},
	editor = {Guyon, Isabelle and Dror, Gideon and Lemaire, Vincent and Taylor, Graham and Silver, Daniel},
	month = jul,
	year = {2012},
	pages = {17--36},
}

@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {0932-4194, 1435-568X},
	url = {http://link.springer.com/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	language = {en},
	number = {4},
	urldate = {2023-06-26},
	journal = {Mathematics of Control, Signals, and Systems},
	author = {Cybenko, G.},
	month = dec,
	year = {1989},
	pages = {303--314},
}

@article{fukushima_visual_1969,
	title = {Visual {Feature} {Extraction} by a {Multilayered} {Network} of {Analog} {Threshold} {Elements}},
	volume = {5},
	issn = {0536-1567},
	url = {http://ieeexplore.ieee.org/document/4082265/},
	doi = {10.1109/TSSC.1969.300225},
	number = {4},
	urldate = {2023-06-26},
	journal = {IEEE Transactions on Systems Science and Cybernetics},
	author = {Fukushima, Kunihiko},
	year = {1969},
	pages = {322--333},
}

@book{hebb_organization_1949,
	address = {Oxford,  England},
	title = {The {Organization} of {Behavior}; {A} {Neuropsychological} {Theory}},
	publisher = {Psychology Press},
	author = {Hebb, Donald O.},
	year = {1949},
}

@book{costandi_neuroplasticity_2016,
	address = {Cambridge, MA},
	series = {The {MIT} {Press} essential knowledge series},
	title = {Neuroplasticity},
	isbn = {978-0-262-52933-4},
	publisher = {The MIT Press},
	author = {Costandi, Moheb},
	year = {2016},
	keywords = {Nervous system, Neural transmission, Neuroplasticity, Physiology},
}

@article{coombs_specific_1955,
	title = {The specific ionic conductances and the ionic movements across the motoneuronal membrane that produce the inhibitory post-synaptic potential},
	volume = {130},
	issn = {00223751},
	url = {https://onlinelibrary.wiley.com/doi/10.1113/jphysiol.1955.sp005412},
	doi = {10.1113/jphysiol.1955.sp005412},
	language = {en},
	number = {2},
	urldate = {2023-06-26},
	journal = {The Journal of Physiology},
	author = {Coombs, Jo S. and Eccles, John C. and Fatt, Paul},
	month = nov,
	year = {1955},
	pages = {326--373},
}

@article{takagi_roles_2000,
	title = {Roles of ion channels in {EPSP} integration at neuronal dendrites},
	volume = {37},
	issn = {01680102},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168010200001206},
	doi = {10.1016/S0168-0102(00)00120-6},
	language = {en},
	number = {3},
	urldate = {2023-06-26},
	journal = {Neuroscience Research},
	author = {Takagi, Hiroshi},
	month = jul,
	year = {2000},
	pages = {167--171},
}

@article{felleman_distributed_1991,
	title = {Distributed {Hierarchical} {Processing} in the {Primate} {Cerebral} {Cortex}},
	volume = {1},
	issn = {1047-3211, 1460-2199},
	url = {https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/1.1.1},
	doi = {10.1093/cercor/1.1.1},
	language = {en},
	number = {1},
	urldate = {2023-06-26},
	journal = {Cerebral Cortex},
	author = {Felleman, Daniel J. and Van Essen, David C.},
	month = jan,
	year = {1991},
	pages = {1--47},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	language = {en},
	number = {6088},
	urldate = {2023-06-26},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	pages = {533--536},
}

@article{herculano-houzel_human_2009,
	title = {The human brain in numbers: a linearly scaled-up primate brain},
	volume = {3},
	issn = {16625161},
	shorttitle = {The human brain in numbers},
	url = {http://journal.frontiersin.org/article/10.3389/neuro.09.031.2009/abstract},
	doi = {10.3389/neuro.09.031.2009},
	urldate = {2023-06-26},
	journal = {Frontiers in Human Neuroscience},
	author = {Herculano-Houzel, Suzana},
	year = {2009},
}

@article{zeng_neuronal_2017,
	title = {Neuronal cell-type classification: challenges, opportunities and the path forward},
	volume = {18},
	issn = {1471-003X, 1471-0048},
	shorttitle = {Neuronal cell-type classification},
	url = {https://www.nature.com/articles/nrn.2017.85},
	doi = {10.1038/nrn.2017.85},
	language = {en},
	number = {9},
	urldate = {2023-06-26},
	journal = {Nature Reviews Neuroscience},
	author = {Zeng, Hongkui and Sanes, Joshua R.},
	month = sep,
	year = {2017},
	pages = {530--546},
}

@book{james_principles_1890,
	title = {The principles of psychology},
	publisher = {Henry Holt and Company},
	author = {James, William},
	year = {1890},
}

@book{bain_mind_1873,
	address = {New York},
	title = {Mind and body: {The} theories of their relation},
	volume = {1},
	publisher = {D. Appleton and Company},
	author = {Bain, Alexander},
	year = {1873},
}

@article{hopfield_neural_1982,
	title = {Neural networks and physical systems with emergent collective computational abilities.},
	volume = {79},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.79.8.2554},
	doi = {10.1073/pnas.79.8.2554},
	abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
	language = {en},
	number = {8},
	urldate = {2023-06-26},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Hopfield, J J},
	month = apr,
	year = {1982},
	pages = {2554--2558},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	language = {en},
	number = {7553},
	urldate = {2023-06-26},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
}

@book{mitchell_machine_1997,
	address = {New York},
	series = {{McGraw}-{Hill} series in computer science},
	title = {Machine {Learning}},
	isbn = {978-0-07-042807-2},
	publisher = {McGraw-Hill},
	author = {Mitchell, Tom M.},
	year = {1997},
	keywords = {Computer algorithms, Machine learning},
}

@misc{radford_improving_2018,
	title = {Improving language understanding by generative pre-training},
	url = {https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf},
	urldate = {2023-06-24},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	year = {2018},
}

@misc{noauthor_improving_nodate,
	title = {Improving language understanding by generative pre-training},
}

@article{wiskott_face_1997,
	title = {Face recognition by elastic bunch graph matching},
	volume = {19},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/598235/},
	doi = {10.1109/34.598235},
	number = {7},
	urldate = {2023-06-14},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wiskott, Laurenz and Fellous, Jean-Marc and Kuiger, Norbert and Von Der Malsburg, Christoph},
	month = jul,
	year = {1997},
	pages = {775--779},
}

@book{ivakhnenko_cybernetic_1965,
	address = {New York},
	title = {Cybernetic {Predicting} {Devices}},
	publisher = {CCM Information Corporation},
	author = {Ivakhnenko, Aleksei G. and Lapa, Valentin G.},
	year = {1965},
}

@article{wolfrum_recurrent_2008,
	title = {A recurrent dynamic model for correspondence-based face recognition},
	volume = {8},
	issn = {1534-7362},
	url = {http://jov.arvojournals.org/article.aspx?doi=10.1167/8.7.34},
	doi = {10.1167/8.7.34},
	language = {en},
	number = {7},
	urldate = {2023-03-06},
	journal = {Journal of Vision},
	author = {Wolfrum, Philipp and Wolff, Christian and Lücke, Jörg and von der Malsburg, Christoph},
	month = dec,
	year = {2008},
	pages = {34},
}

@misc{von_der_malsburg_theory_2022,
	title = {A {Theory} of {Natural} {Intelligence}},
	url = {http://arxiv.org/abs/2205.00002},
	doi = {10.48550/arXiv.2205.00002},
	abstract = {Introduction: In contrast to current AI technology, natural intelligence -- the kind of autonomous intelligence that is realized in the brains of animals and humans to attain in their natural environment goals defined by a repertoire of innate behavioral schemata -- is far superior in terms of learning speed, generalization capabilities, autonomy and creativity. How are these strengths, by what means are ideas and imagination produced in natural neural networks? Methods: Reviewing the literature, we put forward the argument that both our natural environment and the brain are of low complexity, that is, require for their generation very little information and are consequently both highly structured. We further argue that the structures of brain and natural environment are closely related. Results: We propose that the structural regularity of the brain takes the form of net fragments (self-organized network patterns) and that these serve as the powerful inductive bias that enables the brain to learn quickly, generalize from few examples and bridge the gap between abstractly defined general goals and concrete situations. Conclusions: Our results have important bearings on open problems in artificial neural network research.},
	urldate = {2022-07-11},
	publisher = {arXiv},
	author = {von der Malsburg, Christoph and Stadelmann, Thilo and Grewe, Benjamin F.},
	month = apr,
	year = {2022},
	note = {arXiv:2205.00002 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, I.2, Quantitative Biology - Neurons and Cognition},
}

@article{yarats_improving_2021,
	title = {Improving {Sample} {Efficiency} in {Model}-{Free} {Reinforcement} {Learning} from {Images}},
	volume = {35},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17276},
	doi = {10.1609/aaai.v35i12.17276},
	abstract = {Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. A promising approach is to learn a latent representation together with the control policy. However, fitting a high-capacity encoder using a scarce reward signal is sample inefficient and leads to poor performance.
Prior work has shown that auxiliary losses, such as image reconstruction, can aid efficient representation learning.  
However, incorporating reconstruction loss into an off-policy learning algorithm often leads to training instability. We explore the underlying reasons and 
identify variational autoencoders, used by previous investigations, as the cause of the divergence.   
Following these findings, we propose effective techniques to improve training stability. 
This results in a simple approach capable of
matching state-of-the-art model-free and model-based algorithms on MuJoCo control tasks. Furthermore, our approach demonstrates robustness to observational noise, surpassing existing approaches in this setting. Code, results, and videos are anonymously available at https://sites.google.com/view/sac-ae/home.},
	number = {12},
	urldate = {2023-06-24},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Yarats, Denis and Zhang, Amy and Kostrikov, Ilya and Amos, Brandon and Pineau, Joelle and Fergus, Rob},
	month = may,
	year = {2021},
	pages = {10674--10681},
}

@article{sager_unsupervised_2022,
	title = {Unsupervised {Domain} {Adaptation} for {Vertebrae} {Detection} and {Identification} in {3D} {CT} {Volumes} {Using} a {Domain} {Sanity} {Loss}},
	volume = {8},
	copyright = {All rights reserved},
	issn = {2313-433X},
	url = {https://www.mdpi.com/2313-433X/8/8/222},
	doi = {10.3390/jimaging8080222},
	abstract = {A variety of medical computer vision applications analyze 2D slices of computed tomography (CT) scans, whereas axial slices from the body trunk region are usually identified based on their relative position to the spine. A limitation of such systems is that either the correct slices must be extracted manually or labels of the vertebrae are required for each CT scan to develop an automated extraction system. In this paper, we propose an unsupervised domain adaptation (UDA) approach for vertebrae detection and identification based on a novel Domain Sanity Loss (DSL) function. With UDA the model’s knowledge learned on a publicly available (source) data set can be transferred to the target domain without using target labels, where the target domain is defined by the specific setup (CT modality, study protocols, applied pre- and processing) at the point of use (e.g., a specific clinic with its specific CT study protocols). With our approach, a model is trained on the source and target data set in parallel. The model optimizes a supervised loss for labeled samples from the source domain and the DSL loss function based on domain-specific “sanity checks” for samples from the unlabeled target domain. Without using labels from the target domain, we are able to identify vertebra centroids with an accuracy of 72.8\%. By adding only ten target labels during training the accuracy increases to 89.2\%, which is on par with the current state-of-the-art for full supervised learning, while using about 20 times less labels. Thus, our model can be used to extract 2D slices from 3D CT scans on arbitrary data sets fully automatically without requiring an extensive labeling effort, contributing to the clinical adoption of medical imaging by hospitals.},
	language = {en},
	number = {8},
	urldate = {2022-09-01},
	journal = {Journal of Imaging},
	author = {Sager, Pascal and Salzmann, Sebastian and Burn, Felice and Stadelmann, Thilo},
	month = aug,
	year = {2022},
	pages = {222},
}

@article{long_survey_2022,
	title = {A survey on adversarial attacks in computer vision: {Taxonomy}, visualization and future directions},
	volume = {121},
	issn = {01674048},
	shorttitle = {A survey on adversarial attacks in computer vision},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167404822002413},
	doi = {10.1016/j.cose.2022.102847},
	language = {en},
	urldate = {2023-06-24},
	journal = {Computers \& Security},
	author = {Long, Teng and Gao, Qi and Xu, Lili and Zhou, Zhangbing},
	month = oct,
	year = {2022},
	pages = {102847},
}

@misc{rosenbloom_defining_2023,
	title = {Defining and {Explorting} the {Intelligence} {Space}},
	url = {http://arxiv.org/abs/2306.06499},
	doi = {10.48550/arXiv.2306.06499},
	abstract = {Intelligence is a difficult concept to define, despite many attempts at doing so. Rather than trying to settle on a single definition, this article introduces a broad perspective on what intelligence is, by laying out a cascade of definitions that induces both a nested hierarchy of three levels of intelligence and a wider-ranging space that is built around them and approximations to them. Within this intelligence space, regions are identified that correspond to both natural -- most particularly, human -- intelligence and artificial intelligence (AI), along with the crossover notion of humanlike intelligence. These definitions are then exploited in early explorations of four more advanced, and likely more controversial, topics: the singularity, generative AI, ethics, and intellectual property.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {Rosenbloom, Paul S.},
	month = jun,
	year = {2023},
	note = {arXiv:2306.06499 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{mitchell_debate_2023,
	title = {The debate over understanding in {AI}’s large language models},
	volume = {120},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/10.1073/pnas.2215907120},
	doi = {10.1073/pnas.2215907120},
	abstract = {We survey a current, heated debate in the artificial intelligence (AI) research community on whether large pretrained language models can be said to understand language—and the physical and social situations language encodes—in any humanlike sense. We describe arguments that have been made for and against such understanding and key questions for the broader sciences of intelligence that have arisen in light of these arguments. We contend that an extended science of intelligence can be developed that will provide insight into distinct modes of understanding, their strengths and limitations, and the challenge of integrating diverse forms of cognition.},
	language = {en},
	number = {13},
	urldate = {2023-06-24},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mitchell, Melanie and Krakauer, David C.},
	month = mar,
	year = {2023},
	pages = {e2215907120},
}

@article{bertolini_machine_2021,
	title = {Machine {Learning} for industrial applications: {A} comprehensive literature review},
	volume = {175},
	issn = {09574174},
	shorttitle = {Machine {Learning} for industrial applications},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095741742100261X},
	doi = {10.1016/j.eswa.2021.114820},
	language = {en},
	urldate = {2023-06-24},
	journal = {Expert Systems with Applications},
	author = {Bertolini, Massimo and Mezzogori, Davide and Neroni, Mattia and Zammori, Francesco},
	month = aug,
	year = {2021},
	pages = {114820},
}

@article{ning_review_2019,
	title = {A {Review} of {Deep} {Learning} {Based} {Speech} {Synthesis}},
	volume = {9},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/9/19/4050},
	doi = {10.3390/app9194050},
	abstract = {Speech synthesis, also known as text-to-speech (TTS), has attracted increasingly more attention. Recent advances on speech synthesis are overwhelmingly contributed by deep learning or even end-to-end techniques which have been utilized to enhance a wide range of application scenarios such as intelligent speech interaction, chatbot or conversational artificial intelligence (AI). For speech synthesis, deep learning based techniques can leverage a large scale of {\textless}text, speech{\textgreater} pairs to learn effective feature representations to bridge the gap between text and speech, thus better characterizing the properties of events. To better understand the research dynamics in the speech synthesis field, this paper firstly introduces the traditional speech synthesis methods and highlights the importance of the acoustic modeling from the composition of the statistical parametric speech synthesis (SPSS) system. It then gives an overview of the advances on deep learning based speech synthesis, including the end-to-end approaches which have achieved start-of-the-art performance in recent years. Finally, it discusses the problems of the deep learning methods for speech synthesis, and also points out some appealing research directions that can bring the speech synthesis research into a new frontier.},
	language = {en},
	number = {19},
	urldate = {2023-06-24},
	journal = {Applied Sciences},
	author = {Ning, Yishuang and He, Sheng and Wu, Zhiyong and Xing, Chunxiao and Zhang, Liang-Jie},
	month = sep,
	year = {2019},
	pages = {4050},
}

@article{otter_survey_2021,
	title = {A {Survey} of the {Usages} of {Deep} {Learning} for {Natural} {Language} {Processing}},
	volume = {32},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/9075398/},
	doi = {10.1109/TNNLS.2020.2979670},
	number = {2},
	urldate = {2023-06-24},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Otter, Daniel W. and Medina, Julian R. and Kalita, Jugal K.},
	month = feb,
	year = {2021},
	pages = {604--624},
}

@article{bhatt_cnn_2021,
	title = {{CNN} {Variants} for {Computer} {Vision}: {History}, {Architecture}, {Application}, {Challenges} and {Future} {Scope}},
	volume = {10},
	issn = {2079-9292},
	shorttitle = {{CNN} {Variants} for {Computer} {Vision}},
	url = {https://www.mdpi.com/2079-9292/10/20/2470},
	doi = {10.3390/electronics10202470},
	abstract = {Computer vision is becoming an increasingly trendy word in the area of image processing. With the emergence of computer vision applications, there is a significant demand to recognize objects automatically. Deep CNN (convolution neural network) has benefited the computer vision community by producing excellent results in video processing, object recognition, picture classification and segmentation, natural language processing, speech recognition, and many other fields. Furthermore, the introduction of large amounts of data and readily available hardware has opened new avenues for CNN study. Several inspirational concepts for the progress of CNN have been investigated, including alternative activation functions, regularization, parameter optimization, and architectural advances. Furthermore, achieving innovations in architecture results in a tremendous enhancement in the capacity of the deep CNN. Significant emphasis has been given to leveraging channel and spatial information, with a depth of architecture and information processing via multi-path. This survey paper focuses mainly on the primary taxonomy and newly released deep CNN architectures, and it divides numerous recent developments in CNN architectures into eight groups. Spatial exploitation, multi-path, depth, breadth, dimension, channel boosting, feature-map exploitation, and attention-based CNN are the eight categories. The main contribution of this manuscript is in comparing various architectural evolutions in CNN by its architectural change, strengths, and weaknesses. Besides, it also includes an explanation of the CNN’s components, the strengths and weaknesses of various CNN variants, research gap or open challenges, CNN applications, and the future research direction.},
	language = {en},
	number = {20},
	urldate = {2023-06-24},
	journal = {Electronics},
	author = {Bhatt, Dulari and Patel, Chirag and Talsania, Hardik and Patel, Jigar and Vaghela, Rasmika and Pandya, Sharnil and Modi, Kirit and Ghayvat, Hemant},
	month = oct,
	year = {2021},
	pages = {2470},
}

@article{dabre_survey_2021,
	title = {A {Survey} of {Multilingual} {Neural} {Machine} {Translation}},
	volume = {53},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3406095},
	doi = {10.1145/3406095},
	abstract = {We present a survey on multilingual neural machine translation (MNMT), which has gained a lot of traction in recent years. MNMT has been useful in improving translation quality as a result of translation knowledge transfer (transfer learning). MNMT is more promising and interesting than its statistical machine translation counterpart, because end-to-end modeling and distributed representations open new avenues for research on machine translation. Many approaches have been proposed to exploit multilingual parallel corpora for improving translation quality. However, the lack of a comprehensive survey makes it difficult to determine which approaches are promising and, hence, deserve further exploration. In this article, we present an in-depth survey of existing literature on MNMT. We first categorize various approaches based on their central use-case and then further categorize them based on resource scenarios, underlying modeling principles, core-issues, and challenges. Wherever possible, we address the strengths and weaknesses of several techniques by comparing them with each other. We also discuss the future directions for MNMT. This article is aimed towards both beginners and experts in NMT. We hope this article will serve as a starting point as well as a source of new ideas for researchers and engineers interested in MNMT.},
	language = {en},
	number = {5},
	urldate = {2023-06-24},
	journal = {ACM Computing Surveys},
	author = {Dabre, Raj and Chu, Chenhui and Kunchukuttan, Anoop},
	month = sep,
	year = {2021},
	pages = {1--38},
}

@misc{liu_summary_2023,
	title = {Summary of {ChatGPT}/{GPT}-4 {Research} and {Perspective} {Towards} the {Future} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2304.01852},
	doi = {10.48550/arXiv.2304.01852},
	abstract = {This paper presents a comprehensive survey of ChatGPT and GPT-4, state-of-the-art large language models (LLM) from the GPT series, and their prospective applications across diverse domains. Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability and performance. We performed an in-depth analysis of 194 relevant papers on arXiv, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains. The findings reveal a significant and increasing interest in ChatGPT/GPT-4 research, predominantly centered on direct natural language processing applications, while also demonstrating considerable potential in areas ranging from education and history to mathematics, medicine, and physics. This study endeavors to furnish insights into ChatGPT's capabilities, potential implications, ethical concerns, and offer direction for future advancements in this field.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {Liu, Yiheng and Han, Tianle and Ma, Siyuan and Zhang, Jiayue and Yang, Yuanyuan and Tian, Jiaming and He, Hao and Li, Antong and He, Mengshen and Liu, Zhengliang and Wu, Zihao and Zhu, Dajiang and Li, Xiang and Qiang, Ning and Shen, Dingang and Liu, Tianming and Ge, Bao},
	month = may,
	year = {2023},
	note = {arXiv:2304.01852 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{bubeck_sparks_2023,
	title = {Sparks of {Artificial} {General} {Intelligence}: {Early} experiments with {GPT}-4},
	shorttitle = {Sparks of {Artificial} {General} {Intelligence}},
	url = {http://arxiv.org/abs/2303.12712},
	doi = {10.48550/arXiv.2303.12712},
	abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
	month = apr,
	year = {2023},
	note = {arXiv:2303.12712 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}
